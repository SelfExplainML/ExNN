{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T02:57:17.403078Z",
     "start_time": "2020-07-21T02:54:13.775303Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n",
      "Training epoch: 1, train loss: 0.15243, val loss: 0.15319\n",
      "Training epoch: 2, train loss: 0.13356, val loss: 0.13547\n",
      "Training epoch: 3, train loss: 0.11574, val loss: 0.11758\n",
      "Training epoch: 4, train loss: 0.09915, val loss: 0.10049\n",
      "Training epoch: 5, train loss: 0.08545, val loss: 0.08645\n",
      "Training epoch: 6, train loss: 0.07816, val loss: 0.07833\n",
      "Training epoch: 7, train loss: 0.06871, val loss: 0.06867\n",
      "Training epoch: 8, train loss: 0.06548, val loss: 0.06517\n",
      "Training epoch: 9, train loss: 0.05748, val loss: 0.05773\n",
      "Training epoch: 10, train loss: 0.05786, val loss: 0.05819\n",
      "Training epoch: 11, train loss: 0.04962, val loss: 0.05019\n",
      "Training epoch: 12, train loss: 0.04886, val loss: 0.04934\n",
      "Training epoch: 13, train loss: 0.04819, val loss: 0.04860\n",
      "Training epoch: 14, train loss: 0.04446, val loss: 0.04476\n",
      "Training epoch: 15, train loss: 0.04247, val loss: 0.04275\n",
      "Training epoch: 16, train loss: 0.03910, val loss: 0.03954\n",
      "Training epoch: 17, train loss: 0.03977, val loss: 0.03997\n",
      "Training epoch: 18, train loss: 0.03747, val loss: 0.03765\n",
      "Training epoch: 19, train loss: 0.03694, val loss: 0.03718\n",
      "Training epoch: 20, train loss: 0.03497, val loss: 0.03525\n",
      "Training epoch: 21, train loss: 0.03474, val loss: 0.03511\n",
      "Training epoch: 22, train loss: 0.03296, val loss: 0.03318\n",
      "Training epoch: 23, train loss: 0.03140, val loss: 0.03162\n",
      "Training epoch: 24, train loss: 0.03201, val loss: 0.03219\n",
      "Training epoch: 25, train loss: 0.03034, val loss: 0.03039\n",
      "Training epoch: 26, train loss: 0.03030, val loss: 0.03040\n",
      "Training epoch: 27, train loss: 0.02945, val loss: 0.02956\n",
      "Training epoch: 28, train loss: 0.02928, val loss: 0.02940\n",
      "Training epoch: 29, train loss: 0.02858, val loss: 0.02877\n",
      "Training epoch: 30, train loss: 0.02728, val loss: 0.02741\n",
      "Training epoch: 31, train loss: 0.02756, val loss: 0.02781\n",
      "Training epoch: 32, train loss: 0.02609, val loss: 0.02633\n",
      "Training epoch: 33, train loss: 0.02707, val loss: 0.02720\n",
      "Training epoch: 34, train loss: 0.02575, val loss: 0.02602\n",
      "Training epoch: 35, train loss: 0.02546, val loss: 0.02564\n",
      "Training epoch: 36, train loss: 0.02505, val loss: 0.02539\n",
      "Training epoch: 37, train loss: 0.02477, val loss: 0.02494\n",
      "Training epoch: 38, train loss: 0.02417, val loss: 0.02453\n",
      "Training epoch: 39, train loss: 0.02476, val loss: 0.02508\n",
      "Training epoch: 40, train loss: 0.02412, val loss: 0.02446\n",
      "Training epoch: 41, train loss: 0.02417, val loss: 0.02454\n",
      "Training epoch: 42, train loss: 0.02360, val loss: 0.02394\n",
      "Training epoch: 43, train loss: 0.02540, val loss: 0.02582\n",
      "Training epoch: 44, train loss: 0.02327, val loss: 0.02367\n",
      "Training epoch: 45, train loss: 0.02324, val loss: 0.02359\n",
      "Training epoch: 46, train loss: 0.02301, val loss: 0.02338\n",
      "Training epoch: 47, train loss: 0.02247, val loss: 0.02294\n",
      "Training epoch: 48, train loss: 0.02267, val loss: 0.02305\n",
      "Training epoch: 49, train loss: 0.02299, val loss: 0.02346\n",
      "Training epoch: 50, train loss: 0.02266, val loss: 0.02308\n",
      "Training epoch: 51, train loss: 0.02250, val loss: 0.02292\n",
      "Training epoch: 52, train loss: 0.02245, val loss: 0.02294\n",
      "Training epoch: 53, train loss: 0.02282, val loss: 0.02323\n",
      "Training epoch: 54, train loss: 0.02255, val loss: 0.02297\n",
      "Training epoch: 55, train loss: 0.02186, val loss: 0.02236\n",
      "Training epoch: 56, train loss: 0.02196, val loss: 0.02231\n",
      "Training epoch: 57, train loss: 0.02200, val loss: 0.02243\n",
      "Training epoch: 58, train loss: 0.02207, val loss: 0.02245\n",
      "Training epoch: 59, train loss: 0.02207, val loss: 0.02256\n",
      "Training epoch: 60, train loss: 0.02202, val loss: 0.02237\n",
      "Training epoch: 61, train loss: 0.02157, val loss: 0.02212\n",
      "Training epoch: 62, train loss: 0.02153, val loss: 0.02200\n",
      "Training epoch: 63, train loss: 0.02163, val loss: 0.02208\n",
      "Training epoch: 64, train loss: 0.02137, val loss: 0.02186\n",
      "Training epoch: 65, train loss: 0.02140, val loss: 0.02189\n",
      "Training epoch: 66, train loss: 0.02139, val loss: 0.02190\n",
      "Training epoch: 67, train loss: 0.02130, val loss: 0.02179\n",
      "Training epoch: 68, train loss: 0.02139, val loss: 0.02194\n",
      "Training epoch: 69, train loss: 0.02132, val loss: 0.02188\n",
      "Training epoch: 70, train loss: 0.02166, val loss: 0.02219\n",
      "Training epoch: 71, train loss: 0.02121, val loss: 0.02180\n",
      "Training epoch: 72, train loss: 0.02160, val loss: 0.02197\n",
      "Training epoch: 73, train loss: 0.02227, val loss: 0.02272\n",
      "Training epoch: 74, train loss: 0.02112, val loss: 0.02166\n",
      "Training epoch: 75, train loss: 0.02127, val loss: 0.02176\n",
      "Training epoch: 76, train loss: 0.02265, val loss: 0.02316\n",
      "Training epoch: 77, train loss: 0.02110, val loss: 0.02161\n",
      "Training epoch: 78, train loss: 0.02100, val loss: 0.02155\n",
      "Training epoch: 79, train loss: 0.02114, val loss: 0.02161\n",
      "Training epoch: 80, train loss: 0.02117, val loss: 0.02177\n",
      "Training epoch: 81, train loss: 0.02119, val loss: 0.02181\n",
      "Training epoch: 82, train loss: 0.02103, val loss: 0.02162\n",
      "Training epoch: 83, train loss: 0.02103, val loss: 0.02163\n",
      "Training epoch: 84, train loss: 0.02162, val loss: 0.02209\n",
      "Training epoch: 85, train loss: 0.02078, val loss: 0.02131\n",
      "Training epoch: 86, train loss: 0.02090, val loss: 0.02146\n",
      "Training epoch: 87, train loss: 0.02092, val loss: 0.02148\n",
      "Training epoch: 88, train loss: 0.02150, val loss: 0.02216\n",
      "Training epoch: 89, train loss: 0.02096, val loss: 0.02152\n",
      "Training epoch: 90, train loss: 0.02067, val loss: 0.02124\n",
      "Training epoch: 91, train loss: 0.02082, val loss: 0.02140\n",
      "Training epoch: 92, train loss: 0.02075, val loss: 0.02132\n",
      "Training epoch: 93, train loss: 0.02050, val loss: 0.02102\n",
      "Training epoch: 94, train loss: 0.02072, val loss: 0.02123\n",
      "Training epoch: 95, train loss: 0.02057, val loss: 0.02109\n",
      "Training epoch: 96, train loss: 0.02097, val loss: 0.02145\n",
      "Training epoch: 97, train loss: 0.02038, val loss: 0.02088\n",
      "Training epoch: 98, train loss: 0.02071, val loss: 0.02122\n",
      "Training epoch: 99, train loss: 0.02053, val loss: 0.02111\n",
      "Training epoch: 100, train loss: 0.02114, val loss: 0.02179\n",
      "Training epoch: 101, train loss: 0.02045, val loss: 0.02101\n",
      "Training epoch: 102, train loss: 0.02052, val loss: 0.02108\n",
      "Training epoch: 103, train loss: 0.02197, val loss: 0.02242\n",
      "Training epoch: 104, train loss: 0.02095, val loss: 0.02148\n",
      "Training epoch: 105, train loss: 0.02036, val loss: 0.02098\n",
      "Training epoch: 106, train loss: 0.02071, val loss: 0.02119\n",
      "Training epoch: 107, train loss: 0.02070, val loss: 0.02128\n",
      "Training epoch: 108, train loss: 0.02032, val loss: 0.02083\n",
      "Training epoch: 109, train loss: 0.02039, val loss: 0.02095\n",
      "Training epoch: 110, train loss: 0.02037, val loss: 0.02096\n",
      "Training epoch: 111, train loss: 0.02023, val loss: 0.02084\n",
      "Training epoch: 112, train loss: 0.02056, val loss: 0.02103\n",
      "Training epoch: 113, train loss: 0.02061, val loss: 0.02104\n",
      "Training epoch: 114, train loss: 0.02021, val loss: 0.02070\n",
      "Training epoch: 115, train loss: 0.02019, val loss: 0.02076\n",
      "Training epoch: 116, train loss: 0.02033, val loss: 0.02079\n",
      "Training epoch: 117, train loss: 0.02066, val loss: 0.02111\n",
      "Training epoch: 118, train loss: 0.02010, val loss: 0.02054\n",
      "Training epoch: 119, train loss: 0.02042, val loss: 0.02082\n",
      "Training epoch: 120, train loss: 0.02009, val loss: 0.02065\n",
      "Training epoch: 121, train loss: 0.02067, val loss: 0.02119\n",
      "Training epoch: 122, train loss: 0.01999, val loss: 0.02062\n",
      "Training epoch: 123, train loss: 0.02065, val loss: 0.02105\n",
      "Training epoch: 124, train loss: 0.02051, val loss: 0.02094\n",
      "Training epoch: 125, train loss: 0.01990, val loss: 0.02046\n",
      "Training epoch: 126, train loss: 0.02025, val loss: 0.02096\n",
      "Training epoch: 127, train loss: 0.01978, val loss: 0.02026\n",
      "Training epoch: 128, train loss: 0.02000, val loss: 0.02054\n",
      "Training epoch: 129, train loss: 0.01989, val loss: 0.02035\n",
      "Training epoch: 130, train loss: 0.01985, val loss: 0.02043\n",
      "Training epoch: 131, train loss: 0.02005, val loss: 0.02057\n",
      "Training epoch: 132, train loss: 0.02007, val loss: 0.02061\n",
      "Training epoch: 133, train loss: 0.01971, val loss: 0.02038\n",
      "Training epoch: 134, train loss: 0.01955, val loss: 0.02015\n",
      "Training epoch: 135, train loss: 0.01954, val loss: 0.02009\n",
      "Training epoch: 136, train loss: 0.01984, val loss: 0.02047\n",
      "Training epoch: 137, train loss: 0.02013, val loss: 0.02085\n",
      "Training epoch: 138, train loss: 0.01952, val loss: 0.02010\n",
      "Training epoch: 139, train loss: 0.01959, val loss: 0.02017\n",
      "Training epoch: 140, train loss: 0.01953, val loss: 0.02000\n",
      "Training epoch: 141, train loss: 0.01960, val loss: 0.02025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 142, train loss: 0.01934, val loss: 0.01989\n",
      "Training epoch: 143, train loss: 0.01948, val loss: 0.02001\n",
      "Training epoch: 144, train loss: 0.01949, val loss: 0.01998\n",
      "Training epoch: 145, train loss: 0.01964, val loss: 0.02017\n",
      "Training epoch: 146, train loss: 0.01926, val loss: 0.01982\n",
      "Training epoch: 147, train loss: 0.01926, val loss: 0.01988\n",
      "Training epoch: 148, train loss: 0.01964, val loss: 0.02034\n",
      "Training epoch: 149, train loss: 0.01906, val loss: 0.01960\n",
      "Training epoch: 150, train loss: 0.01949, val loss: 0.02020\n",
      "Training epoch: 151, train loss: 0.01913, val loss: 0.01975\n",
      "Training epoch: 152, train loss: 0.01931, val loss: 0.01982\n",
      "Training epoch: 153, train loss: 0.01968, val loss: 0.02048\n",
      "Training epoch: 154, train loss: 0.01911, val loss: 0.01988\n",
      "Training epoch: 155, train loss: 0.01887, val loss: 0.01951\n",
      "Training epoch: 156, train loss: 0.01923, val loss: 0.01992\n",
      "Training epoch: 157, train loss: 0.01891, val loss: 0.01953\n",
      "Training epoch: 158, train loss: 0.01980, val loss: 0.02062\n",
      "Training epoch: 159, train loss: 0.01895, val loss: 0.01946\n",
      "Training epoch: 160, train loss: 0.01916, val loss: 0.01978\n",
      "Training epoch: 161, train loss: 0.01997, val loss: 0.02092\n",
      "Training epoch: 162, train loss: 0.01870, val loss: 0.01924\n",
      "Training epoch: 163, train loss: 0.01864, val loss: 0.01928\n",
      "Training epoch: 164, train loss: 0.01872, val loss: 0.01936\n",
      "Training epoch: 165, train loss: 0.01860, val loss: 0.01919\n",
      "Training epoch: 166, train loss: 0.01924, val loss: 0.02007\n",
      "Training epoch: 167, train loss: 0.01875, val loss: 0.01935\n",
      "Training epoch: 168, train loss: 0.01857, val loss: 0.01924\n",
      "Training epoch: 169, train loss: 0.01902, val loss: 0.01966\n",
      "Training epoch: 170, train loss: 0.01905, val loss: 0.01972\n",
      "Training epoch: 171, train loss: 0.01911, val loss: 0.01989\n",
      "Training epoch: 172, train loss: 0.01837, val loss: 0.01914\n",
      "Training epoch: 173, train loss: 0.01902, val loss: 0.01957\n",
      "Training epoch: 174, train loss: 0.01920, val loss: 0.02009\n",
      "Training epoch: 175, train loss: 0.01899, val loss: 0.01979\n",
      "Training epoch: 176, train loss: 0.01970, val loss: 0.02053\n",
      "Training epoch: 177, train loss: 0.01922, val loss: 0.01999\n",
      "Training epoch: 178, train loss: 0.01823, val loss: 0.01889\n",
      "Training epoch: 179, train loss: 0.01855, val loss: 0.01917\n",
      "Training epoch: 180, train loss: 0.01815, val loss: 0.01887\n",
      "Training epoch: 181, train loss: 0.01901, val loss: 0.01960\n",
      "Training epoch: 182, train loss: 0.01839, val loss: 0.01911\n",
      "Training epoch: 183, train loss: 0.01915, val loss: 0.01996\n",
      "Training epoch: 184, train loss: 0.01888, val loss: 0.01966\n",
      "Training epoch: 185, train loss: 0.01814, val loss: 0.01890\n",
      "Training epoch: 186, train loss: 0.01844, val loss: 0.01931\n",
      "Training epoch: 187, train loss: 0.01820, val loss: 0.01899\n",
      "Training epoch: 188, train loss: 0.01883, val loss: 0.01946\n",
      "Training epoch: 189, train loss: 0.01801, val loss: 0.01875\n",
      "Training epoch: 190, train loss: 0.01803, val loss: 0.01873\n",
      "Training epoch: 191, train loss: 0.01799, val loss: 0.01870\n",
      "Training epoch: 192, train loss: 0.01802, val loss: 0.01881\n",
      "Training epoch: 193, train loss: 0.01784, val loss: 0.01870\n",
      "Training epoch: 194, train loss: 0.01776, val loss: 0.01849\n",
      "Training epoch: 195, train loss: 0.01916, val loss: 0.01998\n",
      "Training epoch: 196, train loss: 0.01830, val loss: 0.01902\n",
      "Training epoch: 197, train loss: 0.01773, val loss: 0.01848\n",
      "Training epoch: 198, train loss: 0.01862, val loss: 0.01928\n",
      "Training epoch: 199, train loss: 0.01820, val loss: 0.01887\n",
      "Training epoch: 200, train loss: 0.01793, val loss: 0.01868\n",
      "Training epoch: 201, train loss: 0.01828, val loss: 0.01896\n",
      "Training epoch: 202, train loss: 0.01795, val loss: 0.01881\n",
      "Training epoch: 203, train loss: 0.01770, val loss: 0.01842\n",
      "Training epoch: 204, train loss: 0.01765, val loss: 0.01841\n",
      "Training epoch: 205, train loss: 0.01804, val loss: 0.01886\n",
      "Training epoch: 206, train loss: 0.01760, val loss: 0.01833\n",
      "Training epoch: 207, train loss: 0.01841, val loss: 0.01926\n",
      "Training epoch: 208, train loss: 0.01927, val loss: 0.01993\n",
      "Training epoch: 209, train loss: 0.01862, val loss: 0.01949\n",
      "Training epoch: 210, train loss: 0.01916, val loss: 0.01982\n",
      "Training epoch: 211, train loss: 0.01847, val loss: 0.01913\n",
      "Training epoch: 212, train loss: 0.01866, val loss: 0.01960\n",
      "Training epoch: 213, train loss: 0.01789, val loss: 0.01867\n",
      "Training epoch: 214, train loss: 0.01995, val loss: 0.02059\n",
      "Training epoch: 215, train loss: 0.01758, val loss: 0.01835\n",
      "Training epoch: 216, train loss: 0.01885, val loss: 0.01981\n",
      "Training epoch: 217, train loss: 0.01829, val loss: 0.01894\n",
      "Training epoch: 218, train loss: 0.01810, val loss: 0.01885\n",
      "Training epoch: 219, train loss: 0.01844, val loss: 0.01928\n",
      "Training epoch: 220, train loss: 0.01776, val loss: 0.01851\n",
      "Training epoch: 221, train loss: 0.01802, val loss: 0.01886\n",
      "Training epoch: 222, train loss: 0.01862, val loss: 0.01932\n",
      "Training epoch: 223, train loss: 0.01981, val loss: 0.02070\n",
      "Training epoch: 224, train loss: 0.01787, val loss: 0.01872\n",
      "Training epoch: 225, train loss: 0.01756, val loss: 0.01831\n",
      "Training epoch: 226, train loss: 0.01778, val loss: 0.01854\n",
      "Training epoch: 227, train loss: 0.01785, val loss: 0.01863\n",
      "Training epoch: 228, train loss: 0.01788, val loss: 0.01869\n",
      "Training epoch: 229, train loss: 0.01844, val loss: 0.01927\n",
      "Training epoch: 230, train loss: 0.01762, val loss: 0.01837\n",
      "Training epoch: 231, train loss: 0.01833, val loss: 0.01903\n",
      "Training epoch: 232, train loss: 0.01758, val loss: 0.01838\n",
      "Training epoch: 233, train loss: 0.01740, val loss: 0.01818\n",
      "Training epoch: 234, train loss: 0.01838, val loss: 0.01905\n",
      "Training epoch: 235, train loss: 0.01760, val loss: 0.01833\n",
      "Training epoch: 236, train loss: 0.01812, val loss: 0.01894\n",
      "Training epoch: 237, train loss: 0.01752, val loss: 0.01831\n",
      "Training epoch: 238, train loss: 0.01743, val loss: 0.01816\n",
      "Training epoch: 239, train loss: 0.01851, val loss: 0.01931\n",
      "Training epoch: 240, train loss: 0.01768, val loss: 0.01843\n",
      "Training epoch: 241, train loss: 0.01767, val loss: 0.01842\n",
      "Training epoch: 242, train loss: 0.01778, val loss: 0.01862\n",
      "Training epoch: 243, train loss: 0.01733, val loss: 0.01807\n",
      "Training epoch: 244, train loss: 0.01799, val loss: 0.01883\n",
      "Training epoch: 245, train loss: 0.01746, val loss: 0.01826\n",
      "Training epoch: 246, train loss: 0.01737, val loss: 0.01809\n",
      "Training epoch: 247, train loss: 0.01823, val loss: 0.01908\n",
      "Training epoch: 248, train loss: 0.01744, val loss: 0.01817\n",
      "Training epoch: 249, train loss: 0.01738, val loss: 0.01810\n",
      "Training epoch: 250, train loss: 0.01760, val loss: 0.01833\n",
      "Training epoch: 251, train loss: 0.01747, val loss: 0.01824\n",
      "Training epoch: 252, train loss: 0.01915, val loss: 0.01974\n",
      "Training epoch: 253, train loss: 0.01801, val loss: 0.01881\n",
      "Training epoch: 254, train loss: 0.01843, val loss: 0.01925\n",
      "Training epoch: 255, train loss: 0.01759, val loss: 0.01837\n",
      "Training epoch: 256, train loss: 0.01736, val loss: 0.01808\n",
      "Training epoch: 257, train loss: 0.01846, val loss: 0.01929\n",
      "Training epoch: 258, train loss: 0.01725, val loss: 0.01804\n",
      "Training epoch: 259, train loss: 0.01728, val loss: 0.01806\n",
      "Training epoch: 260, train loss: 0.01747, val loss: 0.01818\n",
      "Training epoch: 261, train loss: 0.01728, val loss: 0.01806\n",
      "Training epoch: 262, train loss: 0.01765, val loss: 0.01833\n",
      "Training epoch: 263, train loss: 0.01963, val loss: 0.02062\n",
      "Training epoch: 264, train loss: 0.01739, val loss: 0.01812\n",
      "Training epoch: 265, train loss: 0.01751, val loss: 0.01825\n",
      "Training epoch: 266, train loss: 0.01736, val loss: 0.01815\n",
      "Training epoch: 267, train loss: 0.01712, val loss: 0.01789\n",
      "Training epoch: 268, train loss: 0.01746, val loss: 0.01814\n",
      "Training epoch: 269, train loss: 0.01771, val loss: 0.01848\n",
      "Training epoch: 270, train loss: 0.01744, val loss: 0.01816\n",
      "Training epoch: 271, train loss: 0.01729, val loss: 0.01807\n",
      "Training epoch: 272, train loss: 0.01732, val loss: 0.01803\n",
      "Training epoch: 273, train loss: 0.01726, val loss: 0.01797\n",
      "Training epoch: 274, train loss: 0.01722, val loss: 0.01802\n",
      "Training epoch: 275, train loss: 0.01722, val loss: 0.01793\n",
      "Training epoch: 276, train loss: 0.01789, val loss: 0.01860\n",
      "Training epoch: 277, train loss: 0.01802, val loss: 0.01886\n",
      "Training epoch: 278, train loss: 0.01726, val loss: 0.01802\n",
      "Training epoch: 279, train loss: 0.01780, val loss: 0.01845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 280, train loss: 0.01716, val loss: 0.01797\n",
      "Training epoch: 281, train loss: 0.01715, val loss: 0.01793\n",
      "Training epoch: 282, train loss: 0.01720, val loss: 0.01790\n",
      "Training epoch: 283, train loss: 0.01728, val loss: 0.01801\n",
      "Training epoch: 284, train loss: 0.01748, val loss: 0.01822\n",
      "Training epoch: 285, train loss: 0.01726, val loss: 0.01797\n",
      "Training epoch: 286, train loss: 0.01707, val loss: 0.01782\n",
      "Training epoch: 287, train loss: 0.01828, val loss: 0.01905\n",
      "Training epoch: 288, train loss: 0.01722, val loss: 0.01798\n",
      "Training epoch: 289, train loss: 0.01704, val loss: 0.01774\n",
      "Training epoch: 290, train loss: 0.01722, val loss: 0.01802\n",
      "Training epoch: 291, train loss: 0.01735, val loss: 0.01811\n",
      "Training epoch: 292, train loss: 0.01729, val loss: 0.01804\n",
      "Training epoch: 293, train loss: 0.01695, val loss: 0.01765\n",
      "Training epoch: 294, train loss: 0.01729, val loss: 0.01800\n",
      "Training epoch: 295, train loss: 0.01817, val loss: 0.01905\n",
      "Training epoch: 296, train loss: 0.01707, val loss: 0.01776\n",
      "Training epoch: 297, train loss: 0.01758, val loss: 0.01824\n",
      "Training epoch: 298, train loss: 0.01676, val loss: 0.01752\n",
      "Training epoch: 299, train loss: 0.01782, val loss: 0.01859\n",
      "Training epoch: 300, train loss: 0.01696, val loss: 0.01767\n",
      "Training epoch: 301, train loss: 0.01723, val loss: 0.01790\n",
      "Training epoch: 302, train loss: 0.01761, val loss: 0.01843\n",
      "Training epoch: 303, train loss: 0.01697, val loss: 0.01766\n",
      "Training epoch: 304, train loss: 0.01683, val loss: 0.01758\n",
      "Training epoch: 305, train loss: 0.01765, val loss: 0.01835\n",
      "Training epoch: 306, train loss: 0.01703, val loss: 0.01780\n",
      "Training epoch: 307, train loss: 0.01699, val loss: 0.01761\n",
      "Training epoch: 308, train loss: 0.01679, val loss: 0.01745\n",
      "Training epoch: 309, train loss: 0.01703, val loss: 0.01772\n",
      "Training epoch: 310, train loss: 0.01684, val loss: 0.01752\n",
      "Training epoch: 311, train loss: 0.01709, val loss: 0.01777\n",
      "Training epoch: 312, train loss: 0.01694, val loss: 0.01771\n",
      "Training epoch: 313, train loss: 0.01673, val loss: 0.01737\n",
      "Training epoch: 314, train loss: 0.01692, val loss: 0.01762\n",
      "Training epoch: 315, train loss: 0.01679, val loss: 0.01745\n",
      "Training epoch: 316, train loss: 0.01705, val loss: 0.01777\n",
      "Training epoch: 317, train loss: 0.01673, val loss: 0.01733\n",
      "Training epoch: 318, train loss: 0.01672, val loss: 0.01731\n",
      "Training epoch: 319, train loss: 0.01736, val loss: 0.01801\n",
      "Training epoch: 320, train loss: 0.01734, val loss: 0.01806\n",
      "Training epoch: 321, train loss: 0.01706, val loss: 0.01765\n",
      "Training epoch: 322, train loss: 0.01743, val loss: 0.01801\n",
      "Training epoch: 323, train loss: 0.01674, val loss: 0.01739\n",
      "Training epoch: 324, train loss: 0.01686, val loss: 0.01758\n",
      "Training epoch: 325, train loss: 0.01720, val loss: 0.01794\n",
      "Training epoch: 326, train loss: 0.01665, val loss: 0.01728\n",
      "Training epoch: 327, train loss: 0.01684, val loss: 0.01746\n",
      "Training epoch: 328, train loss: 0.01688, val loss: 0.01747\n",
      "Training epoch: 329, train loss: 0.01666, val loss: 0.01727\n",
      "Training epoch: 330, train loss: 0.01644, val loss: 0.01709\n",
      "Training epoch: 331, train loss: 0.01691, val loss: 0.01755\n",
      "Training epoch: 332, train loss: 0.01722, val loss: 0.01796\n",
      "Training epoch: 333, train loss: 0.01923, val loss: 0.02009\n",
      "Training epoch: 334, train loss: 0.01734, val loss: 0.01790\n",
      "Training epoch: 335, train loss: 0.01730, val loss: 0.01783\n",
      "Training epoch: 336, train loss: 0.01652, val loss: 0.01707\n",
      "Training epoch: 337, train loss: 0.01730, val loss: 0.01793\n",
      "Training epoch: 338, train loss: 0.01756, val loss: 0.01817\n",
      "Training epoch: 339, train loss: 0.01649, val loss: 0.01708\n",
      "Training epoch: 340, train loss: 0.01675, val loss: 0.01733\n",
      "Training epoch: 341, train loss: 0.01892, val loss: 0.01974\n",
      "Training epoch: 342, train loss: 0.01865, val loss: 0.01945\n",
      "Training epoch: 343, train loss: 0.01729, val loss: 0.01792\n",
      "Training epoch: 344, train loss: 0.01771, val loss: 0.01827\n",
      "Training epoch: 345, train loss: 0.01641, val loss: 0.01697\n",
      "Training epoch: 346, train loss: 0.01664, val loss: 0.01729\n",
      "Training epoch: 347, train loss: 0.01710, val loss: 0.01770\n",
      "Training epoch: 348, train loss: 0.01635, val loss: 0.01697\n",
      "Training epoch: 349, train loss: 0.01688, val loss: 0.01759\n",
      "Training epoch: 350, train loss: 0.01631, val loss: 0.01689\n",
      "Training epoch: 351, train loss: 0.01659, val loss: 0.01723\n",
      "Training epoch: 352, train loss: 0.01652, val loss: 0.01717\n",
      "Training epoch: 353, train loss: 0.01674, val loss: 0.01730\n",
      "Training epoch: 354, train loss: 0.01670, val loss: 0.01726\n",
      "Training epoch: 355, train loss: 0.01778, val loss: 0.01835\n",
      "Training epoch: 356, train loss: 0.01632, val loss: 0.01689\n",
      "Training epoch: 357, train loss: 0.01654, val loss: 0.01715\n",
      "Training epoch: 358, train loss: 0.01627, val loss: 0.01685\n",
      "Training epoch: 359, train loss: 0.01698, val loss: 0.01761\n",
      "Training epoch: 360, train loss: 0.01667, val loss: 0.01728\n",
      "Training epoch: 361, train loss: 0.01657, val loss: 0.01718\n",
      "Training epoch: 362, train loss: 0.01650, val loss: 0.01709\n",
      "Training epoch: 363, train loss: 0.01631, val loss: 0.01688\n",
      "Training epoch: 364, train loss: 0.01615, val loss: 0.01674\n",
      "Training epoch: 365, train loss: 0.01618, val loss: 0.01675\n",
      "Training epoch: 366, train loss: 0.01635, val loss: 0.01693\n",
      "Training epoch: 367, train loss: 0.01670, val loss: 0.01741\n",
      "Training epoch: 368, train loss: 0.01623, val loss: 0.01679\n",
      "Training epoch: 369, train loss: 0.01653, val loss: 0.01715\n",
      "Training epoch: 370, train loss: 0.01633, val loss: 0.01692\n",
      "Training epoch: 371, train loss: 0.01631, val loss: 0.01690\n",
      "Training epoch: 372, train loss: 0.01631, val loss: 0.01690\n",
      "Training epoch: 373, train loss: 0.01609, val loss: 0.01660\n",
      "Training epoch: 374, train loss: 0.01631, val loss: 0.01693\n",
      "Training epoch: 375, train loss: 0.01630, val loss: 0.01682\n",
      "Training epoch: 376, train loss: 0.01661, val loss: 0.01713\n",
      "Training epoch: 377, train loss: 0.01628, val loss: 0.01691\n",
      "Training epoch: 378, train loss: 0.01641, val loss: 0.01704\n",
      "Training epoch: 379, train loss: 0.01677, val loss: 0.01744\n",
      "Training epoch: 380, train loss: 0.01639, val loss: 0.01689\n",
      "Training epoch: 381, train loss: 0.01621, val loss: 0.01675\n",
      "Training epoch: 382, train loss: 0.01607, val loss: 0.01664\n",
      "Training epoch: 383, train loss: 0.01680, val loss: 0.01742\n",
      "Training epoch: 384, train loss: 0.01652, val loss: 0.01706\n",
      "Training epoch: 385, train loss: 0.01645, val loss: 0.01704\n",
      "Training epoch: 386, train loss: 0.01693, val loss: 0.01753\n",
      "Training epoch: 387, train loss: 0.01612, val loss: 0.01667\n",
      "Training epoch: 388, train loss: 0.01642, val loss: 0.01706\n",
      "Training epoch: 389, train loss: 0.01607, val loss: 0.01661\n",
      "Training epoch: 390, train loss: 0.01607, val loss: 0.01659\n",
      "Training epoch: 391, train loss: 0.01653, val loss: 0.01715\n",
      "Training epoch: 392, train loss: 0.01625, val loss: 0.01688\n",
      "Training epoch: 393, train loss: 0.01642, val loss: 0.01700\n",
      "Training epoch: 394, train loss: 0.01616, val loss: 0.01669\n",
      "Training epoch: 395, train loss: 0.01604, val loss: 0.01664\n",
      "Training epoch: 396, train loss: 0.01589, val loss: 0.01647\n",
      "Training epoch: 397, train loss: 0.01666, val loss: 0.01724\n",
      "Training epoch: 398, train loss: 0.01624, val loss: 0.01671\n",
      "Training epoch: 399, train loss: 0.01651, val loss: 0.01711\n",
      "Training epoch: 400, train loss: 0.01595, val loss: 0.01652\n",
      "Training epoch: 401, train loss: 0.01592, val loss: 0.01649\n",
      "Training epoch: 402, train loss: 0.01613, val loss: 0.01669\n",
      "Training epoch: 403, train loss: 0.01611, val loss: 0.01662\n",
      "Training epoch: 404, train loss: 0.01594, val loss: 0.01652\n",
      "Training epoch: 405, train loss: 0.01645, val loss: 0.01695\n",
      "Training epoch: 406, train loss: 0.01626, val loss: 0.01686\n",
      "Training epoch: 407, train loss: 0.01608, val loss: 0.01666\n",
      "Training epoch: 408, train loss: 0.01628, val loss: 0.01688\n",
      "Training epoch: 409, train loss: 0.01620, val loss: 0.01677\n",
      "Training epoch: 410, train loss: 0.01638, val loss: 0.01683\n",
      "Training epoch: 411, train loss: 0.01698, val loss: 0.01755\n",
      "Training epoch: 412, train loss: 0.01613, val loss: 0.01671\n",
      "Training epoch: 413, train loss: 0.01653, val loss: 0.01713\n",
      "Training epoch: 414, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 415, train loss: 0.01651, val loss: 0.01701\n",
      "Training epoch: 416, train loss: 0.01691, val loss: 0.01761\n",
      "Training epoch: 417, train loss: 0.01660, val loss: 0.01725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 418, train loss: 0.01605, val loss: 0.01656\n",
      "Training epoch: 419, train loss: 0.01608, val loss: 0.01664\n",
      "Training epoch: 420, train loss: 0.01744, val loss: 0.01787\n",
      "Training epoch: 421, train loss: 0.01584, val loss: 0.01636\n",
      "Training epoch: 422, train loss: 0.01644, val loss: 0.01705\n",
      "Training epoch: 423, train loss: 0.01633, val loss: 0.01691\n",
      "Training epoch: 424, train loss: 0.01642, val loss: 0.01688\n",
      "Training epoch: 425, train loss: 0.01614, val loss: 0.01664\n",
      "Training epoch: 426, train loss: 0.01622, val loss: 0.01684\n",
      "Training epoch: 427, train loss: 0.01618, val loss: 0.01668\n",
      "Training epoch: 428, train loss: 0.01653, val loss: 0.01705\n",
      "Training epoch: 429, train loss: 0.01636, val loss: 0.01697\n",
      "Training epoch: 430, train loss: 0.01668, val loss: 0.01730\n",
      "Training epoch: 431, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 432, train loss: 0.01594, val loss: 0.01649\n",
      "Training epoch: 433, train loss: 0.01581, val loss: 0.01631\n",
      "Training epoch: 434, train loss: 0.01597, val loss: 0.01653\n",
      "Training epoch: 435, train loss: 0.01638, val loss: 0.01686\n",
      "Training epoch: 436, train loss: 0.01589, val loss: 0.01647\n",
      "Training epoch: 437, train loss: 0.01610, val loss: 0.01659\n",
      "Training epoch: 438, train loss: 0.01629, val loss: 0.01672\n",
      "Training epoch: 439, train loss: 0.01646, val loss: 0.01699\n",
      "Training epoch: 440, train loss: 0.01621, val loss: 0.01681\n",
      "Training epoch: 441, train loss: 0.01632, val loss: 0.01697\n",
      "Training epoch: 442, train loss: 0.01609, val loss: 0.01658\n",
      "Training epoch: 443, train loss: 0.01592, val loss: 0.01648\n",
      "Training epoch: 444, train loss: 0.01595, val loss: 0.01650\n",
      "Training epoch: 445, train loss: 0.01633, val loss: 0.01695\n",
      "Training epoch: 446, train loss: 0.01592, val loss: 0.01644\n",
      "Training epoch: 447, train loss: 0.01633, val loss: 0.01698\n",
      "Training epoch: 448, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 449, train loss: 0.01589, val loss: 0.01639\n",
      "Training epoch: 450, train loss: 0.01613, val loss: 0.01668\n",
      "Training epoch: 451, train loss: 0.01625, val loss: 0.01675\n",
      "Training epoch: 452, train loss: 0.01701, val loss: 0.01765\n",
      "Training epoch: 453, train loss: 0.01677, val loss: 0.01734\n",
      "Training epoch: 454, train loss: 0.01702, val loss: 0.01764\n",
      "Training epoch: 455, train loss: 0.01604, val loss: 0.01658\n",
      "Training epoch: 456, train loss: 0.01664, val loss: 0.01724\n",
      "Training epoch: 457, train loss: 0.01630, val loss: 0.01682\n",
      "Training epoch: 458, train loss: 0.01625, val loss: 0.01672\n",
      "Training epoch: 459, train loss: 0.01659, val loss: 0.01705\n",
      "Training epoch: 460, train loss: 0.01670, val loss: 0.01727\n",
      "Training epoch: 461, train loss: 0.01649, val loss: 0.01709\n",
      "Training epoch: 462, train loss: 0.01617, val loss: 0.01671\n",
      "Training epoch: 463, train loss: 0.01597, val loss: 0.01653\n",
      "Training epoch: 464, train loss: 0.01616, val loss: 0.01662\n",
      "Training epoch: 465, train loss: 0.01600, val loss: 0.01644\n",
      "Training epoch: 466, train loss: 0.01615, val loss: 0.01669\n",
      "Training epoch: 467, train loss: 0.01650, val loss: 0.01694\n",
      "Training epoch: 468, train loss: 0.01631, val loss: 0.01693\n",
      "Training epoch: 469, train loss: 0.01623, val loss: 0.01671\n",
      "Training epoch: 470, train loss: 0.01653, val loss: 0.01717\n",
      "Training epoch: 471, train loss: 0.01596, val loss: 0.01647\n",
      "Training epoch: 472, train loss: 0.01589, val loss: 0.01642\n",
      "Training epoch: 473, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 474, train loss: 0.01694, val loss: 0.01739\n",
      "Training epoch: 475, train loss: 0.01596, val loss: 0.01647\n",
      "Training epoch: 476, train loss: 0.01606, val loss: 0.01663\n",
      "Training epoch: 477, train loss: 0.01631, val loss: 0.01690\n",
      "Training epoch: 478, train loss: 0.01595, val loss: 0.01642\n",
      "Training epoch: 479, train loss: 0.01608, val loss: 0.01662\n",
      "Training epoch: 480, train loss: 0.01697, val loss: 0.01758\n",
      "Training epoch: 481, train loss: 0.01602, val loss: 0.01656\n",
      "Training epoch: 482, train loss: 0.01614, val loss: 0.01662\n",
      "Training epoch: 483, train loss: 0.01665, val loss: 0.01708\n",
      "Training epoch: 484, train loss: 0.01605, val loss: 0.01660\n",
      "Training epoch: 485, train loss: 0.01659, val loss: 0.01705\n",
      "Training epoch: 486, train loss: 0.01657, val loss: 0.01720\n",
      "Training epoch: 487, train loss: 0.01625, val loss: 0.01672\n",
      "Training epoch: 488, train loss: 0.01596, val loss: 0.01647\n",
      "Training epoch: 489, train loss: 0.01602, val loss: 0.01659\n",
      "Training epoch: 490, train loss: 0.01646, val loss: 0.01685\n",
      "Training epoch: 491, train loss: 0.01626, val loss: 0.01673\n",
      "Training epoch: 492, train loss: 0.01619, val loss: 0.01663\n",
      "Training epoch: 493, train loss: 0.01624, val loss: 0.01683\n",
      "Training epoch: 494, train loss: 0.01598, val loss: 0.01649\n",
      "Training epoch: 495, train loss: 0.01579, val loss: 0.01626\n",
      "Training epoch: 496, train loss: 0.01614, val loss: 0.01664\n",
      "Training epoch: 497, train loss: 0.01592, val loss: 0.01640\n",
      "Training epoch: 498, train loss: 0.01594, val loss: 0.01642\n",
      "Training epoch: 499, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 500, train loss: 0.01591, val loss: 0.01641\n",
      "Training epoch: 501, train loss: 0.01594, val loss: 0.01643\n",
      "Training epoch: 502, train loss: 0.01577, val loss: 0.01624\n",
      "Training epoch: 503, train loss: 0.01648, val loss: 0.01706\n",
      "Training epoch: 504, train loss: 0.01683, val loss: 0.01724\n",
      "Training epoch: 505, train loss: 0.01675, val loss: 0.01726\n",
      "Training epoch: 506, train loss: 0.01594, val loss: 0.01642\n",
      "Training epoch: 507, train loss: 0.01584, val loss: 0.01628\n",
      "Training epoch: 508, train loss: 0.01669, val loss: 0.01722\n",
      "Training epoch: 509, train loss: 0.01615, val loss: 0.01664\n",
      "Training epoch: 510, train loss: 0.01639, val loss: 0.01692\n",
      "Training epoch: 511, train loss: 0.01611, val loss: 0.01660\n",
      "Training epoch: 512, train loss: 0.01597, val loss: 0.01650\n",
      "Training epoch: 513, train loss: 0.01590, val loss: 0.01639\n",
      "Training epoch: 514, train loss: 0.01573, val loss: 0.01619\n",
      "Training epoch: 515, train loss: 0.01591, val loss: 0.01638\n",
      "Training epoch: 516, train loss: 0.01608, val loss: 0.01660\n",
      "Training epoch: 517, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 518, train loss: 0.01640, val loss: 0.01683\n",
      "Training epoch: 519, train loss: 0.01643, val loss: 0.01697\n",
      "Training epoch: 520, train loss: 0.01605, val loss: 0.01654\n",
      "Training epoch: 521, train loss: 0.01693, val loss: 0.01753\n",
      "Training epoch: 522, train loss: 0.01613, val loss: 0.01663\n",
      "Training epoch: 523, train loss: 0.01576, val loss: 0.01626\n",
      "Training epoch: 524, train loss: 0.01607, val loss: 0.01649\n",
      "Training epoch: 525, train loss: 0.01630, val loss: 0.01685\n",
      "Training epoch: 526, train loss: 0.01575, val loss: 0.01625\n",
      "Training epoch: 527, train loss: 0.01601, val loss: 0.01651\n",
      "Training epoch: 528, train loss: 0.01577, val loss: 0.01623\n",
      "Training epoch: 529, train loss: 0.01592, val loss: 0.01639\n",
      "Training epoch: 530, train loss: 0.01665, val loss: 0.01707\n",
      "Training epoch: 531, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 532, train loss: 0.01600, val loss: 0.01647\n",
      "Training epoch: 533, train loss: 0.01619, val loss: 0.01661\n",
      "Training epoch: 534, train loss: 0.01620, val loss: 0.01662\n",
      "Training epoch: 535, train loss: 0.01618, val loss: 0.01669\n",
      "Training epoch: 536, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 537, train loss: 0.01716, val loss: 0.01759\n",
      "Training epoch: 538, train loss: 0.01630, val loss: 0.01682\n",
      "Training epoch: 539, train loss: 0.01641, val loss: 0.01692\n",
      "Training epoch: 540, train loss: 0.01615, val loss: 0.01657\n",
      "Training epoch: 541, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 542, train loss: 0.01615, val loss: 0.01663\n",
      "Training epoch: 543, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 544, train loss: 0.01692, val loss: 0.01730\n",
      "Training epoch: 545, train loss: 0.01632, val loss: 0.01676\n",
      "Training epoch: 546, train loss: 0.01605, val loss: 0.01651\n",
      "Training epoch: 547, train loss: 0.01614, val loss: 0.01666\n",
      "Training epoch: 548, train loss: 0.01634, val loss: 0.01676\n",
      "Training epoch: 549, train loss: 0.01593, val loss: 0.01638\n",
      "Training epoch: 550, train loss: 0.01582, val loss: 0.01626\n",
      "Training epoch: 551, train loss: 0.01608, val loss: 0.01644\n",
      "Training epoch: 552, train loss: 0.01611, val loss: 0.01656\n",
      "Training epoch: 553, train loss: 0.01582, val loss: 0.01628\n",
      "Training epoch: 554, train loss: 0.01665, val loss: 0.01723\n",
      "Training epoch: 555, train loss: 0.01585, val loss: 0.01627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 556, train loss: 0.01610, val loss: 0.01653\n",
      "Training epoch: 557, train loss: 0.01628, val loss: 0.01680\n",
      "Training epoch: 558, train loss: 0.01613, val loss: 0.01652\n",
      "Training epoch: 559, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 560, train loss: 0.01647, val loss: 0.01699\n",
      "Training epoch: 561, train loss: 0.01634, val loss: 0.01688\n",
      "Training epoch: 562, train loss: 0.01634, val loss: 0.01673\n",
      "Training epoch: 563, train loss: 0.01599, val loss: 0.01648\n",
      "Training epoch: 564, train loss: 0.01612, val loss: 0.01653\n",
      "Training epoch: 565, train loss: 0.01621, val loss: 0.01667\n",
      "Training epoch: 566, train loss: 0.01768, val loss: 0.01825\n",
      "Training epoch: 567, train loss: 0.01624, val loss: 0.01668\n",
      "Training epoch: 568, train loss: 0.01631, val loss: 0.01678\n",
      "Training epoch: 569, train loss: 0.01629, val loss: 0.01682\n",
      "Training epoch: 570, train loss: 0.01628, val loss: 0.01670\n",
      "Training epoch: 571, train loss: 0.01631, val loss: 0.01674\n",
      "Training epoch: 572, train loss: 0.01612, val loss: 0.01660\n",
      "Training epoch: 573, train loss: 0.01588, val loss: 0.01639\n",
      "Training epoch: 574, train loss: 0.01686, val loss: 0.01741\n",
      "Training epoch: 575, train loss: 0.01613, val loss: 0.01662\n",
      "Training epoch: 576, train loss: 0.01596, val loss: 0.01635\n",
      "Training epoch: 577, train loss: 0.01612, val loss: 0.01651\n",
      "Training epoch: 578, train loss: 0.01593, val loss: 0.01643\n",
      "Training epoch: 579, train loss: 0.01592, val loss: 0.01637\n",
      "Training epoch: 580, train loss: 0.01665, val loss: 0.01709\n",
      "Training epoch: 581, train loss: 0.01619, val loss: 0.01669\n",
      "Training epoch: 582, train loss: 0.01633, val loss: 0.01675\n",
      "Training epoch: 583, train loss: 0.01634, val loss: 0.01687\n",
      "Training epoch: 584, train loss: 0.01594, val loss: 0.01641\n",
      "Training epoch: 585, train loss: 0.01650, val loss: 0.01700\n",
      "Training epoch: 586, train loss: 0.01646, val loss: 0.01698\n",
      "Training epoch: 587, train loss: 0.01616, val loss: 0.01658\n",
      "Training epoch: 588, train loss: 0.01615, val loss: 0.01655\n",
      "Training epoch: 589, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 590, train loss: 0.01798, val loss: 0.01860\n",
      "Training epoch: 591, train loss: 0.01626, val loss: 0.01663\n",
      "Training epoch: 592, train loss: 0.01628, val loss: 0.01670\n",
      "Training epoch: 593, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 594, train loss: 0.01594, val loss: 0.01643\n",
      "Training epoch: 595, train loss: 0.01615, val loss: 0.01657\n",
      "Training epoch: 596, train loss: 0.01589, val loss: 0.01639\n",
      "Training epoch: 597, train loss: 0.01605, val loss: 0.01645\n",
      "Training epoch: 598, train loss: 0.01598, val loss: 0.01646\n",
      "Training epoch: 599, train loss: 0.01720, val loss: 0.01775\n",
      "Training epoch: 600, train loss: 0.01607, val loss: 0.01655\n",
      "Training epoch: 601, train loss: 0.01632, val loss: 0.01673\n",
      "Training epoch: 602, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 603, train loss: 0.01599, val loss: 0.01646\n",
      "Training epoch: 604, train loss: 0.01573, val loss: 0.01616\n",
      "Training epoch: 605, train loss: 0.01591, val loss: 0.01633\n",
      "Training epoch: 606, train loss: 0.01615, val loss: 0.01665\n",
      "Training epoch: 607, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 608, train loss: 0.01591, val loss: 0.01640\n",
      "Training epoch: 609, train loss: 0.01630, val loss: 0.01678\n",
      "Training epoch: 610, train loss: 0.01664, val loss: 0.01717\n",
      "Training epoch: 611, train loss: 0.01607, val loss: 0.01656\n",
      "Training epoch: 612, train loss: 0.01644, val loss: 0.01682\n",
      "Training epoch: 613, train loss: 0.01598, val loss: 0.01643\n",
      "Training epoch: 614, train loss: 0.01625, val loss: 0.01666\n",
      "Training epoch: 615, train loss: 0.01585, val loss: 0.01631\n",
      "Training epoch: 616, train loss: 0.01624, val loss: 0.01672\n",
      "Training epoch: 617, train loss: 0.01606, val loss: 0.01645\n",
      "Training epoch: 618, train loss: 0.01639, val loss: 0.01685\n",
      "Training epoch: 619, train loss: 0.01613, val loss: 0.01664\n",
      "Training epoch: 620, train loss: 0.01684, val loss: 0.01735\n",
      "Training epoch: 621, train loss: 0.01653, val loss: 0.01689\n",
      "Training epoch: 622, train loss: 0.01594, val loss: 0.01637\n",
      "Training epoch: 623, train loss: 0.01592, val loss: 0.01637\n",
      "Training epoch: 624, train loss: 0.01620, val loss: 0.01669\n",
      "Training epoch: 625, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 626, train loss: 0.01599, val loss: 0.01642\n",
      "Training epoch: 627, train loss: 0.01625, val loss: 0.01673\n",
      "Training epoch: 628, train loss: 0.01600, val loss: 0.01644\n",
      "Training epoch: 629, train loss: 0.01606, val loss: 0.01658\n",
      "Training epoch: 630, train loss: 0.01607, val loss: 0.01650\n",
      "Training epoch: 631, train loss: 0.01613, val loss: 0.01660\n",
      "Training epoch: 632, train loss: 0.01682, val loss: 0.01737\n",
      "Training epoch: 633, train loss: 0.01630, val loss: 0.01670\n",
      "Training epoch: 634, train loss: 0.01580, val loss: 0.01622\n",
      "Training epoch: 635, train loss: 0.01660, val loss: 0.01699\n",
      "Training epoch: 636, train loss: 0.01622, val loss: 0.01674\n",
      "Training epoch: 637, train loss: 0.01666, val loss: 0.01706\n",
      "Training epoch: 638, train loss: 0.01633, val loss: 0.01684\n",
      "Training epoch: 639, train loss: 0.01615, val loss: 0.01660\n",
      "Training epoch: 640, train loss: 0.01632, val loss: 0.01668\n",
      "Training epoch: 641, train loss: 0.01615, val loss: 0.01666\n",
      "Training epoch: 642, train loss: 0.01586, val loss: 0.01631\n",
      "Training epoch: 643, train loss: 0.01601, val loss: 0.01649\n",
      "Training epoch: 644, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 645, train loss: 0.01596, val loss: 0.01648\n",
      "Training epoch: 646, train loss: 0.01600, val loss: 0.01643\n",
      "Training epoch: 647, train loss: 0.01599, val loss: 0.01647\n",
      "Training epoch: 648, train loss: 0.01583, val loss: 0.01627\n",
      "Training epoch: 649, train loss: 0.01636, val loss: 0.01688\n",
      "Training epoch: 650, train loss: 0.01765, val loss: 0.01827\n",
      "Training epoch: 651, train loss: 0.01601, val loss: 0.01640\n",
      "Training epoch: 652, train loss: 0.01615, val loss: 0.01661\n",
      "Training epoch: 653, train loss: 0.01607, val loss: 0.01657\n",
      "Training epoch: 654, train loss: 0.01608, val loss: 0.01654\n",
      "Training epoch: 655, train loss: 0.01654, val loss: 0.01694\n",
      "Training epoch: 656, train loss: 0.01624, val loss: 0.01663\n",
      "Training epoch: 657, train loss: 0.01583, val loss: 0.01632\n",
      "Training epoch: 658, train loss: 0.01601, val loss: 0.01650\n",
      "Training epoch: 659, train loss: 0.01577, val loss: 0.01619\n",
      "Training epoch: 660, train loss: 0.01613, val loss: 0.01655\n",
      "Training epoch: 661, train loss: 0.01651, val loss: 0.01705\n",
      "Training epoch: 662, train loss: 0.01608, val loss: 0.01656\n",
      "Training epoch: 663, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 664, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 665, train loss: 0.01611, val loss: 0.01651\n",
      "Training epoch: 666, train loss: 0.01607, val loss: 0.01655\n",
      "Training epoch: 667, train loss: 0.01705, val loss: 0.01763\n",
      "Training epoch: 668, train loss: 0.01614, val loss: 0.01658\n",
      "Training epoch: 669, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 670, train loss: 0.01693, val loss: 0.01750\n",
      "Training epoch: 671, train loss: 0.01686, val loss: 0.01738\n",
      "Training epoch: 672, train loss: 0.01619, val loss: 0.01663\n",
      "Training epoch: 673, train loss: 0.01606, val loss: 0.01647\n",
      "Training epoch: 674, train loss: 0.01596, val loss: 0.01646\n",
      "Training epoch: 675, train loss: 0.01589, val loss: 0.01635\n",
      "Training epoch: 676, train loss: 0.01646, val loss: 0.01683\n",
      "Training epoch: 677, train loss: 0.01615, val loss: 0.01663\n",
      "Training epoch: 678, train loss: 0.01597, val loss: 0.01640\n",
      "Training epoch: 679, train loss: 0.01600, val loss: 0.01640\n",
      "Training epoch: 680, train loss: 0.01611, val loss: 0.01658\n",
      "Training epoch: 681, train loss: 0.01591, val loss: 0.01636\n",
      "Training epoch: 682, train loss: 0.01604, val loss: 0.01655\n",
      "Training epoch: 683, train loss: 0.01628, val loss: 0.01680\n",
      "Training epoch: 684, train loss: 0.01636, val loss: 0.01684\n",
      "Training epoch: 685, train loss: 0.01585, val loss: 0.01629\n",
      "Training epoch: 686, train loss: 0.01581, val loss: 0.01623\n",
      "Training epoch: 687, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 688, train loss: 0.01761, val loss: 0.01809\n",
      "Training epoch: 689, train loss: 0.01631, val loss: 0.01680\n",
      "Training epoch: 690, train loss: 0.01642, val loss: 0.01678\n",
      "Training epoch: 691, train loss: 0.01598, val loss: 0.01638\n",
      "Training epoch: 692, train loss: 0.01586, val loss: 0.01634\n",
      "Training epoch: 693, train loss: 0.01617, val loss: 0.01665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 694, train loss: 0.01622, val loss: 0.01673\n",
      "Training epoch: 695, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 696, train loss: 0.01608, val loss: 0.01657\n",
      "Training epoch: 697, train loss: 0.01628, val loss: 0.01678\n",
      "Training epoch: 698, train loss: 0.01602, val loss: 0.01649\n",
      "Training epoch: 699, train loss: 0.01616, val loss: 0.01665\n",
      "Training epoch: 700, train loss: 0.01591, val loss: 0.01639\n",
      "Training epoch: 701, train loss: 0.01598, val loss: 0.01643\n",
      "Training epoch: 702, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 703, train loss: 0.01612, val loss: 0.01656\n",
      "Training epoch: 704, train loss: 0.01594, val loss: 0.01640\n",
      "Training epoch: 705, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 706, train loss: 0.01657, val loss: 0.01693\n",
      "Training epoch: 707, train loss: 0.01667, val loss: 0.01715\n",
      "Training epoch: 708, train loss: 0.01642, val loss: 0.01690\n",
      "Training epoch: 709, train loss: 0.01648, val loss: 0.01701\n",
      "Training epoch: 710, train loss: 0.01600, val loss: 0.01649\n",
      "Training epoch: 711, train loss: 0.01661, val loss: 0.01715\n",
      "Training epoch: 712, train loss: 0.01681, val loss: 0.01734\n",
      "Training epoch: 713, train loss: 0.01623, val loss: 0.01663\n",
      "Training epoch: 714, train loss: 0.01609, val loss: 0.01646\n",
      "Training epoch: 715, train loss: 0.01625, val loss: 0.01671\n",
      "Training epoch: 716, train loss: 0.01609, val loss: 0.01658\n",
      "Training epoch: 717, train loss: 0.01630, val loss: 0.01680\n",
      "Training epoch: 718, train loss: 0.01648, val loss: 0.01700\n",
      "Training epoch: 719, train loss: 0.01644, val loss: 0.01693\n",
      "Training epoch: 720, train loss: 0.01603, val loss: 0.01644\n",
      "Training epoch: 721, train loss: 0.01624, val loss: 0.01674\n",
      "Training epoch: 722, train loss: 0.01578, val loss: 0.01621\n",
      "Training epoch: 723, train loss: 0.01596, val loss: 0.01641\n",
      "Training epoch: 724, train loss: 0.01646, val loss: 0.01696\n",
      "Training epoch: 725, train loss: 0.01601, val loss: 0.01646\n",
      "Training epoch: 726, train loss: 0.01615, val loss: 0.01656\n",
      "Training epoch: 727, train loss: 0.01697, val loss: 0.01733\n",
      "Training epoch: 728, train loss: 0.01608, val loss: 0.01648\n",
      "Training epoch: 729, train loss: 0.01602, val loss: 0.01652\n",
      "Training epoch: 730, train loss: 0.01595, val loss: 0.01644\n",
      "Training epoch: 731, train loss: 0.01656, val loss: 0.01698\n",
      "Training epoch: 732, train loss: 0.01610, val loss: 0.01654\n",
      "Training epoch: 733, train loss: 0.01618, val loss: 0.01665\n",
      "Training epoch: 734, train loss: 0.01593, val loss: 0.01639\n",
      "Training epoch: 735, train loss: 0.01593, val loss: 0.01640\n",
      "Training epoch: 736, train loss: 0.01605, val loss: 0.01651\n",
      "Training epoch: 737, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 738, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 739, train loss: 0.01675, val loss: 0.01731\n",
      "Training epoch: 740, train loss: 0.01667, val loss: 0.01705\n",
      "Training epoch: 741, train loss: 0.01570, val loss: 0.01614\n",
      "Training epoch: 742, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 743, train loss: 0.01613, val loss: 0.01655\n",
      "Training epoch: 744, train loss: 0.01588, val loss: 0.01634\n",
      "Training epoch: 745, train loss: 0.01715, val loss: 0.01772\n",
      "Training epoch: 746, train loss: 0.01627, val loss: 0.01676\n",
      "Training epoch: 747, train loss: 0.01632, val loss: 0.01669\n",
      "Training epoch: 748, train loss: 0.01627, val loss: 0.01679\n",
      "Training epoch: 749, train loss: 0.01618, val loss: 0.01665\n",
      "Training epoch: 750, train loss: 0.01574, val loss: 0.01617\n",
      "Training epoch: 751, train loss: 0.01602, val loss: 0.01647\n",
      "Training epoch: 752, train loss: 0.01625, val loss: 0.01668\n",
      "Training epoch: 753, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 754, train loss: 0.01605, val loss: 0.01647\n",
      "Training epoch: 755, train loss: 0.01656, val loss: 0.01712\n",
      "Training epoch: 756, train loss: 0.01579, val loss: 0.01621\n",
      "Training epoch: 757, train loss: 0.01601, val loss: 0.01648\n",
      "Training epoch: 758, train loss: 0.01587, val loss: 0.01632\n",
      "Training epoch: 759, train loss: 0.01619, val loss: 0.01664\n",
      "Training epoch: 760, train loss: 0.01615, val loss: 0.01665\n",
      "Training epoch: 761, train loss: 0.01671, val loss: 0.01723\n",
      "Training epoch: 762, train loss: 0.01739, val loss: 0.01800\n",
      "Training epoch: 763, train loss: 0.01608, val loss: 0.01647\n",
      "Training epoch: 764, train loss: 0.01631, val loss: 0.01669\n",
      "Training epoch: 765, train loss: 0.01676, val loss: 0.01719\n",
      "Training epoch: 766, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 767, train loss: 0.01623, val loss: 0.01668\n",
      "Training epoch: 768, train loss: 0.01597, val loss: 0.01643\n",
      "Training epoch: 769, train loss: 0.01593, val loss: 0.01632\n",
      "Training epoch: 770, train loss: 0.01639, val loss: 0.01684\n",
      "Training epoch: 771, train loss: 0.01638, val loss: 0.01685\n",
      "Training epoch: 772, train loss: 0.01605, val loss: 0.01649\n",
      "Training epoch: 773, train loss: 0.01613, val loss: 0.01655\n",
      "Training epoch: 774, train loss: 0.01613, val loss: 0.01662\n",
      "Training epoch: 775, train loss: 0.01581, val loss: 0.01630\n",
      "Training epoch: 776, train loss: 0.01641, val loss: 0.01690\n",
      "Training epoch: 777, train loss: 0.01632, val loss: 0.01673\n",
      "Training epoch: 778, train loss: 0.01628, val loss: 0.01680\n",
      "Training epoch: 779, train loss: 0.01578, val loss: 0.01621\n",
      "Training epoch: 780, train loss: 0.01624, val loss: 0.01676\n",
      "Training epoch: 781, train loss: 0.01604, val loss: 0.01648\n",
      "Training epoch: 782, train loss: 0.01584, val loss: 0.01629\n",
      "Training epoch: 783, train loss: 0.01592, val loss: 0.01632\n",
      "Training epoch: 784, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 785, train loss: 0.01597, val loss: 0.01648\n",
      "Training epoch: 786, train loss: 0.01635, val loss: 0.01685\n",
      "Training epoch: 787, train loss: 0.01585, val loss: 0.01632\n",
      "Training epoch: 788, train loss: 0.01614, val loss: 0.01651\n",
      "Training epoch: 789, train loss: 0.01611, val loss: 0.01655\n",
      "Training epoch: 790, train loss: 0.01594, val loss: 0.01641\n",
      "Training epoch: 791, train loss: 0.01596, val loss: 0.01642\n",
      "Training epoch: 792, train loss: 0.01604, val loss: 0.01650\n",
      "Training epoch: 793, train loss: 0.01623, val loss: 0.01674\n",
      "Training epoch: 794, train loss: 0.01581, val loss: 0.01628\n",
      "Training epoch: 795, train loss: 0.01573, val loss: 0.01619\n",
      "Training epoch: 796, train loss: 0.01582, val loss: 0.01625\n",
      "Training epoch: 797, train loss: 0.01583, val loss: 0.01623\n",
      "Training epoch: 798, train loss: 0.01729, val loss: 0.01786\n",
      "Training epoch: 799, train loss: 0.01588, val loss: 0.01637\n",
      "Training epoch: 800, train loss: 0.01630, val loss: 0.01674\n",
      "Training epoch: 801, train loss: 0.01605, val loss: 0.01653\n",
      "Training epoch: 802, train loss: 0.01654, val loss: 0.01690\n",
      "Training epoch: 803, train loss: 0.01593, val loss: 0.01637\n",
      "Training epoch: 804, train loss: 0.01621, val loss: 0.01666\n",
      "Training epoch: 805, train loss: 0.01638, val loss: 0.01679\n",
      "Training epoch: 806, train loss: 0.01648, val loss: 0.01699\n",
      "Training epoch: 807, train loss: 0.01636, val loss: 0.01687\n",
      "Training epoch: 808, train loss: 0.01610, val loss: 0.01650\n",
      "Training epoch: 809, train loss: 0.01635, val loss: 0.01682\n",
      "Training epoch: 810, train loss: 0.01591, val loss: 0.01629\n",
      "Training epoch: 811, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 812, train loss: 0.01584, val loss: 0.01632\n",
      "Training epoch: 813, train loss: 0.01645, val loss: 0.01690\n",
      "Training epoch: 814, train loss: 0.01593, val loss: 0.01634\n",
      "Training epoch: 815, train loss: 0.01601, val loss: 0.01646\n",
      "Training epoch: 816, train loss: 0.01604, val loss: 0.01655\n",
      "Training epoch: 817, train loss: 0.01633, val loss: 0.01674\n",
      "Training epoch: 818, train loss: 0.01647, val loss: 0.01699\n",
      "Training epoch: 819, train loss: 0.01643, val loss: 0.01695\n",
      "Training epoch: 820, train loss: 0.01655, val loss: 0.01697\n",
      "Training epoch: 821, train loss: 0.01604, val loss: 0.01653\n",
      "Training epoch: 822, train loss: 0.01611, val loss: 0.01655\n",
      "Training epoch: 823, train loss: 0.01713, val loss: 0.01770\n",
      "Training epoch: 824, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 825, train loss: 0.01644, val loss: 0.01681\n",
      "Training epoch: 826, train loss: 0.01692, val loss: 0.01747\n",
      "Training epoch: 827, train loss: 0.01659, val loss: 0.01708\n",
      "Training epoch: 828, train loss: 0.01634, val loss: 0.01670\n",
      "Training epoch: 829, train loss: 0.01636, val loss: 0.01675\n",
      "Training epoch: 830, train loss: 0.01625, val loss: 0.01670\n",
      "Training epoch: 831, train loss: 0.01582, val loss: 0.01625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 832, train loss: 0.01599, val loss: 0.01645\n",
      "Training epoch: 833, train loss: 0.01608, val loss: 0.01660\n",
      "Training epoch: 834, train loss: 0.01577, val loss: 0.01621\n",
      "Training epoch: 835, train loss: 0.01608, val loss: 0.01658\n",
      "Training epoch: 836, train loss: 0.01611, val loss: 0.01655\n",
      "Training epoch: 837, train loss: 0.01624, val loss: 0.01662\n",
      "Training epoch: 838, train loss: 0.01622, val loss: 0.01664\n",
      "Training epoch: 839, train loss: 0.01590, val loss: 0.01636\n",
      "Training epoch: 840, train loss: 0.01617, val loss: 0.01666\n",
      "Training epoch: 841, train loss: 0.01621, val loss: 0.01662\n",
      "Training epoch: 842, train loss: 0.01590, val loss: 0.01635\n",
      "Training epoch: 843, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 844, train loss: 0.01616, val loss: 0.01667\n",
      "Training epoch: 845, train loss: 0.01579, val loss: 0.01618\n",
      "Training epoch: 846, train loss: 0.01590, val loss: 0.01631\n",
      "Training epoch: 847, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 848, train loss: 0.01645, val loss: 0.01697\n",
      "Training epoch: 849, train loss: 0.01608, val loss: 0.01659\n",
      "Training epoch: 850, train loss: 0.01581, val loss: 0.01626\n",
      "Training epoch: 851, train loss: 0.01713, val loss: 0.01756\n",
      "Training epoch: 852, train loss: 0.01600, val loss: 0.01639\n",
      "Training epoch: 853, train loss: 0.01602, val loss: 0.01651\n",
      "Training epoch: 854, train loss: 0.01589, val loss: 0.01633\n",
      "Training epoch: 855, train loss: 0.01618, val loss: 0.01670\n",
      "Training epoch: 856, train loss: 0.01639, val loss: 0.01680\n",
      "Training epoch: 857, train loss: 0.01616, val loss: 0.01657\n",
      "Training epoch: 858, train loss: 0.01605, val loss: 0.01654\n",
      "Training epoch: 859, train loss: 0.01602, val loss: 0.01653\n",
      "Training epoch: 860, train loss: 0.01585, val loss: 0.01628\n",
      "Training epoch: 861, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 862, train loss: 0.01601, val loss: 0.01650\n",
      "Training epoch: 863, train loss: 0.01603, val loss: 0.01651\n",
      "Training epoch: 864, train loss: 0.01604, val loss: 0.01656\n",
      "Training epoch: 865, train loss: 0.01607, val loss: 0.01653\n",
      "Training epoch: 866, train loss: 0.01626, val loss: 0.01676\n",
      "Training epoch: 867, train loss: 0.01596, val loss: 0.01640\n",
      "Training epoch: 868, train loss: 0.01588, val loss: 0.01634\n",
      "Training epoch: 869, train loss: 0.01612, val loss: 0.01657\n",
      "Training epoch: 870, train loss: 0.01599, val loss: 0.01645\n",
      "Training epoch: 871, train loss: 0.01589, val loss: 0.01628\n",
      "Training epoch: 872, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 873, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 874, train loss: 0.01578, val loss: 0.01623\n",
      "Training epoch: 875, train loss: 0.01613, val loss: 0.01660\n",
      "Training epoch: 876, train loss: 0.01603, val loss: 0.01650\n",
      "Training epoch: 877, train loss: 0.01571, val loss: 0.01614\n",
      "Training epoch: 878, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 879, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 880, train loss: 0.01628, val loss: 0.01682\n",
      "Training epoch: 881, train loss: 0.01590, val loss: 0.01628\n",
      "Training epoch: 882, train loss: 0.01635, val loss: 0.01681\n",
      "Training epoch: 883, train loss: 0.01599, val loss: 0.01646\n",
      "Training epoch: 884, train loss: 0.01639, val loss: 0.01689\n",
      "Training epoch: 885, train loss: 0.01619, val loss: 0.01664\n",
      "Training epoch: 886, train loss: 0.01623, val loss: 0.01674\n",
      "Training epoch: 887, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 888, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 889, train loss: 0.01624, val loss: 0.01667\n",
      "Training epoch: 890, train loss: 0.01603, val loss: 0.01651\n",
      "Training epoch: 891, train loss: 0.01614, val loss: 0.01661\n",
      "Training epoch: 892, train loss: 0.01623, val loss: 0.01659\n",
      "Training epoch: 893, train loss: 0.01646, val loss: 0.01692\n",
      "Training epoch: 894, train loss: 0.01593, val loss: 0.01639\n",
      "Training epoch: 895, train loss: 0.01607, val loss: 0.01646\n",
      "Training epoch: 896, train loss: 0.01647, val loss: 0.01702\n",
      "Training epoch: 897, train loss: 0.01637, val loss: 0.01690\n",
      "Training epoch: 898, train loss: 0.01661, val loss: 0.01696\n",
      "Training epoch: 899, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 900, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 901, train loss: 0.01610, val loss: 0.01656\n",
      "Training epoch: 902, train loss: 0.01620, val loss: 0.01664\n",
      "Training epoch: 903, train loss: 0.01700, val loss: 0.01740\n",
      "Training epoch: 904, train loss: 0.01602, val loss: 0.01649\n",
      "Training epoch: 905, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 906, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 907, train loss: 0.01588, val loss: 0.01635\n",
      "Training epoch: 908, train loss: 0.01597, val loss: 0.01645\n",
      "Training epoch: 909, train loss: 0.01592, val loss: 0.01634\n",
      "Training epoch: 910, train loss: 0.01767, val loss: 0.01801\n",
      "Training epoch: 911, train loss: 0.01639, val loss: 0.01686\n",
      "Training epoch: 912, train loss: 0.01628, val loss: 0.01672\n",
      "Training epoch: 913, train loss: 0.01635, val loss: 0.01679\n",
      "Training epoch: 914, train loss: 0.01586, val loss: 0.01633\n",
      "Training epoch: 915, train loss: 0.01584, val loss: 0.01631\n",
      "Training epoch: 916, train loss: 0.01592, val loss: 0.01639\n",
      "Training epoch: 917, train loss: 0.01648, val loss: 0.01699\n",
      "Training epoch: 918, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 919, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 920, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 921, train loss: 0.01634, val loss: 0.01670\n",
      "Training epoch: 922, train loss: 0.01594, val loss: 0.01642\n",
      "Training epoch: 923, train loss: 0.01593, val loss: 0.01640\n",
      "Training epoch: 924, train loss: 0.01617, val loss: 0.01665\n",
      "Training epoch: 925, train loss: 0.01630, val loss: 0.01679\n",
      "Training epoch: 926, train loss: 0.01654, val loss: 0.01690\n",
      "Training epoch: 927, train loss: 0.01647, val loss: 0.01686\n",
      "Training epoch: 928, train loss: 0.01612, val loss: 0.01652\n",
      "Training epoch: 929, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 930, train loss: 0.01639, val loss: 0.01681\n",
      "Training epoch: 931, train loss: 0.01600, val loss: 0.01643\n",
      "Training epoch: 932, train loss: 0.01623, val loss: 0.01665\n",
      "Training epoch: 933, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 934, train loss: 0.01583, val loss: 0.01631\n",
      "Training epoch: 935, train loss: 0.01603, val loss: 0.01649\n",
      "Training epoch: 936, train loss: 0.01593, val loss: 0.01637\n",
      "Training epoch: 937, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 938, train loss: 0.01618, val loss: 0.01662\n",
      "Training epoch: 939, train loss: 0.01627, val loss: 0.01675\n",
      "Training epoch: 940, train loss: 0.01625, val loss: 0.01665\n",
      "Training epoch: 941, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 942, train loss: 0.01645, val loss: 0.01685\n",
      "Training epoch: 943, train loss: 0.01587, val loss: 0.01630\n",
      "Training epoch: 944, train loss: 0.01632, val loss: 0.01681\n",
      "Training epoch: 945, train loss: 0.01599, val loss: 0.01645\n",
      "Training epoch: 946, train loss: 0.01605, val loss: 0.01657\n",
      "Training epoch: 947, train loss: 0.01615, val loss: 0.01664\n",
      "Training epoch: 948, train loss: 0.01599, val loss: 0.01648\n",
      "Training epoch: 949, train loss: 0.01600, val loss: 0.01651\n",
      "Training epoch: 950, train loss: 0.01593, val loss: 0.01637\n",
      "Training epoch: 951, train loss: 0.01609, val loss: 0.01659\n",
      "Training epoch: 952, train loss: 0.01581, val loss: 0.01621\n",
      "Training epoch: 953, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 954, train loss: 0.01596, val loss: 0.01635\n",
      "Training epoch: 955, train loss: 0.01627, val loss: 0.01671\n",
      "Training epoch: 956, train loss: 0.01596, val loss: 0.01642\n",
      "Training epoch: 957, train loss: 0.01608, val loss: 0.01653\n",
      "Training epoch: 958, train loss: 0.01605, val loss: 0.01646\n",
      "Training epoch: 959, train loss: 0.01607, val loss: 0.01645\n",
      "Training epoch: 960, train loss: 0.01651, val loss: 0.01691\n",
      "Training epoch: 961, train loss: 0.01591, val loss: 0.01640\n",
      "Training epoch: 962, train loss: 0.01592, val loss: 0.01639\n",
      "Training epoch: 963, train loss: 0.01588, val loss: 0.01632\n",
      "Training epoch: 964, train loss: 0.01587, val loss: 0.01627\n",
      "Training epoch: 965, train loss: 0.01574, val loss: 0.01619\n",
      "Training epoch: 966, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 967, train loss: 0.01600, val loss: 0.01643\n",
      "Training epoch: 968, train loss: 0.01592, val loss: 0.01634\n",
      "Training epoch: 969, train loss: 0.01619, val loss: 0.01665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 970, train loss: 0.01597, val loss: 0.01645\n",
      "Training epoch: 971, train loss: 0.01584, val loss: 0.01626\n",
      "Training epoch: 972, train loss: 0.01607, val loss: 0.01655\n",
      "Training epoch: 973, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 974, train loss: 0.01609, val loss: 0.01657\n",
      "Training epoch: 975, train loss: 0.01660, val loss: 0.01695\n",
      "Training epoch: 976, train loss: 0.01614, val loss: 0.01652\n",
      "Training epoch: 977, train loss: 0.01645, val loss: 0.01697\n",
      "Training epoch: 978, train loss: 0.01641, val loss: 0.01692\n",
      "Training epoch: 979, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 980, train loss: 0.01600, val loss: 0.01649\n",
      "Training epoch: 981, train loss: 0.01695, val loss: 0.01749\n",
      "Training epoch: 982, train loss: 0.01604, val loss: 0.01641\n",
      "Training epoch: 983, train loss: 0.01591, val loss: 0.01631\n",
      "Training epoch: 984, train loss: 0.01617, val loss: 0.01657\n",
      "Training epoch: 985, train loss: 0.01600, val loss: 0.01640\n",
      "Training epoch: 986, train loss: 0.01590, val loss: 0.01639\n",
      "Training epoch: 987, train loss: 0.01619, val loss: 0.01670\n",
      "Training epoch: 988, train loss: 0.01593, val loss: 0.01639\n",
      "Training epoch: 989, train loss: 0.01579, val loss: 0.01625\n",
      "Training epoch: 990, train loss: 0.01578, val loss: 0.01623\n",
      "Training epoch: 991, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 992, train loss: 0.01602, val loss: 0.01645\n",
      "Training epoch: 993, train loss: 0.01602, val loss: 0.01652\n",
      "Training epoch: 994, train loss: 0.01590, val loss: 0.01631\n",
      "Training epoch: 995, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 996, train loss: 0.01601, val loss: 0.01643\n",
      "Training epoch: 997, train loss: 0.01593, val loss: 0.01636\n",
      "Training epoch: 998, train loss: 0.01598, val loss: 0.01644\n",
      "Training epoch: 999, train loss: 0.01584, val loss: 0.01629\n",
      "Training epoch: 1000, train loss: 0.01695, val loss: 0.01751\n",
      "Training epoch: 1001, train loss: 0.01647, val loss: 0.01688\n",
      "Training epoch: 1002, train loss: 0.01616, val loss: 0.01655\n",
      "Training epoch: 1003, train loss: 0.01680, val loss: 0.01714\n",
      "Training epoch: 1004, train loss: 0.01632, val loss: 0.01681\n",
      "Training epoch: 1005, train loss: 0.01647, val loss: 0.01701\n",
      "Training epoch: 1006, train loss: 0.01776, val loss: 0.01839\n",
      "Training epoch: 1007, train loss: 0.01666, val loss: 0.01721\n",
      "Training epoch: 1008, train loss: 0.01580, val loss: 0.01623\n",
      "Training epoch: 1009, train loss: 0.01611, val loss: 0.01659\n",
      "Training epoch: 1010, train loss: 0.01632, val loss: 0.01683\n",
      "Training epoch: 1011, train loss: 0.01603, val loss: 0.01652\n",
      "Training epoch: 1012, train loss: 0.01600, val loss: 0.01642\n",
      "Training epoch: 1013, train loss: 0.01604, val loss: 0.01643\n",
      "Training epoch: 1014, train loss: 0.01622, val loss: 0.01670\n",
      "Training epoch: 1015, train loss: 0.01627, val loss: 0.01671\n",
      "Training epoch: 1016, train loss: 0.01615, val loss: 0.01659\n",
      "Training epoch: 1017, train loss: 0.01639, val loss: 0.01686\n",
      "Training epoch: 1018, train loss: 0.01588, val loss: 0.01631\n",
      "Training epoch: 1019, train loss: 0.01597, val loss: 0.01643\n",
      "Training epoch: 1020, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 1021, train loss: 0.01613, val loss: 0.01660\n",
      "Training epoch: 1022, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 1023, train loss: 0.01586, val loss: 0.01631\n",
      "Training epoch: 1024, train loss: 0.01598, val loss: 0.01647\n",
      "Training epoch: 1025, train loss: 0.01612, val loss: 0.01662\n",
      "Training epoch: 1026, train loss: 0.01583, val loss: 0.01628\n",
      "Training epoch: 1027, train loss: 0.01582, val loss: 0.01621\n",
      "Training epoch: 1028, train loss: 0.01600, val loss: 0.01649\n",
      "Training epoch: 1029, train loss: 0.01616, val loss: 0.01661\n",
      "Training epoch: 1030, train loss: 0.01634, val loss: 0.01682\n",
      "Training epoch: 1031, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 1032, train loss: 0.01618, val loss: 0.01659\n",
      "Training epoch: 1033, train loss: 0.01579, val loss: 0.01619\n",
      "Training epoch: 1034, train loss: 0.01586, val loss: 0.01625\n",
      "Training epoch: 1035, train loss: 0.01601, val loss: 0.01639\n",
      "Training epoch: 1036, train loss: 0.01635, val loss: 0.01683\n",
      "Training epoch: 1037, train loss: 0.01585, val loss: 0.01627\n",
      "Training epoch: 1038, train loss: 0.01620, val loss: 0.01666\n",
      "Training epoch: 1039, train loss: 0.01590, val loss: 0.01638\n",
      "Training epoch: 1040, train loss: 0.01611, val loss: 0.01652\n",
      "Training epoch: 1041, train loss: 0.01604, val loss: 0.01643\n",
      "Training epoch: 1042, train loss: 0.01810, val loss: 0.01842\n",
      "Training epoch: 1043, train loss: 0.01610, val loss: 0.01651\n",
      "Training epoch: 1044, train loss: 0.01625, val loss: 0.01667\n",
      "Training epoch: 1045, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 1046, train loss: 0.01601, val loss: 0.01650\n",
      "Training epoch: 1047, train loss: 0.01576, val loss: 0.01625\n",
      "Training epoch: 1048, train loss: 0.01605, val loss: 0.01652\n",
      "Training epoch: 1049, train loss: 0.01634, val loss: 0.01683\n",
      "Training epoch: 1050, train loss: 0.01626, val loss: 0.01678\n",
      "Training epoch: 1051, train loss: 0.01650, val loss: 0.01703\n",
      "Training epoch: 1052, train loss: 0.01596, val loss: 0.01635\n",
      "Training epoch: 1053, train loss: 0.01596, val loss: 0.01643\n",
      "Training epoch: 1054, train loss: 0.01627, val loss: 0.01677\n",
      "Training epoch: 1055, train loss: 0.01589, val loss: 0.01635\n",
      "Training epoch: 1056, train loss: 0.01650, val loss: 0.01702\n",
      "Training epoch: 1057, train loss: 0.01632, val loss: 0.01685\n",
      "Training epoch: 1058, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1059, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1060, train loss: 0.01587, val loss: 0.01635\n",
      "Training epoch: 1061, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 1062, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 1063, train loss: 0.01577, val loss: 0.01621\n",
      "Training epoch: 1064, train loss: 0.01597, val loss: 0.01647\n",
      "Training epoch: 1065, train loss: 0.01647, val loss: 0.01693\n",
      "Training epoch: 1066, train loss: 0.01607, val loss: 0.01654\n",
      "Training epoch: 1067, train loss: 0.01609, val loss: 0.01657\n",
      "Training epoch: 1068, train loss: 0.01619, val loss: 0.01666\n",
      "Training epoch: 1069, train loss: 0.01610, val loss: 0.01660\n",
      "Training epoch: 1070, train loss: 0.01598, val loss: 0.01637\n",
      "Training epoch: 1071, train loss: 0.01600, val loss: 0.01641\n",
      "Training epoch: 1072, train loss: 0.01649, val loss: 0.01687\n",
      "Training epoch: 1073, train loss: 0.01608, val loss: 0.01651\n",
      "Training epoch: 1074, train loss: 0.01602, val loss: 0.01643\n",
      "Training epoch: 1075, train loss: 0.01649, val loss: 0.01697\n",
      "Training epoch: 1076, train loss: 0.01673, val loss: 0.01720\n",
      "Training epoch: 1077, train loss: 0.01619, val loss: 0.01660\n",
      "Training epoch: 1078, train loss: 0.01582, val loss: 0.01628\n",
      "Training epoch: 1079, train loss: 0.01613, val loss: 0.01661\n",
      "Training epoch: 1080, train loss: 0.01582, val loss: 0.01628\n",
      "Training epoch: 1081, train loss: 0.01580, val loss: 0.01623\n",
      "Training epoch: 1082, train loss: 0.01586, val loss: 0.01631\n",
      "Training epoch: 1083, train loss: 0.01591, val loss: 0.01630\n",
      "Training epoch: 1084, train loss: 0.01581, val loss: 0.01624\n",
      "Training epoch: 1085, train loss: 0.01598, val loss: 0.01649\n",
      "Training epoch: 1086, train loss: 0.01607, val loss: 0.01654\n",
      "Training epoch: 1087, train loss: 0.01571, val loss: 0.01615\n",
      "Training epoch: 1088, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1089, train loss: 0.01585, val loss: 0.01630\n",
      "Training epoch: 1090, train loss: 0.01621, val loss: 0.01673\n",
      "Training epoch: 1091, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 1092, train loss: 0.01585, val loss: 0.01632\n",
      "Training epoch: 1093, train loss: 0.01637, val loss: 0.01692\n",
      "Training epoch: 1094, train loss: 0.01650, val loss: 0.01703\n",
      "Training epoch: 1095, train loss: 0.01607, val loss: 0.01650\n",
      "Training epoch: 1096, train loss: 0.01670, val loss: 0.01710\n",
      "Training epoch: 1097, train loss: 0.01608, val loss: 0.01656\n",
      "Training epoch: 1098, train loss: 0.01650, val loss: 0.01703\n",
      "Training epoch: 1099, train loss: 0.01592, val loss: 0.01638\n",
      "Training epoch: 1100, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1101, train loss: 0.01603, val loss: 0.01651\n",
      "Training epoch: 1102, train loss: 0.01646, val loss: 0.01682\n",
      "Training epoch: 1103, train loss: 0.01632, val loss: 0.01672\n",
      "Training epoch: 1104, train loss: 0.01676, val loss: 0.01720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1105, train loss: 0.01624, val loss: 0.01667\n",
      "Training epoch: 1106, train loss: 0.01608, val loss: 0.01651\n",
      "Training epoch: 1107, train loss: 0.01631, val loss: 0.01678\n",
      "Training epoch: 1108, train loss: 0.01590, val loss: 0.01636\n",
      "Training epoch: 1109, train loss: 0.01631, val loss: 0.01676\n",
      "Training epoch: 1110, train loss: 0.01605, val loss: 0.01648\n",
      "Training epoch: 1111, train loss: 0.01579, val loss: 0.01625\n",
      "Training epoch: 1112, train loss: 0.01593, val loss: 0.01639\n",
      "Training epoch: 1113, train loss: 0.01613, val loss: 0.01657\n",
      "Training epoch: 1114, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1115, train loss: 0.01618, val loss: 0.01667\n",
      "Training epoch: 1116, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1117, train loss: 0.01603, val loss: 0.01644\n",
      "Training epoch: 1118, train loss: 0.01598, val loss: 0.01645\n",
      "Training epoch: 1119, train loss: 0.01594, val loss: 0.01644\n",
      "Training epoch: 1120, train loss: 0.01576, val loss: 0.01621\n",
      "Training epoch: 1121, train loss: 0.01581, val loss: 0.01630\n",
      "Training epoch: 1122, train loss: 0.01599, val loss: 0.01648\n",
      "Training epoch: 1123, train loss: 0.01574, val loss: 0.01619\n",
      "Training epoch: 1124, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 1125, train loss: 0.01605, val loss: 0.01652\n",
      "Training epoch: 1126, train loss: 0.01587, val loss: 0.01632\n",
      "Training epoch: 1127, train loss: 0.01641, val loss: 0.01682\n",
      "Training epoch: 1128, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1129, train loss: 0.01634, val loss: 0.01684\n",
      "Training epoch: 1130, train loss: 0.01689, val loss: 0.01745\n",
      "Training epoch: 1131, train loss: 0.01657, val loss: 0.01711\n",
      "Training epoch: 1132, train loss: 0.01598, val loss: 0.01643\n",
      "Training epoch: 1133, train loss: 0.01611, val loss: 0.01655\n",
      "Training epoch: 1134, train loss: 0.01581, val loss: 0.01624\n",
      "Training epoch: 1135, train loss: 0.01607, val loss: 0.01652\n",
      "Training epoch: 1136, train loss: 0.01591, val loss: 0.01638\n",
      "Training epoch: 1137, train loss: 0.01576, val loss: 0.01620\n",
      "Training epoch: 1138, train loss: 0.01596, val loss: 0.01641\n",
      "Training epoch: 1139, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1140, train loss: 0.01593, val loss: 0.01641\n",
      "Training epoch: 1141, train loss: 0.01599, val loss: 0.01650\n",
      "Training epoch: 1142, train loss: 0.01606, val loss: 0.01643\n",
      "Training epoch: 1143, train loss: 0.01585, val loss: 0.01625\n",
      "Training epoch: 1144, train loss: 0.01672, val loss: 0.01709\n",
      "Training epoch: 1145, train loss: 0.01620, val loss: 0.01669\n",
      "Training epoch: 1146, train loss: 0.01594, val loss: 0.01640\n",
      "Training epoch: 1147, train loss: 0.01592, val loss: 0.01643\n",
      "Training epoch: 1148, train loss: 0.01614, val loss: 0.01663\n",
      "Training epoch: 1149, train loss: 0.01626, val loss: 0.01679\n",
      "Training epoch: 1150, train loss: 0.01612, val loss: 0.01664\n",
      "Training epoch: 1151, train loss: 0.01573, val loss: 0.01615\n",
      "Training epoch: 1152, train loss: 0.01586, val loss: 0.01633\n",
      "Training epoch: 1153, train loss: 0.01578, val loss: 0.01623\n",
      "Training epoch: 1154, train loss: 0.01589, val loss: 0.01636\n",
      "Training epoch: 1155, train loss: 0.01627, val loss: 0.01676\n",
      "Training epoch: 1156, train loss: 0.01583, val loss: 0.01629\n",
      "Training epoch: 1157, train loss: 0.01614, val loss: 0.01653\n",
      "Training epoch: 1158, train loss: 0.01592, val loss: 0.01641\n",
      "Training epoch: 1159, train loss: 0.01665, val loss: 0.01710\n",
      "Training epoch: 1160, train loss: 0.01612, val loss: 0.01662\n",
      "Training epoch: 1161, train loss: 0.01588, val loss: 0.01627\n",
      "Training epoch: 1162, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1163, train loss: 0.01707, val loss: 0.01762\n",
      "Training epoch: 1164, train loss: 0.01598, val loss: 0.01640\n",
      "Training epoch: 1165, train loss: 0.01658, val loss: 0.01704\n",
      "Training epoch: 1166, train loss: 0.01618, val loss: 0.01655\n",
      "Training epoch: 1167, train loss: 0.01630, val loss: 0.01667\n",
      "Training epoch: 1168, train loss: 0.01646, val loss: 0.01689\n",
      "Training epoch: 1169, train loss: 0.01802, val loss: 0.01838\n",
      "Training epoch: 1170, train loss: 0.01662, val loss: 0.01709\n",
      "Training epoch: 1171, train loss: 0.01592, val loss: 0.01630\n",
      "Training epoch: 1172, train loss: 0.01624, val loss: 0.01669\n",
      "Training epoch: 1173, train loss: 0.01608, val loss: 0.01654\n",
      "Training epoch: 1174, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 1175, train loss: 0.01588, val loss: 0.01627\n",
      "Training epoch: 1176, train loss: 0.01578, val loss: 0.01623\n",
      "Training epoch: 1177, train loss: 0.01582, val loss: 0.01624\n",
      "Training epoch: 1178, train loss: 0.01595, val loss: 0.01644\n",
      "Training epoch: 1179, train loss: 0.01620, val loss: 0.01659\n",
      "Training epoch: 1180, train loss: 0.01573, val loss: 0.01615\n",
      "Training epoch: 1181, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 1182, train loss: 0.01640, val loss: 0.01678\n",
      "Training epoch: 1183, train loss: 0.01581, val loss: 0.01627\n",
      "Training epoch: 1184, train loss: 0.01582, val loss: 0.01625\n",
      "Training epoch: 1185, train loss: 0.01623, val loss: 0.01677\n",
      "Training epoch: 1186, train loss: 0.01590, val loss: 0.01638\n",
      "Training epoch: 1187, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 1188, train loss: 0.01584, val loss: 0.01630\n",
      "Training epoch: 1189, train loss: 0.01582, val loss: 0.01626\n",
      "Training epoch: 1190, train loss: 0.01590, val loss: 0.01632\n",
      "Training epoch: 1191, train loss: 0.01600, val loss: 0.01642\n",
      "Training epoch: 1192, train loss: 0.01577, val loss: 0.01621\n",
      "Training epoch: 1193, train loss: 0.01615, val loss: 0.01657\n",
      "Training epoch: 1194, train loss: 0.01677, val loss: 0.01714\n",
      "Training epoch: 1195, train loss: 0.01590, val loss: 0.01627\n",
      "Training epoch: 1196, train loss: 0.01581, val loss: 0.01625\n",
      "Training epoch: 1197, train loss: 0.01629, val loss: 0.01679\n",
      "Training epoch: 1198, train loss: 0.01595, val loss: 0.01638\n",
      "Training epoch: 1199, train loss: 0.01661, val loss: 0.01698\n",
      "Training epoch: 1200, train loss: 0.01583, val loss: 0.01629\n",
      "Training epoch: 1201, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1202, train loss: 0.01587, val loss: 0.01626\n",
      "Training epoch: 1203, train loss: 0.01594, val loss: 0.01640\n",
      "Training epoch: 1204, train loss: 0.01628, val loss: 0.01664\n",
      "Training epoch: 1205, train loss: 0.01632, val loss: 0.01668\n",
      "Training epoch: 1206, train loss: 0.01577, val loss: 0.01620\n",
      "Training epoch: 1207, train loss: 0.01602, val loss: 0.01646\n",
      "Training epoch: 1208, train loss: 0.01676, val loss: 0.01727\n",
      "Training epoch: 1209, train loss: 0.01618, val loss: 0.01664\n",
      "Training epoch: 1210, train loss: 0.01600, val loss: 0.01647\n",
      "Training epoch: 1211, train loss: 0.01614, val loss: 0.01655\n",
      "Training epoch: 1212, train loss: 0.01585, val loss: 0.01629\n",
      "Training epoch: 1213, train loss: 0.01590, val loss: 0.01631\n",
      "Training epoch: 1214, train loss: 0.01584, val loss: 0.01628\n",
      "Training epoch: 1215, train loss: 0.01601, val loss: 0.01644\n",
      "Training epoch: 1216, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 1217, train loss: 0.01605, val loss: 0.01650\n",
      "Training epoch: 1218, train loss: 0.01598, val loss: 0.01642\n",
      "Training epoch: 1219, train loss: 0.01617, val loss: 0.01653\n",
      "Training epoch: 1220, train loss: 0.01586, val loss: 0.01626\n",
      "Training epoch: 1221, train loss: 0.01610, val loss: 0.01654\n",
      "Training epoch: 1222, train loss: 0.01582, val loss: 0.01625\n",
      "Training epoch: 1223, train loss: 0.01627, val loss: 0.01664\n",
      "Training epoch: 1224, train loss: 0.01605, val loss: 0.01650\n",
      "Training epoch: 1225, train loss: 0.01603, val loss: 0.01649\n",
      "Training epoch: 1226, train loss: 0.01625, val loss: 0.01671\n",
      "Training epoch: 1227, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1228, train loss: 0.01595, val loss: 0.01642\n",
      "Training epoch: 1229, train loss: 0.01645, val loss: 0.01696\n",
      "Training epoch: 1230, train loss: 0.01574, val loss: 0.01617\n",
      "Training epoch: 1231, train loss: 0.01608, val loss: 0.01658\n",
      "Training epoch: 1232, train loss: 0.01646, val loss: 0.01697\n",
      "Training epoch: 1233, train loss: 0.01608, val loss: 0.01644\n",
      "Training epoch: 1234, train loss: 0.01598, val loss: 0.01651\n",
      "Training epoch: 1235, train loss: 0.01613, val loss: 0.01649\n",
      "Training epoch: 1236, train loss: 0.01617, val loss: 0.01657\n",
      "Training epoch: 1237, train loss: 0.01608, val loss: 0.01651\n",
      "Training epoch: 1238, train loss: 0.01631, val loss: 0.01681\n",
      "Training epoch: 1239, train loss: 0.01585, val loss: 0.01632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1240, train loss: 0.01601, val loss: 0.01646\n",
      "Training epoch: 1241, train loss: 0.01674, val loss: 0.01707\n",
      "Training epoch: 1242, train loss: 0.01631, val loss: 0.01670\n",
      "Training epoch: 1243, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 1244, train loss: 0.01608, val loss: 0.01655\n",
      "Training epoch: 1245, train loss: 0.01628, val loss: 0.01680\n",
      "Training epoch: 1246, train loss: 0.01618, val loss: 0.01665\n",
      "Training epoch: 1247, train loss: 0.01646, val loss: 0.01681\n",
      "Training epoch: 1248, train loss: 0.01601, val loss: 0.01647\n",
      "Training epoch: 1249, train loss: 0.01603, val loss: 0.01649\n",
      "Training epoch: 1250, train loss: 0.01595, val loss: 0.01635\n",
      "Training epoch: 1251, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 1252, train loss: 0.01619, val loss: 0.01671\n",
      "Training epoch: 1253, train loss: 0.01579, val loss: 0.01623\n",
      "Training epoch: 1254, train loss: 0.01622, val loss: 0.01664\n",
      "Training epoch: 1255, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 1256, train loss: 0.01592, val loss: 0.01637\n",
      "Training epoch: 1257, train loss: 0.01574, val loss: 0.01615\n",
      "Training epoch: 1258, train loss: 0.01581, val loss: 0.01627\n",
      "Training epoch: 1259, train loss: 0.01622, val loss: 0.01658\n",
      "Training epoch: 1260, train loss: 0.01582, val loss: 0.01623\n",
      "Training epoch: 1261, train loss: 0.01600, val loss: 0.01639\n",
      "Training epoch: 1262, train loss: 0.01589, val loss: 0.01635\n",
      "Training epoch: 1263, train loss: 0.01619, val loss: 0.01662\n",
      "Training epoch: 1264, train loss: 0.01635, val loss: 0.01682\n",
      "Training epoch: 1265, train loss: 0.01637, val loss: 0.01688\n",
      "Training epoch: 1266, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 1267, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 1268, train loss: 0.01595, val loss: 0.01645\n",
      "Training epoch: 1269, train loss: 0.01585, val loss: 0.01631\n",
      "Training epoch: 1270, train loss: 0.01621, val loss: 0.01667\n",
      "Training epoch: 1271, train loss: 0.01643, val loss: 0.01693\n",
      "Training epoch: 1272, train loss: 0.01583, val loss: 0.01623\n",
      "Training epoch: 1273, train loss: 0.01573, val loss: 0.01615\n",
      "Training epoch: 1274, train loss: 0.01586, val loss: 0.01631\n",
      "Training epoch: 1275, train loss: 0.01601, val loss: 0.01641\n",
      "Training epoch: 1276, train loss: 0.01592, val loss: 0.01632\n",
      "Training epoch: 1277, train loss: 0.01604, val loss: 0.01652\n",
      "Training epoch: 1278, train loss: 0.01587, val loss: 0.01632\n",
      "Training epoch: 1279, train loss: 0.01596, val loss: 0.01642\n",
      "Training epoch: 1280, train loss: 0.01634, val loss: 0.01670\n",
      "Training epoch: 1281, train loss: 0.01605, val loss: 0.01643\n",
      "Training epoch: 1282, train loss: 0.01623, val loss: 0.01671\n",
      "Training epoch: 1283, train loss: 0.01581, val loss: 0.01624\n",
      "Training epoch: 1284, train loss: 0.01637, val loss: 0.01685\n",
      "Training epoch: 1285, train loss: 0.01597, val loss: 0.01645\n",
      "Training epoch: 1286, train loss: 0.01604, val loss: 0.01651\n",
      "Training epoch: 1287, train loss: 0.01582, val loss: 0.01627\n",
      "Training epoch: 1288, train loss: 0.01579, val loss: 0.01621\n",
      "Training epoch: 1289, train loss: 0.01576, val loss: 0.01619\n",
      "Training epoch: 1290, train loss: 0.01636, val loss: 0.01685\n",
      "Training epoch: 1291, train loss: 0.01605, val loss: 0.01640\n",
      "Training epoch: 1292, train loss: 0.01599, val loss: 0.01643\n",
      "Training epoch: 1293, train loss: 0.01606, val loss: 0.01648\n",
      "Training epoch: 1294, train loss: 0.01599, val loss: 0.01646\n",
      "Training epoch: 1295, train loss: 0.01586, val loss: 0.01625\n",
      "Training epoch: 1296, train loss: 0.01593, val loss: 0.01642\n",
      "Training epoch: 1297, train loss: 0.01644, val loss: 0.01688\n",
      "Training epoch: 1298, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1299, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1300, train loss: 0.01597, val loss: 0.01643\n",
      "Training epoch: 1301, train loss: 0.01627, val loss: 0.01677\n",
      "Training epoch: 1302, train loss: 0.01574, val loss: 0.01616\n",
      "Training epoch: 1303, train loss: 0.01596, val loss: 0.01637\n",
      "Training epoch: 1304, train loss: 0.01609, val loss: 0.01658\n",
      "Training epoch: 1305, train loss: 0.01599, val loss: 0.01647\n",
      "Training epoch: 1306, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1307, train loss: 0.01602, val loss: 0.01647\n",
      "Training epoch: 1308, train loss: 0.01595, val loss: 0.01638\n",
      "Training epoch: 1309, train loss: 0.01579, val loss: 0.01619\n",
      "Training epoch: 1310, train loss: 0.01598, val loss: 0.01644\n",
      "Training epoch: 1311, train loss: 0.01591, val loss: 0.01637\n",
      "Training epoch: 1312, train loss: 0.01603, val loss: 0.01646\n",
      "Training epoch: 1313, train loss: 0.01655, val loss: 0.01706\n",
      "Training epoch: 1314, train loss: 0.01618, val loss: 0.01667\n",
      "Training epoch: 1315, train loss: 0.01578, val loss: 0.01620\n",
      "Training epoch: 1316, train loss: 0.01569, val loss: 0.01612\n",
      "Training epoch: 1317, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 1318, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 1319, train loss: 0.01598, val loss: 0.01644\n",
      "Training epoch: 1320, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 1321, train loss: 0.01604, val loss: 0.01652\n",
      "Training epoch: 1322, train loss: 0.01595, val loss: 0.01640\n",
      "Training epoch: 1323, train loss: 0.01619, val loss: 0.01656\n",
      "Training epoch: 1324, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1325, train loss: 0.01621, val loss: 0.01671\n",
      "Training epoch: 1326, train loss: 0.01590, val loss: 0.01637\n",
      "Training epoch: 1327, train loss: 0.01622, val loss: 0.01669\n",
      "Training epoch: 1328, train loss: 0.01617, val loss: 0.01664\n",
      "Training epoch: 1329, train loss: 0.01621, val loss: 0.01657\n",
      "Training epoch: 1330, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 1331, train loss: 0.01604, val loss: 0.01641\n",
      "Training epoch: 1332, train loss: 0.01627, val loss: 0.01673\n",
      "Training epoch: 1333, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1334, train loss: 0.01596, val loss: 0.01641\n",
      "Training epoch: 1335, train loss: 0.01606, val loss: 0.01645\n",
      "Training epoch: 1336, train loss: 0.01598, val loss: 0.01634\n",
      "Training epoch: 1337, train loss: 0.01572, val loss: 0.01615\n",
      "Training epoch: 1338, train loss: 0.01593, val loss: 0.01636\n",
      "Training epoch: 1339, train loss: 0.01610, val loss: 0.01655\n",
      "Training epoch: 1340, train loss: 0.01592, val loss: 0.01635\n",
      "Training epoch: 1341, train loss: 0.01585, val loss: 0.01622\n",
      "Training epoch: 1342, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 1343, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 1344, train loss: 0.01588, val loss: 0.01632\n",
      "Training epoch: 1345, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 1346, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 1347, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1348, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 1349, train loss: 0.01588, val loss: 0.01625\n",
      "Training epoch: 1350, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 1351, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1352, train loss: 0.01597, val loss: 0.01643\n",
      "Training epoch: 1353, train loss: 0.01699, val loss: 0.01751\n",
      "Training epoch: 1354, train loss: 0.01736, val loss: 0.01790\n",
      "Training epoch: 1355, train loss: 0.01626, val loss: 0.01674\n",
      "Training epoch: 1356, train loss: 0.01633, val loss: 0.01680\n",
      "Training epoch: 1357, train loss: 0.01629, val loss: 0.01673\n",
      "Training epoch: 1358, train loss: 0.01622, val loss: 0.01669\n",
      "Training epoch: 1359, train loss: 0.01612, val loss: 0.01653\n",
      "Training epoch: 1360, train loss: 0.01585, val loss: 0.01630\n",
      "Training epoch: 1361, train loss: 0.01611, val loss: 0.01655\n",
      "Training epoch: 1362, train loss: 0.01607, val loss: 0.01649\n",
      "Training epoch: 1363, train loss: 0.01595, val loss: 0.01636\n",
      "Training epoch: 1364, train loss: 0.01660, val loss: 0.01712\n",
      "Training epoch: 1365, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 1366, train loss: 0.01596, val loss: 0.01642\n",
      "Training epoch: 1367, train loss: 0.01608, val loss: 0.01655\n",
      "Training epoch: 1368, train loss: 0.01602, val loss: 0.01644\n",
      "Training epoch: 1369, train loss: 0.01592, val loss: 0.01630\n",
      "Training epoch: 1370, train loss: 0.01584, val loss: 0.01624\n",
      "Training epoch: 1371, train loss: 0.01612, val loss: 0.01654\n",
      "Training epoch: 1372, train loss: 0.01589, val loss: 0.01628\n",
      "Training epoch: 1373, train loss: 0.01617, val loss: 0.01658\n",
      "Training epoch: 1374, train loss: 0.01623, val loss: 0.01671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1375, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 1376, train loss: 0.01586, val loss: 0.01628\n",
      "Training epoch: 1377, train loss: 0.01571, val loss: 0.01611\n",
      "Training epoch: 1378, train loss: 0.01629, val loss: 0.01680\n",
      "Training epoch: 1379, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 1380, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 1381, train loss: 0.01576, val loss: 0.01614\n",
      "Training epoch: 1382, train loss: 0.01593, val loss: 0.01638\n",
      "Training epoch: 1383, train loss: 0.01647, val loss: 0.01698\n",
      "Training epoch: 1384, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 1385, train loss: 0.01648, val loss: 0.01698\n",
      "Training epoch: 1386, train loss: 0.01621, val loss: 0.01671\n",
      "Training epoch: 1387, train loss: 0.01597, val loss: 0.01639\n",
      "Training epoch: 1388, train loss: 0.01610, val loss: 0.01648\n",
      "Training epoch: 1389, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 1390, train loss: 0.01632, val loss: 0.01681\n",
      "Training epoch: 1391, train loss: 0.01596, val loss: 0.01644\n",
      "Training epoch: 1392, train loss: 0.01615, val loss: 0.01663\n",
      "Training epoch: 1393, train loss: 0.01632, val loss: 0.01681\n",
      "Training epoch: 1394, train loss: 0.01602, val loss: 0.01646\n",
      "Training epoch: 1395, train loss: 0.01601, val loss: 0.01648\n",
      "Training epoch: 1396, train loss: 0.01619, val loss: 0.01667\n",
      "Training epoch: 1397, train loss: 0.01617, val loss: 0.01666\n",
      "Training epoch: 1398, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1399, train loss: 0.01633, val loss: 0.01684\n",
      "Training epoch: 1400, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 1401, train loss: 0.01593, val loss: 0.01636\n",
      "Training epoch: 1402, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 1403, train loss: 0.01580, val loss: 0.01623\n",
      "Training epoch: 1404, train loss: 0.01583, val loss: 0.01628\n",
      "Training epoch: 1405, train loss: 0.01589, val loss: 0.01628\n",
      "Training epoch: 1406, train loss: 0.01600, val loss: 0.01643\n",
      "Training epoch: 1407, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 1408, train loss: 0.01660, val loss: 0.01707\n",
      "Training epoch: 1409, train loss: 0.01630, val loss: 0.01665\n",
      "Training epoch: 1410, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 1411, train loss: 0.01607, val loss: 0.01645\n",
      "Training epoch: 1412, train loss: 0.01574, val loss: 0.01618\n",
      "Training epoch: 1413, train loss: 0.01625, val loss: 0.01675\n",
      "Training epoch: 1414, train loss: 0.01604, val loss: 0.01651\n",
      "Training epoch: 1415, train loss: 0.01614, val loss: 0.01660\n",
      "Training epoch: 1416, train loss: 0.01616, val loss: 0.01652\n",
      "Training epoch: 1417, train loss: 0.01616, val loss: 0.01660\n",
      "Training epoch: 1418, train loss: 0.01668, val loss: 0.01713\n",
      "Training epoch: 1419, train loss: 0.01585, val loss: 0.01627\n",
      "Training epoch: 1420, train loss: 0.01608, val loss: 0.01656\n",
      "Training epoch: 1421, train loss: 0.01595, val loss: 0.01633\n",
      "Training epoch: 1422, train loss: 0.01581, val loss: 0.01625\n",
      "Training epoch: 1423, train loss: 0.01582, val loss: 0.01630\n",
      "Training epoch: 1424, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 1425, train loss: 0.01602, val loss: 0.01646\n",
      "Training epoch: 1426, train loss: 0.01616, val loss: 0.01657\n",
      "Training epoch: 1427, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 1428, train loss: 0.01666, val loss: 0.01706\n",
      "Training epoch: 1429, train loss: 0.01645, val loss: 0.01688\n",
      "Training epoch: 1430, train loss: 0.01643, val loss: 0.01692\n",
      "Training epoch: 1431, train loss: 0.01632, val loss: 0.01675\n",
      "Training epoch: 1432, train loss: 0.01581, val loss: 0.01626\n",
      "Training epoch: 1433, train loss: 0.01611, val loss: 0.01654\n",
      "Training epoch: 1434, train loss: 0.01575, val loss: 0.01616\n",
      "Training epoch: 1435, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 1436, train loss: 0.01651, val loss: 0.01701\n",
      "Training epoch: 1437, train loss: 0.01622, val loss: 0.01671\n",
      "Training epoch: 1438, train loss: 0.01606, val loss: 0.01651\n",
      "Training epoch: 1439, train loss: 0.01599, val loss: 0.01642\n",
      "Training epoch: 1440, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1441, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1442, train loss: 0.01581, val loss: 0.01621\n",
      "Training epoch: 1443, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 1444, train loss: 0.01633, val loss: 0.01680\n",
      "Training epoch: 1445, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 1446, train loss: 0.01632, val loss: 0.01669\n",
      "Training epoch: 1447, train loss: 0.01615, val loss: 0.01655\n",
      "Training epoch: 1448, train loss: 0.01644, val loss: 0.01684\n",
      "Training epoch: 1449, train loss: 0.01601, val loss: 0.01647\n",
      "Training epoch: 1450, train loss: 0.01747, val loss: 0.01801\n",
      "Training epoch: 1451, train loss: 0.01675, val loss: 0.01725\n",
      "Training epoch: 1452, train loss: 0.01604, val loss: 0.01645\n",
      "Training epoch: 1453, train loss: 0.01590, val loss: 0.01630\n",
      "Training epoch: 1454, train loss: 0.01624, val loss: 0.01665\n",
      "Training epoch: 1455, train loss: 0.01610, val loss: 0.01644\n",
      "Training epoch: 1456, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 1457, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 1458, train loss: 0.01594, val loss: 0.01637\n",
      "Training epoch: 1459, train loss: 0.01670, val loss: 0.01714\n",
      "Training epoch: 1460, train loss: 0.01646, val loss: 0.01694\n",
      "Training epoch: 1461, train loss: 0.01613, val loss: 0.01657\n",
      "Training epoch: 1462, train loss: 0.01603, val loss: 0.01646\n",
      "Training epoch: 1463, train loss: 0.01608, val loss: 0.01655\n",
      "Training epoch: 1464, train loss: 0.01654, val loss: 0.01689\n",
      "Training epoch: 1465, train loss: 0.01636, val loss: 0.01685\n",
      "Training epoch: 1466, train loss: 0.01682, val loss: 0.01734\n",
      "Training epoch: 1467, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1468, train loss: 0.01587, val loss: 0.01630\n",
      "Training epoch: 1469, train loss: 0.01607, val loss: 0.01641\n",
      "Training epoch: 1470, train loss: 0.01590, val loss: 0.01632\n",
      "Training epoch: 1471, train loss: 0.01581, val loss: 0.01623\n",
      "Training epoch: 1472, train loss: 0.01615, val loss: 0.01664\n",
      "Training epoch: 1473, train loss: 0.01594, val loss: 0.01641\n",
      "Training epoch: 1474, train loss: 0.01678, val loss: 0.01727\n",
      "Training epoch: 1475, train loss: 0.01671, val loss: 0.01723\n",
      "Training epoch: 1476, train loss: 0.01585, val loss: 0.01622\n",
      "Training epoch: 1477, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 1478, train loss: 0.01578, val loss: 0.01616\n",
      "Training epoch: 1479, train loss: 0.01609, val loss: 0.01648\n",
      "Training epoch: 1480, train loss: 0.01610, val loss: 0.01655\n",
      "Training epoch: 1481, train loss: 0.01644, val loss: 0.01694\n",
      "Training epoch: 1482, train loss: 0.01590, val loss: 0.01635\n",
      "Training epoch: 1483, train loss: 0.01582, val loss: 0.01622\n",
      "Training epoch: 1484, train loss: 0.01605, val loss: 0.01645\n",
      "Training epoch: 1485, train loss: 0.01585, val loss: 0.01624\n",
      "Training epoch: 1486, train loss: 0.01681, val loss: 0.01733\n",
      "Training epoch: 1487, train loss: 0.01607, val loss: 0.01654\n",
      "Training epoch: 1488, train loss: 0.01618, val loss: 0.01665\n",
      "Training epoch: 1489, train loss: 0.01671, val loss: 0.01725\n",
      "Training epoch: 1490, train loss: 0.01589, val loss: 0.01628\n",
      "Training epoch: 1491, train loss: 0.01627, val loss: 0.01669\n",
      "Training epoch: 1492, train loss: 0.01577, val loss: 0.01618\n",
      "Training epoch: 1493, train loss: 0.01606, val loss: 0.01650\n",
      "Training epoch: 1494, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 1495, train loss: 0.01625, val loss: 0.01668\n",
      "Training epoch: 1496, train loss: 0.01615, val loss: 0.01662\n",
      "Training epoch: 1497, train loss: 0.01619, val loss: 0.01663\n",
      "Training epoch: 1498, train loss: 0.01624, val loss: 0.01661\n",
      "Training epoch: 1499, train loss: 0.01616, val loss: 0.01663\n",
      "Training epoch: 1500, train loss: 0.01630, val loss: 0.01680\n",
      "Training epoch: 1501, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 1502, train loss: 0.01613, val loss: 0.01658\n",
      "Training epoch: 1503, train loss: 0.01590, val loss: 0.01634\n",
      "Training epoch: 1504, train loss: 0.01599, val loss: 0.01638\n",
      "Training epoch: 1505, train loss: 0.01610, val loss: 0.01654\n",
      "Training epoch: 1506, train loss: 0.01599, val loss: 0.01645\n",
      "Training epoch: 1507, train loss: 0.01612, val loss: 0.01656\n",
      "Training epoch: 1508, train loss: 0.01587, val loss: 0.01628\n",
      "Training epoch: 1509, train loss: 0.01586, val loss: 0.01629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1510, train loss: 0.01587, val loss: 0.01627\n",
      "Training epoch: 1511, train loss: 0.01590, val loss: 0.01634\n",
      "Training epoch: 1512, train loss: 0.01626, val loss: 0.01668\n",
      "Training epoch: 1513, train loss: 0.01588, val loss: 0.01625\n",
      "Training epoch: 1514, train loss: 0.01604, val loss: 0.01649\n",
      "Training epoch: 1515, train loss: 0.01640, val loss: 0.01690\n",
      "Training epoch: 1516, train loss: 0.01594, val loss: 0.01637\n",
      "Training epoch: 1517, train loss: 0.01587, val loss: 0.01625\n",
      "Training epoch: 1518, train loss: 0.01635, val loss: 0.01681\n",
      "Training epoch: 1519, train loss: 0.01602, val loss: 0.01647\n",
      "Training epoch: 1520, train loss: 0.01601, val loss: 0.01639\n",
      "Training epoch: 1521, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 1522, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 1523, train loss: 0.01626, val loss: 0.01659\n",
      "Training epoch: 1524, train loss: 0.01580, val loss: 0.01620\n",
      "Training epoch: 1525, train loss: 0.01632, val loss: 0.01673\n",
      "Training epoch: 1526, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 1527, train loss: 0.01658, val loss: 0.01698\n",
      "Training epoch: 1528, train loss: 0.01598, val loss: 0.01641\n",
      "Training epoch: 1529, train loss: 0.01630, val loss: 0.01675\n",
      "Training epoch: 1530, train loss: 0.01588, val loss: 0.01630\n",
      "Training epoch: 1531, train loss: 0.01599, val loss: 0.01640\n",
      "Training epoch: 1532, train loss: 0.01621, val loss: 0.01663\n",
      "Training epoch: 1533, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 1534, train loss: 0.01643, val loss: 0.01691\n",
      "Training epoch: 1535, train loss: 0.01611, val loss: 0.01650\n",
      "Training epoch: 1536, train loss: 0.01601, val loss: 0.01648\n",
      "Training epoch: 1537, train loss: 0.01587, val loss: 0.01624\n",
      "Training epoch: 1538, train loss: 0.01617, val loss: 0.01662\n",
      "Training epoch: 1539, train loss: 0.01623, val loss: 0.01657\n",
      "Training epoch: 1540, train loss: 0.01604, val loss: 0.01649\n",
      "Training epoch: 1541, train loss: 0.01655, val loss: 0.01706\n",
      "Training epoch: 1542, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 1543, train loss: 0.01586, val loss: 0.01623\n",
      "Training epoch: 1544, train loss: 0.01620, val loss: 0.01662\n",
      "Training epoch: 1545, train loss: 0.01622, val loss: 0.01673\n",
      "Training epoch: 1546, train loss: 0.01807, val loss: 0.01862\n",
      "Training epoch: 1547, train loss: 0.01599, val loss: 0.01641\n",
      "Training epoch: 1548, train loss: 0.01592, val loss: 0.01635\n",
      "Training epoch: 1549, train loss: 0.01609, val loss: 0.01658\n",
      "Training epoch: 1550, train loss: 0.01608, val loss: 0.01653\n",
      "Training epoch: 1551, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 1552, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 1553, train loss: 0.01614, val loss: 0.01650\n",
      "Training epoch: 1554, train loss: 0.01602, val loss: 0.01643\n",
      "Training epoch: 1555, train loss: 0.01601, val loss: 0.01637\n",
      "Training epoch: 1556, train loss: 0.01618, val loss: 0.01659\n",
      "Training epoch: 1557, train loss: 0.01606, val loss: 0.01639\n",
      "Training epoch: 1558, train loss: 0.01637, val loss: 0.01671\n",
      "Training epoch: 1559, train loss: 0.01591, val loss: 0.01631\n",
      "Training epoch: 1560, train loss: 0.01584, val loss: 0.01630\n",
      "Training epoch: 1561, train loss: 0.01638, val loss: 0.01688\n",
      "Training epoch: 1562, train loss: 0.01614, val loss: 0.01657\n",
      "Training epoch: 1563, train loss: 0.01717, val loss: 0.01773\n",
      "Training epoch: 1564, train loss: 0.01628, val loss: 0.01675\n",
      "Training epoch: 1565, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1566, train loss: 0.01582, val loss: 0.01623\n",
      "Training epoch: 1567, train loss: 0.01587, val loss: 0.01625\n",
      "Training epoch: 1568, train loss: 0.01619, val loss: 0.01657\n",
      "Training epoch: 1569, train loss: 0.01611, val loss: 0.01654\n",
      "Training epoch: 1570, train loss: 0.01604, val loss: 0.01643\n",
      "Training epoch: 1571, train loss: 0.01608, val loss: 0.01649\n",
      "Training epoch: 1572, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 1573, train loss: 0.01585, val loss: 0.01628\n",
      "Training epoch: 1574, train loss: 0.01578, val loss: 0.01620\n",
      "Training epoch: 1575, train loss: 0.01611, val loss: 0.01656\n",
      "Training epoch: 1576, train loss: 0.01671, val loss: 0.01723\n",
      "Training epoch: 1577, train loss: 0.01600, val loss: 0.01644\n",
      "Training epoch: 1578, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 1579, train loss: 0.01598, val loss: 0.01643\n",
      "Training epoch: 1580, train loss: 0.01612, val loss: 0.01656\n",
      "Training epoch: 1581, train loss: 0.01611, val loss: 0.01652\n",
      "Training epoch: 1582, train loss: 0.01594, val loss: 0.01634\n",
      "Training epoch: 1583, train loss: 0.01590, val loss: 0.01632\n",
      "Training epoch: 1584, train loss: 0.01635, val loss: 0.01678\n",
      "Training epoch: 1585, train loss: 0.01626, val loss: 0.01674\n",
      "Training epoch: 1586, train loss: 0.01607, val loss: 0.01648\n",
      "Training epoch: 1587, train loss: 0.01605, val loss: 0.01650\n",
      "Training epoch: 1588, train loss: 0.01585, val loss: 0.01629\n",
      "Training epoch: 1589, train loss: 0.01584, val loss: 0.01625\n",
      "Training epoch: 1590, train loss: 0.01598, val loss: 0.01641\n",
      "Training epoch: 1591, train loss: 0.01641, val loss: 0.01680\n",
      "Training epoch: 1592, train loss: 0.01587, val loss: 0.01632\n",
      "Training epoch: 1593, train loss: 0.01597, val loss: 0.01635\n",
      "Training epoch: 1594, train loss: 0.01638, val loss: 0.01673\n",
      "Training epoch: 1595, train loss: 0.01706, val loss: 0.01757\n",
      "Training epoch: 1596, train loss: 0.01703, val loss: 0.01754\n",
      "Training epoch: 1597, train loss: 0.01607, val loss: 0.01647\n",
      "Training epoch: 1598, train loss: 0.01601, val loss: 0.01644\n",
      "Training epoch: 1599, train loss: 0.01602, val loss: 0.01637\n",
      "Training epoch: 1600, train loss: 0.01619, val loss: 0.01664\n",
      "Training epoch: 1601, train loss: 0.01588, val loss: 0.01631\n",
      "Training epoch: 1602, train loss: 0.01602, val loss: 0.01645\n",
      "Training epoch: 1603, train loss: 0.01584, val loss: 0.01626\n",
      "Training epoch: 1604, train loss: 0.01646, val loss: 0.01695\n",
      "Training epoch: 1605, train loss: 0.01630, val loss: 0.01678\n",
      "Training epoch: 1606, train loss: 0.01580, val loss: 0.01614\n",
      "Training epoch: 1607, train loss: 0.01586, val loss: 0.01624\n",
      "Training epoch: 1608, train loss: 0.01626, val loss: 0.01668\n",
      "Training epoch: 1609, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1610, train loss: 0.01615, val loss: 0.01661\n",
      "Training epoch: 1611, train loss: 0.01586, val loss: 0.01623\n",
      "Training epoch: 1612, train loss: 0.01580, val loss: 0.01616\n",
      "Training epoch: 1613, train loss: 0.01613, val loss: 0.01658\n",
      "Training epoch: 1614, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 1615, train loss: 0.01599, val loss: 0.01642\n",
      "Training epoch: 1616, train loss: 0.01639, val loss: 0.01685\n",
      "Training epoch: 1617, train loss: 0.01624, val loss: 0.01667\n",
      "Training epoch: 1618, train loss: 0.01626, val loss: 0.01666\n",
      "Training epoch: 1619, train loss: 0.01618, val loss: 0.01655\n",
      "Training epoch: 1620, train loss: 0.01615, val loss: 0.01659\n",
      "Training epoch: 1621, train loss: 0.01601, val loss: 0.01641\n",
      "Training epoch: 1622, train loss: 0.01621, val loss: 0.01658\n",
      "Training epoch: 1623, train loss: 0.01644, val loss: 0.01695\n",
      "Training epoch: 1624, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 1625, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1626, train loss: 0.01623, val loss: 0.01666\n",
      "Training epoch: 1627, train loss: 0.01603, val loss: 0.01645\n",
      "Training epoch: 1628, train loss: 0.01585, val loss: 0.01627\n",
      "Training epoch: 1629, train loss: 0.01589, val loss: 0.01630\n",
      "Training epoch: 1630, train loss: 0.01631, val loss: 0.01679\n",
      "Training epoch: 1631, train loss: 0.01601, val loss: 0.01644\n",
      "Training epoch: 1632, train loss: 0.01645, val loss: 0.01694\n",
      "Training epoch: 1633, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 1634, train loss: 0.01668, val loss: 0.01717\n",
      "Training epoch: 1635, train loss: 0.01619, val loss: 0.01668\n",
      "Training epoch: 1636, train loss: 0.01610, val loss: 0.01656\n",
      "Training epoch: 1637, train loss: 0.01633, val loss: 0.01681\n",
      "Training epoch: 1638, train loss: 0.01605, val loss: 0.01648\n",
      "Training epoch: 1639, train loss: 0.01592, val loss: 0.01632\n",
      "Training epoch: 1640, train loss: 0.01588, val loss: 0.01631\n",
      "Training epoch: 1641, train loss: 0.01591, val loss: 0.01632\n",
      "Training epoch: 1642, train loss: 0.01593, val loss: 0.01625\n",
      "Training epoch: 1643, train loss: 0.01608, val loss: 0.01649\n",
      "Training epoch: 1644, train loss: 0.01607, val loss: 0.01652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1645, train loss: 0.01710, val loss: 0.01761\n",
      "Training epoch: 1646, train loss: 0.01658, val loss: 0.01708\n",
      "Training epoch: 1647, train loss: 0.01643, val loss: 0.01688\n",
      "Training epoch: 1648, train loss: 0.01595, val loss: 0.01631\n",
      "Training epoch: 1649, train loss: 0.01582, val loss: 0.01623\n",
      "Training epoch: 1650, train loss: 0.01649, val loss: 0.01691\n",
      "Training epoch: 1651, train loss: 0.01633, val loss: 0.01679\n",
      "Training epoch: 1652, train loss: 0.01603, val loss: 0.01644\n",
      "Training epoch: 1653, train loss: 0.01690, val loss: 0.01720\n",
      "Training epoch: 1654, train loss: 0.01621, val loss: 0.01660\n",
      "Training epoch: 1655, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 1656, train loss: 0.01611, val loss: 0.01657\n",
      "Training epoch: 1657, train loss: 0.01607, val loss: 0.01653\n",
      "Training epoch: 1658, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 1659, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1660, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1661, train loss: 0.01605, val loss: 0.01650\n",
      "Training epoch: 1662, train loss: 0.01591, val loss: 0.01630\n",
      "Training epoch: 1663, train loss: 0.01616, val loss: 0.01660\n",
      "Training epoch: 1664, train loss: 0.01602, val loss: 0.01648\n",
      "Training epoch: 1665, train loss: 0.01616, val loss: 0.01662\n",
      "Training epoch: 1666, train loss: 0.01636, val loss: 0.01682\n",
      "Training epoch: 1667, train loss: 0.01576, val loss: 0.01615\n",
      "Training epoch: 1668, train loss: 0.01632, val loss: 0.01669\n",
      "Training epoch: 1669, train loss: 0.01597, val loss: 0.01637\n",
      "Training epoch: 1670, train loss: 0.01611, val loss: 0.01649\n",
      "Training epoch: 1671, train loss: 0.01605, val loss: 0.01642\n",
      "Training epoch: 1672, train loss: 0.01628, val loss: 0.01675\n",
      "Training epoch: 1673, train loss: 0.01604, val loss: 0.01648\n",
      "Training epoch: 1674, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 1675, train loss: 0.01665, val loss: 0.01713\n",
      "Training epoch: 1676, train loss: 0.01591, val loss: 0.01633\n",
      "Training epoch: 1677, train loss: 0.01601, val loss: 0.01643\n",
      "Training epoch: 1678, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 1679, train loss: 0.01617, val loss: 0.01664\n",
      "Training epoch: 1680, train loss: 0.01624, val loss: 0.01670\n",
      "Training epoch: 1681, train loss: 0.01650, val loss: 0.01683\n",
      "Training epoch: 1682, train loss: 0.01680, val loss: 0.01711\n",
      "Training epoch: 1683, train loss: 0.01618, val loss: 0.01657\n",
      "Training epoch: 1684, train loss: 0.01587, val loss: 0.01627\n",
      "Training epoch: 1685, train loss: 0.01596, val loss: 0.01641\n",
      "Training epoch: 1686, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 1687, train loss: 0.01579, val loss: 0.01617\n",
      "Training epoch: 1688, train loss: 0.01596, val loss: 0.01640\n",
      "Training epoch: 1689, train loss: 0.01588, val loss: 0.01627\n",
      "Training epoch: 1690, train loss: 0.01648, val loss: 0.01693\n",
      "Training epoch: 1691, train loss: 0.01609, val loss: 0.01646\n",
      "Training epoch: 1692, train loss: 0.01603, val loss: 0.01637\n",
      "Training epoch: 1693, train loss: 0.01611, val loss: 0.01654\n",
      "Training epoch: 1694, train loss: 0.01588, val loss: 0.01627\n",
      "Training epoch: 1695, train loss: 0.01613, val loss: 0.01659\n",
      "Training epoch: 1696, train loss: 0.01636, val loss: 0.01684\n",
      "Training epoch: 1697, train loss: 0.01598, val loss: 0.01641\n",
      "Training epoch: 1698, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 1699, train loss: 0.01648, val loss: 0.01699\n",
      "Training epoch: 1700, train loss: 0.01622, val loss: 0.01664\n",
      "Training epoch: 1701, train loss: 0.01618, val loss: 0.01658\n",
      "Training epoch: 1702, train loss: 0.01604, val loss: 0.01642\n",
      "Training epoch: 1703, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1704, train loss: 0.01646, val loss: 0.01696\n",
      "Training epoch: 1705, train loss: 0.01644, val loss: 0.01689\n",
      "Training epoch: 1706, train loss: 0.01601, val loss: 0.01641\n",
      "Training epoch: 1707, train loss: 0.01657, val loss: 0.01698\n",
      "Training epoch: 1708, train loss: 0.01663, val loss: 0.01710\n",
      "Training epoch: 1709, train loss: 0.01606, val loss: 0.01649\n",
      "Training epoch: 1710, train loss: 0.01682, val loss: 0.01725\n",
      "Training epoch: 1711, train loss: 0.01663, val loss: 0.01709\n",
      "Training epoch: 1712, train loss: 0.01583, val loss: 0.01622\n",
      "Training epoch: 1713, train loss: 0.01583, val loss: 0.01620\n",
      "Training epoch: 1714, train loss: 0.01596, val loss: 0.01638\n",
      "Training epoch: 1715, train loss: 0.01615, val loss: 0.01652\n",
      "Training epoch: 1716, train loss: 0.01603, val loss: 0.01643\n",
      "Training epoch: 1717, train loss: 0.01585, val loss: 0.01623\n",
      "Training epoch: 1718, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 1719, train loss: 0.01609, val loss: 0.01649\n",
      "Training epoch: 1720, train loss: 0.01579, val loss: 0.01620\n",
      "Training epoch: 1721, train loss: 0.01591, val loss: 0.01630\n",
      "Training epoch: 1722, train loss: 0.01614, val loss: 0.01660\n",
      "Training epoch: 1723, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1724, train loss: 0.01609, val loss: 0.01656\n",
      "Training epoch: 1725, train loss: 0.01625, val loss: 0.01673\n",
      "Training epoch: 1726, train loss: 0.01582, val loss: 0.01626\n",
      "Training epoch: 1727, train loss: 0.01587, val loss: 0.01629\n",
      "Training epoch: 1728, train loss: 0.01587, val loss: 0.01625\n",
      "Training epoch: 1729, train loss: 0.01612, val loss: 0.01659\n",
      "Training epoch: 1730, train loss: 0.01588, val loss: 0.01630\n",
      "Training epoch: 1731, train loss: 0.01591, val loss: 0.01632\n",
      "Training epoch: 1732, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 1733, train loss: 0.01593, val loss: 0.01631\n",
      "Training epoch: 1734, train loss: 0.01583, val loss: 0.01622\n",
      "Training epoch: 1735, train loss: 0.01594, val loss: 0.01636\n",
      "Training epoch: 1736, train loss: 0.01655, val loss: 0.01700\n",
      "Training epoch: 1737, train loss: 0.01587, val loss: 0.01630\n",
      "Training epoch: 1738, train loss: 0.01633, val loss: 0.01679\n",
      "Training epoch: 1739, train loss: 0.01596, val loss: 0.01636\n",
      "Training epoch: 1740, train loss: 0.01592, val loss: 0.01629\n",
      "Training epoch: 1741, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 1742, train loss: 0.01590, val loss: 0.01630\n",
      "Training epoch: 1743, train loss: 0.01595, val loss: 0.01637\n",
      "Training epoch: 1744, train loss: 0.01586, val loss: 0.01626\n",
      "Training epoch: 1745, train loss: 0.01618, val loss: 0.01663\n",
      "Training epoch: 1746, train loss: 0.01579, val loss: 0.01619\n",
      "Training epoch: 1747, train loss: 0.01679, val loss: 0.01731\n",
      "Training epoch: 1748, train loss: 0.01590, val loss: 0.01633\n",
      "Training epoch: 1749, train loss: 0.01710, val loss: 0.01740\n",
      "Training epoch: 1750, train loss: 0.01592, val loss: 0.01630\n",
      "Training epoch: 1751, train loss: 0.01594, val loss: 0.01635\n",
      "Training epoch: 1752, train loss: 0.01628, val loss: 0.01666\n",
      "Training epoch: 1753, train loss: 0.01645, val loss: 0.01682\n",
      "Training epoch: 1754, train loss: 0.01642, val loss: 0.01688\n",
      "Training epoch: 1755, train loss: 0.01612, val loss: 0.01655\n",
      "Training epoch: 1756, train loss: 0.01649, val loss: 0.01699\n",
      "Training epoch: 1757, train loss: 0.01618, val loss: 0.01663\n",
      "Training epoch: 1758, train loss: 0.01589, val loss: 0.01629\n",
      "Training epoch: 1759, train loss: 0.01587, val loss: 0.01624\n",
      "Training epoch: 1760, train loss: 0.01593, val loss: 0.01633\n",
      "Training epoch: 1761, train loss: 0.01607, val loss: 0.01649\n",
      "Training epoch: 1762, train loss: 0.01643, val loss: 0.01687\n",
      "Training epoch: 1763, train loss: 0.01610, val loss: 0.01650\n",
      "Training epoch: 1764, train loss: 0.01625, val loss: 0.01670\n",
      "Training epoch: 1765, train loss: 0.01607, val loss: 0.01651\n",
      "Training epoch: 1766, train loss: 0.01588, val loss: 0.01624\n",
      "Training epoch: 1767, train loss: 0.01634, val loss: 0.01682\n",
      "Training epoch: 1768, train loss: 0.01583, val loss: 0.01619\n",
      "Training epoch: 1769, train loss: 0.01612, val loss: 0.01657\n",
      "Training epoch: 1770, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1771, train loss: 0.01574, val loss: 0.01612\n",
      "Training epoch: 1772, train loss: 0.01615, val loss: 0.01653\n",
      "Training epoch: 1773, train loss: 0.01584, val loss: 0.01623\n",
      "Training epoch: 1774, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1775, train loss: 0.01605, val loss: 0.01649\n",
      "Training epoch: 1776, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 1777, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1778, train loss: 0.01599, val loss: 0.01638\n",
      "Training epoch: 1779, train loss: 0.01585, val loss: 0.01628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1780, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 1781, train loss: 0.01613, val loss: 0.01651\n",
      "Training epoch: 1782, train loss: 0.01583, val loss: 0.01623\n",
      "Training epoch: 1783, train loss: 0.01591, val loss: 0.01630\n",
      "Training epoch: 1784, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1785, train loss: 0.01638, val loss: 0.01671\n",
      "Training epoch: 1786, train loss: 0.01611, val loss: 0.01656\n",
      "Training epoch: 1787, train loss: 0.01625, val loss: 0.01668\n",
      "Training epoch: 1788, train loss: 0.01607, val loss: 0.01653\n",
      "Training epoch: 1789, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 1790, train loss: 0.01639, val loss: 0.01688\n",
      "Training epoch: 1791, train loss: 0.01633, val loss: 0.01680\n",
      "Training epoch: 1792, train loss: 0.01588, val loss: 0.01628\n",
      "Training epoch: 1793, train loss: 0.01599, val loss: 0.01641\n",
      "Training epoch: 1794, train loss: 0.01618, val loss: 0.01652\n",
      "Training epoch: 1795, train loss: 0.01595, val loss: 0.01633\n",
      "Training epoch: 1796, train loss: 0.01604, val loss: 0.01645\n",
      "Training epoch: 1797, train loss: 0.01594, val loss: 0.01638\n",
      "Training epoch: 1798, train loss: 0.01597, val loss: 0.01640\n",
      "Training epoch: 1799, train loss: 0.01660, val loss: 0.01699\n",
      "Training epoch: 1800, train loss: 0.01651, val loss: 0.01699\n",
      "Training epoch: 1801, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 1802, train loss: 0.01633, val loss: 0.01668\n",
      "Training epoch: 1803, train loss: 0.01596, val loss: 0.01639\n",
      "Training epoch: 1804, train loss: 0.01587, val loss: 0.01630\n",
      "Training epoch: 1805, train loss: 0.01595, val loss: 0.01638\n",
      "Training epoch: 1806, train loss: 0.01616, val loss: 0.01663\n",
      "Training epoch: 1807, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1808, train loss: 0.01618, val loss: 0.01655\n",
      "Training epoch: 1809, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 1810, train loss: 0.01594, val loss: 0.01636\n",
      "Training epoch: 1811, train loss: 0.01602, val loss: 0.01645\n",
      "Training epoch: 1812, train loss: 0.01610, val loss: 0.01655\n",
      "Training epoch: 1813, train loss: 0.01616, val loss: 0.01660\n",
      "Training epoch: 1814, train loss: 0.01627, val loss: 0.01673\n",
      "Training epoch: 1815, train loss: 0.01577, val loss: 0.01620\n",
      "Training epoch: 1816, train loss: 0.01579, val loss: 0.01619\n",
      "Training epoch: 1817, train loss: 0.01623, val loss: 0.01666\n",
      "Training epoch: 1818, train loss: 0.01643, val loss: 0.01689\n",
      "Training epoch: 1819, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1820, train loss: 0.01603, val loss: 0.01643\n",
      "Training epoch: 1821, train loss: 0.01599, val loss: 0.01639\n",
      "Training epoch: 1822, train loss: 0.01684, val loss: 0.01717\n",
      "Training epoch: 1823, train loss: 0.01625, val loss: 0.01666\n",
      "Training epoch: 1824, train loss: 0.01598, val loss: 0.01637\n",
      "Training epoch: 1825, train loss: 0.01636, val loss: 0.01672\n",
      "Training epoch: 1826, train loss: 0.01641, val loss: 0.01681\n",
      "Training epoch: 1827, train loss: 0.01602, val loss: 0.01644\n",
      "Training epoch: 1828, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 1829, train loss: 0.01606, val loss: 0.01652\n",
      "Training epoch: 1830, train loss: 0.01608, val loss: 0.01650\n",
      "Training epoch: 1831, train loss: 0.01654, val loss: 0.01704\n",
      "Training epoch: 1832, train loss: 0.01603, val loss: 0.01645\n",
      "Training epoch: 1833, train loss: 0.01582, val loss: 0.01620\n",
      "Training epoch: 1834, train loss: 0.01593, val loss: 0.01637\n",
      "Training epoch: 1835, train loss: 0.01619, val loss: 0.01656\n",
      "Training epoch: 1836, train loss: 0.01594, val loss: 0.01635\n",
      "Training epoch: 1837, train loss: 0.01576, val loss: 0.01617\n",
      "Training epoch: 1838, train loss: 0.01613, val loss: 0.01657\n",
      "Training epoch: 1839, train loss: 0.01619, val loss: 0.01655\n",
      "Training epoch: 1840, train loss: 0.01608, val loss: 0.01650\n",
      "Training epoch: 1841, train loss: 0.01635, val loss: 0.01676\n",
      "Training epoch: 1842, train loss: 0.01599, val loss: 0.01635\n",
      "Training epoch: 1843, train loss: 0.01590, val loss: 0.01633\n",
      "Training epoch: 1844, train loss: 0.01639, val loss: 0.01676\n",
      "Training epoch: 1845, train loss: 0.01585, val loss: 0.01624\n",
      "Training epoch: 1846, train loss: 0.01654, val loss: 0.01689\n",
      "Training epoch: 1847, train loss: 0.01608, val loss: 0.01650\n",
      "Training epoch: 1848, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1849, train loss: 0.01602, val loss: 0.01644\n",
      "Training epoch: 1850, train loss: 0.01591, val loss: 0.01625\n",
      "Training epoch: 1851, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 1852, train loss: 0.01621, val loss: 0.01666\n",
      "Training epoch: 1853, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 1854, train loss: 0.01605, val loss: 0.01649\n",
      "Training epoch: 1855, train loss: 0.01619, val loss: 0.01663\n",
      "Training epoch: 1856, train loss: 0.01655, val loss: 0.01705\n",
      "Training epoch: 1857, train loss: 0.01602, val loss: 0.01646\n",
      "Training epoch: 1858, train loss: 0.01614, val loss: 0.01651\n",
      "Training epoch: 1859, train loss: 0.01608, val loss: 0.01654\n",
      "Training epoch: 1860, train loss: 0.01583, val loss: 0.01623\n",
      "Training epoch: 1861, train loss: 0.01589, val loss: 0.01629\n",
      "Training epoch: 1862, train loss: 0.01612, val loss: 0.01657\n",
      "Training epoch: 1863, train loss: 0.01630, val loss: 0.01678\n",
      "Training epoch: 1864, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 1865, train loss: 0.01614, val loss: 0.01654\n",
      "Training epoch: 1866, train loss: 0.01594, val loss: 0.01637\n",
      "Training epoch: 1867, train loss: 0.01606, val loss: 0.01650\n",
      "Training epoch: 1868, train loss: 0.01597, val loss: 0.01637\n",
      "Training epoch: 1869, train loss: 0.01592, val loss: 0.01633\n",
      "Training epoch: 1870, train loss: 0.01631, val loss: 0.01673\n",
      "Training epoch: 1871, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 1872, train loss: 0.01642, val loss: 0.01677\n",
      "Training epoch: 1873, train loss: 0.01606, val loss: 0.01648\n",
      "Training epoch: 1874, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 1875, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1876, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 1877, train loss: 0.01639, val loss: 0.01687\n",
      "Training epoch: 1878, train loss: 0.01591, val loss: 0.01628\n",
      "Early stop at epoch 1878, With Testing Error: 0.01628\n",
      "Subnetwork pruning.\n",
      "Fine tuning.\n",
      "Tuning epoch: 1, train loss: 0.01585, val loss: 0.01628\n",
      "Tuning epoch: 2, train loss: 0.01584, val loss: 0.01626\n",
      "Tuning epoch: 3, train loss: 0.01585, val loss: 0.01622\n",
      "Tuning epoch: 4, train loss: 0.01570, val loss: 0.01606\n",
      "Tuning epoch: 5, train loss: 0.01596, val loss: 0.01630\n",
      "Tuning epoch: 6, train loss: 0.01655, val loss: 0.01698\n",
      "Tuning epoch: 7, train loss: 0.01607, val loss: 0.01646\n",
      "Tuning epoch: 8, train loss: 0.01581, val loss: 0.01620\n",
      "Tuning epoch: 9, train loss: 0.01568, val loss: 0.01607\n",
      "Tuning epoch: 10, train loss: 0.01610, val loss: 0.01647\n",
      "Tuning epoch: 11, train loss: 0.01577, val loss: 0.01617\n",
      "Tuning epoch: 12, train loss: 0.01570, val loss: 0.01608\n",
      "Tuning epoch: 13, train loss: 0.01573, val loss: 0.01613\n",
      "Tuning epoch: 14, train loss: 0.01583, val loss: 0.01622\n",
      "Tuning epoch: 15, train loss: 0.01575, val loss: 0.01607\n",
      "Tuning epoch: 16, train loss: 0.01573, val loss: 0.01612\n",
      "Tuning epoch: 17, train loss: 0.01601, val loss: 0.01637\n",
      "Tuning epoch: 18, train loss: 0.01572, val loss: 0.01608\n",
      "Tuning epoch: 19, train loss: 0.01591, val loss: 0.01623\n",
      "Tuning epoch: 20, train loss: 0.01610, val loss: 0.01651\n",
      "Tuning epoch: 21, train loss: 0.01586, val loss: 0.01623\n",
      "Tuning epoch: 22, train loss: 0.01631, val loss: 0.01659\n",
      "Tuning epoch: 23, train loss: 0.01581, val loss: 0.01621\n",
      "Tuning epoch: 24, train loss: 0.01578, val loss: 0.01614\n",
      "Tuning epoch: 25, train loss: 0.01588, val loss: 0.01625\n",
      "Tuning epoch: 26, train loss: 0.01600, val loss: 0.01637\n",
      "Tuning epoch: 27, train loss: 0.01581, val loss: 0.01618\n",
      "Tuning epoch: 28, train loss: 0.01596, val loss: 0.01634\n",
      "Tuning epoch: 29, train loss: 0.01568, val loss: 0.01602\n",
      "Tuning epoch: 30, train loss: 0.01564, val loss: 0.01604\n",
      "Tuning epoch: 31, train loss: 0.01591, val loss: 0.01627\n",
      "Tuning epoch: 32, train loss: 0.01606, val loss: 0.01649\n",
      "Tuning epoch: 33, train loss: 0.01600, val loss: 0.01636\n",
      "Tuning epoch: 34, train loss: 0.01574, val loss: 0.01609\n",
      "Tuning epoch: 35, train loss: 0.01586, val loss: 0.01622\n",
      "Tuning epoch: 36, train loss: 0.01593, val loss: 0.01636\n",
      "Tuning epoch: 37, train loss: 0.01571, val loss: 0.01611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning epoch: 38, train loss: 0.01577, val loss: 0.01615\n",
      "Tuning epoch: 39, train loss: 0.01571, val loss: 0.01609\n",
      "Tuning epoch: 40, train loss: 0.01573, val loss: 0.01610\n",
      "Tuning epoch: 41, train loss: 0.01593, val loss: 0.01636\n",
      "Tuning epoch: 42, train loss: 0.01589, val loss: 0.01628\n",
      "Tuning epoch: 43, train loss: 0.01570, val loss: 0.01608\n",
      "Tuning epoch: 44, train loss: 0.01603, val loss: 0.01636\n",
      "Tuning epoch: 45, train loss: 0.01568, val loss: 0.01604\n",
      "Tuning epoch: 46, train loss: 0.01598, val loss: 0.01634\n",
      "Tuning epoch: 47, train loss: 0.01590, val loss: 0.01633\n",
      "Tuning epoch: 48, train loss: 0.01578, val loss: 0.01617\n",
      "Tuning epoch: 49, train loss: 0.01601, val loss: 0.01644\n",
      "Tuning epoch: 50, train loss: 0.01611, val loss: 0.01655\n",
      "Tuning epoch: 51, train loss: 0.01675, val loss: 0.01721\n",
      "Tuning epoch: 52, train loss: 0.01603, val loss: 0.01646\n",
      "Tuning epoch: 53, train loss: 0.01640, val loss: 0.01683\n",
      "Tuning epoch: 54, train loss: 0.01677, val loss: 0.01726\n",
      "Tuning epoch: 55, train loss: 0.01598, val loss: 0.01638\n",
      "Tuning epoch: 56, train loss: 0.01648, val loss: 0.01681\n",
      "Tuning epoch: 57, train loss: 0.01646, val loss: 0.01675\n",
      "Tuning epoch: 58, train loss: 0.01582, val loss: 0.01617\n",
      "Tuning epoch: 59, train loss: 0.01592, val loss: 0.01630\n",
      "Tuning epoch: 60, train loss: 0.01586, val loss: 0.01620\n",
      "Tuning epoch: 61, train loss: 0.01593, val loss: 0.01624\n",
      "Tuning epoch: 62, train loss: 0.01639, val loss: 0.01669\n",
      "Tuning epoch: 63, train loss: 0.01569, val loss: 0.01606\n",
      "Tuning epoch: 64, train loss: 0.01619, val loss: 0.01656\n",
      "Tuning epoch: 65, train loss: 0.01576, val loss: 0.01611\n",
      "Tuning epoch: 66, train loss: 0.01591, val loss: 0.01629\n",
      "Tuning epoch: 67, train loss: 0.01606, val loss: 0.01649\n",
      "Tuning epoch: 68, train loss: 0.01570, val loss: 0.01608\n",
      "Tuning epoch: 69, train loss: 0.01587, val loss: 0.01620\n",
      "Tuning epoch: 70, train loss: 0.01601, val loss: 0.01638\n",
      "Tuning epoch: 71, train loss: 0.01595, val loss: 0.01635\n",
      "Tuning epoch: 72, train loss: 0.01641, val loss: 0.01680\n",
      "Tuning epoch: 73, train loss: 0.01611, val loss: 0.01642\n",
      "Tuning epoch: 74, train loss: 0.01588, val loss: 0.01622\n",
      "Tuning epoch: 75, train loss: 0.01628, val loss: 0.01668\n",
      "Tuning epoch: 76, train loss: 0.01571, val loss: 0.01607\n",
      "Tuning epoch: 77, train loss: 0.01578, val loss: 0.01617\n",
      "Tuning epoch: 78, train loss: 0.01588, val loss: 0.01623\n",
      "Tuning epoch: 79, train loss: 0.01567, val loss: 0.01601\n",
      "Tuning epoch: 80, train loss: 0.01615, val loss: 0.01648\n",
      "Tuning epoch: 81, train loss: 0.01721, val loss: 0.01755\n",
      "Tuning epoch: 82, train loss: 0.01600, val loss: 0.01632\n",
      "Tuning epoch: 83, train loss: 0.01580, val loss: 0.01617\n",
      "Tuning epoch: 84, train loss: 0.01578, val loss: 0.01615\n",
      "Tuning epoch: 85, train loss: 0.01596, val loss: 0.01633\n",
      "Tuning epoch: 86, train loss: 0.01583, val loss: 0.01623\n",
      "Tuning epoch: 87, train loss: 0.01590, val loss: 0.01633\n",
      "Tuning epoch: 88, train loss: 0.01578, val loss: 0.01617\n",
      "Tuning epoch: 89, train loss: 0.01583, val loss: 0.01624\n",
      "Tuning epoch: 90, train loss: 0.01611, val loss: 0.01654\n",
      "Tuning epoch: 91, train loss: 0.01619, val loss: 0.01661\n",
      "Tuning epoch: 92, train loss: 0.01597, val loss: 0.01636\n",
      "Tuning epoch: 93, train loss: 0.01602, val loss: 0.01637\n",
      "Tuning epoch: 94, train loss: 0.01597, val loss: 0.01634\n",
      "Tuning epoch: 95, train loss: 0.01585, val loss: 0.01620\n",
      "Tuning epoch: 96, train loss: 0.01571, val loss: 0.01612\n",
      "Tuning epoch: 97, train loss: 0.01570, val loss: 0.01610\n",
      "Tuning epoch: 98, train loss: 0.01585, val loss: 0.01617\n",
      "Tuning epoch: 99, train loss: 0.01570, val loss: 0.01606\n",
      "Tuning epoch: 100, train loss: 0.01567, val loss: 0.01603\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from exnn import ExNN\n",
    "\n",
    "def data_generator1(datanum, testnum=10000, noise_sigma=1, rand_seed=0):\n",
    "    \n",
    "    corr = 0.5\n",
    "    np.random.seed(rand_seed)\n",
    "    proj_matrix = np.zeros((10, 4))\n",
    "    proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "    proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "    proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "    proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "    u = np.random.uniform(-1, 1, [datanum + testnum, 1])\n",
    "    t = np.sqrt(corr / (1 - corr))\n",
    "    x = np.zeros((datanum + testnum, 10))\n",
    "    for i in range(10):\n",
    "        x[:, i:i + 1] = (np.random.uniform(-1, 1, [datanum + testnum, 1]) + t * u) / (1 + t)\n",
    "\n",
    "    y = np.reshape(2 * np.dot(x, proj_matrix[:, 0]) + 0.2 * np.exp(-4 * np.dot(x, proj_matrix[:, 1])) + \\\n",
    "                   3 * (np.dot(x, proj_matrix[:, 2]))**2 + 2.5 * np.sin(np.pi * np.dot(x, proj_matrix[:, 3])), [-1, 1]) + \\\n",
    "              noise_sigma * np.random.normal(0, 1, [datanum + testnum, 1])\n",
    "    \n",
    "    task_type = \"Regression\"\n",
    "    meta_info = {\"X1\":{\"type\":\"continuous\"},\n",
    "             \"X2\":{\"type\":\"continuous\"},\n",
    "             \"X3\":{\"type\":\"continuous\"},\n",
    "             \"X4\":{\"type\":\"continuous\"},\n",
    "             \"X5\":{\"type\":\"continuous\"},\n",
    "             \"X6\":{\"type\":\"continuous\"},\n",
    "             \"X7\":{\"type\":\"continuous\"},\n",
    "             \"X8\":{\"type\":\"continuous\"},\n",
    "             \"X9\":{\"type\":\"continuous\"},\n",
    "             \"X10\":{\"type\":\"continuous\"},\n",
    "             \"Y\":{\"type\":\"target\"}}\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == \"target\":\n",
    "            sy = MinMaxScaler((-1, 1))\n",
    "            y = sy.fit_transform(y)\n",
    "            meta_info[key][\"scaler\"] = sy\n",
    "        elif item['type'] == \"categorical\":\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(x[:,[i]])\n",
    "            ordinal_feature = enc.transform(x[:,[i]])\n",
    "            x[:,[i]] = ordinal_feature\n",
    "            meta_info[key][\"values\"] = enc.categories_[0].tolist()\n",
    "        else:\n",
    "            sx = MinMaxScaler((-1, 1))\n",
    "            x[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "            meta_info[key][\"scaler\"] = sx\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=testnum, random_state=rand_seed)\n",
    "    return train_x, test_x, train_y, test_y, task_type, meta_info\n",
    "\n",
    "train_x, test_x, train_y, test_y, task_type, meta_info = data_generator1(datanum=10000, testnum=10000, noise_sigma=1, rand_seed=0)\n",
    "model = ExNN(meta_info=meta_info,\n",
    "               subnet_num=10,\n",
    "               subnet_arch=[10, 6],\n",
    "               task_type=task_type,\n",
    "               activation_func=tf.tanh,\n",
    "               batch_size=min(1000, int(train_x.shape[0] * 0.2)),\n",
    "               training_epochs=2000,\n",
    "               lr_bp=0.001,\n",
    "               lr_cl=0.1,\n",
    "               beta_threshold=0.05,\n",
    "               tuning_epochs=100,\n",
    "               l1_proj=0.001,\n",
    "               l1_subnet=0.01,\n",
    "               l2_smooth=10**(-6),\n",
    "               verbose=True,\n",
    "               val_ratio=0.2,\n",
    "               early_stop_thres=500)\n",
    "\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T03:04:37.783711Z",
     "start_time": "2020-07-21T03:04:37.769171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X1': {'type': 'continuous', 'scaler': MinMaxScaler(feature_range=(-1, 1))},\n",
       " 'X2': {'type': 'continuous', 'scaler': MinMaxScaler(feature_range=(-1, 1))},\n",
       " 'X3': {'type': 'continuous', 'scaler': MinMaxScaler(feature_range=(-1, 1))},\n",
       " 'X4': {'type': 'continuous', 'scaler': MinMaxScaler(feature_range=(-1, 1))},\n",
       " 'X5': {'type': 'continuous', 'scaler': MinMaxScaler(feature_range=(-1, 1))},\n",
       " 'X6': {'type': 'continuous', 'scaler': MinMaxScaler(feature_range=(-1, 1))},\n",
       " 'X7': {'type': 'continuous', 'scaler': MinMaxScaler(feature_range=(-1, 1))},\n",
       " 'X8': {'type': 'continuous', 'scaler': MinMaxScaler(feature_range=(-1, 1))},\n",
       " 'X9': {'type': 'continuous', 'scaler': MinMaxScaler(feature_range=(-1, 1))},\n",
       " 'X10': {'type': 'continuous', 'scaler': MinMaxScaler(feature_range=(-1, 1))},\n",
       " 'Y': {'type': 'target', 'scaler': MinMaxScaler(feature_range=(-1, 1))}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T02:45:26.581844Z",
     "start_time": "2020-07-21T02:45:24.205872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x662.4 with 15 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.visualize_new(cols_per_row=3, folder=\"./\", name=\"exnn_demo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
