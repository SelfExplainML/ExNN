{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T02:09:59.087832Z",
     "start_time": "2020-07-21T02:06:55.797285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n",
      "Training epoch: 1, train loss: 0.15243, val loss: 0.15319\n",
      "Training epoch: 2, train loss: 0.13356, val loss: 0.13547\n",
      "Training epoch: 3, train loss: 0.11574, val loss: 0.11758\n",
      "Training epoch: 4, train loss: 0.09915, val loss: 0.10049\n",
      "Training epoch: 5, train loss: 0.08545, val loss: 0.08645\n",
      "Training epoch: 6, train loss: 0.07816, val loss: 0.07833\n",
      "Training epoch: 7, train loss: 0.06871, val loss: 0.06867\n",
      "Training epoch: 8, train loss: 0.06548, val loss: 0.06517\n",
      "Training epoch: 9, train loss: 0.05748, val loss: 0.05773\n",
      "Training epoch: 10, train loss: 0.05786, val loss: 0.05819\n",
      "Training epoch: 11, train loss: 0.04962, val loss: 0.05019\n",
      "Training epoch: 12, train loss: 0.04886, val loss: 0.04934\n",
      "Training epoch: 13, train loss: 0.04819, val loss: 0.04860\n",
      "Training epoch: 14, train loss: 0.04446, val loss: 0.04476\n",
      "Training epoch: 15, train loss: 0.04247, val loss: 0.04275\n",
      "Training epoch: 16, train loss: 0.03910, val loss: 0.03954\n",
      "Training epoch: 17, train loss: 0.03977, val loss: 0.03997\n",
      "Training epoch: 18, train loss: 0.03747, val loss: 0.03765\n",
      "Training epoch: 19, train loss: 0.03694, val loss: 0.03718\n",
      "Training epoch: 20, train loss: 0.03497, val loss: 0.03525\n",
      "Training epoch: 21, train loss: 0.03474, val loss: 0.03511\n",
      "Training epoch: 22, train loss: 0.03296, val loss: 0.03318\n",
      "Training epoch: 23, train loss: 0.03140, val loss: 0.03162\n",
      "Training epoch: 24, train loss: 0.03201, val loss: 0.03219\n",
      "Training epoch: 25, train loss: 0.03034, val loss: 0.03039\n",
      "Training epoch: 26, train loss: 0.03030, val loss: 0.03040\n",
      "Training epoch: 27, train loss: 0.02945, val loss: 0.02956\n",
      "Training epoch: 28, train loss: 0.02928, val loss: 0.02940\n",
      "Training epoch: 29, train loss: 0.02858, val loss: 0.02877\n",
      "Training epoch: 30, train loss: 0.02728, val loss: 0.02741\n",
      "Training epoch: 31, train loss: 0.02756, val loss: 0.02781\n",
      "Training epoch: 32, train loss: 0.02609, val loss: 0.02633\n",
      "Training epoch: 33, train loss: 0.02707, val loss: 0.02720\n",
      "Training epoch: 34, train loss: 0.02575, val loss: 0.02602\n",
      "Training epoch: 35, train loss: 0.02546, val loss: 0.02564\n",
      "Training epoch: 36, train loss: 0.02505, val loss: 0.02539\n",
      "Training epoch: 37, train loss: 0.02477, val loss: 0.02494\n",
      "Training epoch: 38, train loss: 0.02417, val loss: 0.02453\n",
      "Training epoch: 39, train loss: 0.02476, val loss: 0.02508\n",
      "Training epoch: 40, train loss: 0.02412, val loss: 0.02446\n",
      "Training epoch: 41, train loss: 0.02417, val loss: 0.02454\n",
      "Training epoch: 42, train loss: 0.02360, val loss: 0.02394\n",
      "Training epoch: 43, train loss: 0.02540, val loss: 0.02582\n",
      "Training epoch: 44, train loss: 0.02327, val loss: 0.02367\n",
      "Training epoch: 45, train loss: 0.02324, val loss: 0.02359\n",
      "Training epoch: 46, train loss: 0.02301, val loss: 0.02338\n",
      "Training epoch: 47, train loss: 0.02247, val loss: 0.02294\n",
      "Training epoch: 48, train loss: 0.02267, val loss: 0.02305\n",
      "Training epoch: 49, train loss: 0.02299, val loss: 0.02346\n",
      "Training epoch: 50, train loss: 0.02266, val loss: 0.02308\n",
      "Training epoch: 51, train loss: 0.02250, val loss: 0.02292\n",
      "Training epoch: 52, train loss: 0.02245, val loss: 0.02294\n",
      "Training epoch: 53, train loss: 0.02282, val loss: 0.02323\n",
      "Training epoch: 54, train loss: 0.02255, val loss: 0.02297\n",
      "Training epoch: 55, train loss: 0.02186, val loss: 0.02236\n",
      "Training epoch: 56, train loss: 0.02196, val loss: 0.02231\n",
      "Training epoch: 57, train loss: 0.02200, val loss: 0.02243\n",
      "Training epoch: 58, train loss: 0.02207, val loss: 0.02245\n",
      "Training epoch: 59, train loss: 0.02207, val loss: 0.02256\n",
      "Training epoch: 60, train loss: 0.02202, val loss: 0.02237\n",
      "Training epoch: 61, train loss: 0.02157, val loss: 0.02212\n",
      "Training epoch: 62, train loss: 0.02153, val loss: 0.02200\n",
      "Training epoch: 63, train loss: 0.02163, val loss: 0.02208\n",
      "Training epoch: 64, train loss: 0.02137, val loss: 0.02186\n",
      "Training epoch: 65, train loss: 0.02140, val loss: 0.02189\n",
      "Training epoch: 66, train loss: 0.02139, val loss: 0.02190\n",
      "Training epoch: 67, train loss: 0.02130, val loss: 0.02179\n",
      "Training epoch: 68, train loss: 0.02139, val loss: 0.02194\n",
      "Training epoch: 69, train loss: 0.02132, val loss: 0.02188\n",
      "Training epoch: 70, train loss: 0.02166, val loss: 0.02219\n",
      "Training epoch: 71, train loss: 0.02121, val loss: 0.02180\n",
      "Training epoch: 72, train loss: 0.02160, val loss: 0.02197\n",
      "Training epoch: 73, train loss: 0.02227, val loss: 0.02272\n",
      "Training epoch: 74, train loss: 0.02112, val loss: 0.02166\n",
      "Training epoch: 75, train loss: 0.02127, val loss: 0.02176\n",
      "Training epoch: 76, train loss: 0.02265, val loss: 0.02316\n",
      "Training epoch: 77, train loss: 0.02110, val loss: 0.02161\n",
      "Training epoch: 78, train loss: 0.02100, val loss: 0.02155\n",
      "Training epoch: 79, train loss: 0.02114, val loss: 0.02161\n",
      "Training epoch: 80, train loss: 0.02117, val loss: 0.02177\n",
      "Training epoch: 81, train loss: 0.02119, val loss: 0.02181\n",
      "Training epoch: 82, train loss: 0.02103, val loss: 0.02162\n",
      "Training epoch: 83, train loss: 0.02103, val loss: 0.02163\n",
      "Training epoch: 84, train loss: 0.02162, val loss: 0.02209\n",
      "Training epoch: 85, train loss: 0.02078, val loss: 0.02131\n",
      "Training epoch: 86, train loss: 0.02090, val loss: 0.02146\n",
      "Training epoch: 87, train loss: 0.02092, val loss: 0.02148\n",
      "Training epoch: 88, train loss: 0.02150, val loss: 0.02216\n",
      "Training epoch: 89, train loss: 0.02096, val loss: 0.02152\n",
      "Training epoch: 90, train loss: 0.02067, val loss: 0.02124\n",
      "Training epoch: 91, train loss: 0.02082, val loss: 0.02140\n",
      "Training epoch: 92, train loss: 0.02075, val loss: 0.02132\n",
      "Training epoch: 93, train loss: 0.02050, val loss: 0.02102\n",
      "Training epoch: 94, train loss: 0.02072, val loss: 0.02123\n",
      "Training epoch: 95, train loss: 0.02057, val loss: 0.02109\n",
      "Training epoch: 96, train loss: 0.02097, val loss: 0.02145\n",
      "Training epoch: 97, train loss: 0.02038, val loss: 0.02088\n",
      "Training epoch: 98, train loss: 0.02071, val loss: 0.02122\n",
      "Training epoch: 99, train loss: 0.02053, val loss: 0.02111\n",
      "Training epoch: 100, train loss: 0.02114, val loss: 0.02179\n",
      "Training epoch: 101, train loss: 0.02045, val loss: 0.02101\n",
      "Training epoch: 102, train loss: 0.02052, val loss: 0.02108\n",
      "Training epoch: 103, train loss: 0.02197, val loss: 0.02242\n",
      "Training epoch: 104, train loss: 0.02095, val loss: 0.02148\n",
      "Training epoch: 105, train loss: 0.02036, val loss: 0.02098\n",
      "Training epoch: 106, train loss: 0.02071, val loss: 0.02119\n",
      "Training epoch: 107, train loss: 0.02070, val loss: 0.02128\n",
      "Training epoch: 108, train loss: 0.02032, val loss: 0.02083\n",
      "Training epoch: 109, train loss: 0.02039, val loss: 0.02095\n",
      "Training epoch: 110, train loss: 0.02037, val loss: 0.02096\n",
      "Training epoch: 111, train loss: 0.02023, val loss: 0.02084\n",
      "Training epoch: 112, train loss: 0.02056, val loss: 0.02103\n",
      "Training epoch: 113, train loss: 0.02061, val loss: 0.02104\n",
      "Training epoch: 114, train loss: 0.02021, val loss: 0.02070\n",
      "Training epoch: 115, train loss: 0.02019, val loss: 0.02076\n",
      "Training epoch: 116, train loss: 0.02033, val loss: 0.02079\n",
      "Training epoch: 117, train loss: 0.02066, val loss: 0.02111\n",
      "Training epoch: 118, train loss: 0.02010, val loss: 0.02054\n",
      "Training epoch: 119, train loss: 0.02042, val loss: 0.02082\n",
      "Training epoch: 120, train loss: 0.02009, val loss: 0.02065\n",
      "Training epoch: 121, train loss: 0.02067, val loss: 0.02119\n",
      "Training epoch: 122, train loss: 0.01999, val loss: 0.02062\n",
      "Training epoch: 123, train loss: 0.02065, val loss: 0.02105\n",
      "Training epoch: 124, train loss: 0.02051, val loss: 0.02094\n",
      "Training epoch: 125, train loss: 0.01990, val loss: 0.02046\n",
      "Training epoch: 126, train loss: 0.02025, val loss: 0.02096\n",
      "Training epoch: 127, train loss: 0.01978, val loss: 0.02026\n",
      "Training epoch: 128, train loss: 0.02000, val loss: 0.02054\n",
      "Training epoch: 129, train loss: 0.01989, val loss: 0.02035\n",
      "Training epoch: 130, train loss: 0.01985, val loss: 0.02043\n",
      "Training epoch: 131, train loss: 0.02005, val loss: 0.02057\n",
      "Training epoch: 132, train loss: 0.02007, val loss: 0.02061\n",
      "Training epoch: 133, train loss: 0.01971, val loss: 0.02038\n",
      "Training epoch: 134, train loss: 0.01955, val loss: 0.02015\n",
      "Training epoch: 135, train loss: 0.01954, val loss: 0.02009\n",
      "Training epoch: 136, train loss: 0.01984, val loss: 0.02047\n",
      "Training epoch: 137, train loss: 0.02013, val loss: 0.02085\n",
      "Training epoch: 138, train loss: 0.01952, val loss: 0.02010\n",
      "Training epoch: 139, train loss: 0.01959, val loss: 0.02017\n",
      "Training epoch: 140, train loss: 0.01953, val loss: 0.02000\n",
      "Training epoch: 141, train loss: 0.01960, val loss: 0.02025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 142, train loss: 0.01934, val loss: 0.01989\n",
      "Training epoch: 143, train loss: 0.01948, val loss: 0.02001\n",
      "Training epoch: 144, train loss: 0.01949, val loss: 0.01998\n",
      "Training epoch: 145, train loss: 0.01964, val loss: 0.02017\n",
      "Training epoch: 146, train loss: 0.01926, val loss: 0.01982\n",
      "Training epoch: 147, train loss: 0.01926, val loss: 0.01988\n",
      "Training epoch: 148, train loss: 0.01964, val loss: 0.02034\n",
      "Training epoch: 149, train loss: 0.01906, val loss: 0.01960\n",
      "Training epoch: 150, train loss: 0.01949, val loss: 0.02020\n",
      "Training epoch: 151, train loss: 0.01913, val loss: 0.01975\n",
      "Training epoch: 152, train loss: 0.01931, val loss: 0.01982\n",
      "Training epoch: 153, train loss: 0.01968, val loss: 0.02048\n",
      "Training epoch: 154, train loss: 0.01911, val loss: 0.01988\n",
      "Training epoch: 155, train loss: 0.01887, val loss: 0.01951\n",
      "Training epoch: 156, train loss: 0.01923, val loss: 0.01992\n",
      "Training epoch: 157, train loss: 0.01891, val loss: 0.01953\n",
      "Training epoch: 158, train loss: 0.01980, val loss: 0.02062\n",
      "Training epoch: 159, train loss: 0.01895, val loss: 0.01946\n",
      "Training epoch: 160, train loss: 0.01916, val loss: 0.01978\n",
      "Training epoch: 161, train loss: 0.01997, val loss: 0.02092\n",
      "Training epoch: 162, train loss: 0.01870, val loss: 0.01924\n",
      "Training epoch: 163, train loss: 0.01864, val loss: 0.01928\n",
      "Training epoch: 164, train loss: 0.01872, val loss: 0.01936\n",
      "Training epoch: 165, train loss: 0.01860, val loss: 0.01919\n",
      "Training epoch: 166, train loss: 0.01924, val loss: 0.02007\n",
      "Training epoch: 167, train loss: 0.01875, val loss: 0.01935\n",
      "Training epoch: 168, train loss: 0.01857, val loss: 0.01924\n",
      "Training epoch: 169, train loss: 0.01902, val loss: 0.01966\n",
      "Training epoch: 170, train loss: 0.01905, val loss: 0.01972\n",
      "Training epoch: 171, train loss: 0.01911, val loss: 0.01989\n",
      "Training epoch: 172, train loss: 0.01837, val loss: 0.01914\n",
      "Training epoch: 173, train loss: 0.01902, val loss: 0.01957\n",
      "Training epoch: 174, train loss: 0.01920, val loss: 0.02009\n",
      "Training epoch: 175, train loss: 0.01899, val loss: 0.01979\n",
      "Training epoch: 176, train loss: 0.01970, val loss: 0.02053\n",
      "Training epoch: 177, train loss: 0.01922, val loss: 0.01999\n",
      "Training epoch: 178, train loss: 0.01823, val loss: 0.01889\n",
      "Training epoch: 179, train loss: 0.01855, val loss: 0.01917\n",
      "Training epoch: 180, train loss: 0.01815, val loss: 0.01887\n",
      "Training epoch: 181, train loss: 0.01901, val loss: 0.01960\n",
      "Training epoch: 182, train loss: 0.01839, val loss: 0.01911\n",
      "Training epoch: 183, train loss: 0.01915, val loss: 0.01996\n",
      "Training epoch: 184, train loss: 0.01888, val loss: 0.01966\n",
      "Training epoch: 185, train loss: 0.01814, val loss: 0.01890\n",
      "Training epoch: 186, train loss: 0.01844, val loss: 0.01931\n",
      "Training epoch: 187, train loss: 0.01820, val loss: 0.01899\n",
      "Training epoch: 188, train loss: 0.01883, val loss: 0.01946\n",
      "Training epoch: 189, train loss: 0.01801, val loss: 0.01875\n",
      "Training epoch: 190, train loss: 0.01803, val loss: 0.01873\n",
      "Training epoch: 191, train loss: 0.01799, val loss: 0.01870\n",
      "Training epoch: 192, train loss: 0.01802, val loss: 0.01881\n",
      "Training epoch: 193, train loss: 0.01784, val loss: 0.01870\n",
      "Training epoch: 194, train loss: 0.01776, val loss: 0.01849\n",
      "Training epoch: 195, train loss: 0.01916, val loss: 0.01998\n",
      "Training epoch: 196, train loss: 0.01830, val loss: 0.01902\n",
      "Training epoch: 197, train loss: 0.01773, val loss: 0.01848\n",
      "Training epoch: 198, train loss: 0.01862, val loss: 0.01928\n",
      "Training epoch: 199, train loss: 0.01820, val loss: 0.01887\n",
      "Training epoch: 200, train loss: 0.01793, val loss: 0.01868\n",
      "Training epoch: 201, train loss: 0.01828, val loss: 0.01896\n",
      "Training epoch: 202, train loss: 0.01795, val loss: 0.01881\n",
      "Training epoch: 203, train loss: 0.01770, val loss: 0.01842\n",
      "Training epoch: 204, train loss: 0.01765, val loss: 0.01841\n",
      "Training epoch: 205, train loss: 0.01804, val loss: 0.01886\n",
      "Training epoch: 206, train loss: 0.01760, val loss: 0.01833\n",
      "Training epoch: 207, train loss: 0.01841, val loss: 0.01926\n",
      "Training epoch: 208, train loss: 0.01927, val loss: 0.01993\n",
      "Training epoch: 209, train loss: 0.01862, val loss: 0.01949\n",
      "Training epoch: 210, train loss: 0.01916, val loss: 0.01982\n",
      "Training epoch: 211, train loss: 0.01847, val loss: 0.01913\n",
      "Training epoch: 212, train loss: 0.01866, val loss: 0.01960\n",
      "Training epoch: 213, train loss: 0.01789, val loss: 0.01867\n",
      "Training epoch: 214, train loss: 0.01995, val loss: 0.02059\n",
      "Training epoch: 215, train loss: 0.01758, val loss: 0.01835\n",
      "Training epoch: 216, train loss: 0.01885, val loss: 0.01981\n",
      "Training epoch: 217, train loss: 0.01829, val loss: 0.01894\n",
      "Training epoch: 218, train loss: 0.01810, val loss: 0.01885\n",
      "Training epoch: 219, train loss: 0.01844, val loss: 0.01928\n",
      "Training epoch: 220, train loss: 0.01776, val loss: 0.01851\n",
      "Training epoch: 221, train loss: 0.01802, val loss: 0.01886\n",
      "Training epoch: 222, train loss: 0.01862, val loss: 0.01932\n",
      "Training epoch: 223, train loss: 0.01981, val loss: 0.02070\n",
      "Training epoch: 224, train loss: 0.01787, val loss: 0.01872\n",
      "Training epoch: 225, train loss: 0.01756, val loss: 0.01831\n",
      "Training epoch: 226, train loss: 0.01778, val loss: 0.01854\n",
      "Training epoch: 227, train loss: 0.01785, val loss: 0.01863\n",
      "Training epoch: 228, train loss: 0.01788, val loss: 0.01869\n",
      "Training epoch: 229, train loss: 0.01844, val loss: 0.01927\n",
      "Training epoch: 230, train loss: 0.01762, val loss: 0.01837\n",
      "Training epoch: 231, train loss: 0.01833, val loss: 0.01903\n",
      "Training epoch: 232, train loss: 0.01758, val loss: 0.01838\n",
      "Training epoch: 233, train loss: 0.01740, val loss: 0.01818\n",
      "Training epoch: 234, train loss: 0.01838, val loss: 0.01905\n",
      "Training epoch: 235, train loss: 0.01760, val loss: 0.01833\n",
      "Training epoch: 236, train loss: 0.01812, val loss: 0.01894\n",
      "Training epoch: 237, train loss: 0.01752, val loss: 0.01831\n",
      "Training epoch: 238, train loss: 0.01743, val loss: 0.01816\n",
      "Training epoch: 239, train loss: 0.01851, val loss: 0.01931\n",
      "Training epoch: 240, train loss: 0.01768, val loss: 0.01843\n",
      "Training epoch: 241, train loss: 0.01767, val loss: 0.01842\n",
      "Training epoch: 242, train loss: 0.01778, val loss: 0.01862\n",
      "Training epoch: 243, train loss: 0.01733, val loss: 0.01807\n",
      "Training epoch: 244, train loss: 0.01799, val loss: 0.01883\n",
      "Training epoch: 245, train loss: 0.01746, val loss: 0.01826\n",
      "Training epoch: 246, train loss: 0.01737, val loss: 0.01809\n",
      "Training epoch: 247, train loss: 0.01823, val loss: 0.01908\n",
      "Training epoch: 248, train loss: 0.01744, val loss: 0.01817\n",
      "Training epoch: 249, train loss: 0.01738, val loss: 0.01810\n",
      "Training epoch: 250, train loss: 0.01760, val loss: 0.01833\n",
      "Training epoch: 251, train loss: 0.01747, val loss: 0.01824\n",
      "Training epoch: 252, train loss: 0.01915, val loss: 0.01974\n",
      "Training epoch: 253, train loss: 0.01801, val loss: 0.01881\n",
      "Training epoch: 254, train loss: 0.01843, val loss: 0.01925\n",
      "Training epoch: 255, train loss: 0.01759, val loss: 0.01837\n",
      "Training epoch: 256, train loss: 0.01736, val loss: 0.01808\n",
      "Training epoch: 257, train loss: 0.01846, val loss: 0.01929\n",
      "Training epoch: 258, train loss: 0.01725, val loss: 0.01804\n",
      "Training epoch: 259, train loss: 0.01728, val loss: 0.01806\n",
      "Training epoch: 260, train loss: 0.01747, val loss: 0.01818\n",
      "Training epoch: 261, train loss: 0.01728, val loss: 0.01806\n",
      "Training epoch: 262, train loss: 0.01765, val loss: 0.01833\n",
      "Training epoch: 263, train loss: 0.01963, val loss: 0.02062\n",
      "Training epoch: 264, train loss: 0.01739, val loss: 0.01812\n",
      "Training epoch: 265, train loss: 0.01751, val loss: 0.01825\n",
      "Training epoch: 266, train loss: 0.01736, val loss: 0.01815\n",
      "Training epoch: 267, train loss: 0.01712, val loss: 0.01789\n",
      "Training epoch: 268, train loss: 0.01746, val loss: 0.01814\n",
      "Training epoch: 269, train loss: 0.01771, val loss: 0.01848\n",
      "Training epoch: 270, train loss: 0.01744, val loss: 0.01816\n",
      "Training epoch: 271, train loss: 0.01729, val loss: 0.01807\n",
      "Training epoch: 272, train loss: 0.01732, val loss: 0.01803\n",
      "Training epoch: 273, train loss: 0.01726, val loss: 0.01797\n",
      "Training epoch: 274, train loss: 0.01722, val loss: 0.01802\n",
      "Training epoch: 275, train loss: 0.01722, val loss: 0.01793\n",
      "Training epoch: 276, train loss: 0.01789, val loss: 0.01860\n",
      "Training epoch: 277, train loss: 0.01802, val loss: 0.01886\n",
      "Training epoch: 278, train loss: 0.01726, val loss: 0.01802\n",
      "Training epoch: 279, train loss: 0.01780, val loss: 0.01845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 280, train loss: 0.01716, val loss: 0.01797\n",
      "Training epoch: 281, train loss: 0.01715, val loss: 0.01793\n",
      "Training epoch: 282, train loss: 0.01720, val loss: 0.01790\n",
      "Training epoch: 283, train loss: 0.01728, val loss: 0.01801\n",
      "Training epoch: 284, train loss: 0.01748, val loss: 0.01822\n",
      "Training epoch: 285, train loss: 0.01726, val loss: 0.01797\n",
      "Training epoch: 286, train loss: 0.01707, val loss: 0.01782\n",
      "Training epoch: 287, train loss: 0.01828, val loss: 0.01905\n",
      "Training epoch: 288, train loss: 0.01722, val loss: 0.01798\n",
      "Training epoch: 289, train loss: 0.01704, val loss: 0.01774\n",
      "Training epoch: 290, train loss: 0.01722, val loss: 0.01802\n",
      "Training epoch: 291, train loss: 0.01735, val loss: 0.01811\n",
      "Training epoch: 292, train loss: 0.01729, val loss: 0.01804\n",
      "Training epoch: 293, train loss: 0.01695, val loss: 0.01765\n",
      "Training epoch: 294, train loss: 0.01729, val loss: 0.01800\n",
      "Training epoch: 295, train loss: 0.01817, val loss: 0.01905\n",
      "Training epoch: 296, train loss: 0.01707, val loss: 0.01776\n",
      "Training epoch: 297, train loss: 0.01758, val loss: 0.01824\n",
      "Training epoch: 298, train loss: 0.01676, val loss: 0.01752\n",
      "Training epoch: 299, train loss: 0.01782, val loss: 0.01859\n",
      "Training epoch: 300, train loss: 0.01696, val loss: 0.01767\n",
      "Training epoch: 301, train loss: 0.01723, val loss: 0.01790\n",
      "Training epoch: 302, train loss: 0.01761, val loss: 0.01843\n",
      "Training epoch: 303, train loss: 0.01697, val loss: 0.01766\n",
      "Training epoch: 304, train loss: 0.01683, val loss: 0.01758\n",
      "Training epoch: 305, train loss: 0.01765, val loss: 0.01835\n",
      "Training epoch: 306, train loss: 0.01703, val loss: 0.01780\n",
      "Training epoch: 307, train loss: 0.01699, val loss: 0.01761\n",
      "Training epoch: 308, train loss: 0.01679, val loss: 0.01745\n",
      "Training epoch: 309, train loss: 0.01703, val loss: 0.01772\n",
      "Training epoch: 310, train loss: 0.01684, val loss: 0.01752\n",
      "Training epoch: 311, train loss: 0.01709, val loss: 0.01777\n",
      "Training epoch: 312, train loss: 0.01694, val loss: 0.01771\n",
      "Training epoch: 313, train loss: 0.01673, val loss: 0.01737\n",
      "Training epoch: 314, train loss: 0.01692, val loss: 0.01762\n",
      "Training epoch: 315, train loss: 0.01679, val loss: 0.01745\n",
      "Training epoch: 316, train loss: 0.01705, val loss: 0.01777\n",
      "Training epoch: 317, train loss: 0.01673, val loss: 0.01733\n",
      "Training epoch: 318, train loss: 0.01672, val loss: 0.01731\n",
      "Training epoch: 319, train loss: 0.01736, val loss: 0.01801\n",
      "Training epoch: 320, train loss: 0.01734, val loss: 0.01806\n",
      "Training epoch: 321, train loss: 0.01706, val loss: 0.01765\n",
      "Training epoch: 322, train loss: 0.01743, val loss: 0.01801\n",
      "Training epoch: 323, train loss: 0.01674, val loss: 0.01739\n",
      "Training epoch: 324, train loss: 0.01686, val loss: 0.01758\n",
      "Training epoch: 325, train loss: 0.01720, val loss: 0.01794\n",
      "Training epoch: 326, train loss: 0.01665, val loss: 0.01728\n",
      "Training epoch: 327, train loss: 0.01684, val loss: 0.01746\n",
      "Training epoch: 328, train loss: 0.01688, val loss: 0.01747\n",
      "Training epoch: 329, train loss: 0.01666, val loss: 0.01727\n",
      "Training epoch: 330, train loss: 0.01644, val loss: 0.01709\n",
      "Training epoch: 331, train loss: 0.01691, val loss: 0.01755\n",
      "Training epoch: 332, train loss: 0.01722, val loss: 0.01796\n",
      "Training epoch: 333, train loss: 0.01923, val loss: 0.02009\n",
      "Training epoch: 334, train loss: 0.01734, val loss: 0.01790\n",
      "Training epoch: 335, train loss: 0.01730, val loss: 0.01783\n",
      "Training epoch: 336, train loss: 0.01652, val loss: 0.01707\n",
      "Training epoch: 337, train loss: 0.01730, val loss: 0.01793\n",
      "Training epoch: 338, train loss: 0.01756, val loss: 0.01817\n",
      "Training epoch: 339, train loss: 0.01649, val loss: 0.01708\n",
      "Training epoch: 340, train loss: 0.01675, val loss: 0.01733\n",
      "Training epoch: 341, train loss: 0.01892, val loss: 0.01974\n",
      "Training epoch: 342, train loss: 0.01865, val loss: 0.01945\n",
      "Training epoch: 343, train loss: 0.01729, val loss: 0.01792\n",
      "Training epoch: 344, train loss: 0.01771, val loss: 0.01827\n",
      "Training epoch: 345, train loss: 0.01641, val loss: 0.01697\n",
      "Training epoch: 346, train loss: 0.01664, val loss: 0.01729\n",
      "Training epoch: 347, train loss: 0.01710, val loss: 0.01770\n",
      "Training epoch: 348, train loss: 0.01635, val loss: 0.01697\n",
      "Training epoch: 349, train loss: 0.01688, val loss: 0.01759\n",
      "Training epoch: 350, train loss: 0.01631, val loss: 0.01689\n",
      "Training epoch: 351, train loss: 0.01659, val loss: 0.01723\n",
      "Training epoch: 352, train loss: 0.01652, val loss: 0.01717\n",
      "Training epoch: 353, train loss: 0.01674, val loss: 0.01730\n",
      "Training epoch: 354, train loss: 0.01670, val loss: 0.01726\n",
      "Training epoch: 355, train loss: 0.01778, val loss: 0.01835\n",
      "Training epoch: 356, train loss: 0.01632, val loss: 0.01689\n",
      "Training epoch: 357, train loss: 0.01654, val loss: 0.01715\n",
      "Training epoch: 358, train loss: 0.01627, val loss: 0.01685\n",
      "Training epoch: 359, train loss: 0.01698, val loss: 0.01761\n",
      "Training epoch: 360, train loss: 0.01667, val loss: 0.01728\n",
      "Training epoch: 361, train loss: 0.01657, val loss: 0.01718\n",
      "Training epoch: 362, train loss: 0.01650, val loss: 0.01709\n",
      "Training epoch: 363, train loss: 0.01631, val loss: 0.01688\n",
      "Training epoch: 364, train loss: 0.01615, val loss: 0.01674\n",
      "Training epoch: 365, train loss: 0.01618, val loss: 0.01675\n",
      "Training epoch: 366, train loss: 0.01635, val loss: 0.01693\n",
      "Training epoch: 367, train loss: 0.01670, val loss: 0.01741\n",
      "Training epoch: 368, train loss: 0.01623, val loss: 0.01679\n",
      "Training epoch: 369, train loss: 0.01653, val loss: 0.01715\n",
      "Training epoch: 370, train loss: 0.01633, val loss: 0.01692\n",
      "Training epoch: 371, train loss: 0.01631, val loss: 0.01690\n",
      "Training epoch: 372, train loss: 0.01631, val loss: 0.01690\n",
      "Training epoch: 373, train loss: 0.01609, val loss: 0.01660\n",
      "Training epoch: 374, train loss: 0.01631, val loss: 0.01693\n",
      "Training epoch: 375, train loss: 0.01630, val loss: 0.01682\n",
      "Training epoch: 376, train loss: 0.01661, val loss: 0.01713\n",
      "Training epoch: 377, train loss: 0.01628, val loss: 0.01691\n",
      "Training epoch: 378, train loss: 0.01641, val loss: 0.01704\n",
      "Training epoch: 379, train loss: 0.01677, val loss: 0.01744\n",
      "Training epoch: 380, train loss: 0.01639, val loss: 0.01689\n",
      "Training epoch: 381, train loss: 0.01621, val loss: 0.01675\n",
      "Training epoch: 382, train loss: 0.01607, val loss: 0.01664\n",
      "Training epoch: 383, train loss: 0.01680, val loss: 0.01742\n",
      "Training epoch: 384, train loss: 0.01652, val loss: 0.01706\n",
      "Training epoch: 385, train loss: 0.01645, val loss: 0.01704\n",
      "Training epoch: 386, train loss: 0.01693, val loss: 0.01753\n",
      "Training epoch: 387, train loss: 0.01612, val loss: 0.01667\n",
      "Training epoch: 388, train loss: 0.01642, val loss: 0.01706\n",
      "Training epoch: 389, train loss: 0.01607, val loss: 0.01661\n",
      "Training epoch: 390, train loss: 0.01607, val loss: 0.01659\n",
      "Training epoch: 391, train loss: 0.01653, val loss: 0.01715\n",
      "Training epoch: 392, train loss: 0.01625, val loss: 0.01688\n",
      "Training epoch: 393, train loss: 0.01642, val loss: 0.01700\n",
      "Training epoch: 394, train loss: 0.01616, val loss: 0.01669\n",
      "Training epoch: 395, train loss: 0.01604, val loss: 0.01664\n",
      "Training epoch: 396, train loss: 0.01589, val loss: 0.01647\n",
      "Training epoch: 397, train loss: 0.01666, val loss: 0.01724\n",
      "Training epoch: 398, train loss: 0.01624, val loss: 0.01671\n",
      "Training epoch: 399, train loss: 0.01651, val loss: 0.01711\n",
      "Training epoch: 400, train loss: 0.01595, val loss: 0.01652\n",
      "Training epoch: 401, train loss: 0.01592, val loss: 0.01649\n",
      "Training epoch: 402, train loss: 0.01613, val loss: 0.01669\n",
      "Training epoch: 403, train loss: 0.01611, val loss: 0.01662\n",
      "Training epoch: 404, train loss: 0.01594, val loss: 0.01652\n",
      "Training epoch: 405, train loss: 0.01645, val loss: 0.01695\n",
      "Training epoch: 406, train loss: 0.01626, val loss: 0.01686\n",
      "Training epoch: 407, train loss: 0.01608, val loss: 0.01666\n",
      "Training epoch: 408, train loss: 0.01628, val loss: 0.01688\n",
      "Training epoch: 409, train loss: 0.01620, val loss: 0.01677\n",
      "Training epoch: 410, train loss: 0.01638, val loss: 0.01683\n",
      "Training epoch: 411, train loss: 0.01698, val loss: 0.01755\n",
      "Training epoch: 412, train loss: 0.01613, val loss: 0.01671\n",
      "Training epoch: 413, train loss: 0.01653, val loss: 0.01713\n",
      "Training epoch: 414, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 415, train loss: 0.01651, val loss: 0.01701\n",
      "Training epoch: 416, train loss: 0.01691, val loss: 0.01761\n",
      "Training epoch: 417, train loss: 0.01660, val loss: 0.01725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 418, train loss: 0.01605, val loss: 0.01656\n",
      "Training epoch: 419, train loss: 0.01608, val loss: 0.01664\n",
      "Training epoch: 420, train loss: 0.01744, val loss: 0.01787\n",
      "Training epoch: 421, train loss: 0.01584, val loss: 0.01636\n",
      "Training epoch: 422, train loss: 0.01644, val loss: 0.01705\n",
      "Training epoch: 423, train loss: 0.01633, val loss: 0.01691\n",
      "Training epoch: 424, train loss: 0.01642, val loss: 0.01688\n",
      "Training epoch: 425, train loss: 0.01614, val loss: 0.01664\n",
      "Training epoch: 426, train loss: 0.01622, val loss: 0.01684\n",
      "Training epoch: 427, train loss: 0.01618, val loss: 0.01668\n",
      "Training epoch: 428, train loss: 0.01653, val loss: 0.01705\n",
      "Training epoch: 429, train loss: 0.01636, val loss: 0.01697\n",
      "Training epoch: 430, train loss: 0.01668, val loss: 0.01730\n",
      "Training epoch: 431, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 432, train loss: 0.01594, val loss: 0.01649\n",
      "Training epoch: 433, train loss: 0.01581, val loss: 0.01631\n",
      "Training epoch: 434, train loss: 0.01597, val loss: 0.01653\n",
      "Training epoch: 435, train loss: 0.01638, val loss: 0.01686\n",
      "Training epoch: 436, train loss: 0.01589, val loss: 0.01647\n",
      "Training epoch: 437, train loss: 0.01610, val loss: 0.01659\n",
      "Training epoch: 438, train loss: 0.01629, val loss: 0.01672\n",
      "Training epoch: 439, train loss: 0.01646, val loss: 0.01699\n",
      "Training epoch: 440, train loss: 0.01621, val loss: 0.01681\n",
      "Training epoch: 441, train loss: 0.01632, val loss: 0.01697\n",
      "Training epoch: 442, train loss: 0.01609, val loss: 0.01658\n",
      "Training epoch: 443, train loss: 0.01592, val loss: 0.01648\n",
      "Training epoch: 444, train loss: 0.01595, val loss: 0.01650\n",
      "Training epoch: 445, train loss: 0.01633, val loss: 0.01695\n",
      "Training epoch: 446, train loss: 0.01592, val loss: 0.01644\n",
      "Training epoch: 447, train loss: 0.01633, val loss: 0.01698\n",
      "Training epoch: 448, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 449, train loss: 0.01589, val loss: 0.01639\n",
      "Training epoch: 450, train loss: 0.01613, val loss: 0.01668\n",
      "Training epoch: 451, train loss: 0.01625, val loss: 0.01675\n",
      "Training epoch: 452, train loss: 0.01701, val loss: 0.01765\n",
      "Training epoch: 453, train loss: 0.01677, val loss: 0.01734\n",
      "Training epoch: 454, train loss: 0.01702, val loss: 0.01764\n",
      "Training epoch: 455, train loss: 0.01604, val loss: 0.01658\n",
      "Training epoch: 456, train loss: 0.01664, val loss: 0.01724\n",
      "Training epoch: 457, train loss: 0.01630, val loss: 0.01682\n",
      "Training epoch: 458, train loss: 0.01625, val loss: 0.01672\n",
      "Training epoch: 459, train loss: 0.01659, val loss: 0.01705\n",
      "Training epoch: 460, train loss: 0.01670, val loss: 0.01727\n",
      "Training epoch: 461, train loss: 0.01649, val loss: 0.01709\n",
      "Training epoch: 462, train loss: 0.01617, val loss: 0.01671\n",
      "Training epoch: 463, train loss: 0.01597, val loss: 0.01653\n",
      "Training epoch: 464, train loss: 0.01616, val loss: 0.01662\n",
      "Training epoch: 465, train loss: 0.01600, val loss: 0.01644\n",
      "Training epoch: 466, train loss: 0.01615, val loss: 0.01669\n",
      "Training epoch: 467, train loss: 0.01650, val loss: 0.01694\n",
      "Training epoch: 468, train loss: 0.01631, val loss: 0.01693\n",
      "Training epoch: 469, train loss: 0.01623, val loss: 0.01671\n",
      "Training epoch: 470, train loss: 0.01653, val loss: 0.01717\n",
      "Training epoch: 471, train loss: 0.01596, val loss: 0.01647\n",
      "Training epoch: 472, train loss: 0.01589, val loss: 0.01642\n",
      "Training epoch: 473, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 474, train loss: 0.01694, val loss: 0.01739\n",
      "Training epoch: 475, train loss: 0.01596, val loss: 0.01647\n",
      "Training epoch: 476, train loss: 0.01606, val loss: 0.01663\n",
      "Training epoch: 477, train loss: 0.01631, val loss: 0.01690\n",
      "Training epoch: 478, train loss: 0.01595, val loss: 0.01642\n",
      "Training epoch: 479, train loss: 0.01608, val loss: 0.01662\n",
      "Training epoch: 480, train loss: 0.01697, val loss: 0.01758\n",
      "Training epoch: 481, train loss: 0.01602, val loss: 0.01656\n",
      "Training epoch: 482, train loss: 0.01614, val loss: 0.01662\n",
      "Training epoch: 483, train loss: 0.01665, val loss: 0.01708\n",
      "Training epoch: 484, train loss: 0.01605, val loss: 0.01660\n",
      "Training epoch: 485, train loss: 0.01659, val loss: 0.01705\n",
      "Training epoch: 486, train loss: 0.01657, val loss: 0.01720\n",
      "Training epoch: 487, train loss: 0.01625, val loss: 0.01672\n",
      "Training epoch: 488, train loss: 0.01596, val loss: 0.01647\n",
      "Training epoch: 489, train loss: 0.01602, val loss: 0.01659\n",
      "Training epoch: 490, train loss: 0.01646, val loss: 0.01685\n",
      "Training epoch: 491, train loss: 0.01626, val loss: 0.01673\n",
      "Training epoch: 492, train loss: 0.01619, val loss: 0.01663\n",
      "Training epoch: 493, train loss: 0.01624, val loss: 0.01683\n",
      "Training epoch: 494, train loss: 0.01598, val loss: 0.01649\n",
      "Training epoch: 495, train loss: 0.01579, val loss: 0.01626\n",
      "Training epoch: 496, train loss: 0.01614, val loss: 0.01664\n",
      "Training epoch: 497, train loss: 0.01592, val loss: 0.01640\n",
      "Training epoch: 498, train loss: 0.01594, val loss: 0.01642\n",
      "Training epoch: 499, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 500, train loss: 0.01591, val loss: 0.01641\n",
      "Training epoch: 501, train loss: 0.01594, val loss: 0.01643\n",
      "Training epoch: 502, train loss: 0.01577, val loss: 0.01624\n",
      "Training epoch: 503, train loss: 0.01648, val loss: 0.01706\n",
      "Training epoch: 504, train loss: 0.01683, val loss: 0.01724\n",
      "Training epoch: 505, train loss: 0.01675, val loss: 0.01726\n",
      "Training epoch: 506, train loss: 0.01594, val loss: 0.01642\n",
      "Training epoch: 507, train loss: 0.01584, val loss: 0.01628\n",
      "Training epoch: 508, train loss: 0.01669, val loss: 0.01722\n",
      "Training epoch: 509, train loss: 0.01615, val loss: 0.01664\n",
      "Training epoch: 510, train loss: 0.01639, val loss: 0.01692\n",
      "Training epoch: 511, train loss: 0.01611, val loss: 0.01660\n",
      "Training epoch: 512, train loss: 0.01597, val loss: 0.01650\n",
      "Training epoch: 513, train loss: 0.01590, val loss: 0.01639\n",
      "Training epoch: 514, train loss: 0.01573, val loss: 0.01619\n",
      "Training epoch: 515, train loss: 0.01591, val loss: 0.01638\n",
      "Training epoch: 516, train loss: 0.01608, val loss: 0.01660\n",
      "Training epoch: 517, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 518, train loss: 0.01640, val loss: 0.01683\n",
      "Training epoch: 519, train loss: 0.01643, val loss: 0.01697\n",
      "Training epoch: 520, train loss: 0.01605, val loss: 0.01654\n",
      "Training epoch: 521, train loss: 0.01693, val loss: 0.01753\n",
      "Training epoch: 522, train loss: 0.01613, val loss: 0.01663\n",
      "Training epoch: 523, train loss: 0.01576, val loss: 0.01626\n",
      "Training epoch: 524, train loss: 0.01607, val loss: 0.01649\n",
      "Training epoch: 525, train loss: 0.01630, val loss: 0.01685\n",
      "Training epoch: 526, train loss: 0.01575, val loss: 0.01625\n",
      "Training epoch: 527, train loss: 0.01601, val loss: 0.01651\n",
      "Training epoch: 528, train loss: 0.01577, val loss: 0.01623\n",
      "Training epoch: 529, train loss: 0.01592, val loss: 0.01639\n",
      "Training epoch: 530, train loss: 0.01665, val loss: 0.01707\n",
      "Training epoch: 531, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 532, train loss: 0.01600, val loss: 0.01647\n",
      "Training epoch: 533, train loss: 0.01619, val loss: 0.01661\n",
      "Training epoch: 534, train loss: 0.01620, val loss: 0.01662\n",
      "Training epoch: 535, train loss: 0.01618, val loss: 0.01669\n",
      "Training epoch: 536, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 537, train loss: 0.01716, val loss: 0.01759\n",
      "Training epoch: 538, train loss: 0.01630, val loss: 0.01682\n",
      "Training epoch: 539, train loss: 0.01641, val loss: 0.01692\n",
      "Training epoch: 540, train loss: 0.01615, val loss: 0.01657\n",
      "Training epoch: 541, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 542, train loss: 0.01615, val loss: 0.01663\n",
      "Training epoch: 543, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 544, train loss: 0.01692, val loss: 0.01730\n",
      "Training epoch: 545, train loss: 0.01632, val loss: 0.01676\n",
      "Training epoch: 546, train loss: 0.01605, val loss: 0.01651\n",
      "Training epoch: 547, train loss: 0.01614, val loss: 0.01666\n",
      "Training epoch: 548, train loss: 0.01634, val loss: 0.01676\n",
      "Training epoch: 549, train loss: 0.01593, val loss: 0.01638\n",
      "Training epoch: 550, train loss: 0.01582, val loss: 0.01626\n",
      "Training epoch: 551, train loss: 0.01608, val loss: 0.01644\n",
      "Training epoch: 552, train loss: 0.01611, val loss: 0.01656\n",
      "Training epoch: 553, train loss: 0.01582, val loss: 0.01628\n",
      "Training epoch: 554, train loss: 0.01665, val loss: 0.01723\n",
      "Training epoch: 555, train loss: 0.01585, val loss: 0.01627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 556, train loss: 0.01610, val loss: 0.01653\n",
      "Training epoch: 557, train loss: 0.01628, val loss: 0.01680\n",
      "Training epoch: 558, train loss: 0.01613, val loss: 0.01652\n",
      "Training epoch: 559, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 560, train loss: 0.01647, val loss: 0.01699\n",
      "Training epoch: 561, train loss: 0.01634, val loss: 0.01688\n",
      "Training epoch: 562, train loss: 0.01634, val loss: 0.01673\n",
      "Training epoch: 563, train loss: 0.01599, val loss: 0.01648\n",
      "Training epoch: 564, train loss: 0.01612, val loss: 0.01653\n",
      "Training epoch: 565, train loss: 0.01621, val loss: 0.01667\n",
      "Training epoch: 566, train loss: 0.01768, val loss: 0.01825\n",
      "Training epoch: 567, train loss: 0.01624, val loss: 0.01668\n",
      "Training epoch: 568, train loss: 0.01631, val loss: 0.01678\n",
      "Training epoch: 569, train loss: 0.01629, val loss: 0.01682\n",
      "Training epoch: 570, train loss: 0.01628, val loss: 0.01670\n",
      "Training epoch: 571, train loss: 0.01631, val loss: 0.01674\n",
      "Training epoch: 572, train loss: 0.01612, val loss: 0.01660\n",
      "Training epoch: 573, train loss: 0.01588, val loss: 0.01639\n",
      "Training epoch: 574, train loss: 0.01686, val loss: 0.01741\n",
      "Training epoch: 575, train loss: 0.01613, val loss: 0.01662\n",
      "Training epoch: 576, train loss: 0.01596, val loss: 0.01635\n",
      "Training epoch: 577, train loss: 0.01612, val loss: 0.01651\n",
      "Training epoch: 578, train loss: 0.01593, val loss: 0.01643\n",
      "Training epoch: 579, train loss: 0.01592, val loss: 0.01637\n",
      "Training epoch: 580, train loss: 0.01665, val loss: 0.01709\n",
      "Training epoch: 581, train loss: 0.01619, val loss: 0.01669\n",
      "Training epoch: 582, train loss: 0.01633, val loss: 0.01675\n",
      "Training epoch: 583, train loss: 0.01634, val loss: 0.01687\n",
      "Training epoch: 584, train loss: 0.01594, val loss: 0.01641\n",
      "Training epoch: 585, train loss: 0.01650, val loss: 0.01700\n",
      "Training epoch: 586, train loss: 0.01646, val loss: 0.01698\n",
      "Training epoch: 587, train loss: 0.01616, val loss: 0.01658\n",
      "Training epoch: 588, train loss: 0.01615, val loss: 0.01655\n",
      "Training epoch: 589, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 590, train loss: 0.01798, val loss: 0.01860\n",
      "Training epoch: 591, train loss: 0.01626, val loss: 0.01663\n",
      "Training epoch: 592, train loss: 0.01628, val loss: 0.01670\n",
      "Training epoch: 593, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 594, train loss: 0.01594, val loss: 0.01643\n",
      "Training epoch: 595, train loss: 0.01615, val loss: 0.01657\n",
      "Training epoch: 596, train loss: 0.01589, val loss: 0.01639\n",
      "Training epoch: 597, train loss: 0.01605, val loss: 0.01645\n",
      "Training epoch: 598, train loss: 0.01598, val loss: 0.01646\n",
      "Training epoch: 599, train loss: 0.01720, val loss: 0.01775\n",
      "Training epoch: 600, train loss: 0.01607, val loss: 0.01655\n",
      "Training epoch: 601, train loss: 0.01632, val loss: 0.01673\n",
      "Training epoch: 602, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 603, train loss: 0.01599, val loss: 0.01646\n",
      "Training epoch: 604, train loss: 0.01573, val loss: 0.01616\n",
      "Training epoch: 605, train loss: 0.01591, val loss: 0.01633\n",
      "Training epoch: 606, train loss: 0.01615, val loss: 0.01665\n",
      "Training epoch: 607, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 608, train loss: 0.01591, val loss: 0.01640\n",
      "Training epoch: 609, train loss: 0.01630, val loss: 0.01678\n",
      "Training epoch: 610, train loss: 0.01664, val loss: 0.01717\n",
      "Training epoch: 611, train loss: 0.01607, val loss: 0.01656\n",
      "Training epoch: 612, train loss: 0.01644, val loss: 0.01682\n",
      "Training epoch: 613, train loss: 0.01598, val loss: 0.01643\n",
      "Training epoch: 614, train loss: 0.01625, val loss: 0.01666\n",
      "Training epoch: 615, train loss: 0.01585, val loss: 0.01631\n",
      "Training epoch: 616, train loss: 0.01624, val loss: 0.01672\n",
      "Training epoch: 617, train loss: 0.01606, val loss: 0.01645\n",
      "Training epoch: 618, train loss: 0.01639, val loss: 0.01685\n",
      "Training epoch: 619, train loss: 0.01613, val loss: 0.01664\n",
      "Training epoch: 620, train loss: 0.01684, val loss: 0.01735\n",
      "Training epoch: 621, train loss: 0.01653, val loss: 0.01689\n",
      "Training epoch: 622, train loss: 0.01594, val loss: 0.01637\n",
      "Training epoch: 623, train loss: 0.01592, val loss: 0.01637\n",
      "Training epoch: 624, train loss: 0.01620, val loss: 0.01669\n",
      "Training epoch: 625, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 626, train loss: 0.01599, val loss: 0.01642\n",
      "Training epoch: 627, train loss: 0.01625, val loss: 0.01673\n",
      "Training epoch: 628, train loss: 0.01600, val loss: 0.01644\n",
      "Training epoch: 629, train loss: 0.01606, val loss: 0.01658\n",
      "Training epoch: 630, train loss: 0.01607, val loss: 0.01650\n",
      "Training epoch: 631, train loss: 0.01613, val loss: 0.01660\n",
      "Training epoch: 632, train loss: 0.01682, val loss: 0.01737\n",
      "Training epoch: 633, train loss: 0.01630, val loss: 0.01670\n",
      "Training epoch: 634, train loss: 0.01580, val loss: 0.01622\n",
      "Training epoch: 635, train loss: 0.01660, val loss: 0.01699\n",
      "Training epoch: 636, train loss: 0.01622, val loss: 0.01674\n",
      "Training epoch: 637, train loss: 0.01666, val loss: 0.01706\n",
      "Training epoch: 638, train loss: 0.01633, val loss: 0.01684\n",
      "Training epoch: 639, train loss: 0.01615, val loss: 0.01660\n",
      "Training epoch: 640, train loss: 0.01632, val loss: 0.01668\n",
      "Training epoch: 641, train loss: 0.01615, val loss: 0.01666\n",
      "Training epoch: 642, train loss: 0.01586, val loss: 0.01631\n",
      "Training epoch: 643, train loss: 0.01601, val loss: 0.01649\n",
      "Training epoch: 644, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 645, train loss: 0.01596, val loss: 0.01648\n",
      "Training epoch: 646, train loss: 0.01600, val loss: 0.01643\n",
      "Training epoch: 647, train loss: 0.01599, val loss: 0.01647\n",
      "Training epoch: 648, train loss: 0.01583, val loss: 0.01627\n",
      "Training epoch: 649, train loss: 0.01636, val loss: 0.01688\n",
      "Training epoch: 650, train loss: 0.01765, val loss: 0.01827\n",
      "Training epoch: 651, train loss: 0.01601, val loss: 0.01640\n",
      "Training epoch: 652, train loss: 0.01615, val loss: 0.01661\n",
      "Training epoch: 653, train loss: 0.01607, val loss: 0.01657\n",
      "Training epoch: 654, train loss: 0.01608, val loss: 0.01654\n",
      "Training epoch: 655, train loss: 0.01654, val loss: 0.01694\n",
      "Training epoch: 656, train loss: 0.01624, val loss: 0.01663\n",
      "Training epoch: 657, train loss: 0.01583, val loss: 0.01632\n",
      "Training epoch: 658, train loss: 0.01601, val loss: 0.01650\n",
      "Training epoch: 659, train loss: 0.01577, val loss: 0.01619\n",
      "Training epoch: 660, train loss: 0.01613, val loss: 0.01655\n",
      "Training epoch: 661, train loss: 0.01651, val loss: 0.01705\n",
      "Training epoch: 662, train loss: 0.01608, val loss: 0.01656\n",
      "Training epoch: 663, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 664, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 665, train loss: 0.01611, val loss: 0.01651\n",
      "Training epoch: 666, train loss: 0.01607, val loss: 0.01655\n",
      "Training epoch: 667, train loss: 0.01705, val loss: 0.01763\n",
      "Training epoch: 668, train loss: 0.01614, val loss: 0.01658\n",
      "Training epoch: 669, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 670, train loss: 0.01693, val loss: 0.01750\n",
      "Training epoch: 671, train loss: 0.01686, val loss: 0.01738\n",
      "Training epoch: 672, train loss: 0.01619, val loss: 0.01663\n",
      "Training epoch: 673, train loss: 0.01606, val loss: 0.01647\n",
      "Training epoch: 674, train loss: 0.01596, val loss: 0.01646\n",
      "Training epoch: 675, train loss: 0.01589, val loss: 0.01635\n",
      "Training epoch: 676, train loss: 0.01646, val loss: 0.01683\n",
      "Training epoch: 677, train loss: 0.01615, val loss: 0.01663\n",
      "Training epoch: 678, train loss: 0.01597, val loss: 0.01640\n",
      "Training epoch: 679, train loss: 0.01600, val loss: 0.01640\n",
      "Training epoch: 680, train loss: 0.01611, val loss: 0.01658\n",
      "Training epoch: 681, train loss: 0.01591, val loss: 0.01636\n",
      "Training epoch: 682, train loss: 0.01604, val loss: 0.01655\n",
      "Training epoch: 683, train loss: 0.01628, val loss: 0.01680\n",
      "Training epoch: 684, train loss: 0.01636, val loss: 0.01684\n",
      "Training epoch: 685, train loss: 0.01585, val loss: 0.01629\n",
      "Training epoch: 686, train loss: 0.01581, val loss: 0.01623\n",
      "Training epoch: 687, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 688, train loss: 0.01761, val loss: 0.01809\n",
      "Training epoch: 689, train loss: 0.01631, val loss: 0.01680\n",
      "Training epoch: 690, train loss: 0.01642, val loss: 0.01678\n",
      "Training epoch: 691, train loss: 0.01598, val loss: 0.01638\n",
      "Training epoch: 692, train loss: 0.01586, val loss: 0.01634\n",
      "Training epoch: 693, train loss: 0.01617, val loss: 0.01665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 694, train loss: 0.01622, val loss: 0.01673\n",
      "Training epoch: 695, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 696, train loss: 0.01608, val loss: 0.01657\n",
      "Training epoch: 697, train loss: 0.01628, val loss: 0.01678\n",
      "Training epoch: 698, train loss: 0.01602, val loss: 0.01649\n",
      "Training epoch: 699, train loss: 0.01616, val loss: 0.01665\n",
      "Training epoch: 700, train loss: 0.01591, val loss: 0.01639\n",
      "Training epoch: 701, train loss: 0.01598, val loss: 0.01643\n",
      "Training epoch: 702, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 703, train loss: 0.01612, val loss: 0.01656\n",
      "Training epoch: 704, train loss: 0.01594, val loss: 0.01640\n",
      "Training epoch: 705, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 706, train loss: 0.01657, val loss: 0.01693\n",
      "Training epoch: 707, train loss: 0.01667, val loss: 0.01715\n",
      "Training epoch: 708, train loss: 0.01642, val loss: 0.01690\n",
      "Training epoch: 709, train loss: 0.01648, val loss: 0.01701\n",
      "Training epoch: 710, train loss: 0.01600, val loss: 0.01649\n",
      "Training epoch: 711, train loss: 0.01661, val loss: 0.01715\n",
      "Training epoch: 712, train loss: 0.01681, val loss: 0.01734\n",
      "Training epoch: 713, train loss: 0.01623, val loss: 0.01663\n",
      "Training epoch: 714, train loss: 0.01609, val loss: 0.01646\n",
      "Training epoch: 715, train loss: 0.01625, val loss: 0.01671\n",
      "Training epoch: 716, train loss: 0.01609, val loss: 0.01658\n",
      "Training epoch: 717, train loss: 0.01630, val loss: 0.01680\n",
      "Training epoch: 718, train loss: 0.01648, val loss: 0.01700\n",
      "Training epoch: 719, train loss: 0.01644, val loss: 0.01693\n",
      "Training epoch: 720, train loss: 0.01603, val loss: 0.01644\n",
      "Training epoch: 721, train loss: 0.01624, val loss: 0.01674\n",
      "Training epoch: 722, train loss: 0.01578, val loss: 0.01621\n",
      "Training epoch: 723, train loss: 0.01596, val loss: 0.01641\n",
      "Training epoch: 724, train loss: 0.01646, val loss: 0.01696\n",
      "Training epoch: 725, train loss: 0.01601, val loss: 0.01646\n",
      "Training epoch: 726, train loss: 0.01615, val loss: 0.01656\n",
      "Training epoch: 727, train loss: 0.01697, val loss: 0.01733\n",
      "Training epoch: 728, train loss: 0.01608, val loss: 0.01648\n",
      "Training epoch: 729, train loss: 0.01602, val loss: 0.01652\n",
      "Training epoch: 730, train loss: 0.01595, val loss: 0.01644\n",
      "Training epoch: 731, train loss: 0.01656, val loss: 0.01698\n",
      "Training epoch: 732, train loss: 0.01610, val loss: 0.01654\n",
      "Training epoch: 733, train loss: 0.01618, val loss: 0.01665\n",
      "Training epoch: 734, train loss: 0.01593, val loss: 0.01639\n",
      "Training epoch: 735, train loss: 0.01593, val loss: 0.01640\n",
      "Training epoch: 736, train loss: 0.01605, val loss: 0.01651\n",
      "Training epoch: 737, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 738, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 739, train loss: 0.01675, val loss: 0.01731\n",
      "Training epoch: 740, train loss: 0.01667, val loss: 0.01705\n",
      "Training epoch: 741, train loss: 0.01570, val loss: 0.01614\n",
      "Training epoch: 742, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 743, train loss: 0.01613, val loss: 0.01655\n",
      "Training epoch: 744, train loss: 0.01588, val loss: 0.01634\n",
      "Training epoch: 745, train loss: 0.01715, val loss: 0.01772\n",
      "Training epoch: 746, train loss: 0.01627, val loss: 0.01676\n",
      "Training epoch: 747, train loss: 0.01632, val loss: 0.01669\n",
      "Training epoch: 748, train loss: 0.01627, val loss: 0.01679\n",
      "Training epoch: 749, train loss: 0.01618, val loss: 0.01665\n",
      "Training epoch: 750, train loss: 0.01574, val loss: 0.01617\n",
      "Training epoch: 751, train loss: 0.01602, val loss: 0.01647\n",
      "Training epoch: 752, train loss: 0.01625, val loss: 0.01668\n",
      "Training epoch: 753, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 754, train loss: 0.01605, val loss: 0.01647\n",
      "Training epoch: 755, train loss: 0.01656, val loss: 0.01712\n",
      "Training epoch: 756, train loss: 0.01579, val loss: 0.01621\n",
      "Training epoch: 757, train loss: 0.01601, val loss: 0.01648\n",
      "Training epoch: 758, train loss: 0.01587, val loss: 0.01632\n",
      "Training epoch: 759, train loss: 0.01619, val loss: 0.01664\n",
      "Training epoch: 760, train loss: 0.01615, val loss: 0.01665\n",
      "Training epoch: 761, train loss: 0.01671, val loss: 0.01723\n",
      "Training epoch: 762, train loss: 0.01739, val loss: 0.01800\n",
      "Training epoch: 763, train loss: 0.01608, val loss: 0.01647\n",
      "Training epoch: 764, train loss: 0.01631, val loss: 0.01669\n",
      "Training epoch: 765, train loss: 0.01676, val loss: 0.01719\n",
      "Training epoch: 766, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 767, train loss: 0.01623, val loss: 0.01668\n",
      "Training epoch: 768, train loss: 0.01597, val loss: 0.01643\n",
      "Training epoch: 769, train loss: 0.01593, val loss: 0.01632\n",
      "Training epoch: 770, train loss: 0.01639, val loss: 0.01684\n",
      "Training epoch: 771, train loss: 0.01638, val loss: 0.01685\n",
      "Training epoch: 772, train loss: 0.01605, val loss: 0.01649\n",
      "Training epoch: 773, train loss: 0.01613, val loss: 0.01655\n",
      "Training epoch: 774, train loss: 0.01613, val loss: 0.01662\n",
      "Training epoch: 775, train loss: 0.01581, val loss: 0.01630\n",
      "Training epoch: 776, train loss: 0.01641, val loss: 0.01690\n",
      "Training epoch: 777, train loss: 0.01632, val loss: 0.01673\n",
      "Training epoch: 778, train loss: 0.01628, val loss: 0.01680\n",
      "Training epoch: 779, train loss: 0.01578, val loss: 0.01621\n",
      "Training epoch: 780, train loss: 0.01624, val loss: 0.01676\n",
      "Training epoch: 781, train loss: 0.01604, val loss: 0.01648\n",
      "Training epoch: 782, train loss: 0.01584, val loss: 0.01629\n",
      "Training epoch: 783, train loss: 0.01592, val loss: 0.01632\n",
      "Training epoch: 784, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 785, train loss: 0.01597, val loss: 0.01648\n",
      "Training epoch: 786, train loss: 0.01635, val loss: 0.01685\n",
      "Training epoch: 787, train loss: 0.01585, val loss: 0.01632\n",
      "Training epoch: 788, train loss: 0.01614, val loss: 0.01651\n",
      "Training epoch: 789, train loss: 0.01611, val loss: 0.01655\n",
      "Training epoch: 790, train loss: 0.01594, val loss: 0.01641\n",
      "Training epoch: 791, train loss: 0.01596, val loss: 0.01642\n",
      "Training epoch: 792, train loss: 0.01604, val loss: 0.01650\n",
      "Training epoch: 793, train loss: 0.01623, val loss: 0.01674\n",
      "Training epoch: 794, train loss: 0.01581, val loss: 0.01628\n",
      "Training epoch: 795, train loss: 0.01573, val loss: 0.01619\n",
      "Training epoch: 796, train loss: 0.01582, val loss: 0.01625\n",
      "Training epoch: 797, train loss: 0.01583, val loss: 0.01623\n",
      "Training epoch: 798, train loss: 0.01729, val loss: 0.01786\n",
      "Training epoch: 799, train loss: 0.01588, val loss: 0.01637\n",
      "Training epoch: 800, train loss: 0.01630, val loss: 0.01674\n",
      "Training epoch: 801, train loss: 0.01605, val loss: 0.01653\n",
      "Training epoch: 802, train loss: 0.01654, val loss: 0.01690\n",
      "Training epoch: 803, train loss: 0.01593, val loss: 0.01637\n",
      "Training epoch: 804, train loss: 0.01621, val loss: 0.01666\n",
      "Training epoch: 805, train loss: 0.01638, val loss: 0.01679\n",
      "Training epoch: 806, train loss: 0.01648, val loss: 0.01699\n",
      "Training epoch: 807, train loss: 0.01636, val loss: 0.01687\n",
      "Training epoch: 808, train loss: 0.01610, val loss: 0.01650\n",
      "Training epoch: 809, train loss: 0.01635, val loss: 0.01682\n",
      "Training epoch: 810, train loss: 0.01591, val loss: 0.01629\n",
      "Training epoch: 811, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 812, train loss: 0.01584, val loss: 0.01632\n",
      "Training epoch: 813, train loss: 0.01645, val loss: 0.01690\n",
      "Training epoch: 814, train loss: 0.01593, val loss: 0.01634\n",
      "Training epoch: 815, train loss: 0.01601, val loss: 0.01646\n",
      "Training epoch: 816, train loss: 0.01604, val loss: 0.01655\n",
      "Training epoch: 817, train loss: 0.01633, val loss: 0.01674\n",
      "Training epoch: 818, train loss: 0.01647, val loss: 0.01699\n",
      "Training epoch: 819, train loss: 0.01643, val loss: 0.01695\n",
      "Training epoch: 820, train loss: 0.01655, val loss: 0.01697\n",
      "Training epoch: 821, train loss: 0.01604, val loss: 0.01653\n",
      "Training epoch: 822, train loss: 0.01611, val loss: 0.01655\n",
      "Training epoch: 823, train loss: 0.01713, val loss: 0.01770\n",
      "Training epoch: 824, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 825, train loss: 0.01644, val loss: 0.01681\n",
      "Training epoch: 826, train loss: 0.01692, val loss: 0.01747\n",
      "Training epoch: 827, train loss: 0.01659, val loss: 0.01708\n",
      "Training epoch: 828, train loss: 0.01634, val loss: 0.01670\n",
      "Training epoch: 829, train loss: 0.01636, val loss: 0.01675\n",
      "Training epoch: 830, train loss: 0.01625, val loss: 0.01670\n",
      "Training epoch: 831, train loss: 0.01582, val loss: 0.01625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 832, train loss: 0.01599, val loss: 0.01645\n",
      "Training epoch: 833, train loss: 0.01608, val loss: 0.01660\n",
      "Training epoch: 834, train loss: 0.01577, val loss: 0.01621\n",
      "Training epoch: 835, train loss: 0.01608, val loss: 0.01658\n",
      "Training epoch: 836, train loss: 0.01611, val loss: 0.01655\n",
      "Training epoch: 837, train loss: 0.01624, val loss: 0.01662\n",
      "Training epoch: 838, train loss: 0.01622, val loss: 0.01664\n",
      "Training epoch: 839, train loss: 0.01590, val loss: 0.01636\n",
      "Training epoch: 840, train loss: 0.01617, val loss: 0.01666\n",
      "Training epoch: 841, train loss: 0.01621, val loss: 0.01662\n",
      "Training epoch: 842, train loss: 0.01590, val loss: 0.01635\n",
      "Training epoch: 843, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 844, train loss: 0.01616, val loss: 0.01667\n",
      "Training epoch: 845, train loss: 0.01579, val loss: 0.01618\n",
      "Training epoch: 846, train loss: 0.01590, val loss: 0.01631\n",
      "Training epoch: 847, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 848, train loss: 0.01645, val loss: 0.01697\n",
      "Training epoch: 849, train loss: 0.01608, val loss: 0.01659\n",
      "Training epoch: 850, train loss: 0.01581, val loss: 0.01626\n",
      "Training epoch: 851, train loss: 0.01713, val loss: 0.01756\n",
      "Training epoch: 852, train loss: 0.01600, val loss: 0.01639\n",
      "Training epoch: 853, train loss: 0.01602, val loss: 0.01651\n",
      "Training epoch: 854, train loss: 0.01589, val loss: 0.01633\n",
      "Training epoch: 855, train loss: 0.01618, val loss: 0.01670\n",
      "Training epoch: 856, train loss: 0.01639, val loss: 0.01680\n",
      "Training epoch: 857, train loss: 0.01616, val loss: 0.01657\n",
      "Training epoch: 858, train loss: 0.01605, val loss: 0.01654\n",
      "Training epoch: 859, train loss: 0.01602, val loss: 0.01653\n",
      "Training epoch: 860, train loss: 0.01585, val loss: 0.01628\n",
      "Training epoch: 861, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 862, train loss: 0.01601, val loss: 0.01650\n",
      "Training epoch: 863, train loss: 0.01603, val loss: 0.01651\n",
      "Training epoch: 864, train loss: 0.01604, val loss: 0.01656\n",
      "Training epoch: 865, train loss: 0.01607, val loss: 0.01653\n",
      "Training epoch: 866, train loss: 0.01626, val loss: 0.01676\n",
      "Training epoch: 867, train loss: 0.01596, val loss: 0.01640\n",
      "Training epoch: 868, train loss: 0.01588, val loss: 0.01634\n",
      "Training epoch: 869, train loss: 0.01612, val loss: 0.01657\n",
      "Training epoch: 870, train loss: 0.01599, val loss: 0.01645\n",
      "Training epoch: 871, train loss: 0.01589, val loss: 0.01628\n",
      "Training epoch: 872, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 873, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 874, train loss: 0.01578, val loss: 0.01623\n",
      "Training epoch: 875, train loss: 0.01613, val loss: 0.01660\n",
      "Training epoch: 876, train loss: 0.01603, val loss: 0.01650\n",
      "Training epoch: 877, train loss: 0.01571, val loss: 0.01614\n",
      "Training epoch: 878, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 879, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 880, train loss: 0.01628, val loss: 0.01682\n",
      "Training epoch: 881, train loss: 0.01590, val loss: 0.01628\n",
      "Training epoch: 882, train loss: 0.01635, val loss: 0.01681\n",
      "Training epoch: 883, train loss: 0.01599, val loss: 0.01646\n",
      "Training epoch: 884, train loss: 0.01639, val loss: 0.01689\n",
      "Training epoch: 885, train loss: 0.01619, val loss: 0.01664\n",
      "Training epoch: 886, train loss: 0.01623, val loss: 0.01674\n",
      "Training epoch: 887, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 888, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 889, train loss: 0.01624, val loss: 0.01667\n",
      "Training epoch: 890, train loss: 0.01603, val loss: 0.01651\n",
      "Training epoch: 891, train loss: 0.01614, val loss: 0.01661\n",
      "Training epoch: 892, train loss: 0.01623, val loss: 0.01659\n",
      "Training epoch: 893, train loss: 0.01646, val loss: 0.01692\n",
      "Training epoch: 894, train loss: 0.01593, val loss: 0.01639\n",
      "Training epoch: 895, train loss: 0.01607, val loss: 0.01646\n",
      "Training epoch: 896, train loss: 0.01647, val loss: 0.01702\n",
      "Training epoch: 897, train loss: 0.01637, val loss: 0.01690\n",
      "Training epoch: 898, train loss: 0.01661, val loss: 0.01696\n",
      "Training epoch: 899, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 900, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 901, train loss: 0.01610, val loss: 0.01656\n",
      "Training epoch: 902, train loss: 0.01620, val loss: 0.01664\n",
      "Training epoch: 903, train loss: 0.01700, val loss: 0.01740\n",
      "Training epoch: 904, train loss: 0.01602, val loss: 0.01649\n",
      "Training epoch: 905, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 906, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 907, train loss: 0.01588, val loss: 0.01635\n",
      "Training epoch: 908, train loss: 0.01597, val loss: 0.01645\n",
      "Training epoch: 909, train loss: 0.01592, val loss: 0.01634\n",
      "Training epoch: 910, train loss: 0.01767, val loss: 0.01801\n",
      "Training epoch: 911, train loss: 0.01639, val loss: 0.01686\n",
      "Training epoch: 912, train loss: 0.01628, val loss: 0.01672\n",
      "Training epoch: 913, train loss: 0.01635, val loss: 0.01679\n",
      "Training epoch: 914, train loss: 0.01586, val loss: 0.01633\n",
      "Training epoch: 915, train loss: 0.01584, val loss: 0.01631\n",
      "Training epoch: 916, train loss: 0.01592, val loss: 0.01639\n",
      "Training epoch: 917, train loss: 0.01648, val loss: 0.01699\n",
      "Training epoch: 918, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 919, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 920, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 921, train loss: 0.01634, val loss: 0.01670\n",
      "Training epoch: 922, train loss: 0.01594, val loss: 0.01642\n",
      "Training epoch: 923, train loss: 0.01593, val loss: 0.01640\n",
      "Training epoch: 924, train loss: 0.01617, val loss: 0.01665\n",
      "Training epoch: 925, train loss: 0.01630, val loss: 0.01679\n",
      "Training epoch: 926, train loss: 0.01654, val loss: 0.01690\n",
      "Training epoch: 927, train loss: 0.01647, val loss: 0.01686\n",
      "Training epoch: 928, train loss: 0.01612, val loss: 0.01652\n",
      "Training epoch: 929, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 930, train loss: 0.01639, val loss: 0.01681\n",
      "Training epoch: 931, train loss: 0.01600, val loss: 0.01643\n",
      "Training epoch: 932, train loss: 0.01623, val loss: 0.01665\n",
      "Training epoch: 933, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 934, train loss: 0.01583, val loss: 0.01631\n",
      "Training epoch: 935, train loss: 0.01603, val loss: 0.01649\n",
      "Training epoch: 936, train loss: 0.01593, val loss: 0.01637\n",
      "Training epoch: 937, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 938, train loss: 0.01618, val loss: 0.01662\n",
      "Training epoch: 939, train loss: 0.01627, val loss: 0.01675\n",
      "Training epoch: 940, train loss: 0.01625, val loss: 0.01665\n",
      "Training epoch: 941, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 942, train loss: 0.01645, val loss: 0.01685\n",
      "Training epoch: 943, train loss: 0.01587, val loss: 0.01630\n",
      "Training epoch: 944, train loss: 0.01632, val loss: 0.01681\n",
      "Training epoch: 945, train loss: 0.01599, val loss: 0.01645\n",
      "Training epoch: 946, train loss: 0.01605, val loss: 0.01657\n",
      "Training epoch: 947, train loss: 0.01615, val loss: 0.01664\n",
      "Training epoch: 948, train loss: 0.01599, val loss: 0.01648\n",
      "Training epoch: 949, train loss: 0.01600, val loss: 0.01651\n",
      "Training epoch: 950, train loss: 0.01593, val loss: 0.01637\n",
      "Training epoch: 951, train loss: 0.01609, val loss: 0.01659\n",
      "Training epoch: 952, train loss: 0.01581, val loss: 0.01621\n",
      "Training epoch: 953, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 954, train loss: 0.01596, val loss: 0.01635\n",
      "Training epoch: 955, train loss: 0.01627, val loss: 0.01671\n",
      "Training epoch: 956, train loss: 0.01596, val loss: 0.01642\n",
      "Training epoch: 957, train loss: 0.01608, val loss: 0.01653\n",
      "Training epoch: 958, train loss: 0.01605, val loss: 0.01646\n",
      "Training epoch: 959, train loss: 0.01607, val loss: 0.01645\n",
      "Training epoch: 960, train loss: 0.01651, val loss: 0.01691\n",
      "Training epoch: 961, train loss: 0.01591, val loss: 0.01640\n",
      "Training epoch: 962, train loss: 0.01592, val loss: 0.01639\n",
      "Training epoch: 963, train loss: 0.01588, val loss: 0.01632\n",
      "Training epoch: 964, train loss: 0.01587, val loss: 0.01627\n",
      "Training epoch: 965, train loss: 0.01574, val loss: 0.01619\n",
      "Training epoch: 966, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 967, train loss: 0.01600, val loss: 0.01643\n",
      "Training epoch: 968, train loss: 0.01592, val loss: 0.01634\n",
      "Training epoch: 969, train loss: 0.01619, val loss: 0.01665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 970, train loss: 0.01597, val loss: 0.01645\n",
      "Training epoch: 971, train loss: 0.01584, val loss: 0.01626\n",
      "Training epoch: 972, train loss: 0.01607, val loss: 0.01655\n",
      "Training epoch: 973, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 974, train loss: 0.01609, val loss: 0.01657\n",
      "Training epoch: 975, train loss: 0.01660, val loss: 0.01695\n",
      "Training epoch: 976, train loss: 0.01614, val loss: 0.01652\n",
      "Training epoch: 977, train loss: 0.01645, val loss: 0.01697\n",
      "Training epoch: 978, train loss: 0.01641, val loss: 0.01692\n",
      "Training epoch: 979, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 980, train loss: 0.01600, val loss: 0.01649\n",
      "Training epoch: 981, train loss: 0.01695, val loss: 0.01749\n",
      "Training epoch: 982, train loss: 0.01604, val loss: 0.01641\n",
      "Training epoch: 983, train loss: 0.01591, val loss: 0.01631\n",
      "Training epoch: 984, train loss: 0.01617, val loss: 0.01657\n",
      "Training epoch: 985, train loss: 0.01600, val loss: 0.01640\n",
      "Training epoch: 986, train loss: 0.01590, val loss: 0.01639\n",
      "Training epoch: 987, train loss: 0.01619, val loss: 0.01670\n",
      "Training epoch: 988, train loss: 0.01593, val loss: 0.01639\n",
      "Training epoch: 989, train loss: 0.01579, val loss: 0.01625\n",
      "Training epoch: 990, train loss: 0.01578, val loss: 0.01623\n",
      "Training epoch: 991, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 992, train loss: 0.01602, val loss: 0.01645\n",
      "Training epoch: 993, train loss: 0.01602, val loss: 0.01652\n",
      "Training epoch: 994, train loss: 0.01590, val loss: 0.01631\n",
      "Training epoch: 995, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 996, train loss: 0.01601, val loss: 0.01643\n",
      "Training epoch: 997, train loss: 0.01593, val loss: 0.01636\n",
      "Training epoch: 998, train loss: 0.01598, val loss: 0.01644\n",
      "Training epoch: 999, train loss: 0.01584, val loss: 0.01629\n",
      "Training epoch: 1000, train loss: 0.01695, val loss: 0.01751\n",
      "Training epoch: 1001, train loss: 0.01647, val loss: 0.01688\n",
      "Training epoch: 1002, train loss: 0.01616, val loss: 0.01655\n",
      "Training epoch: 1003, train loss: 0.01680, val loss: 0.01714\n",
      "Training epoch: 1004, train loss: 0.01632, val loss: 0.01681\n",
      "Training epoch: 1005, train loss: 0.01647, val loss: 0.01701\n",
      "Training epoch: 1006, train loss: 0.01776, val loss: 0.01839\n",
      "Training epoch: 1007, train loss: 0.01666, val loss: 0.01721\n",
      "Training epoch: 1008, train loss: 0.01580, val loss: 0.01623\n",
      "Training epoch: 1009, train loss: 0.01611, val loss: 0.01659\n",
      "Training epoch: 1010, train loss: 0.01632, val loss: 0.01683\n",
      "Training epoch: 1011, train loss: 0.01603, val loss: 0.01652\n",
      "Training epoch: 1012, train loss: 0.01600, val loss: 0.01642\n",
      "Training epoch: 1013, train loss: 0.01604, val loss: 0.01643\n",
      "Training epoch: 1014, train loss: 0.01622, val loss: 0.01670\n",
      "Training epoch: 1015, train loss: 0.01627, val loss: 0.01671\n",
      "Training epoch: 1016, train loss: 0.01615, val loss: 0.01659\n",
      "Training epoch: 1017, train loss: 0.01639, val loss: 0.01686\n",
      "Training epoch: 1018, train loss: 0.01588, val loss: 0.01631\n",
      "Training epoch: 1019, train loss: 0.01597, val loss: 0.01643\n",
      "Training epoch: 1020, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 1021, train loss: 0.01613, val loss: 0.01660\n",
      "Training epoch: 1022, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 1023, train loss: 0.01586, val loss: 0.01631\n",
      "Training epoch: 1024, train loss: 0.01598, val loss: 0.01647\n",
      "Training epoch: 1025, train loss: 0.01612, val loss: 0.01662\n",
      "Training epoch: 1026, train loss: 0.01583, val loss: 0.01628\n",
      "Training epoch: 1027, train loss: 0.01582, val loss: 0.01621\n",
      "Training epoch: 1028, train loss: 0.01600, val loss: 0.01649\n",
      "Training epoch: 1029, train loss: 0.01616, val loss: 0.01661\n",
      "Training epoch: 1030, train loss: 0.01634, val loss: 0.01682\n",
      "Training epoch: 1031, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 1032, train loss: 0.01618, val loss: 0.01659\n",
      "Training epoch: 1033, train loss: 0.01579, val loss: 0.01619\n",
      "Training epoch: 1034, train loss: 0.01586, val loss: 0.01625\n",
      "Training epoch: 1035, train loss: 0.01601, val loss: 0.01639\n",
      "Training epoch: 1036, train loss: 0.01635, val loss: 0.01683\n",
      "Training epoch: 1037, train loss: 0.01585, val loss: 0.01627\n",
      "Training epoch: 1038, train loss: 0.01620, val loss: 0.01666\n",
      "Training epoch: 1039, train loss: 0.01590, val loss: 0.01638\n",
      "Training epoch: 1040, train loss: 0.01611, val loss: 0.01652\n",
      "Training epoch: 1041, train loss: 0.01604, val loss: 0.01643\n",
      "Training epoch: 1042, train loss: 0.01810, val loss: 0.01842\n",
      "Training epoch: 1043, train loss: 0.01610, val loss: 0.01651\n",
      "Training epoch: 1044, train loss: 0.01625, val loss: 0.01667\n",
      "Training epoch: 1045, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 1046, train loss: 0.01601, val loss: 0.01650\n",
      "Training epoch: 1047, train loss: 0.01576, val loss: 0.01625\n",
      "Training epoch: 1048, train loss: 0.01605, val loss: 0.01652\n",
      "Training epoch: 1049, train loss: 0.01634, val loss: 0.01683\n",
      "Training epoch: 1050, train loss: 0.01626, val loss: 0.01678\n",
      "Training epoch: 1051, train loss: 0.01650, val loss: 0.01703\n",
      "Training epoch: 1052, train loss: 0.01596, val loss: 0.01635\n",
      "Training epoch: 1053, train loss: 0.01596, val loss: 0.01643\n",
      "Training epoch: 1054, train loss: 0.01627, val loss: 0.01677\n",
      "Training epoch: 1055, train loss: 0.01589, val loss: 0.01635\n",
      "Training epoch: 1056, train loss: 0.01650, val loss: 0.01702\n",
      "Training epoch: 1057, train loss: 0.01632, val loss: 0.01685\n",
      "Training epoch: 1058, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1059, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1060, train loss: 0.01587, val loss: 0.01635\n",
      "Training epoch: 1061, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 1062, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 1063, train loss: 0.01577, val loss: 0.01621\n",
      "Training epoch: 1064, train loss: 0.01597, val loss: 0.01647\n",
      "Training epoch: 1065, train loss: 0.01647, val loss: 0.01693\n",
      "Training epoch: 1066, train loss: 0.01607, val loss: 0.01654\n",
      "Training epoch: 1067, train loss: 0.01609, val loss: 0.01657\n",
      "Training epoch: 1068, train loss: 0.01619, val loss: 0.01666\n",
      "Training epoch: 1069, train loss: 0.01610, val loss: 0.01660\n",
      "Training epoch: 1070, train loss: 0.01598, val loss: 0.01637\n",
      "Training epoch: 1071, train loss: 0.01600, val loss: 0.01641\n",
      "Training epoch: 1072, train loss: 0.01649, val loss: 0.01687\n",
      "Training epoch: 1073, train loss: 0.01608, val loss: 0.01651\n",
      "Training epoch: 1074, train loss: 0.01602, val loss: 0.01643\n",
      "Training epoch: 1075, train loss: 0.01649, val loss: 0.01697\n",
      "Training epoch: 1076, train loss: 0.01673, val loss: 0.01720\n",
      "Training epoch: 1077, train loss: 0.01619, val loss: 0.01660\n",
      "Training epoch: 1078, train loss: 0.01582, val loss: 0.01628\n",
      "Training epoch: 1079, train loss: 0.01613, val loss: 0.01661\n",
      "Training epoch: 1080, train loss: 0.01582, val loss: 0.01628\n",
      "Training epoch: 1081, train loss: 0.01580, val loss: 0.01623\n",
      "Training epoch: 1082, train loss: 0.01586, val loss: 0.01631\n",
      "Training epoch: 1083, train loss: 0.01591, val loss: 0.01630\n",
      "Training epoch: 1084, train loss: 0.01581, val loss: 0.01624\n",
      "Training epoch: 1085, train loss: 0.01598, val loss: 0.01649\n",
      "Training epoch: 1086, train loss: 0.01607, val loss: 0.01654\n",
      "Training epoch: 1087, train loss: 0.01571, val loss: 0.01615\n",
      "Training epoch: 1088, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1089, train loss: 0.01585, val loss: 0.01630\n",
      "Training epoch: 1090, train loss: 0.01621, val loss: 0.01673\n",
      "Training epoch: 1091, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 1092, train loss: 0.01585, val loss: 0.01632\n",
      "Training epoch: 1093, train loss: 0.01637, val loss: 0.01692\n",
      "Training epoch: 1094, train loss: 0.01650, val loss: 0.01703\n",
      "Training epoch: 1095, train loss: 0.01607, val loss: 0.01650\n",
      "Training epoch: 1096, train loss: 0.01670, val loss: 0.01710\n",
      "Training epoch: 1097, train loss: 0.01608, val loss: 0.01656\n",
      "Training epoch: 1098, train loss: 0.01650, val loss: 0.01703\n",
      "Training epoch: 1099, train loss: 0.01592, val loss: 0.01638\n",
      "Training epoch: 1100, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1101, train loss: 0.01603, val loss: 0.01651\n",
      "Training epoch: 1102, train loss: 0.01646, val loss: 0.01682\n",
      "Training epoch: 1103, train loss: 0.01632, val loss: 0.01672\n",
      "Training epoch: 1104, train loss: 0.01676, val loss: 0.01720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1105, train loss: 0.01624, val loss: 0.01667\n",
      "Training epoch: 1106, train loss: 0.01608, val loss: 0.01651\n",
      "Training epoch: 1107, train loss: 0.01631, val loss: 0.01678\n",
      "Training epoch: 1108, train loss: 0.01590, val loss: 0.01636\n",
      "Training epoch: 1109, train loss: 0.01631, val loss: 0.01676\n",
      "Training epoch: 1110, train loss: 0.01605, val loss: 0.01648\n",
      "Training epoch: 1111, train loss: 0.01579, val loss: 0.01625\n",
      "Training epoch: 1112, train loss: 0.01593, val loss: 0.01639\n",
      "Training epoch: 1113, train loss: 0.01613, val loss: 0.01657\n",
      "Training epoch: 1114, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1115, train loss: 0.01618, val loss: 0.01667\n",
      "Training epoch: 1116, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1117, train loss: 0.01603, val loss: 0.01644\n",
      "Training epoch: 1118, train loss: 0.01598, val loss: 0.01645\n",
      "Training epoch: 1119, train loss: 0.01594, val loss: 0.01644\n",
      "Training epoch: 1120, train loss: 0.01576, val loss: 0.01621\n",
      "Training epoch: 1121, train loss: 0.01581, val loss: 0.01630\n",
      "Training epoch: 1122, train loss: 0.01599, val loss: 0.01648\n",
      "Training epoch: 1123, train loss: 0.01574, val loss: 0.01619\n",
      "Training epoch: 1124, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 1125, train loss: 0.01605, val loss: 0.01652\n",
      "Training epoch: 1126, train loss: 0.01587, val loss: 0.01632\n",
      "Training epoch: 1127, train loss: 0.01641, val loss: 0.01682\n",
      "Training epoch: 1128, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1129, train loss: 0.01634, val loss: 0.01684\n",
      "Training epoch: 1130, train loss: 0.01689, val loss: 0.01745\n",
      "Training epoch: 1131, train loss: 0.01657, val loss: 0.01711\n",
      "Training epoch: 1132, train loss: 0.01598, val loss: 0.01643\n",
      "Training epoch: 1133, train loss: 0.01611, val loss: 0.01655\n",
      "Training epoch: 1134, train loss: 0.01581, val loss: 0.01624\n",
      "Training epoch: 1135, train loss: 0.01607, val loss: 0.01652\n",
      "Training epoch: 1136, train loss: 0.01591, val loss: 0.01638\n",
      "Training epoch: 1137, train loss: 0.01576, val loss: 0.01620\n",
      "Training epoch: 1138, train loss: 0.01596, val loss: 0.01641\n",
      "Training epoch: 1139, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1140, train loss: 0.01593, val loss: 0.01641\n",
      "Training epoch: 1141, train loss: 0.01599, val loss: 0.01650\n",
      "Training epoch: 1142, train loss: 0.01606, val loss: 0.01643\n",
      "Training epoch: 1143, train loss: 0.01585, val loss: 0.01625\n",
      "Training epoch: 1144, train loss: 0.01672, val loss: 0.01709\n",
      "Training epoch: 1145, train loss: 0.01620, val loss: 0.01669\n",
      "Training epoch: 1146, train loss: 0.01594, val loss: 0.01640\n",
      "Training epoch: 1147, train loss: 0.01592, val loss: 0.01643\n",
      "Training epoch: 1148, train loss: 0.01614, val loss: 0.01663\n",
      "Training epoch: 1149, train loss: 0.01626, val loss: 0.01679\n",
      "Training epoch: 1150, train loss: 0.01612, val loss: 0.01664\n",
      "Training epoch: 1151, train loss: 0.01573, val loss: 0.01615\n",
      "Training epoch: 1152, train loss: 0.01586, val loss: 0.01633\n",
      "Training epoch: 1153, train loss: 0.01578, val loss: 0.01623\n",
      "Training epoch: 1154, train loss: 0.01589, val loss: 0.01636\n",
      "Training epoch: 1155, train loss: 0.01627, val loss: 0.01676\n",
      "Training epoch: 1156, train loss: 0.01583, val loss: 0.01629\n",
      "Training epoch: 1157, train loss: 0.01614, val loss: 0.01653\n",
      "Training epoch: 1158, train loss: 0.01592, val loss: 0.01641\n",
      "Training epoch: 1159, train loss: 0.01665, val loss: 0.01710\n",
      "Training epoch: 1160, train loss: 0.01612, val loss: 0.01662\n",
      "Training epoch: 1161, train loss: 0.01588, val loss: 0.01627\n",
      "Training epoch: 1162, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1163, train loss: 0.01707, val loss: 0.01762\n",
      "Training epoch: 1164, train loss: 0.01598, val loss: 0.01640\n",
      "Training epoch: 1165, train loss: 0.01658, val loss: 0.01704\n",
      "Training epoch: 1166, train loss: 0.01618, val loss: 0.01655\n",
      "Training epoch: 1167, train loss: 0.01630, val loss: 0.01667\n",
      "Training epoch: 1168, train loss: 0.01646, val loss: 0.01689\n",
      "Training epoch: 1169, train loss: 0.01802, val loss: 0.01838\n",
      "Training epoch: 1170, train loss: 0.01662, val loss: 0.01709\n",
      "Training epoch: 1171, train loss: 0.01592, val loss: 0.01630\n",
      "Training epoch: 1172, train loss: 0.01624, val loss: 0.01669\n",
      "Training epoch: 1173, train loss: 0.01608, val loss: 0.01654\n",
      "Training epoch: 1174, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 1175, train loss: 0.01588, val loss: 0.01627\n",
      "Training epoch: 1176, train loss: 0.01578, val loss: 0.01623\n",
      "Training epoch: 1177, train loss: 0.01582, val loss: 0.01624\n",
      "Training epoch: 1178, train loss: 0.01595, val loss: 0.01644\n",
      "Training epoch: 1179, train loss: 0.01620, val loss: 0.01659\n",
      "Training epoch: 1180, train loss: 0.01573, val loss: 0.01615\n",
      "Training epoch: 1181, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 1182, train loss: 0.01640, val loss: 0.01678\n",
      "Training epoch: 1183, train loss: 0.01581, val loss: 0.01627\n",
      "Training epoch: 1184, train loss: 0.01582, val loss: 0.01625\n",
      "Training epoch: 1185, train loss: 0.01623, val loss: 0.01677\n",
      "Training epoch: 1186, train loss: 0.01590, val loss: 0.01638\n",
      "Training epoch: 1187, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 1188, train loss: 0.01584, val loss: 0.01630\n",
      "Training epoch: 1189, train loss: 0.01582, val loss: 0.01626\n",
      "Training epoch: 1190, train loss: 0.01590, val loss: 0.01632\n",
      "Training epoch: 1191, train loss: 0.01600, val loss: 0.01642\n",
      "Training epoch: 1192, train loss: 0.01577, val loss: 0.01621\n",
      "Training epoch: 1193, train loss: 0.01615, val loss: 0.01657\n",
      "Training epoch: 1194, train loss: 0.01677, val loss: 0.01714\n",
      "Training epoch: 1195, train loss: 0.01590, val loss: 0.01627\n",
      "Training epoch: 1196, train loss: 0.01581, val loss: 0.01625\n",
      "Training epoch: 1197, train loss: 0.01629, val loss: 0.01679\n",
      "Training epoch: 1198, train loss: 0.01595, val loss: 0.01638\n",
      "Training epoch: 1199, train loss: 0.01661, val loss: 0.01698\n",
      "Training epoch: 1200, train loss: 0.01583, val loss: 0.01629\n",
      "Training epoch: 1201, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1202, train loss: 0.01587, val loss: 0.01626\n",
      "Training epoch: 1203, train loss: 0.01594, val loss: 0.01640\n",
      "Training epoch: 1204, train loss: 0.01628, val loss: 0.01664\n",
      "Training epoch: 1205, train loss: 0.01632, val loss: 0.01668\n",
      "Training epoch: 1206, train loss: 0.01577, val loss: 0.01620\n",
      "Training epoch: 1207, train loss: 0.01602, val loss: 0.01646\n",
      "Training epoch: 1208, train loss: 0.01676, val loss: 0.01727\n",
      "Training epoch: 1209, train loss: 0.01618, val loss: 0.01664\n",
      "Training epoch: 1210, train loss: 0.01600, val loss: 0.01647\n",
      "Training epoch: 1211, train loss: 0.01614, val loss: 0.01655\n",
      "Training epoch: 1212, train loss: 0.01585, val loss: 0.01629\n",
      "Training epoch: 1213, train loss: 0.01590, val loss: 0.01631\n",
      "Training epoch: 1214, train loss: 0.01584, val loss: 0.01628\n",
      "Training epoch: 1215, train loss: 0.01601, val loss: 0.01644\n",
      "Training epoch: 1216, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 1217, train loss: 0.01605, val loss: 0.01650\n",
      "Training epoch: 1218, train loss: 0.01598, val loss: 0.01642\n",
      "Training epoch: 1219, train loss: 0.01617, val loss: 0.01653\n",
      "Training epoch: 1220, train loss: 0.01586, val loss: 0.01626\n",
      "Training epoch: 1221, train loss: 0.01610, val loss: 0.01654\n",
      "Training epoch: 1222, train loss: 0.01582, val loss: 0.01625\n",
      "Training epoch: 1223, train loss: 0.01627, val loss: 0.01664\n",
      "Training epoch: 1224, train loss: 0.01605, val loss: 0.01650\n",
      "Training epoch: 1225, train loss: 0.01603, val loss: 0.01649\n",
      "Training epoch: 1226, train loss: 0.01625, val loss: 0.01671\n",
      "Training epoch: 1227, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1228, train loss: 0.01595, val loss: 0.01642\n",
      "Training epoch: 1229, train loss: 0.01645, val loss: 0.01696\n",
      "Training epoch: 1230, train loss: 0.01574, val loss: 0.01617\n",
      "Training epoch: 1231, train loss: 0.01608, val loss: 0.01658\n",
      "Training epoch: 1232, train loss: 0.01646, val loss: 0.01697\n",
      "Training epoch: 1233, train loss: 0.01608, val loss: 0.01644\n",
      "Training epoch: 1234, train loss: 0.01598, val loss: 0.01651\n",
      "Training epoch: 1235, train loss: 0.01613, val loss: 0.01649\n",
      "Training epoch: 1236, train loss: 0.01617, val loss: 0.01657\n",
      "Training epoch: 1237, train loss: 0.01608, val loss: 0.01651\n",
      "Training epoch: 1238, train loss: 0.01631, val loss: 0.01681\n",
      "Training epoch: 1239, train loss: 0.01585, val loss: 0.01632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1240, train loss: 0.01601, val loss: 0.01646\n",
      "Training epoch: 1241, train loss: 0.01674, val loss: 0.01707\n",
      "Training epoch: 1242, train loss: 0.01631, val loss: 0.01670\n",
      "Training epoch: 1243, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 1244, train loss: 0.01608, val loss: 0.01655\n",
      "Training epoch: 1245, train loss: 0.01628, val loss: 0.01680\n",
      "Training epoch: 1246, train loss: 0.01618, val loss: 0.01665\n",
      "Training epoch: 1247, train loss: 0.01646, val loss: 0.01681\n",
      "Training epoch: 1248, train loss: 0.01601, val loss: 0.01647\n",
      "Training epoch: 1249, train loss: 0.01603, val loss: 0.01649\n",
      "Training epoch: 1250, train loss: 0.01595, val loss: 0.01635\n",
      "Training epoch: 1251, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 1252, train loss: 0.01619, val loss: 0.01671\n",
      "Training epoch: 1253, train loss: 0.01579, val loss: 0.01623\n",
      "Training epoch: 1254, train loss: 0.01622, val loss: 0.01664\n",
      "Training epoch: 1255, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 1256, train loss: 0.01592, val loss: 0.01637\n",
      "Training epoch: 1257, train loss: 0.01574, val loss: 0.01615\n",
      "Training epoch: 1258, train loss: 0.01581, val loss: 0.01627\n",
      "Training epoch: 1259, train loss: 0.01622, val loss: 0.01658\n",
      "Training epoch: 1260, train loss: 0.01582, val loss: 0.01623\n",
      "Training epoch: 1261, train loss: 0.01600, val loss: 0.01639\n",
      "Training epoch: 1262, train loss: 0.01589, val loss: 0.01635\n",
      "Training epoch: 1263, train loss: 0.01619, val loss: 0.01662\n",
      "Training epoch: 1264, train loss: 0.01635, val loss: 0.01682\n",
      "Training epoch: 1265, train loss: 0.01637, val loss: 0.01688\n",
      "Training epoch: 1266, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 1267, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 1268, train loss: 0.01595, val loss: 0.01645\n",
      "Training epoch: 1269, train loss: 0.01585, val loss: 0.01631\n",
      "Training epoch: 1270, train loss: 0.01621, val loss: 0.01667\n",
      "Training epoch: 1271, train loss: 0.01643, val loss: 0.01693\n",
      "Training epoch: 1272, train loss: 0.01583, val loss: 0.01623\n",
      "Training epoch: 1273, train loss: 0.01573, val loss: 0.01615\n",
      "Training epoch: 1274, train loss: 0.01586, val loss: 0.01631\n",
      "Training epoch: 1275, train loss: 0.01601, val loss: 0.01641\n",
      "Training epoch: 1276, train loss: 0.01592, val loss: 0.01632\n",
      "Training epoch: 1277, train loss: 0.01604, val loss: 0.01652\n",
      "Training epoch: 1278, train loss: 0.01587, val loss: 0.01632\n",
      "Training epoch: 1279, train loss: 0.01596, val loss: 0.01642\n",
      "Training epoch: 1280, train loss: 0.01634, val loss: 0.01670\n",
      "Training epoch: 1281, train loss: 0.01605, val loss: 0.01643\n",
      "Training epoch: 1282, train loss: 0.01623, val loss: 0.01671\n",
      "Training epoch: 1283, train loss: 0.01581, val loss: 0.01624\n",
      "Training epoch: 1284, train loss: 0.01637, val loss: 0.01685\n",
      "Training epoch: 1285, train loss: 0.01597, val loss: 0.01645\n",
      "Training epoch: 1286, train loss: 0.01604, val loss: 0.01651\n",
      "Training epoch: 1287, train loss: 0.01582, val loss: 0.01627\n",
      "Training epoch: 1288, train loss: 0.01579, val loss: 0.01621\n",
      "Training epoch: 1289, train loss: 0.01576, val loss: 0.01619\n",
      "Training epoch: 1290, train loss: 0.01636, val loss: 0.01685\n",
      "Training epoch: 1291, train loss: 0.01605, val loss: 0.01640\n",
      "Training epoch: 1292, train loss: 0.01599, val loss: 0.01643\n",
      "Training epoch: 1293, train loss: 0.01606, val loss: 0.01648\n",
      "Training epoch: 1294, train loss: 0.01599, val loss: 0.01646\n",
      "Training epoch: 1295, train loss: 0.01586, val loss: 0.01625\n",
      "Training epoch: 1296, train loss: 0.01593, val loss: 0.01642\n",
      "Training epoch: 1297, train loss: 0.01644, val loss: 0.01688\n",
      "Training epoch: 1298, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1299, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1300, train loss: 0.01597, val loss: 0.01643\n",
      "Training epoch: 1301, train loss: 0.01627, val loss: 0.01677\n",
      "Training epoch: 1302, train loss: 0.01574, val loss: 0.01616\n",
      "Training epoch: 1303, train loss: 0.01596, val loss: 0.01637\n",
      "Training epoch: 1304, train loss: 0.01609, val loss: 0.01658\n",
      "Training epoch: 1305, train loss: 0.01599, val loss: 0.01647\n",
      "Training epoch: 1306, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1307, train loss: 0.01602, val loss: 0.01647\n",
      "Training epoch: 1308, train loss: 0.01595, val loss: 0.01638\n",
      "Training epoch: 1309, train loss: 0.01579, val loss: 0.01619\n",
      "Training epoch: 1310, train loss: 0.01598, val loss: 0.01644\n",
      "Training epoch: 1311, train loss: 0.01591, val loss: 0.01637\n",
      "Training epoch: 1312, train loss: 0.01603, val loss: 0.01646\n",
      "Training epoch: 1313, train loss: 0.01655, val loss: 0.01706\n",
      "Training epoch: 1314, train loss: 0.01618, val loss: 0.01667\n",
      "Training epoch: 1315, train loss: 0.01578, val loss: 0.01620\n",
      "Training epoch: 1316, train loss: 0.01569, val loss: 0.01612\n",
      "Training epoch: 1317, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 1318, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 1319, train loss: 0.01598, val loss: 0.01644\n",
      "Training epoch: 1320, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 1321, train loss: 0.01604, val loss: 0.01652\n",
      "Training epoch: 1322, train loss: 0.01595, val loss: 0.01640\n",
      "Training epoch: 1323, train loss: 0.01619, val loss: 0.01656\n",
      "Training epoch: 1324, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1325, train loss: 0.01621, val loss: 0.01671\n",
      "Training epoch: 1326, train loss: 0.01590, val loss: 0.01637\n",
      "Training epoch: 1327, train loss: 0.01622, val loss: 0.01669\n",
      "Training epoch: 1328, train loss: 0.01617, val loss: 0.01664\n",
      "Training epoch: 1329, train loss: 0.01621, val loss: 0.01657\n",
      "Training epoch: 1330, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 1331, train loss: 0.01604, val loss: 0.01641\n",
      "Training epoch: 1332, train loss: 0.01627, val loss: 0.01673\n",
      "Training epoch: 1333, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1334, train loss: 0.01596, val loss: 0.01641\n",
      "Training epoch: 1335, train loss: 0.01606, val loss: 0.01645\n",
      "Training epoch: 1336, train loss: 0.01598, val loss: 0.01634\n",
      "Training epoch: 1337, train loss: 0.01572, val loss: 0.01615\n",
      "Training epoch: 1338, train loss: 0.01593, val loss: 0.01636\n",
      "Training epoch: 1339, train loss: 0.01610, val loss: 0.01655\n",
      "Training epoch: 1340, train loss: 0.01592, val loss: 0.01635\n",
      "Training epoch: 1341, train loss: 0.01585, val loss: 0.01622\n",
      "Training epoch: 1342, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 1343, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 1344, train loss: 0.01588, val loss: 0.01632\n",
      "Training epoch: 1345, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 1346, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 1347, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1348, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 1349, train loss: 0.01588, val loss: 0.01625\n",
      "Training epoch: 1350, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 1351, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1352, train loss: 0.01597, val loss: 0.01643\n",
      "Training epoch: 1353, train loss: 0.01699, val loss: 0.01751\n",
      "Training epoch: 1354, train loss: 0.01736, val loss: 0.01790\n",
      "Training epoch: 1355, train loss: 0.01626, val loss: 0.01674\n",
      "Training epoch: 1356, train loss: 0.01633, val loss: 0.01680\n",
      "Training epoch: 1357, train loss: 0.01629, val loss: 0.01673\n",
      "Training epoch: 1358, train loss: 0.01622, val loss: 0.01669\n",
      "Training epoch: 1359, train loss: 0.01612, val loss: 0.01653\n",
      "Training epoch: 1360, train loss: 0.01585, val loss: 0.01630\n",
      "Training epoch: 1361, train loss: 0.01611, val loss: 0.01655\n",
      "Training epoch: 1362, train loss: 0.01607, val loss: 0.01649\n",
      "Training epoch: 1363, train loss: 0.01595, val loss: 0.01636\n",
      "Training epoch: 1364, train loss: 0.01660, val loss: 0.01712\n",
      "Training epoch: 1365, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 1366, train loss: 0.01596, val loss: 0.01642\n",
      "Training epoch: 1367, train loss: 0.01608, val loss: 0.01655\n",
      "Training epoch: 1368, train loss: 0.01602, val loss: 0.01644\n",
      "Training epoch: 1369, train loss: 0.01592, val loss: 0.01630\n",
      "Training epoch: 1370, train loss: 0.01584, val loss: 0.01624\n",
      "Training epoch: 1371, train loss: 0.01612, val loss: 0.01654\n",
      "Training epoch: 1372, train loss: 0.01589, val loss: 0.01628\n",
      "Training epoch: 1373, train loss: 0.01617, val loss: 0.01658\n",
      "Training epoch: 1374, train loss: 0.01623, val loss: 0.01671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1375, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 1376, train loss: 0.01586, val loss: 0.01628\n",
      "Training epoch: 1377, train loss: 0.01571, val loss: 0.01611\n",
      "Training epoch: 1378, train loss: 0.01629, val loss: 0.01680\n",
      "Training epoch: 1379, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 1380, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 1381, train loss: 0.01576, val loss: 0.01614\n",
      "Training epoch: 1382, train loss: 0.01593, val loss: 0.01638\n",
      "Training epoch: 1383, train loss: 0.01647, val loss: 0.01698\n",
      "Training epoch: 1384, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 1385, train loss: 0.01648, val loss: 0.01698\n",
      "Training epoch: 1386, train loss: 0.01621, val loss: 0.01671\n",
      "Training epoch: 1387, train loss: 0.01597, val loss: 0.01639\n",
      "Training epoch: 1388, train loss: 0.01610, val loss: 0.01648\n",
      "Training epoch: 1389, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 1390, train loss: 0.01632, val loss: 0.01681\n",
      "Training epoch: 1391, train loss: 0.01596, val loss: 0.01644\n",
      "Training epoch: 1392, train loss: 0.01615, val loss: 0.01663\n",
      "Training epoch: 1393, train loss: 0.01632, val loss: 0.01681\n",
      "Training epoch: 1394, train loss: 0.01602, val loss: 0.01646\n",
      "Training epoch: 1395, train loss: 0.01601, val loss: 0.01648\n",
      "Training epoch: 1396, train loss: 0.01619, val loss: 0.01667\n",
      "Training epoch: 1397, train loss: 0.01617, val loss: 0.01666\n",
      "Training epoch: 1398, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1399, train loss: 0.01633, val loss: 0.01684\n",
      "Training epoch: 1400, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 1401, train loss: 0.01593, val loss: 0.01636\n",
      "Training epoch: 1402, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 1403, train loss: 0.01580, val loss: 0.01623\n",
      "Training epoch: 1404, train loss: 0.01583, val loss: 0.01628\n",
      "Training epoch: 1405, train loss: 0.01589, val loss: 0.01628\n",
      "Training epoch: 1406, train loss: 0.01600, val loss: 0.01643\n",
      "Training epoch: 1407, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 1408, train loss: 0.01660, val loss: 0.01707\n",
      "Training epoch: 1409, train loss: 0.01630, val loss: 0.01665\n",
      "Training epoch: 1410, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 1411, train loss: 0.01607, val loss: 0.01645\n",
      "Training epoch: 1412, train loss: 0.01574, val loss: 0.01618\n",
      "Training epoch: 1413, train loss: 0.01625, val loss: 0.01675\n",
      "Training epoch: 1414, train loss: 0.01604, val loss: 0.01651\n",
      "Training epoch: 1415, train loss: 0.01614, val loss: 0.01660\n",
      "Training epoch: 1416, train loss: 0.01616, val loss: 0.01652\n",
      "Training epoch: 1417, train loss: 0.01616, val loss: 0.01660\n",
      "Training epoch: 1418, train loss: 0.01668, val loss: 0.01713\n",
      "Training epoch: 1419, train loss: 0.01585, val loss: 0.01627\n",
      "Training epoch: 1420, train loss: 0.01608, val loss: 0.01656\n",
      "Training epoch: 1421, train loss: 0.01595, val loss: 0.01633\n",
      "Training epoch: 1422, train loss: 0.01581, val loss: 0.01625\n",
      "Training epoch: 1423, train loss: 0.01582, val loss: 0.01630\n",
      "Training epoch: 1424, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 1425, train loss: 0.01602, val loss: 0.01646\n",
      "Training epoch: 1426, train loss: 0.01616, val loss: 0.01657\n",
      "Training epoch: 1427, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 1428, train loss: 0.01666, val loss: 0.01706\n",
      "Training epoch: 1429, train loss: 0.01645, val loss: 0.01688\n",
      "Training epoch: 1430, train loss: 0.01643, val loss: 0.01692\n",
      "Training epoch: 1431, train loss: 0.01632, val loss: 0.01675\n",
      "Training epoch: 1432, train loss: 0.01581, val loss: 0.01626\n",
      "Training epoch: 1433, train loss: 0.01611, val loss: 0.01654\n",
      "Training epoch: 1434, train loss: 0.01575, val loss: 0.01616\n",
      "Training epoch: 1435, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 1436, train loss: 0.01651, val loss: 0.01701\n",
      "Training epoch: 1437, train loss: 0.01622, val loss: 0.01671\n",
      "Training epoch: 1438, train loss: 0.01606, val loss: 0.01651\n",
      "Training epoch: 1439, train loss: 0.01599, val loss: 0.01642\n",
      "Training epoch: 1440, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1441, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1442, train loss: 0.01581, val loss: 0.01621\n",
      "Training epoch: 1443, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 1444, train loss: 0.01633, val loss: 0.01680\n",
      "Training epoch: 1445, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 1446, train loss: 0.01632, val loss: 0.01669\n",
      "Training epoch: 1447, train loss: 0.01615, val loss: 0.01655\n",
      "Training epoch: 1448, train loss: 0.01644, val loss: 0.01684\n",
      "Training epoch: 1449, train loss: 0.01601, val loss: 0.01647\n",
      "Training epoch: 1450, train loss: 0.01747, val loss: 0.01801\n",
      "Training epoch: 1451, train loss: 0.01675, val loss: 0.01725\n",
      "Training epoch: 1452, train loss: 0.01604, val loss: 0.01645\n",
      "Training epoch: 1453, train loss: 0.01590, val loss: 0.01630\n",
      "Training epoch: 1454, train loss: 0.01624, val loss: 0.01665\n",
      "Training epoch: 1455, train loss: 0.01610, val loss: 0.01644\n",
      "Training epoch: 1456, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 1457, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 1458, train loss: 0.01594, val loss: 0.01637\n",
      "Training epoch: 1459, train loss: 0.01670, val loss: 0.01714\n",
      "Training epoch: 1460, train loss: 0.01646, val loss: 0.01694\n",
      "Training epoch: 1461, train loss: 0.01613, val loss: 0.01657\n",
      "Training epoch: 1462, train loss: 0.01603, val loss: 0.01646\n",
      "Training epoch: 1463, train loss: 0.01608, val loss: 0.01655\n",
      "Training epoch: 1464, train loss: 0.01654, val loss: 0.01689\n",
      "Training epoch: 1465, train loss: 0.01636, val loss: 0.01685\n",
      "Training epoch: 1466, train loss: 0.01682, val loss: 0.01734\n",
      "Training epoch: 1467, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1468, train loss: 0.01587, val loss: 0.01630\n",
      "Training epoch: 1469, train loss: 0.01607, val loss: 0.01641\n",
      "Training epoch: 1470, train loss: 0.01590, val loss: 0.01632\n",
      "Training epoch: 1471, train loss: 0.01581, val loss: 0.01623\n",
      "Training epoch: 1472, train loss: 0.01615, val loss: 0.01664\n",
      "Training epoch: 1473, train loss: 0.01594, val loss: 0.01641\n",
      "Training epoch: 1474, train loss: 0.01678, val loss: 0.01727\n",
      "Training epoch: 1475, train loss: 0.01671, val loss: 0.01723\n",
      "Training epoch: 1476, train loss: 0.01585, val loss: 0.01622\n",
      "Training epoch: 1477, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 1478, train loss: 0.01578, val loss: 0.01616\n",
      "Training epoch: 1479, train loss: 0.01609, val loss: 0.01648\n",
      "Training epoch: 1480, train loss: 0.01610, val loss: 0.01655\n",
      "Training epoch: 1481, train loss: 0.01644, val loss: 0.01694\n",
      "Training epoch: 1482, train loss: 0.01590, val loss: 0.01635\n",
      "Training epoch: 1483, train loss: 0.01582, val loss: 0.01622\n",
      "Training epoch: 1484, train loss: 0.01605, val loss: 0.01645\n",
      "Training epoch: 1485, train loss: 0.01585, val loss: 0.01624\n",
      "Training epoch: 1486, train loss: 0.01681, val loss: 0.01733\n",
      "Training epoch: 1487, train loss: 0.01607, val loss: 0.01654\n",
      "Training epoch: 1488, train loss: 0.01618, val loss: 0.01665\n",
      "Training epoch: 1489, train loss: 0.01671, val loss: 0.01725\n",
      "Training epoch: 1490, train loss: 0.01589, val loss: 0.01628\n",
      "Training epoch: 1491, train loss: 0.01627, val loss: 0.01669\n",
      "Training epoch: 1492, train loss: 0.01577, val loss: 0.01618\n",
      "Training epoch: 1493, train loss: 0.01606, val loss: 0.01650\n",
      "Training epoch: 1494, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 1495, train loss: 0.01625, val loss: 0.01668\n",
      "Training epoch: 1496, train loss: 0.01615, val loss: 0.01662\n",
      "Training epoch: 1497, train loss: 0.01619, val loss: 0.01663\n",
      "Training epoch: 1498, train loss: 0.01624, val loss: 0.01661\n",
      "Training epoch: 1499, train loss: 0.01616, val loss: 0.01663\n",
      "Training epoch: 1500, train loss: 0.01630, val loss: 0.01680\n",
      "Training epoch: 1501, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 1502, train loss: 0.01613, val loss: 0.01658\n",
      "Training epoch: 1503, train loss: 0.01590, val loss: 0.01634\n",
      "Training epoch: 1504, train loss: 0.01599, val loss: 0.01638\n",
      "Training epoch: 1505, train loss: 0.01610, val loss: 0.01654\n",
      "Training epoch: 1506, train loss: 0.01599, val loss: 0.01645\n",
      "Training epoch: 1507, train loss: 0.01612, val loss: 0.01656\n",
      "Training epoch: 1508, train loss: 0.01587, val loss: 0.01628\n",
      "Training epoch: 1509, train loss: 0.01586, val loss: 0.01629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1510, train loss: 0.01587, val loss: 0.01627\n",
      "Training epoch: 1511, train loss: 0.01590, val loss: 0.01634\n",
      "Training epoch: 1512, train loss: 0.01626, val loss: 0.01668\n",
      "Training epoch: 1513, train loss: 0.01588, val loss: 0.01625\n",
      "Training epoch: 1514, train loss: 0.01604, val loss: 0.01649\n",
      "Training epoch: 1515, train loss: 0.01640, val loss: 0.01690\n",
      "Training epoch: 1516, train loss: 0.01594, val loss: 0.01637\n",
      "Training epoch: 1517, train loss: 0.01587, val loss: 0.01625\n",
      "Training epoch: 1518, train loss: 0.01635, val loss: 0.01681\n",
      "Training epoch: 1519, train loss: 0.01602, val loss: 0.01647\n",
      "Training epoch: 1520, train loss: 0.01601, val loss: 0.01639\n",
      "Training epoch: 1521, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 1522, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 1523, train loss: 0.01626, val loss: 0.01659\n",
      "Training epoch: 1524, train loss: 0.01580, val loss: 0.01620\n",
      "Training epoch: 1525, train loss: 0.01632, val loss: 0.01673\n",
      "Training epoch: 1526, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 1527, train loss: 0.01658, val loss: 0.01698\n",
      "Training epoch: 1528, train loss: 0.01598, val loss: 0.01641\n",
      "Training epoch: 1529, train loss: 0.01630, val loss: 0.01675\n",
      "Training epoch: 1530, train loss: 0.01588, val loss: 0.01630\n",
      "Training epoch: 1531, train loss: 0.01599, val loss: 0.01640\n",
      "Training epoch: 1532, train loss: 0.01621, val loss: 0.01663\n",
      "Training epoch: 1533, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 1534, train loss: 0.01643, val loss: 0.01691\n",
      "Training epoch: 1535, train loss: 0.01611, val loss: 0.01650\n",
      "Training epoch: 1536, train loss: 0.01601, val loss: 0.01648\n",
      "Training epoch: 1537, train loss: 0.01587, val loss: 0.01624\n",
      "Training epoch: 1538, train loss: 0.01617, val loss: 0.01662\n",
      "Training epoch: 1539, train loss: 0.01623, val loss: 0.01657\n",
      "Training epoch: 1540, train loss: 0.01604, val loss: 0.01649\n",
      "Training epoch: 1541, train loss: 0.01655, val loss: 0.01706\n",
      "Training epoch: 1542, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 1543, train loss: 0.01586, val loss: 0.01623\n",
      "Training epoch: 1544, train loss: 0.01620, val loss: 0.01662\n",
      "Training epoch: 1545, train loss: 0.01622, val loss: 0.01673\n",
      "Training epoch: 1546, train loss: 0.01807, val loss: 0.01862\n",
      "Training epoch: 1547, train loss: 0.01599, val loss: 0.01641\n",
      "Training epoch: 1548, train loss: 0.01592, val loss: 0.01635\n",
      "Training epoch: 1549, train loss: 0.01609, val loss: 0.01658\n",
      "Training epoch: 1550, train loss: 0.01608, val loss: 0.01653\n",
      "Training epoch: 1551, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 1552, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 1553, train loss: 0.01614, val loss: 0.01650\n",
      "Training epoch: 1554, train loss: 0.01602, val loss: 0.01643\n",
      "Training epoch: 1555, train loss: 0.01601, val loss: 0.01637\n",
      "Training epoch: 1556, train loss: 0.01618, val loss: 0.01659\n",
      "Training epoch: 1557, train loss: 0.01606, val loss: 0.01639\n",
      "Training epoch: 1558, train loss: 0.01637, val loss: 0.01671\n",
      "Training epoch: 1559, train loss: 0.01591, val loss: 0.01631\n",
      "Training epoch: 1560, train loss: 0.01584, val loss: 0.01630\n",
      "Training epoch: 1561, train loss: 0.01638, val loss: 0.01688\n",
      "Training epoch: 1562, train loss: 0.01614, val loss: 0.01657\n",
      "Training epoch: 1563, train loss: 0.01717, val loss: 0.01773\n",
      "Training epoch: 1564, train loss: 0.01628, val loss: 0.01675\n",
      "Training epoch: 1565, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1566, train loss: 0.01582, val loss: 0.01623\n",
      "Training epoch: 1567, train loss: 0.01587, val loss: 0.01625\n",
      "Training epoch: 1568, train loss: 0.01619, val loss: 0.01657\n",
      "Training epoch: 1569, train loss: 0.01611, val loss: 0.01654\n",
      "Training epoch: 1570, train loss: 0.01604, val loss: 0.01643\n",
      "Training epoch: 1571, train loss: 0.01608, val loss: 0.01649\n",
      "Training epoch: 1572, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 1573, train loss: 0.01585, val loss: 0.01628\n",
      "Training epoch: 1574, train loss: 0.01578, val loss: 0.01620\n",
      "Training epoch: 1575, train loss: 0.01611, val loss: 0.01656\n",
      "Training epoch: 1576, train loss: 0.01671, val loss: 0.01723\n",
      "Training epoch: 1577, train loss: 0.01600, val loss: 0.01644\n",
      "Training epoch: 1578, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 1579, train loss: 0.01598, val loss: 0.01643\n",
      "Training epoch: 1580, train loss: 0.01612, val loss: 0.01656\n",
      "Training epoch: 1581, train loss: 0.01611, val loss: 0.01652\n",
      "Training epoch: 1582, train loss: 0.01594, val loss: 0.01634\n",
      "Training epoch: 1583, train loss: 0.01590, val loss: 0.01632\n",
      "Training epoch: 1584, train loss: 0.01635, val loss: 0.01678\n",
      "Training epoch: 1585, train loss: 0.01626, val loss: 0.01674\n",
      "Training epoch: 1586, train loss: 0.01607, val loss: 0.01648\n",
      "Training epoch: 1587, train loss: 0.01605, val loss: 0.01650\n",
      "Training epoch: 1588, train loss: 0.01585, val loss: 0.01629\n",
      "Training epoch: 1589, train loss: 0.01584, val loss: 0.01625\n",
      "Training epoch: 1590, train loss: 0.01598, val loss: 0.01641\n",
      "Training epoch: 1591, train loss: 0.01641, val loss: 0.01680\n",
      "Training epoch: 1592, train loss: 0.01587, val loss: 0.01632\n",
      "Training epoch: 1593, train loss: 0.01597, val loss: 0.01635\n",
      "Training epoch: 1594, train loss: 0.01638, val loss: 0.01673\n",
      "Training epoch: 1595, train loss: 0.01706, val loss: 0.01757\n",
      "Training epoch: 1596, train loss: 0.01703, val loss: 0.01754\n",
      "Training epoch: 1597, train loss: 0.01607, val loss: 0.01647\n",
      "Training epoch: 1598, train loss: 0.01601, val loss: 0.01644\n",
      "Training epoch: 1599, train loss: 0.01602, val loss: 0.01637\n",
      "Training epoch: 1600, train loss: 0.01619, val loss: 0.01664\n",
      "Training epoch: 1601, train loss: 0.01588, val loss: 0.01631\n",
      "Training epoch: 1602, train loss: 0.01602, val loss: 0.01645\n",
      "Training epoch: 1603, train loss: 0.01584, val loss: 0.01626\n",
      "Training epoch: 1604, train loss: 0.01646, val loss: 0.01695\n",
      "Training epoch: 1605, train loss: 0.01630, val loss: 0.01678\n",
      "Training epoch: 1606, train loss: 0.01580, val loss: 0.01614\n",
      "Training epoch: 1607, train loss: 0.01586, val loss: 0.01624\n",
      "Training epoch: 1608, train loss: 0.01626, val loss: 0.01668\n",
      "Training epoch: 1609, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1610, train loss: 0.01615, val loss: 0.01661\n",
      "Training epoch: 1611, train loss: 0.01586, val loss: 0.01623\n",
      "Training epoch: 1612, train loss: 0.01580, val loss: 0.01616\n",
      "Training epoch: 1613, train loss: 0.01613, val loss: 0.01658\n",
      "Training epoch: 1614, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 1615, train loss: 0.01599, val loss: 0.01642\n",
      "Training epoch: 1616, train loss: 0.01639, val loss: 0.01685\n",
      "Training epoch: 1617, train loss: 0.01624, val loss: 0.01667\n",
      "Training epoch: 1618, train loss: 0.01626, val loss: 0.01666\n",
      "Training epoch: 1619, train loss: 0.01618, val loss: 0.01655\n",
      "Training epoch: 1620, train loss: 0.01615, val loss: 0.01659\n",
      "Training epoch: 1621, train loss: 0.01601, val loss: 0.01641\n",
      "Training epoch: 1622, train loss: 0.01621, val loss: 0.01658\n",
      "Training epoch: 1623, train loss: 0.01644, val loss: 0.01695\n",
      "Training epoch: 1624, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 1625, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1626, train loss: 0.01623, val loss: 0.01666\n",
      "Training epoch: 1627, train loss: 0.01603, val loss: 0.01645\n",
      "Training epoch: 1628, train loss: 0.01585, val loss: 0.01627\n",
      "Training epoch: 1629, train loss: 0.01589, val loss: 0.01630\n",
      "Training epoch: 1630, train loss: 0.01631, val loss: 0.01679\n",
      "Training epoch: 1631, train loss: 0.01601, val loss: 0.01644\n",
      "Training epoch: 1632, train loss: 0.01645, val loss: 0.01694\n",
      "Training epoch: 1633, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 1634, train loss: 0.01668, val loss: 0.01717\n",
      "Training epoch: 1635, train loss: 0.01619, val loss: 0.01668\n",
      "Training epoch: 1636, train loss: 0.01610, val loss: 0.01656\n",
      "Training epoch: 1637, train loss: 0.01633, val loss: 0.01681\n",
      "Training epoch: 1638, train loss: 0.01605, val loss: 0.01648\n",
      "Training epoch: 1639, train loss: 0.01592, val loss: 0.01632\n",
      "Training epoch: 1640, train loss: 0.01588, val loss: 0.01631\n",
      "Training epoch: 1641, train loss: 0.01591, val loss: 0.01632\n",
      "Training epoch: 1642, train loss: 0.01593, val loss: 0.01625\n",
      "Training epoch: 1643, train loss: 0.01608, val loss: 0.01649\n",
      "Training epoch: 1644, train loss: 0.01607, val loss: 0.01652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1645, train loss: 0.01710, val loss: 0.01761\n",
      "Training epoch: 1646, train loss: 0.01658, val loss: 0.01708\n",
      "Training epoch: 1647, train loss: 0.01643, val loss: 0.01688\n",
      "Training epoch: 1648, train loss: 0.01595, val loss: 0.01631\n",
      "Training epoch: 1649, train loss: 0.01582, val loss: 0.01623\n",
      "Training epoch: 1650, train loss: 0.01649, val loss: 0.01691\n",
      "Training epoch: 1651, train loss: 0.01633, val loss: 0.01679\n",
      "Training epoch: 1652, train loss: 0.01603, val loss: 0.01644\n",
      "Training epoch: 1653, train loss: 0.01690, val loss: 0.01720\n",
      "Training epoch: 1654, train loss: 0.01621, val loss: 0.01660\n",
      "Training epoch: 1655, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 1656, train loss: 0.01611, val loss: 0.01657\n",
      "Training epoch: 1657, train loss: 0.01607, val loss: 0.01653\n",
      "Training epoch: 1658, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 1659, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1660, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1661, train loss: 0.01605, val loss: 0.01650\n",
      "Training epoch: 1662, train loss: 0.01591, val loss: 0.01630\n",
      "Training epoch: 1663, train loss: 0.01616, val loss: 0.01660\n",
      "Training epoch: 1664, train loss: 0.01602, val loss: 0.01648\n",
      "Training epoch: 1665, train loss: 0.01616, val loss: 0.01662\n",
      "Training epoch: 1666, train loss: 0.01636, val loss: 0.01682\n",
      "Training epoch: 1667, train loss: 0.01576, val loss: 0.01615\n",
      "Training epoch: 1668, train loss: 0.01632, val loss: 0.01669\n",
      "Training epoch: 1669, train loss: 0.01597, val loss: 0.01637\n",
      "Training epoch: 1670, train loss: 0.01611, val loss: 0.01649\n",
      "Training epoch: 1671, train loss: 0.01605, val loss: 0.01642\n",
      "Training epoch: 1672, train loss: 0.01628, val loss: 0.01675\n",
      "Training epoch: 1673, train loss: 0.01604, val loss: 0.01648\n",
      "Training epoch: 1674, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 1675, train loss: 0.01665, val loss: 0.01713\n",
      "Training epoch: 1676, train loss: 0.01591, val loss: 0.01633\n",
      "Training epoch: 1677, train loss: 0.01601, val loss: 0.01643\n",
      "Training epoch: 1678, train loss: 0.01608, val loss: 0.01652\n",
      "Training epoch: 1679, train loss: 0.01617, val loss: 0.01664\n",
      "Training epoch: 1680, train loss: 0.01624, val loss: 0.01670\n",
      "Training epoch: 1681, train loss: 0.01650, val loss: 0.01683\n",
      "Training epoch: 1682, train loss: 0.01680, val loss: 0.01711\n",
      "Training epoch: 1683, train loss: 0.01618, val loss: 0.01657\n",
      "Training epoch: 1684, train loss: 0.01587, val loss: 0.01627\n",
      "Training epoch: 1685, train loss: 0.01596, val loss: 0.01641\n",
      "Training epoch: 1686, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 1687, train loss: 0.01579, val loss: 0.01617\n",
      "Training epoch: 1688, train loss: 0.01596, val loss: 0.01640\n",
      "Training epoch: 1689, train loss: 0.01588, val loss: 0.01627\n",
      "Training epoch: 1690, train loss: 0.01648, val loss: 0.01693\n",
      "Training epoch: 1691, train loss: 0.01609, val loss: 0.01646\n",
      "Training epoch: 1692, train loss: 0.01603, val loss: 0.01637\n",
      "Training epoch: 1693, train loss: 0.01611, val loss: 0.01654\n",
      "Training epoch: 1694, train loss: 0.01588, val loss: 0.01627\n",
      "Training epoch: 1695, train loss: 0.01613, val loss: 0.01659\n",
      "Training epoch: 1696, train loss: 0.01636, val loss: 0.01684\n",
      "Training epoch: 1697, train loss: 0.01598, val loss: 0.01641\n",
      "Training epoch: 1698, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 1699, train loss: 0.01648, val loss: 0.01699\n",
      "Training epoch: 1700, train loss: 0.01622, val loss: 0.01664\n",
      "Training epoch: 1701, train loss: 0.01618, val loss: 0.01658\n",
      "Training epoch: 1702, train loss: 0.01604, val loss: 0.01642\n",
      "Training epoch: 1703, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1704, train loss: 0.01646, val loss: 0.01696\n",
      "Training epoch: 1705, train loss: 0.01644, val loss: 0.01689\n",
      "Training epoch: 1706, train loss: 0.01601, val loss: 0.01641\n",
      "Training epoch: 1707, train loss: 0.01657, val loss: 0.01698\n",
      "Training epoch: 1708, train loss: 0.01663, val loss: 0.01710\n",
      "Training epoch: 1709, train loss: 0.01606, val loss: 0.01649\n",
      "Training epoch: 1710, train loss: 0.01682, val loss: 0.01725\n",
      "Training epoch: 1711, train loss: 0.01663, val loss: 0.01709\n",
      "Training epoch: 1712, train loss: 0.01583, val loss: 0.01622\n",
      "Training epoch: 1713, train loss: 0.01583, val loss: 0.01620\n",
      "Training epoch: 1714, train loss: 0.01596, val loss: 0.01638\n",
      "Training epoch: 1715, train loss: 0.01615, val loss: 0.01652\n",
      "Training epoch: 1716, train loss: 0.01603, val loss: 0.01643\n",
      "Training epoch: 1717, train loss: 0.01585, val loss: 0.01623\n",
      "Training epoch: 1718, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 1719, train loss: 0.01609, val loss: 0.01649\n",
      "Training epoch: 1720, train loss: 0.01579, val loss: 0.01620\n",
      "Training epoch: 1721, train loss: 0.01591, val loss: 0.01630\n",
      "Training epoch: 1722, train loss: 0.01614, val loss: 0.01660\n",
      "Training epoch: 1723, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1724, train loss: 0.01609, val loss: 0.01656\n",
      "Training epoch: 1725, train loss: 0.01625, val loss: 0.01673\n",
      "Training epoch: 1726, train loss: 0.01582, val loss: 0.01626\n",
      "Training epoch: 1727, train loss: 0.01587, val loss: 0.01629\n",
      "Training epoch: 1728, train loss: 0.01587, val loss: 0.01625\n",
      "Training epoch: 1729, train loss: 0.01612, val loss: 0.01659\n",
      "Training epoch: 1730, train loss: 0.01588, val loss: 0.01630\n",
      "Training epoch: 1731, train loss: 0.01591, val loss: 0.01632\n",
      "Training epoch: 1732, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 1733, train loss: 0.01593, val loss: 0.01631\n",
      "Training epoch: 1734, train loss: 0.01583, val loss: 0.01622\n",
      "Training epoch: 1735, train loss: 0.01594, val loss: 0.01636\n",
      "Training epoch: 1736, train loss: 0.01655, val loss: 0.01700\n",
      "Training epoch: 1737, train loss: 0.01587, val loss: 0.01630\n",
      "Training epoch: 1738, train loss: 0.01633, val loss: 0.01679\n",
      "Training epoch: 1739, train loss: 0.01596, val loss: 0.01636\n",
      "Training epoch: 1740, train loss: 0.01592, val loss: 0.01629\n",
      "Training epoch: 1741, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 1742, train loss: 0.01590, val loss: 0.01630\n",
      "Training epoch: 1743, train loss: 0.01595, val loss: 0.01637\n",
      "Training epoch: 1744, train loss: 0.01586, val loss: 0.01626\n",
      "Training epoch: 1745, train loss: 0.01618, val loss: 0.01663\n",
      "Training epoch: 1746, train loss: 0.01579, val loss: 0.01619\n",
      "Training epoch: 1747, train loss: 0.01679, val loss: 0.01731\n",
      "Training epoch: 1748, train loss: 0.01590, val loss: 0.01633\n",
      "Training epoch: 1749, train loss: 0.01710, val loss: 0.01740\n",
      "Training epoch: 1750, train loss: 0.01592, val loss: 0.01630\n",
      "Training epoch: 1751, train loss: 0.01594, val loss: 0.01635\n",
      "Training epoch: 1752, train loss: 0.01628, val loss: 0.01666\n",
      "Training epoch: 1753, train loss: 0.01645, val loss: 0.01682\n",
      "Training epoch: 1754, train loss: 0.01642, val loss: 0.01688\n",
      "Training epoch: 1755, train loss: 0.01612, val loss: 0.01655\n",
      "Training epoch: 1756, train loss: 0.01649, val loss: 0.01699\n",
      "Training epoch: 1757, train loss: 0.01618, val loss: 0.01663\n",
      "Training epoch: 1758, train loss: 0.01589, val loss: 0.01629\n",
      "Training epoch: 1759, train loss: 0.01587, val loss: 0.01624\n",
      "Training epoch: 1760, train loss: 0.01593, val loss: 0.01633\n",
      "Training epoch: 1761, train loss: 0.01607, val loss: 0.01649\n",
      "Training epoch: 1762, train loss: 0.01643, val loss: 0.01687\n",
      "Training epoch: 1763, train loss: 0.01610, val loss: 0.01650\n",
      "Training epoch: 1764, train loss: 0.01625, val loss: 0.01670\n",
      "Training epoch: 1765, train loss: 0.01607, val loss: 0.01651\n",
      "Training epoch: 1766, train loss: 0.01588, val loss: 0.01624\n",
      "Training epoch: 1767, train loss: 0.01634, val loss: 0.01682\n",
      "Training epoch: 1768, train loss: 0.01583, val loss: 0.01619\n",
      "Training epoch: 1769, train loss: 0.01612, val loss: 0.01657\n",
      "Training epoch: 1770, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1771, train loss: 0.01574, val loss: 0.01612\n",
      "Training epoch: 1772, train loss: 0.01615, val loss: 0.01653\n",
      "Training epoch: 1773, train loss: 0.01584, val loss: 0.01623\n",
      "Training epoch: 1774, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1775, train loss: 0.01605, val loss: 0.01649\n",
      "Training epoch: 1776, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 1777, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1778, train loss: 0.01599, val loss: 0.01638\n",
      "Training epoch: 1779, train loss: 0.01585, val loss: 0.01628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1780, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 1781, train loss: 0.01613, val loss: 0.01651\n",
      "Training epoch: 1782, train loss: 0.01583, val loss: 0.01623\n",
      "Training epoch: 1783, train loss: 0.01591, val loss: 0.01630\n",
      "Training epoch: 1784, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1785, train loss: 0.01638, val loss: 0.01671\n",
      "Training epoch: 1786, train loss: 0.01611, val loss: 0.01656\n",
      "Training epoch: 1787, train loss: 0.01625, val loss: 0.01668\n",
      "Training epoch: 1788, train loss: 0.01607, val loss: 0.01653\n",
      "Training epoch: 1789, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 1790, train loss: 0.01639, val loss: 0.01688\n",
      "Training epoch: 1791, train loss: 0.01633, val loss: 0.01680\n",
      "Training epoch: 1792, train loss: 0.01588, val loss: 0.01628\n",
      "Training epoch: 1793, train loss: 0.01599, val loss: 0.01641\n",
      "Training epoch: 1794, train loss: 0.01618, val loss: 0.01652\n",
      "Training epoch: 1795, train loss: 0.01595, val loss: 0.01633\n",
      "Training epoch: 1796, train loss: 0.01604, val loss: 0.01645\n",
      "Training epoch: 1797, train loss: 0.01594, val loss: 0.01638\n",
      "Training epoch: 1798, train loss: 0.01597, val loss: 0.01640\n",
      "Training epoch: 1799, train loss: 0.01660, val loss: 0.01699\n",
      "Training epoch: 1800, train loss: 0.01651, val loss: 0.01699\n",
      "Training epoch: 1801, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 1802, train loss: 0.01633, val loss: 0.01668\n",
      "Training epoch: 1803, train loss: 0.01596, val loss: 0.01639\n",
      "Training epoch: 1804, train loss: 0.01587, val loss: 0.01630\n",
      "Training epoch: 1805, train loss: 0.01595, val loss: 0.01638\n",
      "Training epoch: 1806, train loss: 0.01616, val loss: 0.01663\n",
      "Training epoch: 1807, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1808, train loss: 0.01618, val loss: 0.01655\n",
      "Training epoch: 1809, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 1810, train loss: 0.01594, val loss: 0.01636\n",
      "Training epoch: 1811, train loss: 0.01602, val loss: 0.01645\n",
      "Training epoch: 1812, train loss: 0.01610, val loss: 0.01655\n",
      "Training epoch: 1813, train loss: 0.01616, val loss: 0.01660\n",
      "Training epoch: 1814, train loss: 0.01627, val loss: 0.01673\n",
      "Training epoch: 1815, train loss: 0.01577, val loss: 0.01620\n",
      "Training epoch: 1816, train loss: 0.01579, val loss: 0.01619\n",
      "Training epoch: 1817, train loss: 0.01623, val loss: 0.01666\n",
      "Training epoch: 1818, train loss: 0.01643, val loss: 0.01689\n",
      "Training epoch: 1819, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 1820, train loss: 0.01603, val loss: 0.01643\n",
      "Training epoch: 1821, train loss: 0.01599, val loss: 0.01639\n",
      "Training epoch: 1822, train loss: 0.01684, val loss: 0.01717\n",
      "Training epoch: 1823, train loss: 0.01625, val loss: 0.01666\n",
      "Training epoch: 1824, train loss: 0.01598, val loss: 0.01637\n",
      "Training epoch: 1825, train loss: 0.01636, val loss: 0.01672\n",
      "Training epoch: 1826, train loss: 0.01641, val loss: 0.01681\n",
      "Training epoch: 1827, train loss: 0.01602, val loss: 0.01644\n",
      "Training epoch: 1828, train loss: 0.01583, val loss: 0.01624\n",
      "Training epoch: 1829, train loss: 0.01606, val loss: 0.01652\n",
      "Training epoch: 1830, train loss: 0.01608, val loss: 0.01650\n",
      "Training epoch: 1831, train loss: 0.01654, val loss: 0.01704\n",
      "Training epoch: 1832, train loss: 0.01603, val loss: 0.01645\n",
      "Training epoch: 1833, train loss: 0.01582, val loss: 0.01620\n",
      "Training epoch: 1834, train loss: 0.01593, val loss: 0.01637\n",
      "Training epoch: 1835, train loss: 0.01619, val loss: 0.01656\n",
      "Training epoch: 1836, train loss: 0.01594, val loss: 0.01635\n",
      "Training epoch: 1837, train loss: 0.01576, val loss: 0.01617\n",
      "Training epoch: 1838, train loss: 0.01613, val loss: 0.01657\n",
      "Training epoch: 1839, train loss: 0.01619, val loss: 0.01655\n",
      "Training epoch: 1840, train loss: 0.01608, val loss: 0.01650\n",
      "Training epoch: 1841, train loss: 0.01635, val loss: 0.01676\n",
      "Training epoch: 1842, train loss: 0.01599, val loss: 0.01635\n",
      "Training epoch: 1843, train loss: 0.01590, val loss: 0.01633\n",
      "Training epoch: 1844, train loss: 0.01639, val loss: 0.01676\n",
      "Training epoch: 1845, train loss: 0.01585, val loss: 0.01624\n",
      "Training epoch: 1846, train loss: 0.01654, val loss: 0.01689\n",
      "Training epoch: 1847, train loss: 0.01608, val loss: 0.01650\n",
      "Training epoch: 1848, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 1849, train loss: 0.01602, val loss: 0.01644\n",
      "Training epoch: 1850, train loss: 0.01591, val loss: 0.01625\n",
      "Training epoch: 1851, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 1852, train loss: 0.01621, val loss: 0.01666\n",
      "Training epoch: 1853, train loss: 0.01603, val loss: 0.01647\n",
      "Training epoch: 1854, train loss: 0.01605, val loss: 0.01649\n",
      "Training epoch: 1855, train loss: 0.01619, val loss: 0.01663\n",
      "Training epoch: 1856, train loss: 0.01655, val loss: 0.01705\n",
      "Training epoch: 1857, train loss: 0.01602, val loss: 0.01646\n",
      "Training epoch: 1858, train loss: 0.01614, val loss: 0.01651\n",
      "Training epoch: 1859, train loss: 0.01608, val loss: 0.01654\n",
      "Training epoch: 1860, train loss: 0.01583, val loss: 0.01623\n",
      "Training epoch: 1861, train loss: 0.01589, val loss: 0.01629\n",
      "Training epoch: 1862, train loss: 0.01612, val loss: 0.01657\n",
      "Training epoch: 1863, train loss: 0.01630, val loss: 0.01678\n",
      "Training epoch: 1864, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 1865, train loss: 0.01614, val loss: 0.01654\n",
      "Training epoch: 1866, train loss: 0.01594, val loss: 0.01637\n",
      "Training epoch: 1867, train loss: 0.01606, val loss: 0.01650\n",
      "Training epoch: 1868, train loss: 0.01597, val loss: 0.01637\n",
      "Training epoch: 1869, train loss: 0.01592, val loss: 0.01633\n",
      "Training epoch: 1870, train loss: 0.01631, val loss: 0.01673\n",
      "Training epoch: 1871, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 1872, train loss: 0.01642, val loss: 0.01677\n",
      "Training epoch: 1873, train loss: 0.01606, val loss: 0.01648\n",
      "Training epoch: 1874, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 1875, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1876, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 1877, train loss: 0.01639, val loss: 0.01687\n",
      "Training epoch: 1878, train loss: 0.01591, val loss: 0.01628\n",
      "Early stop at epoch 1878, With Testing Error: 0.01628\n",
      "Subnetwork pruning.\n",
      "Fine tuning.\n",
      "Tuning epoch: 1, train loss: 0.01585, val loss: 0.01628\n",
      "Tuning epoch: 2, train loss: 0.01584, val loss: 0.01626\n",
      "Tuning epoch: 3, train loss: 0.01585, val loss: 0.01622\n",
      "Tuning epoch: 4, train loss: 0.01570, val loss: 0.01606\n",
      "Tuning epoch: 5, train loss: 0.01596, val loss: 0.01630\n",
      "Tuning epoch: 6, train loss: 0.01655, val loss: 0.01698\n",
      "Tuning epoch: 7, train loss: 0.01607, val loss: 0.01646\n",
      "Tuning epoch: 8, train loss: 0.01581, val loss: 0.01620\n",
      "Tuning epoch: 9, train loss: 0.01568, val loss: 0.01607\n",
      "Tuning epoch: 10, train loss: 0.01610, val loss: 0.01647\n",
      "Tuning epoch: 11, train loss: 0.01577, val loss: 0.01617\n",
      "Tuning epoch: 12, train loss: 0.01570, val loss: 0.01608\n",
      "Tuning epoch: 13, train loss: 0.01573, val loss: 0.01613\n",
      "Tuning epoch: 14, train loss: 0.01583, val loss: 0.01622\n",
      "Tuning epoch: 15, train loss: 0.01575, val loss: 0.01607\n",
      "Tuning epoch: 16, train loss: 0.01573, val loss: 0.01612\n",
      "Tuning epoch: 17, train loss: 0.01601, val loss: 0.01637\n",
      "Tuning epoch: 18, train loss: 0.01572, val loss: 0.01608\n",
      "Tuning epoch: 19, train loss: 0.01591, val loss: 0.01623\n",
      "Tuning epoch: 20, train loss: 0.01610, val loss: 0.01651\n",
      "Tuning epoch: 21, train loss: 0.01586, val loss: 0.01623\n",
      "Tuning epoch: 22, train loss: 0.01631, val loss: 0.01659\n",
      "Tuning epoch: 23, train loss: 0.01581, val loss: 0.01621\n",
      "Tuning epoch: 24, train loss: 0.01578, val loss: 0.01614\n",
      "Tuning epoch: 25, train loss: 0.01588, val loss: 0.01625\n",
      "Tuning epoch: 26, train loss: 0.01600, val loss: 0.01637\n",
      "Tuning epoch: 27, train loss: 0.01581, val loss: 0.01618\n",
      "Tuning epoch: 28, train loss: 0.01596, val loss: 0.01634\n",
      "Tuning epoch: 29, train loss: 0.01568, val loss: 0.01602\n",
      "Tuning epoch: 30, train loss: 0.01564, val loss: 0.01604\n",
      "Tuning epoch: 31, train loss: 0.01591, val loss: 0.01627\n",
      "Tuning epoch: 32, train loss: 0.01606, val loss: 0.01649\n",
      "Tuning epoch: 33, train loss: 0.01600, val loss: 0.01636\n",
      "Tuning epoch: 34, train loss: 0.01574, val loss: 0.01609\n",
      "Tuning epoch: 35, train loss: 0.01586, val loss: 0.01622\n",
      "Tuning epoch: 36, train loss: 0.01593, val loss: 0.01636\n",
      "Tuning epoch: 37, train loss: 0.01571, val loss: 0.01611\n",
      "Tuning epoch: 38, train loss: 0.01577, val loss: 0.01615\n",
      "Tuning epoch: 39, train loss: 0.01571, val loss: 0.01609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning epoch: 40, train loss: 0.01573, val loss: 0.01610\n",
      "Tuning epoch: 41, train loss: 0.01593, val loss: 0.01636\n",
      "Tuning epoch: 42, train loss: 0.01589, val loss: 0.01628\n",
      "Tuning epoch: 43, train loss: 0.01570, val loss: 0.01608\n",
      "Tuning epoch: 44, train loss: 0.01603, val loss: 0.01636\n",
      "Tuning epoch: 45, train loss: 0.01568, val loss: 0.01604\n",
      "Tuning epoch: 46, train loss: 0.01598, val loss: 0.01634\n",
      "Tuning epoch: 47, train loss: 0.01590, val loss: 0.01633\n",
      "Tuning epoch: 48, train loss: 0.01578, val loss: 0.01617\n",
      "Tuning epoch: 49, train loss: 0.01601, val loss: 0.01644\n",
      "Tuning epoch: 50, train loss: 0.01611, val loss: 0.01655\n",
      "Tuning epoch: 51, train loss: 0.01675, val loss: 0.01721\n",
      "Tuning epoch: 52, train loss: 0.01603, val loss: 0.01646\n",
      "Tuning epoch: 53, train loss: 0.01640, val loss: 0.01683\n",
      "Tuning epoch: 54, train loss: 0.01677, val loss: 0.01726\n",
      "Tuning epoch: 55, train loss: 0.01598, val loss: 0.01638\n",
      "Tuning epoch: 56, train loss: 0.01648, val loss: 0.01681\n",
      "Tuning epoch: 57, train loss: 0.01646, val loss: 0.01675\n",
      "Tuning epoch: 58, train loss: 0.01582, val loss: 0.01617\n",
      "Tuning epoch: 59, train loss: 0.01592, val loss: 0.01630\n",
      "Tuning epoch: 60, train loss: 0.01586, val loss: 0.01620\n",
      "Tuning epoch: 61, train loss: 0.01593, val loss: 0.01624\n",
      "Tuning epoch: 62, train loss: 0.01639, val loss: 0.01669\n",
      "Tuning epoch: 63, train loss: 0.01569, val loss: 0.01606\n",
      "Tuning epoch: 64, train loss: 0.01619, val loss: 0.01656\n",
      "Tuning epoch: 65, train loss: 0.01576, val loss: 0.01611\n",
      "Tuning epoch: 66, train loss: 0.01591, val loss: 0.01629\n",
      "Tuning epoch: 67, train loss: 0.01606, val loss: 0.01649\n",
      "Tuning epoch: 68, train loss: 0.01570, val loss: 0.01608\n",
      "Tuning epoch: 69, train loss: 0.01587, val loss: 0.01620\n",
      "Tuning epoch: 70, train loss: 0.01601, val loss: 0.01638\n",
      "Tuning epoch: 71, train loss: 0.01595, val loss: 0.01635\n",
      "Tuning epoch: 72, train loss: 0.01641, val loss: 0.01680\n",
      "Tuning epoch: 73, train loss: 0.01611, val loss: 0.01642\n",
      "Tuning epoch: 74, train loss: 0.01588, val loss: 0.01622\n",
      "Tuning epoch: 75, train loss: 0.01628, val loss: 0.01668\n",
      "Tuning epoch: 76, train loss: 0.01571, val loss: 0.01607\n",
      "Tuning epoch: 77, train loss: 0.01578, val loss: 0.01617\n",
      "Tuning epoch: 78, train loss: 0.01588, val loss: 0.01623\n",
      "Tuning epoch: 79, train loss: 0.01567, val loss: 0.01601\n",
      "Tuning epoch: 80, train loss: 0.01615, val loss: 0.01648\n",
      "Tuning epoch: 81, train loss: 0.01721, val loss: 0.01755\n",
      "Tuning epoch: 82, train loss: 0.01600, val loss: 0.01632\n",
      "Tuning epoch: 83, train loss: 0.01580, val loss: 0.01617\n",
      "Tuning epoch: 84, train loss: 0.01578, val loss: 0.01615\n",
      "Tuning epoch: 85, train loss: 0.01596, val loss: 0.01633\n",
      "Tuning epoch: 86, train loss: 0.01583, val loss: 0.01623\n",
      "Tuning epoch: 87, train loss: 0.01590, val loss: 0.01633\n",
      "Tuning epoch: 88, train loss: 0.01578, val loss: 0.01617\n",
      "Tuning epoch: 89, train loss: 0.01583, val loss: 0.01624\n",
      "Tuning epoch: 90, train loss: 0.01611, val loss: 0.01654\n",
      "Tuning epoch: 91, train loss: 0.01619, val loss: 0.01661\n",
      "Tuning epoch: 92, train loss: 0.01597, val loss: 0.01636\n",
      "Tuning epoch: 93, train loss: 0.01602, val loss: 0.01637\n",
      "Tuning epoch: 94, train loss: 0.01597, val loss: 0.01634\n",
      "Tuning epoch: 95, train loss: 0.01585, val loss: 0.01620\n",
      "Tuning epoch: 96, train loss: 0.01571, val loss: 0.01612\n",
      "Tuning epoch: 97, train loss: 0.01570, val loss: 0.01610\n",
      "Tuning epoch: 98, train loss: 0.01585, val loss: 0.01617\n",
      "Tuning epoch: 99, train loss: 0.01570, val loss: 0.01606\n",
      "Tuning epoch: 100, train loss: 0.01567, val loss: 0.01603\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_depth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bce4212f1784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exnn_demo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2_local/envs/tf2/lib/python3.6/site-packages/exnn/base.py\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(self, folder, name, save_png, save_eps)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0max3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" (\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msubnets_scale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"%)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msave_eps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_depth' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from exnn import ExNN\n",
    "\n",
    "def data_generator1(datanum, testnum=10000, noise_sigma=1, rand_seed=0):\n",
    "    \n",
    "    corr = 0.5\n",
    "    np.random.seed(rand_seed)\n",
    "    proj_matrix = np.zeros((10, 4))\n",
    "    proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "    proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "    proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "    proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "    u = np.random.uniform(-1, 1, [datanum + testnum, 1])\n",
    "    t = np.sqrt(corr / (1 - corr))\n",
    "    x = np.zeros((datanum + testnum, 10))\n",
    "    for i in range(10):\n",
    "        x[:, i:i + 1] = (np.random.uniform(-1, 1, [datanum + testnum, 1]) + t * u) / (1 + t)\n",
    "\n",
    "    y = np.reshape(2 * np.dot(x, proj_matrix[:, 0]) + 0.2 * np.exp(-4 * np.dot(x, proj_matrix[:, 1])) + \\\n",
    "                   3 * (np.dot(x, proj_matrix[:, 2]))**2 + 2.5 * np.sin(np.pi * np.dot(x, proj_matrix[:, 3])), [-1, 1]) + \\\n",
    "              noise_sigma * np.random.normal(0, 1, [datanum + testnum, 1])\n",
    "    \n",
    "    task_type = \"Regression\"\n",
    "    meta_info = {\"X1\":{\"type\":\"continuous\"},\n",
    "             \"X2\":{\"type\":\"continuous\"},\n",
    "             \"X3\":{\"type\":\"continuous\"},\n",
    "             \"X4\":{\"type\":\"continuous\"},\n",
    "             \"X5\":{\"type\":\"continuous\"},\n",
    "             \"X6\":{\"type\":\"continuous\"},\n",
    "             \"X7\":{\"type\":\"continuous\"},\n",
    "             \"X8\":{\"type\":\"continuous\"},\n",
    "             \"X9\":{\"type\":\"continuous\"},\n",
    "             \"X10\":{\"type\":\"continuous\"},\n",
    "             \"Y\":{\"type\":\"target\"}}\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == \"target\":\n",
    "            sy = MinMaxScaler((-1, 1))\n",
    "            y = sy.fit_transform(y)\n",
    "            meta_info[key][\"scaler\"] = sy\n",
    "        elif item['type'] == \"categorical\":\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(x[:,[i]])\n",
    "            ordinal_feature = enc.transform(x[:,[i]])\n",
    "            x[:,[i]] = ordinal_feature\n",
    "            meta_info[key][\"values\"] = enc.categories_[0].tolist()\n",
    "        else:\n",
    "            sx = MinMaxScaler((-1, 1))\n",
    "            x[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "            meta_info[key][\"scaler\"] = sx\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=testnum, random_state=rand_seed)\n",
    "    return train_x, test_x, train_y, test_y, task_type, meta_info\n",
    "\n",
    "train_x, test_x, train_y, test_y, task_type, meta_info = data_generator1(datanum=10000, testnum=10000, noise_sigma=1, rand_seed=0)\n",
    "model = ExNN(meta_info=meta_info,\n",
    "               subnet_num=10,\n",
    "               subnet_arch=[10, 6],\n",
    "               task_type=task_type,\n",
    "               activation_func=tf.tanh,\n",
    "               batch_size=min(1000, int(train_x.shape[0] * 0.2)),\n",
    "               training_epochs=10000,\n",
    "               lr_bp=0.001,\n",
    "               lr_cl=0.1,\n",
    "               beta_threshold=0.05,\n",
    "               tuning_epochs=100,\n",
    "               l1_proj=0.001,\n",
    "               l1_subnet=0.01,\n",
    "               l2_smooth=10**(-6),\n",
    "               verbose=True,\n",
    "               val_ratio=0.2,\n",
    "               early_stop_thres=500)\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"exnn_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
