{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for hyperparameter tunning with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xnn.sosxnn import SOSxNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "corr = 0.5\n",
    "noise_sigma = 1\n",
    "DummyNum = 0\n",
    "FeatureNum = 10\n",
    "TestNum = 10000\n",
    "DataNum = 10000\n",
    "\n",
    "# Optimization \n",
    "training_epochs = 10000\n",
    "num_cores = 10\n",
    "repeat_num = 10\n",
    "\n",
    "ortho_matrix = np.zeros((FeatureNum,4))\n",
    "ortho_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "ortho_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "ortho_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "ortho_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator1(DataNum, FeatureNum, corr, proj_matrix, noise_sigma):\n",
    "    u = np.random.uniform(-1,1, [DataNum, 1])\n",
    "    t= np.sqrt(corr/(1-corr))\n",
    "    X = np.zeros((DataNum, FeatureNum))\n",
    "    for i in range(FeatureNum):\n",
    "        X[:, i:i+1] = (np.random.uniform(-1,1,[DataNum,1])+t*u)/(1+t)\n",
    "    Y = np.reshape(2*np.dot(X, proj_matrix[:,0])+0.2*np.exp(-4*np.dot(X, proj_matrix[:,1])) + \\\n",
    "              3*(np.dot(X, proj_matrix[:,2]))**2+2.5*np.sin(np.pi*np.dot(X, proj_matrix[:,3])), [-1,1]) + \\\n",
    "              noise_sigma*np.random.normal(0,1, [DataNum,1])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X, Y = data_generator1(DataNum+TestNum, FeatureNum, corr, ortho_matrix, noise_sigma)\n",
    "scaler_x = MinMaxScaler((-1, 1)); scaler_y = MinMaxScaler((-1, 1))\n",
    "sX = scaler_x.fit_transform(X); sY = scaler_y.fit_transform(Y)\n",
    "train_x, test_x, train_y, test_y = train_test_split(sX, sY, test_size = TestNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1_Penalty_Subnet = [$10^{-1}, 10^{-2}, 10^{-3}$]\n",
    "- L1_Penalty_Proj = [$10^{-1}, 10^{-2}, 10^{-3}$]\n",
    "- Smooth_Labmda = [$10^{-5}, 10^{-6}, 10^{-7}$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(scaler_y):\n",
    "    def metric(label, pred):\n",
    "        pred = scaler_y.inverse_transform(pred)\n",
    "        label = scaler_y.inverse_transform(label)\n",
    "        return np.mean((pred - label)**2)\n",
    "    return metric\n",
    "\n",
    "def get_auc(label, pred):\n",
    "    return roc_auc_score\n",
    "\n",
    "def sosxnn_tune(simu_dir, task_name, train_x, train_y, test_x, test_y, metric=None,\n",
    "         input_dummy_num=0,\n",
    "         subnet_num=10,\n",
    "         subnet_arch=[10, 6],\n",
    "         task=\"Regression\",\n",
    "         activation_func=tf.tanh,\n",
    "         bn_flag=True,\n",
    "         lr_bp=0.001,\n",
    "         lr_cl=0.1,\n",
    "         l1_proj=0.001,\n",
    "         l1_subnet=0.001,\n",
    "         smooth_lambda=0.00001,\n",
    "         batch_size=1000,\n",
    "         training_epochs=10000,\n",
    "         tuning_epochs=500,\n",
    "         beta_threshold=0.01,\n",
    "         verbose=False,\n",
    "         val_ratio=0.2,\n",
    "         early_stop_thres=1000,\n",
    "         dummy_name=None):\n",
    "\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(1)\n",
    "    input_num = train_x.shape[1] - input_dummy_num\n",
    "    model = SOSxNN(input_num=input_num, \n",
    "                input_dummy_num=input_dummy_num,\n",
    "                subnet_num=min(input_num, 10), \n",
    "                subnet_arch=subnet_arch,\n",
    "                task=task,\n",
    "                activation_func=tf.tanh,\n",
    "                batch_size=batch_size,\n",
    "                training_epochs=training_epochs,\n",
    "                lr_bp=lr_bp,\n",
    "                lr_cl=lr_cl,\n",
    "                beta_threshold=beta_threshold,\n",
    "                tuning_epochs=tuning_epochs,\n",
    "                l1_proj=l1_proj,\n",
    "                l1_subnet=l1_subnet,\n",
    "                smooth_lambda=smooth_lambda,\n",
    "                verbose=True,\n",
    "                val_ratio=val_ratio,\n",
    "                early_stop_thres=early_stop_thres)\n",
    "    model.fit(train_x, train_y)  \n",
    "    model.visualize(folder=simu_dir + task_name + \"/\", \n",
    "              name=str(-np.log10(l1_proj)).zfill(2) + \"_\" + str(-np.log10(l1_subnet)).zfill(2) + \"_\" +\n",
    "                    str(-np.log10(smooth_lambda)).zfill(2), \n",
    "              dummy_name=dummy_name,\n",
    "              save_eps=False)\n",
    "    \n",
    "    tr_pred = model.predict(model.tr_x) \n",
    "    val_pred = model.predict(model.val_x) \n",
    "    pred_test = model.predict(test_x)\n",
    "\n",
    "    if task==\"Regression\":\n",
    "        stat = np.hstack([np.round(metric(model.tr_y, tr_pred),5),\\\n",
    "               np.round(metric(model.val_y, val_pred),5),\\\n",
    "               np.round(metric(test_y, pred_test),5)])\n",
    "    elif task==\"Classification\":\n",
    "        stat = np.hstack([np.round(metric(model.tr_y, tr_pred),5),\\\n",
    "               np.round(metric(model.val_y, val_pred),5),\\\n",
    "               np.round(metric(test_y, pred_test),5)])\n",
    "\n",
    "    res_stat = pd.DataFrame(np.vstack([stat[0],stat[1],stat[2]]).T, columns = ['train_metric', \"val_metric\", \"test_metric\"])\n",
    "    res_stat[\"Subnet_Number\"] = min(input_num, 10)\n",
    "    res_stat[\"lr_BP\"] = lr_bp\n",
    "    res_stat[\"lr_CL\"] = lr_cl\n",
    "    res_stat[\"L1_Penalty_Proj\"] = l1_proj\n",
    "    res_stat[\"L1_Penalty_Subnet\"] = l1_subnet\n",
    "    res_stat[\"Smooth_labmda\"] = smooth_lambda\n",
    "    res_stat[\"Training_Epochs\"] = training_epochs\n",
    "    return res_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "cv_results = Parallel(n_jobs=9)(delayed(sosxnn_tune)(\"./results/\", \"S1_tune\", train_x, train_y, test_x, test_y,\\\n",
    "                                                      subnet_arch=[10,6], metric=get_mse(scaler_y), input_dummy_num=0,\\\n",
    "                      l1_proj=10**(-2-i), l1_subnet=10**(-2-j), smooth_lambda=10**(-5-k),  beta_threshold=0.05,\\\n",
    "                      training_epochs=5000, lr_bp=0.001, lr_cl=0.1, batch_size=1000, early_stop_thres=1000, tuning_epochs=500, \\\n",
    "                      dummy_name=None) for i in range(3) for j in range(3) for k in range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = pd.concat(cv_results)\n",
    "stat.sort_values(\"val_metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
