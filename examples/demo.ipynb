{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-21T06:03:53.751Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n",
      "Training epoch: 1, train loss: 0.15248, val loss: 0.15325\n",
      "Training epoch: 2, train loss: 0.13367, val loss: 0.13557\n",
      "Training epoch: 3, train loss: 0.11582, val loss: 0.11764\n",
      "Training epoch: 4, train loss: 0.09918, val loss: 0.10050\n",
      "Training epoch: 5, train loss: 0.08543, val loss: 0.08640\n",
      "Training epoch: 6, train loss: 0.07818, val loss: 0.07827\n",
      "Training epoch: 7, train loss: 0.06875, val loss: 0.06865\n",
      "Training epoch: 8, train loss: 0.06563, val loss: 0.06523\n",
      "Training epoch: 9, train loss: 0.05757, val loss: 0.05775\n",
      "Training epoch: 10, train loss: 0.05770, val loss: 0.05791\n",
      "Training epoch: 11, train loss: 0.04955, val loss: 0.05008\n",
      "Training epoch: 12, train loss: 0.04903, val loss: 0.04945\n",
      "Training epoch: 13, train loss: 0.04844, val loss: 0.04874\n",
      "Training epoch: 14, train loss: 0.04466, val loss: 0.04488\n",
      "Training epoch: 15, train loss: 0.04277, val loss: 0.04296\n",
      "Training epoch: 16, train loss: 0.03933, val loss: 0.03972\n",
      "Training epoch: 17, train loss: 0.04030, val loss: 0.04039\n",
      "Training epoch: 18, train loss: 0.03780, val loss: 0.03791\n",
      "Training epoch: 19, train loss: 0.03727, val loss: 0.03747\n",
      "Training epoch: 20, train loss: 0.03549, val loss: 0.03573\n",
      "Training epoch: 21, train loss: 0.03513, val loss: 0.03545\n",
      "Training epoch: 22, train loss: 0.03354, val loss: 0.03372\n",
      "Training epoch: 23, train loss: 0.03204, val loss: 0.03219\n",
      "Training epoch: 24, train loss: 0.03263, val loss: 0.03277\n",
      "Training epoch: 25, train loss: 0.03101, val loss: 0.03101\n",
      "Training epoch: 26, train loss: 0.03086, val loss: 0.03092\n",
      "Training epoch: 27, train loss: 0.03000, val loss: 0.03007\n",
      "Training epoch: 28, train loss: 0.02992, val loss: 0.03000\n",
      "Training epoch: 29, train loss: 0.02911, val loss: 0.02925\n",
      "Training epoch: 30, train loss: 0.02797, val loss: 0.02806\n",
      "Training epoch: 31, train loss: 0.02812, val loss: 0.02838\n",
      "Training epoch: 32, train loss: 0.02673, val loss: 0.02695\n",
      "Training epoch: 33, train loss: 0.02774, val loss: 0.02781\n",
      "Training epoch: 34, train loss: 0.02628, val loss: 0.02649\n",
      "Training epoch: 35, train loss: 0.02623, val loss: 0.02635\n",
      "Training epoch: 36, train loss: 0.02549, val loss: 0.02578\n",
      "Training epoch: 37, train loss: 0.02553, val loss: 0.02563\n",
      "Training epoch: 38, train loss: 0.02468, val loss: 0.02498\n",
      "Training epoch: 39, train loss: 0.02540, val loss: 0.02566\n",
      "Training epoch: 40, train loss: 0.02479, val loss: 0.02508\n",
      "Training epoch: 41, train loss: 0.02460, val loss: 0.02492\n",
      "Training epoch: 42, train loss: 0.02424, val loss: 0.02450\n",
      "Training epoch: 43, train loss: 0.02567, val loss: 0.02605\n",
      "Training epoch: 44, train loss: 0.02370, val loss: 0.02399\n",
      "Training epoch: 45, train loss: 0.02404, val loss: 0.02429\n",
      "Training epoch: 46, train loss: 0.02341, val loss: 0.02370\n",
      "Training epoch: 47, train loss: 0.02293, val loss: 0.02330\n",
      "Training epoch: 48, train loss: 0.02338, val loss: 0.02367\n",
      "Training epoch: 49, train loss: 0.02346, val loss: 0.02388\n",
      "Training epoch: 50, train loss: 0.02285, val loss: 0.02318\n",
      "Training epoch: 51, train loss: 0.02293, val loss: 0.02328\n",
      "Training epoch: 52, train loss: 0.02311, val loss: 0.02353\n",
      "Training epoch: 53, train loss: 0.02336, val loss: 0.02372\n",
      "Training epoch: 54, train loss: 0.02251, val loss: 0.02283\n",
      "Training epoch: 55, train loss: 0.02229, val loss: 0.02269\n",
      "Training epoch: 56, train loss: 0.02248, val loss: 0.02274\n",
      "Training epoch: 57, train loss: 0.02236, val loss: 0.02270\n",
      "Training epoch: 58, train loss: 0.02234, val loss: 0.02260\n",
      "Training epoch: 59, train loss: 0.02227, val loss: 0.02268\n",
      "Training epoch: 60, train loss: 0.02227, val loss: 0.02253\n",
      "Training epoch: 61, train loss: 0.02176, val loss: 0.02218\n",
      "Training epoch: 62, train loss: 0.02179, val loss: 0.02214\n",
      "Training epoch: 63, train loss: 0.02194, val loss: 0.02234\n",
      "Training epoch: 64, train loss: 0.02170, val loss: 0.02212\n",
      "Training epoch: 65, train loss: 0.02156, val loss: 0.02192\n",
      "Training epoch: 66, train loss: 0.02155, val loss: 0.02194\n",
      "Training epoch: 67, train loss: 0.02151, val loss: 0.02190\n",
      "Training epoch: 68, train loss: 0.02176, val loss: 0.02225\n",
      "Training epoch: 69, train loss: 0.02162, val loss: 0.02209\n",
      "Training epoch: 70, train loss: 0.02210, val loss: 0.02251\n",
      "Training epoch: 71, train loss: 0.02157, val loss: 0.02207\n",
      "Training epoch: 72, train loss: 0.02201, val loss: 0.02233\n",
      "Training epoch: 73, train loss: 0.02172, val loss: 0.02213\n",
      "Training epoch: 74, train loss: 0.02128, val loss: 0.02171\n",
      "Training epoch: 75, train loss: 0.02127, val loss: 0.02170\n",
      "Training epoch: 76, train loss: 0.02196, val loss: 0.02235\n",
      "Training epoch: 77, train loss: 0.02139, val loss: 0.02185\n",
      "Training epoch: 78, train loss: 0.02117, val loss: 0.02163\n",
      "Training epoch: 79, train loss: 0.02113, val loss: 0.02145\n",
      "Training epoch: 80, train loss: 0.02107, val loss: 0.02149\n",
      "Training epoch: 81, train loss: 0.02117, val loss: 0.02167\n",
      "Training epoch: 82, train loss: 0.02104, val loss: 0.02153\n",
      "Training epoch: 83, train loss: 0.02119, val loss: 0.02169\n",
      "Training epoch: 84, train loss: 0.02120, val loss: 0.02164\n",
      "Training epoch: 85, train loss: 0.02086, val loss: 0.02128\n",
      "Training epoch: 86, train loss: 0.02103, val loss: 0.02152\n",
      "Training epoch: 87, train loss: 0.02097, val loss: 0.02140\n",
      "Training epoch: 88, train loss: 0.02114, val loss: 0.02166\n",
      "Training epoch: 89, train loss: 0.02094, val loss: 0.02137\n",
      "Training epoch: 90, train loss: 0.02103, val loss: 0.02148\n",
      "Training epoch: 91, train loss: 0.02078, val loss: 0.02123\n",
      "Training epoch: 92, train loss: 0.02069, val loss: 0.02117\n",
      "Training epoch: 93, train loss: 0.02075, val loss: 0.02113\n",
      "Training epoch: 94, train loss: 0.02070, val loss: 0.02115\n",
      "Training epoch: 95, train loss: 0.02103, val loss: 0.02139\n",
      "Training epoch: 96, train loss: 0.02168, val loss: 0.02211\n",
      "Training epoch: 97, train loss: 0.02069, val loss: 0.02112\n",
      "Training epoch: 98, train loss: 0.02099, val loss: 0.02142\n",
      "Training epoch: 99, train loss: 0.02063, val loss: 0.02106\n",
      "Training epoch: 100, train loss: 0.02091, val loss: 0.02142\n",
      "Training epoch: 101, train loss: 0.02058, val loss: 0.02108\n",
      "Training epoch: 102, train loss: 0.02081, val loss: 0.02137\n",
      "Training epoch: 103, train loss: 0.02073, val loss: 0.02113\n",
      "Training epoch: 104, train loss: 0.02057, val loss: 0.02107\n",
      "Training epoch: 105, train loss: 0.02045, val loss: 0.02098\n",
      "Training epoch: 106, train loss: 0.02070, val loss: 0.02109\n",
      "Training epoch: 107, train loss: 0.02077, val loss: 0.02126\n",
      "Training epoch: 108, train loss: 0.02040, val loss: 0.02090\n",
      "Training epoch: 109, train loss: 0.02043, val loss: 0.02088\n",
      "Training epoch: 110, train loss: 0.02051, val loss: 0.02102\n",
      "Training epoch: 111, train loss: 0.02036, val loss: 0.02085\n",
      "Training epoch: 112, train loss: 0.02072, val loss: 0.02112\n",
      "Training epoch: 113, train loss: 0.02076, val loss: 0.02123\n",
      "Training epoch: 114, train loss: 0.02029, val loss: 0.02074\n",
      "Training epoch: 115, train loss: 0.02036, val loss: 0.02094\n",
      "Training epoch: 116, train loss: 0.02026, val loss: 0.02067\n",
      "Training epoch: 117, train loss: 0.02057, val loss: 0.02102\n",
      "Training epoch: 118, train loss: 0.02042, val loss: 0.02089\n",
      "Training epoch: 119, train loss: 0.02053, val loss: 0.02092\n",
      "Training epoch: 120, train loss: 0.02030, val loss: 0.02073\n",
      "Training epoch: 121, train loss: 0.02057, val loss: 0.02104\n",
      "Training epoch: 122, train loss: 0.02018, val loss: 0.02074\n",
      "Training epoch: 123, train loss: 0.02116, val loss: 0.02158\n",
      "Training epoch: 124, train loss: 0.02046, val loss: 0.02100\n",
      "Training epoch: 125, train loss: 0.02084, val loss: 0.02151\n",
      "Training epoch: 126, train loss: 0.02116, val loss: 0.02184\n",
      "Training epoch: 127, train loss: 0.02028, val loss: 0.02078\n",
      "Training epoch: 128, train loss: 0.02006, val loss: 0.02061\n",
      "Training epoch: 129, train loss: 0.02056, val loss: 0.02110\n",
      "Training epoch: 130, train loss: 0.02039, val loss: 0.02098\n",
      "Training epoch: 131, train loss: 0.02034, val loss: 0.02081\n",
      "Training epoch: 132, train loss: 0.02032, val loss: 0.02083\n",
      "Training epoch: 133, train loss: 0.02000, val loss: 0.02053\n",
      "Training epoch: 134, train loss: 0.02005, val loss: 0.02063\n",
      "Training epoch: 135, train loss: 0.02015, val loss: 0.02071\n",
      "Training epoch: 136, train loss: 0.02004, val loss: 0.02058\n",
      "Training epoch: 137, train loss: 0.02041, val loss: 0.02100\n",
      "Training epoch: 138, train loss: 0.01996, val loss: 0.02046\n",
      "Training epoch: 139, train loss: 0.01989, val loss: 0.02045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 140, train loss: 0.02012, val loss: 0.02056\n",
      "Training epoch: 141, train loss: 0.02019, val loss: 0.02076\n",
      "Training epoch: 142, train loss: 0.01987, val loss: 0.02043\n",
      "Training epoch: 143, train loss: 0.01997, val loss: 0.02046\n",
      "Training epoch: 144, train loss: 0.01988, val loss: 0.02035\n",
      "Training epoch: 145, train loss: 0.01988, val loss: 0.02046\n",
      "Training epoch: 146, train loss: 0.01991, val loss: 0.02040\n",
      "Training epoch: 147, train loss: 0.01995, val loss: 0.02058\n",
      "Training epoch: 148, train loss: 0.02026, val loss: 0.02087\n",
      "Training epoch: 149, train loss: 0.01972, val loss: 0.02024\n",
      "Training epoch: 150, train loss: 0.02097, val loss: 0.02165\n",
      "Training epoch: 151, train loss: 0.01993, val loss: 0.02054\n",
      "Training epoch: 152, train loss: 0.02037, val loss: 0.02087\n",
      "Training epoch: 153, train loss: 0.01970, val loss: 0.02024\n",
      "Training epoch: 154, train loss: 0.02011, val loss: 0.02082\n",
      "Training epoch: 155, train loss: 0.01970, val loss: 0.02024\n",
      "Training epoch: 156, train loss: 0.01988, val loss: 0.02050\n",
      "Training epoch: 157, train loss: 0.01963, val loss: 0.02016\n",
      "Training epoch: 158, train loss: 0.01995, val loss: 0.02059\n",
      "Training epoch: 159, train loss: 0.01971, val loss: 0.02024\n",
      "Training epoch: 160, train loss: 0.02027, val loss: 0.02081\n",
      "Training epoch: 161, train loss: 0.01959, val loss: 0.02017\n",
      "Training epoch: 162, train loss: 0.01970, val loss: 0.02024\n",
      "Training epoch: 163, train loss: 0.01978, val loss: 0.02046\n",
      "Training epoch: 164, train loss: 0.01968, val loss: 0.02032\n",
      "Training epoch: 165, train loss: 0.01962, val loss: 0.02018\n",
      "Training epoch: 166, train loss: 0.01969, val loss: 0.02037\n",
      "Training epoch: 167, train loss: 0.01964, val loss: 0.02014\n",
      "Training epoch: 168, train loss: 0.01977, val loss: 0.02027\n",
      "Training epoch: 169, train loss: 0.01996, val loss: 0.02058\n",
      "Training epoch: 170, train loss: 0.01975, val loss: 0.02024\n",
      "Training epoch: 171, train loss: 0.01984, val loss: 0.02056\n",
      "Training epoch: 172, train loss: 0.01955, val loss: 0.02017\n",
      "Training epoch: 173, train loss: 0.01968, val loss: 0.02026\n",
      "Training epoch: 174, train loss: 0.01960, val loss: 0.02010\n",
      "Training epoch: 175, train loss: 0.02007, val loss: 0.02085\n",
      "Training epoch: 176, train loss: 0.01938, val loss: 0.01997\n",
      "Training epoch: 177, train loss: 0.02073, val loss: 0.02118\n",
      "Training epoch: 178, train loss: 0.01971, val loss: 0.02031\n",
      "Training epoch: 179, train loss: 0.01969, val loss: 0.02022\n",
      "Training epoch: 180, train loss: 0.01938, val loss: 0.01998\n",
      "Training epoch: 181, train loss: 0.01933, val loss: 0.01998\n",
      "Training epoch: 182, train loss: 0.01950, val loss: 0.01999\n",
      "Training epoch: 183, train loss: 0.01977, val loss: 0.02028\n",
      "Training epoch: 184, train loss: 0.01961, val loss: 0.02027\n",
      "Training epoch: 185, train loss: 0.01964, val loss: 0.02039\n",
      "Training epoch: 186, train loss: 0.01928, val loss: 0.01990\n",
      "Training epoch: 187, train loss: 0.01976, val loss: 0.02051\n",
      "Training epoch: 188, train loss: 0.01936, val loss: 0.02002\n",
      "Training epoch: 189, train loss: 0.01941, val loss: 0.02006\n",
      "Training epoch: 190, train loss: 0.01927, val loss: 0.01981\n",
      "Training epoch: 191, train loss: 0.02006, val loss: 0.02060\n",
      "Training epoch: 192, train loss: 0.01964, val loss: 0.02017\n",
      "Training epoch: 193, train loss: 0.01946, val loss: 0.02000\n",
      "Training epoch: 194, train loss: 0.01938, val loss: 0.01990\n",
      "Training epoch: 195, train loss: 0.01924, val loss: 0.01993\n",
      "Training epoch: 196, train loss: 0.01952, val loss: 0.02023\n",
      "Training epoch: 197, train loss: 0.02008, val loss: 0.02054\n",
      "Training epoch: 198, train loss: 0.01918, val loss: 0.01973\n",
      "Training epoch: 199, train loss: 0.01936, val loss: 0.01988\n",
      "Training epoch: 200, train loss: 0.01926, val loss: 0.01999\n",
      "Training epoch: 201, train loss: 0.01936, val loss: 0.01991\n",
      "Training epoch: 202, train loss: 0.01965, val loss: 0.02043\n",
      "Training epoch: 203, train loss: 0.01957, val loss: 0.02026\n",
      "Training epoch: 204, train loss: 0.01907, val loss: 0.01960\n",
      "Training epoch: 205, train loss: 0.01902, val loss: 0.01965\n",
      "Training epoch: 206, train loss: 0.01931, val loss: 0.01991\n",
      "Training epoch: 207, train loss: 0.01904, val loss: 0.01968\n",
      "Training epoch: 208, train loss: 0.01895, val loss: 0.01959\n",
      "Training epoch: 209, train loss: 0.01893, val loss: 0.01959\n",
      "Training epoch: 210, train loss: 0.01917, val loss: 0.01975\n",
      "Training epoch: 211, train loss: 0.01912, val loss: 0.01977\n",
      "Training epoch: 212, train loss: 0.02022, val loss: 0.02107\n",
      "Training epoch: 213, train loss: 0.01902, val loss: 0.01972\n",
      "Training epoch: 214, train loss: 0.01885, val loss: 0.01945\n",
      "Training epoch: 215, train loss: 0.01887, val loss: 0.01949\n",
      "Training epoch: 216, train loss: 0.01922, val loss: 0.01976\n",
      "Training epoch: 217, train loss: 0.01913, val loss: 0.01985\n",
      "Training epoch: 218, train loss: 0.01909, val loss: 0.01972\n",
      "Training epoch: 219, train loss: 0.01888, val loss: 0.01954\n",
      "Training epoch: 220, train loss: 0.01918, val loss: 0.01995\n",
      "Training epoch: 221, train loss: 0.02130, val loss: 0.02227\n",
      "Training epoch: 222, train loss: 0.01902, val loss: 0.01972\n",
      "Training epoch: 223, train loss: 0.01939, val loss: 0.02019\n",
      "Training epoch: 224, train loss: 0.01899, val loss: 0.01974\n",
      "Training epoch: 225, train loss: 0.01913, val loss: 0.01988\n",
      "Training epoch: 226, train loss: 0.01902, val loss: 0.01974\n",
      "Training epoch: 227, train loss: 0.01873, val loss: 0.01948\n",
      "Training epoch: 228, train loss: 0.01870, val loss: 0.01933\n",
      "Training epoch: 229, train loss: 0.01917, val loss: 0.01987\n",
      "Training epoch: 230, train loss: 0.01929, val loss: 0.02003\n",
      "Training epoch: 231, train loss: 0.01867, val loss: 0.01932\n",
      "Training epoch: 232, train loss: 0.01863, val loss: 0.01927\n",
      "Training epoch: 233, train loss: 0.01853, val loss: 0.01920\n",
      "Training epoch: 234, train loss: 0.01895, val loss: 0.01945\n",
      "Training epoch: 235, train loss: 0.01878, val loss: 0.01928\n",
      "Training epoch: 236, train loss: 0.01855, val loss: 0.01920\n",
      "Training epoch: 237, train loss: 0.01872, val loss: 0.01943\n",
      "Training epoch: 238, train loss: 0.01887, val loss: 0.01958\n",
      "Training epoch: 239, train loss: 0.01864, val loss: 0.01940\n",
      "Training epoch: 240, train loss: 0.01857, val loss: 0.01915\n",
      "Training epoch: 241, train loss: 0.01868, val loss: 0.01933\n",
      "Training epoch: 242, train loss: 0.01838, val loss: 0.01907\n",
      "Training epoch: 243, train loss: 0.01854, val loss: 0.01917\n",
      "Training epoch: 244, train loss: 0.01868, val loss: 0.01940\n",
      "Training epoch: 245, train loss: 0.01930, val loss: 0.02020\n",
      "Training epoch: 246, train loss: 0.01875, val loss: 0.01955\n",
      "Training epoch: 247, train loss: 0.01869, val loss: 0.01949\n",
      "Training epoch: 248, train loss: 0.01834, val loss: 0.01901\n",
      "Training epoch: 249, train loss: 0.01852, val loss: 0.01929\n",
      "Training epoch: 250, train loss: 0.01872, val loss: 0.01923\n",
      "Training epoch: 251, train loss: 0.01874, val loss: 0.01943\n",
      "Training epoch: 252, train loss: 0.01843, val loss: 0.01900\n",
      "Training epoch: 253, train loss: 0.01838, val loss: 0.01896\n",
      "Training epoch: 254, train loss: 0.01856, val loss: 0.01921\n",
      "Training epoch: 255, train loss: 0.01835, val loss: 0.01915\n",
      "Training epoch: 256, train loss: 0.01815, val loss: 0.01885\n",
      "Training epoch: 257, train loss: 0.01887, val loss: 0.01971\n",
      "Training epoch: 258, train loss: 0.01812, val loss: 0.01883\n",
      "Training epoch: 259, train loss: 0.01826, val loss: 0.01885\n",
      "Training epoch: 260, train loss: 0.01826, val loss: 0.01888\n",
      "Training epoch: 261, train loss: 0.01807, val loss: 0.01877\n",
      "Training epoch: 262, train loss: 0.01858, val loss: 0.01915\n",
      "Training epoch: 263, train loss: 0.01931, val loss: 0.02029\n",
      "Training epoch: 264, train loss: 0.01809, val loss: 0.01878\n",
      "Training epoch: 265, train loss: 0.01811, val loss: 0.01874\n",
      "Training epoch: 266, train loss: 0.01822, val loss: 0.01900\n",
      "Training epoch: 267, train loss: 0.01831, val loss: 0.01894\n",
      "Training epoch: 268, train loss: 0.01856, val loss: 0.01909\n",
      "Training epoch: 269, train loss: 0.01872, val loss: 0.01945\n",
      "Training epoch: 270, train loss: 0.01791, val loss: 0.01862\n",
      "Training epoch: 271, train loss: 0.01807, val loss: 0.01878\n",
      "Training epoch: 272, train loss: 0.01829, val loss: 0.01888\n",
      "Training epoch: 273, train loss: 0.01803, val loss: 0.01868\n",
      "Training epoch: 274, train loss: 0.01798, val loss: 0.01862\n",
      "Training epoch: 275, train loss: 0.01851, val loss: 0.01919\n",
      "Training epoch: 276, train loss: 0.01792, val loss: 0.01859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 277, train loss: 0.01799, val loss: 0.01862\n",
      "Training epoch: 278, train loss: 0.01796, val loss: 0.01860\n",
      "Training epoch: 279, train loss: 0.01776, val loss: 0.01848\n",
      "Training epoch: 280, train loss: 0.01773, val loss: 0.01839\n",
      "Training epoch: 281, train loss: 0.01813, val loss: 0.01881\n",
      "Training epoch: 282, train loss: 0.01782, val loss: 0.01853\n",
      "Training epoch: 283, train loss: 0.01800, val loss: 0.01867\n",
      "Training epoch: 284, train loss: 0.01795, val loss: 0.01855\n",
      "Training epoch: 285, train loss: 0.01767, val loss: 0.01831\n",
      "Training epoch: 286, train loss: 0.01791, val loss: 0.01875\n",
      "Training epoch: 287, train loss: 0.01810, val loss: 0.01886\n",
      "Training epoch: 288, train loss: 0.01813, val loss: 0.01887\n",
      "Training epoch: 289, train loss: 0.01770, val loss: 0.01835\n",
      "Training epoch: 290, train loss: 0.01797, val loss: 0.01858\n",
      "Training epoch: 291, train loss: 0.01809, val loss: 0.01890\n",
      "Training epoch: 292, train loss: 0.01807, val loss: 0.01889\n",
      "Training epoch: 293, train loss: 0.01761, val loss: 0.01832\n",
      "Training epoch: 294, train loss: 0.01783, val loss: 0.01867\n",
      "Training epoch: 295, train loss: 0.01821, val loss: 0.01909\n",
      "Training epoch: 296, train loss: 0.01758, val loss: 0.01834\n",
      "Training epoch: 297, train loss: 0.01767, val loss: 0.01839\n",
      "Training epoch: 298, train loss: 0.01821, val loss: 0.01882\n",
      "Training epoch: 299, train loss: 0.01768, val loss: 0.01840\n",
      "Training epoch: 300, train loss: 0.01781, val loss: 0.01863\n",
      "Training epoch: 301, train loss: 0.01745, val loss: 0.01820\n",
      "Training epoch: 302, train loss: 0.01812, val loss: 0.01892\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from exnn import ExNN\n",
    "\n",
    "def data_generator1(datanum, testnum=10000, noise_sigma=1, rand_seed=0):\n",
    "    \n",
    "    corr = 0.5\n",
    "    np.random.seed(rand_seed)\n",
    "    proj_matrix = np.zeros((10, 4))\n",
    "    proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "    proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "    proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "    proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "    u = np.random.uniform(-1, 1, [datanum + testnum, 1])\n",
    "    t = np.sqrt(corr / (1 - corr))\n",
    "    x = np.zeros((datanum + testnum, 10))\n",
    "    for i in range(10):\n",
    "        x[:, i:i + 1] = (np.random.uniform(-1, 1, [datanum + testnum, 1]) + t * u) / (1 + t)\n",
    "\n",
    "    y = np.reshape(2 * np.dot(x, proj_matrix[:, 0]) + 0.2 * np.exp(-4 * np.dot(x, proj_matrix[:, 1])) + \\\n",
    "                   3 * (np.dot(x, proj_matrix[:, 2]))**2 + 2.5 * np.sin(np.pi * np.dot(x, proj_matrix[:, 3])), [-1, 1]) + \\\n",
    "              noise_sigma * np.random.normal(0, 1, [datanum + testnum, 1])\n",
    "    \n",
    "    task_type = \"Regression\"\n",
    "    meta_info = {\"X1\":{\"type\":\"continuous\"},\n",
    "             \"X2\":{\"type\":\"continuous\"},\n",
    "             \"X3\":{\"type\":\"continuous\"},\n",
    "             \"X4\":{\"type\":\"continuous\"},\n",
    "             \"X5\":{\"type\":\"continuous\"},\n",
    "             \"X6\":{\"type\":\"continuous\"},\n",
    "             \"X7\":{\"type\":\"continuous\"},\n",
    "             \"X8\":{\"type\":\"continuous\"},\n",
    "             \"X9\":{\"type\":\"continuous\"},\n",
    "             \"X10\":{\"type\":\"continuous\"},\n",
    "             \"Y\":{\"type\":\"target\"}}\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == \"target\":\n",
    "            sy = MinMaxScaler((-1, 1))\n",
    "            y = sy.fit_transform(y)\n",
    "            meta_info[key][\"scaler\"] = sy\n",
    "        elif item['type'] == \"categorical\":\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(x[:,[i]])\n",
    "            ordinal_feature = enc.transform(x[:,[i]])\n",
    "            x[:,[i]] = ordinal_feature\n",
    "            meta_info[key][\"values\"] = enc.categories_[0].tolist()\n",
    "        else:\n",
    "            sx = MinMaxScaler((-1, 1))\n",
    "            x[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "            meta_info[key][\"scaler\"] = sx\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=testnum, random_state=rand_seed)\n",
    "    return train_x, test_x, train_y, test_y, task_type, meta_info\n",
    "\n",
    "train_x, test_x, train_y, test_y, task_type, meta_info = data_generator1(datanum=10000, testnum=10000, noise_sigma=1, rand_seed=0)\n",
    "model = ExNN(meta_info=meta_info,\n",
    "               subnet_num=10,\n",
    "               subnet_arch=[10, 6],\n",
    "               task_type=task_type,\n",
    "               activation_func=tf.tanh,\n",
    "               batch_size=min(1000, int(train_x.shape[0] * 0.2)),\n",
    "               training_epochs=5000,\n",
    "               lr_bp=0.001,\n",
    "               lr_cl=0.1,\n",
    "               beta_threshold=0.05,\n",
    "               tuning_epochs=100,\n",
    "               l1_proj=0.0001,\n",
    "               l1_subnet=0.00316,\n",
    "               l2_smooth=10**(-6),\n",
    "               verbose=True,\n",
    "               val_ratio=0.2,\n",
    "               early_stop_thres=100)\n",
    "\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-21T06:03:53.759Z"
    }
   },
   "outputs": [],
   "source": [
    "model.visualize(folder=\"./\", name=\"exnn_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-21T06:03:53.769Z"
    }
   },
   "outputs": [],
   "source": [
    "model.visualize_new(cols_per_row=3, subnet_num=3, dummy_subnet_num=0, folder=\"./\", name=\"exnn_demo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
