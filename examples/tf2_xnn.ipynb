{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import comb\n",
    "import matplotlib.lines as mlines\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.stats import ortho_group\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "from xnn.xnn import xNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "corr = 0.5\n",
    "noise_sigma = 1\n",
    "dummy_num = 0\n",
    "feature_num = 10\n",
    "test_num = 10000\n",
    "data_num = 10000\n",
    "\n",
    "proj_matrix = np.zeros((feature_num,4))\n",
    "proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "\n",
    "def data_generator1(data_num, feature_num, corr, proj_matrix, noise_sigma):\n",
    "    u = np.random.uniform(-1,1, [data_num, 1])\n",
    "    t= np.sqrt(corr/(1-corr))\n",
    "    X = np.zeros((data_num, feature_num))\n",
    "    for i in range(feature_num):\n",
    "        X[:, i:i+1] = (np.random.uniform(-1,1,[data_num,1])+t*u)/(1+t)\n",
    "    Y = np.reshape(2*np.dot(X, proj_matrix[:,0])+0.2*np.exp(-4*np.dot(X, proj_matrix[:,1])) + \\\n",
    "              3*(np.dot(X, proj_matrix[:,2]))**2+2.5*np.sin(np.pi*np.dot(X, proj_matrix[:,3])), [-1,1]) + \\\n",
    "              noise_sigma*np.random.normal(0,1, [data_num,1])\n",
    "    return X, Y\n",
    "\n",
    "np.random.seed(0)\n",
    "X, Y = data_generator1(data_num+test_num, feature_num+dummy_num, corr, proj_matrix, noise_sigma)\n",
    "scaler_x = MinMaxScaler((-1, 1)); scaler_y = MinMaxScaler((-1, 1))\n",
    "sX = scaler_x.fit_transform(X); sY = scaler_y.fit_transform(Y)\n",
    "train_x, test_x, train_y, test_y = train_test_split(sX, sY, test_size = test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n",
      "Training epoch: 1, train loss: 0.06969, val loss: 0.06630\n",
      "Training epoch: 2, train loss: 0.06052, val loss: 0.05732\n",
      "Training epoch: 3, train loss: 0.05369, val loss: 0.05104\n",
      "Training epoch: 4, train loss: 0.04928, val loss: 0.04727\n",
      "Training epoch: 5, train loss: 0.04722, val loss: 0.04571\n",
      "Training epoch: 6, train loss: 0.04564, val loss: 0.04451\n",
      "Training epoch: 7, train loss: 0.04413, val loss: 0.04323\n",
      "Training epoch: 8, train loss: 0.04285, val loss: 0.04221\n",
      "Training epoch: 9, train loss: 0.04172, val loss: 0.04139\n",
      "Training epoch: 10, train loss: 0.04079, val loss: 0.04072\n",
      "Training epoch: 11, train loss: 0.03997, val loss: 0.04016\n",
      "Training epoch: 12, train loss: 0.03927, val loss: 0.03973\n",
      "Training epoch: 13, train loss: 0.03867, val loss: 0.03931\n",
      "Training epoch: 14, train loss: 0.03815, val loss: 0.03900\n",
      "Training epoch: 15, train loss: 0.03768, val loss: 0.03864\n",
      "Training epoch: 16, train loss: 0.03727, val loss: 0.03835\n",
      "Training epoch: 17, train loss: 0.03690, val loss: 0.03812\n",
      "Training epoch: 18, train loss: 0.03658, val loss: 0.03787\n",
      "Training epoch: 19, train loss: 0.03627, val loss: 0.03764\n",
      "Training epoch: 20, train loss: 0.03601, val loss: 0.03743\n",
      "Training epoch: 21, train loss: 0.03576, val loss: 0.03717\n",
      "Training epoch: 22, train loss: 0.03552, val loss: 0.03701\n",
      "Training epoch: 23, train loss: 0.03529, val loss: 0.03677\n",
      "Training epoch: 24, train loss: 0.03508, val loss: 0.03659\n",
      "Training epoch: 25, train loss: 0.03489, val loss: 0.03639\n",
      "Training epoch: 26, train loss: 0.03471, val loss: 0.03622\n",
      "Training epoch: 27, train loss: 0.03454, val loss: 0.03605\n",
      "Training epoch: 28, train loss: 0.03439, val loss: 0.03588\n",
      "Training epoch: 29, train loss: 0.03425, val loss: 0.03579\n",
      "Training epoch: 30, train loss: 0.03411, val loss: 0.03560\n",
      "Training epoch: 31, train loss: 0.03397, val loss: 0.03543\n",
      "Training epoch: 32, train loss: 0.03383, val loss: 0.03526\n",
      "Training epoch: 33, train loss: 0.03369, val loss: 0.03508\n",
      "Training epoch: 34, train loss: 0.03355, val loss: 0.03493\n",
      "Training epoch: 35, train loss: 0.03340, val loss: 0.03473\n",
      "Training epoch: 36, train loss: 0.03326, val loss: 0.03455\n",
      "Training epoch: 37, train loss: 0.03309, val loss: 0.03435\n",
      "Training epoch: 38, train loss: 0.03292, val loss: 0.03414\n",
      "Training epoch: 39, train loss: 0.03273, val loss: 0.03394\n",
      "Training epoch: 40, train loss: 0.03253, val loss: 0.03366\n",
      "Training epoch: 41, train loss: 0.03228, val loss: 0.03336\n",
      "Training epoch: 42, train loss: 0.03199, val loss: 0.03306\n",
      "Training epoch: 43, train loss: 0.03167, val loss: 0.03273\n",
      "Training epoch: 44, train loss: 0.03131, val loss: 0.03232\n",
      "Training epoch: 45, train loss: 0.03084, val loss: 0.03185\n",
      "Training epoch: 46, train loss: 0.03031, val loss: 0.03129\n",
      "Training epoch: 47, train loss: 0.02964, val loss: 0.03061\n",
      "Training epoch: 48, train loss: 0.02884, val loss: 0.02980\n",
      "Training epoch: 49, train loss: 0.02783, val loss: 0.02878\n",
      "Training epoch: 50, train loss: 0.02667, val loss: 0.02759\n",
      "Training epoch: 51, train loss: 0.02539, val loss: 0.02638\n",
      "Training epoch: 52, train loss: 0.02405, val loss: 0.02504\n",
      "Training epoch: 53, train loss: 0.02283, val loss: 0.02384\n",
      "Training epoch: 54, train loss: 0.02177, val loss: 0.02283\n",
      "Training epoch: 55, train loss: 0.02089, val loss: 0.02197\n",
      "Training epoch: 56, train loss: 0.02013, val loss: 0.02123\n",
      "Training epoch: 57, train loss: 0.01954, val loss: 0.02064\n",
      "Training epoch: 58, train loss: 0.01904, val loss: 0.02013\n",
      "Training epoch: 59, train loss: 0.01863, val loss: 0.01971\n",
      "Training epoch: 60, train loss: 0.01830, val loss: 0.01937\n",
      "Training epoch: 61, train loss: 0.01802, val loss: 0.01906\n",
      "Training epoch: 62, train loss: 0.01782, val loss: 0.01884\n",
      "Training epoch: 63, train loss: 0.01766, val loss: 0.01867\n",
      "Training epoch: 64, train loss: 0.01743, val loss: 0.01841\n",
      "Training epoch: 65, train loss: 0.01728, val loss: 0.01824\n",
      "Training epoch: 66, train loss: 0.01723, val loss: 0.01818\n",
      "Training epoch: 67, train loss: 0.01710, val loss: 0.01800\n",
      "Training epoch: 68, train loss: 0.01706, val loss: 0.01798\n",
      "Training epoch: 69, train loss: 0.01693, val loss: 0.01779\n",
      "Training epoch: 70, train loss: 0.01691, val loss: 0.01778\n",
      "Training epoch: 71, train loss: 0.01676, val loss: 0.01761\n",
      "Training epoch: 72, train loss: 0.01673, val loss: 0.01759\n",
      "Training epoch: 73, train loss: 0.01660, val loss: 0.01741\n",
      "Training epoch: 74, train loss: 0.01659, val loss: 0.01738\n",
      "Training epoch: 75, train loss: 0.01650, val loss: 0.01728\n",
      "Training epoch: 76, train loss: 0.01646, val loss: 0.01723\n",
      "Training epoch: 77, train loss: 0.01640, val loss: 0.01716\n",
      "Training epoch: 78, train loss: 0.01636, val loss: 0.01712\n",
      "Training epoch: 79, train loss: 0.01638, val loss: 0.01713\n",
      "Training epoch: 80, train loss: 0.01628, val loss: 0.01701\n",
      "Training epoch: 81, train loss: 0.01636, val loss: 0.01707\n",
      "Training epoch: 82, train loss: 0.01621, val loss: 0.01692\n",
      "Training epoch: 83, train loss: 0.01623, val loss: 0.01693\n",
      "Training epoch: 84, train loss: 0.01617, val loss: 0.01687\n",
      "Training epoch: 85, train loss: 0.01613, val loss: 0.01680\n",
      "Training epoch: 86, train loss: 0.01608, val loss: 0.01674\n",
      "Training epoch: 87, train loss: 0.01605, val loss: 0.01671\n",
      "Training epoch: 88, train loss: 0.01602, val loss: 0.01667\n",
      "Training epoch: 89, train loss: 0.01599, val loss: 0.01664\n",
      "Training epoch: 90, train loss: 0.01596, val loss: 0.01659\n",
      "Training epoch: 91, train loss: 0.01595, val loss: 0.01658\n",
      "Training epoch: 92, train loss: 0.01594, val loss: 0.01656\n",
      "Training epoch: 93, train loss: 0.01591, val loss: 0.01653\n",
      "Training epoch: 94, train loss: 0.01591, val loss: 0.01651\n",
      "Training epoch: 95, train loss: 0.01588, val loss: 0.01648\n",
      "Training epoch: 96, train loss: 0.01584, val loss: 0.01643\n",
      "Training epoch: 97, train loss: 0.01584, val loss: 0.01642\n",
      "Training epoch: 98, train loss: 0.01583, val loss: 0.01641\n",
      "Training epoch: 99, train loss: 0.01582, val loss: 0.01642\n",
      "Training epoch: 100, train loss: 0.01596, val loss: 0.01652\n",
      "Training epoch: 101, train loss: 0.01589, val loss: 0.01643\n",
      "Training epoch: 102, train loss: 0.01576, val loss: 0.01636\n",
      "Training epoch: 103, train loss: 0.01577, val loss: 0.01633\n",
      "Training epoch: 104, train loss: 0.01574, val loss: 0.01628\n",
      "Training epoch: 105, train loss: 0.01576, val loss: 0.01630\n",
      "Training epoch: 106, train loss: 0.01571, val loss: 0.01625\n",
      "Training epoch: 107, train loss: 0.01569, val loss: 0.01623\n",
      "Training epoch: 108, train loss: 0.01570, val loss: 0.01623\n",
      "Training epoch: 109, train loss: 0.01575, val loss: 0.01628\n",
      "Training epoch: 110, train loss: 0.01567, val loss: 0.01620\n",
      "Training epoch: 111, train loss: 0.01568, val loss: 0.01620\n",
      "Training epoch: 112, train loss: 0.01566, val loss: 0.01616\n",
      "Training epoch: 113, train loss: 0.01563, val loss: 0.01615\n",
      "Training epoch: 114, train loss: 0.01563, val loss: 0.01618\n",
      "Training epoch: 115, train loss: 0.01564, val loss: 0.01617\n",
      "Training epoch: 116, train loss: 0.01561, val loss: 0.01611\n",
      "Training epoch: 117, train loss: 0.01561, val loss: 0.01611\n",
      "Training epoch: 118, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 119, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 120, train loss: 0.01558, val loss: 0.01608\n",
      "Training epoch: 121, train loss: 0.01560, val loss: 0.01611\n",
      "Training epoch: 122, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 123, train loss: 0.01557, val loss: 0.01606\n",
      "Training epoch: 124, train loss: 0.01580, val loss: 0.01629\n",
      "Training epoch: 125, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 126, train loss: 0.01568, val loss: 0.01617\n",
      "Training epoch: 127, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 128, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 129, train loss: 0.01556, val loss: 0.01607\n",
      "Training epoch: 130, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 131, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 132, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 133, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 134, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 135, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 136, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 137, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 138, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 139, train loss: 0.01550, val loss: 0.01597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 140, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 141, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 142, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 143, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 144, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 145, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 146, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 147, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 148, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 149, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 150, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 151, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 152, train loss: 0.01544, val loss: 0.01588\n",
      "Training epoch: 153, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 154, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 155, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 156, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 157, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 158, train loss: 0.01542, val loss: 0.01586\n",
      "Training epoch: 159, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 160, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 161, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 162, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 163, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 164, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 165, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 166, train loss: 0.01541, val loss: 0.01583\n",
      "Training epoch: 167, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 168, train loss: 0.01544, val loss: 0.01588\n",
      "Training epoch: 169, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 170, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 171, train loss: 0.01538, val loss: 0.01581\n",
      "Training epoch: 172, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 173, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 174, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 175, train loss: 0.01537, val loss: 0.01580\n",
      "Training epoch: 176, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 177, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 178, train loss: 0.01547, val loss: 0.01591\n",
      "Training epoch: 179, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 180, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 181, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 182, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 183, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 184, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 185, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 186, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 187, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 188, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 189, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 190, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 191, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 192, train loss: 0.01539, val loss: 0.01582\n",
      "Training epoch: 193, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 194, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 195, train loss: 0.01536, val loss: 0.01579\n",
      "Training epoch: 196, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 197, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 198, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 199, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 200, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 201, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 202, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 203, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 204, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 205, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 206, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 207, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 208, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 209, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 210, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 211, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 212, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 213, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 214, train loss: 0.01544, val loss: 0.01588\n",
      "Training epoch: 215, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 216, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 217, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 218, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 219, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 220, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 221, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 222, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 223, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 224, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 225, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 226, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 227, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 228, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 229, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 230, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 231, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 232, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 233, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 234, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 235, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 236, train loss: 0.01524, val loss: 0.01566\n",
      "Training epoch: 237, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 238, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 239, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 240, train loss: 0.01538, val loss: 0.01581\n",
      "Training epoch: 241, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 242, train loss: 0.01534, val loss: 0.01575\n",
      "Training epoch: 243, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 244, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 245, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 246, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 247, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 248, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 249, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 250, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 251, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 252, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 253, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 254, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 255, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 256, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 257, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 258, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 259, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 260, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 261, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 262, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 263, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 264, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 265, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 266, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 267, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 268, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 269, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 270, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 271, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 272, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 273, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 274, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 275, train loss: 0.01520, val loss: 0.01561\n",
      "Training epoch: 276, train loss: 0.01531, val loss: 0.01570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 277, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 278, train loss: 0.01519, val loss: 0.01559\n",
      "Training epoch: 279, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 280, train loss: 0.01520, val loss: 0.01561\n",
      "Training epoch: 281, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 282, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 283, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 284, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 285, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 286, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 287, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 288, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 289, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 290, train loss: 0.01520, val loss: 0.01559\n",
      "Training epoch: 291, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 292, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 293, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 294, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 295, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 296, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 297, train loss: 0.01518, val loss: 0.01557\n",
      "Training epoch: 298, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 299, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 300, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 301, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 302, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 303, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 304, train loss: 0.01518, val loss: 0.01558\n",
      "Training epoch: 305, train loss: 0.01524, val loss: 0.01568\n",
      "Training epoch: 306, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 307, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 308, train loss: 0.01526, val loss: 0.01574\n",
      "Training epoch: 309, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 310, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 311, train loss: 0.01518, val loss: 0.01558\n",
      "Training epoch: 312, train loss: 0.01517, val loss: 0.01557\n",
      "Training epoch: 313, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 314, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 315, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 316, train loss: 0.01518, val loss: 0.01558\n",
      "Training epoch: 317, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 318, train loss: 0.01517, val loss: 0.01557\n",
      "Training epoch: 319, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 320, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 321, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 322, train loss: 0.01521, val loss: 0.01561\n",
      "Training epoch: 323, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 324, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 325, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 326, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 327, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 328, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 329, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 330, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 331, train loss: 0.01538, val loss: 0.01572\n",
      "Training epoch: 332, train loss: 0.01537, val loss: 0.01580\n",
      "Training epoch: 333, train loss: 0.01516, val loss: 0.01555\n",
      "Training epoch: 334, train loss: 0.01518, val loss: 0.01557\n",
      "Training epoch: 335, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 336, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 337, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 338, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 339, train loss: 0.01519, val loss: 0.01558\n",
      "Training epoch: 340, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 341, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 342, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 343, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 344, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 345, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 346, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 347, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 348, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 349, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 350, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 351, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 352, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 353, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 354, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 355, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 356, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 357, train loss: 0.01517, val loss: 0.01557\n",
      "Training epoch: 358, train loss: 0.01516, val loss: 0.01556\n",
      "Training epoch: 359, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 360, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 361, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 362, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 363, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 364, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 365, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 366, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 367, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 368, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 369, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 370, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 371, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 372, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 373, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 374, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 375, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 376, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 377, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 378, train loss: 0.01518, val loss: 0.01557\n",
      "Training epoch: 379, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 380, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 381, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 382, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 383, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 384, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 385, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 386, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 387, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 388, train loss: 0.01514, val loss: 0.01553\n",
      "Training epoch: 389, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 390, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 391, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 392, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 393, train loss: 0.01515, val loss: 0.01554\n",
      "Training epoch: 394, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 395, train loss: 0.01519, val loss: 0.01559\n",
      "Training epoch: 396, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 397, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 398, train loss: 0.01526, val loss: 0.01574\n",
      "Training epoch: 399, train loss: 0.01512, val loss: 0.01552\n",
      "Training epoch: 400, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 401, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 402, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 403, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 404, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 405, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 406, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 407, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 408, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 409, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 410, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 411, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 412, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 413, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 414, train loss: 0.01526, val loss: 0.01566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 415, train loss: 0.01513, val loss: 0.01551\n",
      "Training epoch: 416, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 417, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 418, train loss: 0.01520, val loss: 0.01560\n",
      "Training epoch: 419, train loss: 0.01520, val loss: 0.01561\n",
      "Training epoch: 420, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 421, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 422, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 423, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 424, train loss: 0.01520, val loss: 0.01560\n",
      "Training epoch: 425, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 426, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 427, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 428, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 429, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 430, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 431, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 432, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 433, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 434, train loss: 0.01514, val loss: 0.01553\n",
      "Training epoch: 435, train loss: 0.01517, val loss: 0.01557\n",
      "Training epoch: 436, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 437, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 438, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 439, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 440, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 441, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 442, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 443, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 444, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 445, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 446, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 447, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 448, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 449, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 450, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 451, train loss: 0.01514, val loss: 0.01553\n",
      "Training epoch: 452, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 453, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 454, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 455, train loss: 0.01512, val loss: 0.01552\n",
      "Training epoch: 456, train loss: 0.01513, val loss: 0.01551\n",
      "Training epoch: 457, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 458, train loss: 0.01524, val loss: 0.01566\n",
      "Training epoch: 459, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 460, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 461, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 462, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 463, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 464, train loss: 0.01513, val loss: 0.01550\n",
      "Training epoch: 465, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 466, train loss: 0.01512, val loss: 0.01550\n",
      "Training epoch: 467, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 468, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 469, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 470, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 471, train loss: 0.01514, val loss: 0.01553\n",
      "Training epoch: 472, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 473, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 474, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 475, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 476, train loss: 0.01512, val loss: 0.01550\n",
      "Training epoch: 477, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 478, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 479, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 480, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 481, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 482, train loss: 0.01514, val loss: 0.01553\n",
      "Training epoch: 483, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 484, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 485, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 486, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 487, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 488, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 489, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 490, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 491, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 492, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 493, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 494, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 495, train loss: 0.01510, val loss: 0.01550\n",
      "Training epoch: 496, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 497, train loss: 0.01511, val loss: 0.01548\n",
      "Training epoch: 498, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 499, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 500, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 501, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 502, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 503, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 504, train loss: 0.01513, val loss: 0.01550\n",
      "Training epoch: 505, train loss: 0.01512, val loss: 0.01550\n",
      "Training epoch: 506, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 507, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 508, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 509, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 510, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 511, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 512, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 513, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 514, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 515, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 516, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 517, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 518, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 519, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 520, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 521, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 522, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 523, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 524, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 525, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 526, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 527, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 528, train loss: 0.01520, val loss: 0.01560\n",
      "Training epoch: 529, train loss: 0.01513, val loss: 0.01553\n",
      "Training epoch: 530, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 531, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 532, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 533, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 534, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 535, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 536, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 537, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 538, train loss: 0.01511, val loss: 0.01550\n",
      "Training epoch: 539, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 540, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 541, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 542, train loss: 0.01511, val loss: 0.01549\n",
      "Training epoch: 543, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 544, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 545, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 546, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 547, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 548, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 549, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 550, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 551, train loss: 0.01511, val loss: 0.01553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 552, train loss: 0.01516, val loss: 0.01556\n",
      "Training epoch: 553, train loss: 0.01511, val loss: 0.01550\n",
      "Training epoch: 554, train loss: 0.01512, val loss: 0.01552\n",
      "Training epoch: 555, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 556, train loss: 0.01511, val loss: 0.01550\n",
      "Training epoch: 557, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 558, train loss: 0.01513, val loss: 0.01553\n",
      "Training epoch: 559, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 560, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 561, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 562, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 563, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 564, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 565, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 566, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 567, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 568, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 569, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 570, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 571, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 572, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 573, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 574, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 575, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 576, train loss: 0.01511, val loss: 0.01550\n",
      "Training epoch: 577, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 578, train loss: 0.01513, val loss: 0.01551\n",
      "Training epoch: 579, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 580, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 581, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 582, train loss: 0.01514, val loss: 0.01553\n",
      "Training epoch: 583, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 584, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 585, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 586, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 587, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 588, train loss: 0.01509, val loss: 0.01547\n",
      "Training epoch: 589, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 590, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 591, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 592, train loss: 0.01511, val loss: 0.01549\n",
      "Training epoch: 593, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 594, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 595, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 596, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 597, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 598, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 599, train loss: 0.01510, val loss: 0.01550\n",
      "Training epoch: 600, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 601, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 602, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 603, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 604, train loss: 0.01509, val loss: 0.01549\n",
      "Training epoch: 605, train loss: 0.01516, val loss: 0.01555\n",
      "Training epoch: 606, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 607, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 608, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 609, train loss: 0.01517, val loss: 0.01557\n",
      "Training epoch: 610, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 611, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 612, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 613, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 614, train loss: 0.01512, val loss: 0.01549\n",
      "Training epoch: 615, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 616, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 617, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 618, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 619, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 620, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 621, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 622, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 623, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 624, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 625, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 626, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 627, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 628, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 629, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 630, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 631, train loss: 0.01516, val loss: 0.01555\n",
      "Training epoch: 632, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 633, train loss: 0.01512, val loss: 0.01552\n",
      "Training epoch: 634, train loss: 0.01510, val loss: 0.01548\n",
      "Training epoch: 635, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 636, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 637, train loss: 0.01516, val loss: 0.01556\n",
      "Training epoch: 638, train loss: 0.01510, val loss: 0.01550\n",
      "Training epoch: 639, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 640, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 641, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 642, train loss: 0.01512, val loss: 0.01552\n",
      "Training epoch: 643, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 644, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 645, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 646, train loss: 0.01508, val loss: 0.01547\n",
      "Training epoch: 647, train loss: 0.01513, val loss: 0.01550\n",
      "Training epoch: 648, train loss: 0.01509, val loss: 0.01549\n",
      "Training epoch: 649, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 650, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 651, train loss: 0.01511, val loss: 0.01548\n",
      "Training epoch: 652, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 653, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 654, train loss: 0.01520, val loss: 0.01560\n",
      "Training epoch: 655, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 656, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 657, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 658, train loss: 0.01515, val loss: 0.01554\n",
      "Training epoch: 659, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 660, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 661, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 662, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 663, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 664, train loss: 0.01508, val loss: 0.01546\n",
      "Training epoch: 665, train loss: 0.01519, val loss: 0.01559\n",
      "Training epoch: 666, train loss: 0.01511, val loss: 0.01551\n",
      "Training epoch: 667, train loss: 0.01509, val loss: 0.01546\n",
      "Training epoch: 668, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 669, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 670, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 671, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 672, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 673, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 674, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 675, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 676, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 677, train loss: 0.01511, val loss: 0.01549\n",
      "Training epoch: 678, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 679, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 680, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 681, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 682, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 683, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 684, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 685, train loss: 0.01511, val loss: 0.01549\n",
      "Training epoch: 686, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 687, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 688, train loss: 0.01510, val loss: 0.01551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 689, train loss: 0.01508, val loss: 0.01547\n",
      "Training epoch: 690, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 691, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 692, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 693, train loss: 0.01508, val loss: 0.01547\n",
      "Training epoch: 694, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 695, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 696, train loss: 0.01509, val loss: 0.01549\n",
      "Training epoch: 697, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 698, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 699, train loss: 0.01507, val loss: 0.01549\n",
      "Training epoch: 700, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 701, train loss: 0.01533, val loss: 0.01566\n",
      "Training epoch: 702, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 703, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 704, train loss: 0.01507, val loss: 0.01549\n",
      "Training epoch: 705, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 706, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 707, train loss: 0.01508, val loss: 0.01547\n",
      "Training epoch: 708, train loss: 0.01513, val loss: 0.01550\n",
      "Training epoch: 709, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 710, train loss: 0.01506, val loss: 0.01548\n",
      "Training epoch: 711, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 712, train loss: 0.01507, val loss: 0.01549\n",
      "Training epoch: 713, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 714, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 715, train loss: 0.01507, val loss: 0.01546\n",
      "Training epoch: 716, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 717, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 718, train loss: 0.01506, val loss: 0.01548\n",
      "Training epoch: 719, train loss: 0.01509, val loss: 0.01549\n",
      "Training epoch: 720, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 721, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 722, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 723, train loss: 0.01506, val loss: 0.01547\n",
      "Training epoch: 724, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 725, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 726, train loss: 0.01520, val loss: 0.01560\n",
      "Training epoch: 727, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 728, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 729, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 730, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 731, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 732, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 733, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 734, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 735, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 736, train loss: 0.01507, val loss: 0.01546\n",
      "Training epoch: 737, train loss: 0.01506, val loss: 0.01544\n",
      "Training epoch: 738, train loss: 0.01506, val loss: 0.01547\n",
      "Training epoch: 739, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 740, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 741, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 742, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 743, train loss: 0.01520, val loss: 0.01560\n",
      "Training epoch: 744, train loss: 0.01509, val loss: 0.01547\n",
      "Training epoch: 745, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 746, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 747, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 748, train loss: 0.01509, val loss: 0.01548\n",
      "Training epoch: 749, train loss: 0.01511, val loss: 0.01549\n",
      "Training epoch: 750, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 751, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 752, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 753, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 754, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 755, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 756, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 757, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 758, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 759, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 760, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 761, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 762, train loss: 0.01505, val loss: 0.01546\n",
      "Training epoch: 763, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 764, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 765, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 766, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 767, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 768, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 769, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 770, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 771, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 772, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 773, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 774, train loss: 0.01506, val loss: 0.01550\n",
      "Training epoch: 775, train loss: 0.01506, val loss: 0.01544\n",
      "Training epoch: 776, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 777, train loss: 0.01506, val loss: 0.01548\n",
      "Training epoch: 778, train loss: 0.01505, val loss: 0.01547\n",
      "Training epoch: 779, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 780, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 781, train loss: 0.01507, val loss: 0.01549\n",
      "Training epoch: 782, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 783, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 784, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 785, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 786, train loss: 0.01509, val loss: 0.01547\n",
      "Training epoch: 787, train loss: 0.01511, val loss: 0.01551\n",
      "Training epoch: 788, train loss: 0.01505, val loss: 0.01546\n",
      "Training epoch: 789, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 790, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 791, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 792, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 793, train loss: 0.01508, val loss: 0.01547\n",
      "Training epoch: 794, train loss: 0.01513, val loss: 0.01550\n",
      "Training epoch: 795, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 796, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 797, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 798, train loss: 0.01506, val loss: 0.01543\n",
      "Training epoch: 799, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 800, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 801, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 802, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 803, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 804, train loss: 0.01510, val loss: 0.01550\n",
      "Training epoch: 805, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 806, train loss: 0.01504, val loss: 0.01544\n",
      "Training epoch: 807, train loss: 0.01505, val loss: 0.01543\n",
      "Training epoch: 808, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 809, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 810, train loss: 0.01508, val loss: 0.01544\n",
      "Training epoch: 811, train loss: 0.01506, val loss: 0.01551\n",
      "Training epoch: 812, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 813, train loss: 0.01506, val loss: 0.01543\n",
      "Training epoch: 814, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 815, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 816, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 817, train loss: 0.01513, val loss: 0.01551\n",
      "Training epoch: 818, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 819, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 820, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 821, train loss: 0.01514, val loss: 0.01553\n",
      "Training epoch: 822, train loss: 0.01506, val loss: 0.01548\n",
      "Training epoch: 823, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 824, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 825, train loss: 0.01513, val loss: 0.01551\n",
      "Training epoch: 826, train loss: 0.01515, val loss: 0.01555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 827, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 828, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 829, train loss: 0.01510, val loss: 0.01550\n",
      "Training epoch: 830, train loss: 0.01513, val loss: 0.01551\n",
      "Training epoch: 831, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 832, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 833, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 834, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 835, train loss: 0.01511, val loss: 0.01550\n",
      "Training epoch: 836, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 837, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 838, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 839, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 840, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 841, train loss: 0.01504, val loss: 0.01549\n",
      "Training epoch: 842, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 843, train loss: 0.01503, val loss: 0.01541\n",
      "Training epoch: 844, train loss: 0.01504, val loss: 0.01544\n",
      "Training epoch: 845, train loss: 0.01506, val loss: 0.01547\n",
      "Training epoch: 846, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 847, train loss: 0.01516, val loss: 0.01556\n",
      "Training epoch: 848, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 849, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 850, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 851, train loss: 0.01505, val loss: 0.01547\n",
      "Training epoch: 852, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 853, train loss: 0.01504, val loss: 0.01544\n",
      "Training epoch: 854, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 855, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 856, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 857, train loss: 0.01503, val loss: 0.01542\n",
      "Training epoch: 858, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 859, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 860, train loss: 0.01514, val loss: 0.01562\n",
      "Training epoch: 861, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 862, train loss: 0.01504, val loss: 0.01544\n",
      "Training epoch: 863, train loss: 0.01510, val loss: 0.01547\n",
      "Training epoch: 864, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 865, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 866, train loss: 0.01505, val loss: 0.01543\n",
      "Training epoch: 867, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 868, train loss: 0.01504, val loss: 0.01545\n",
      "Training epoch: 869, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 870, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 871, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 872, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 873, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 874, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 875, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 876, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 877, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 878, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 879, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 880, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 881, train loss: 0.01505, val loss: 0.01546\n",
      "Training epoch: 882, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 883, train loss: 0.01512, val loss: 0.01549\n",
      "Training epoch: 884, train loss: 0.01507, val loss: 0.01543\n",
      "Training epoch: 885, train loss: 0.01503, val loss: 0.01543\n",
      "Training epoch: 886, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 887, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 888, train loss: 0.01503, val loss: 0.01543\n",
      "Training epoch: 889, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 890, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 891, train loss: 0.01508, val loss: 0.01545\n",
      "Training epoch: 892, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 893, train loss: 0.01509, val loss: 0.01548\n",
      "Training epoch: 894, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 895, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 896, train loss: 0.01508, val loss: 0.01545\n",
      "Training epoch: 897, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 898, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 899, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 900, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 901, train loss: 0.01505, val loss: 0.01543\n",
      "Training epoch: 902, train loss: 0.01505, val loss: 0.01547\n",
      "Training epoch: 903, train loss: 0.01505, val loss: 0.01547\n",
      "Training epoch: 904, train loss: 0.01503, val loss: 0.01543\n",
      "Training epoch: 905, train loss: 0.01504, val loss: 0.01544\n",
      "Training epoch: 906, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 907, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 908, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 909, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 910, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 911, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 912, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 913, train loss: 0.01507, val loss: 0.01544\n",
      "Training epoch: 914, train loss: 0.01516, val loss: 0.01556\n",
      "Training epoch: 915, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 916, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 917, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 918, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 919, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 920, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 921, train loss: 0.01505, val loss: 0.01550\n",
      "Training epoch: 922, train loss: 0.01506, val loss: 0.01543\n",
      "Training epoch: 923, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 924, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 925, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 926, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 927, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 928, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 929, train loss: 0.01511, val loss: 0.01551\n",
      "Training epoch: 930, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 931, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 932, train loss: 0.01505, val loss: 0.01550\n",
      "Training epoch: 933, train loss: 0.01503, val loss: 0.01542\n",
      "Training epoch: 934, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 935, train loss: 0.01504, val loss: 0.01545\n",
      "Training epoch: 936, train loss: 0.01505, val loss: 0.01542\n",
      "Training epoch: 937, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 938, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 939, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 940, train loss: 0.01504, val loss: 0.01549\n",
      "Training epoch: 941, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 942, train loss: 0.01505, val loss: 0.01542\n",
      "Training epoch: 943, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 944, train loss: 0.01514, val loss: 0.01550\n",
      "Training epoch: 945, train loss: 0.01508, val loss: 0.01545\n",
      "Training epoch: 946, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 947, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 948, train loss: 0.01505, val loss: 0.01546\n",
      "Training epoch: 949, train loss: 0.01504, val loss: 0.01541\n",
      "Training epoch: 950, train loss: 0.01505, val loss: 0.01542\n",
      "Training epoch: 951, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 952, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 953, train loss: 0.01510, val loss: 0.01546\n",
      "Training epoch: 954, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 955, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 956, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 957, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 958, train loss: 0.01502, val loss: 0.01540\n",
      "Training epoch: 959, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 960, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 961, train loss: 0.01505, val loss: 0.01546\n",
      "Training epoch: 962, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 963, train loss: 0.01506, val loss: 0.01549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 964, train loss: 0.01504, val loss: 0.01544\n",
      "Training epoch: 965, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 966, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 967, train loss: 0.01504, val loss: 0.01549\n",
      "Training epoch: 968, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 969, train loss: 0.01501, val loss: 0.01540\n",
      "Training epoch: 970, train loss: 0.01504, val loss: 0.01548\n",
      "Training epoch: 971, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 972, train loss: 0.01508, val loss: 0.01546\n",
      "Training epoch: 973, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 974, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 975, train loss: 0.01503, val loss: 0.01541\n",
      "Training epoch: 976, train loss: 0.01511, val loss: 0.01550\n",
      "Training epoch: 977, train loss: 0.01513, val loss: 0.01551\n",
      "Training epoch: 978, train loss: 0.01511, val loss: 0.01550\n",
      "Training epoch: 979, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 980, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 981, train loss: 0.01533, val loss: 0.01583\n",
      "Training epoch: 982, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 983, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 984, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 985, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 986, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 987, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 988, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 989, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 990, train loss: 0.01503, val loss: 0.01542\n",
      "Training epoch: 991, train loss: 0.01502, val loss: 0.01541\n",
      "Training epoch: 992, train loss: 0.01509, val loss: 0.01548\n",
      "Training epoch: 993, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 994, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 995, train loss: 0.01504, val loss: 0.01542\n",
      "Training epoch: 996, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 997, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 998, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 999, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1000, train loss: 0.01505, val loss: 0.01547\n",
      "Training epoch: 1001, train loss: 0.01509, val loss: 0.01546\n",
      "Training epoch: 1002, train loss: 0.01541, val loss: 0.01577\n",
      "Training epoch: 1003, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 1004, train loss: 0.01505, val loss: 0.01550\n",
      "Training epoch: 1005, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 1006, train loss: 0.01505, val loss: 0.01540\n",
      "Training epoch: 1007, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1008, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1009, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1010, train loss: 0.01511, val loss: 0.01559\n",
      "Training epoch: 1011, train loss: 0.01506, val loss: 0.01544\n",
      "Training epoch: 1012, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1013, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1014, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 1015, train loss: 0.01513, val loss: 0.01548\n",
      "Training epoch: 1016, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 1017, train loss: 0.01510, val loss: 0.01547\n",
      "Training epoch: 1018, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 1019, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1020, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 1021, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 1022, train loss: 0.01503, val loss: 0.01543\n",
      "Training epoch: 1023, train loss: 0.01506, val loss: 0.01551\n",
      "Training epoch: 1024, train loss: 0.01501, val loss: 0.01540\n",
      "Training epoch: 1025, train loss: 0.01501, val loss: 0.01540\n",
      "Training epoch: 1026, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 1027, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 1028, train loss: 0.01507, val loss: 0.01545\n",
      "Training epoch: 1029, train loss: 0.01506, val loss: 0.01547\n",
      "Training epoch: 1030, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1031, train loss: 0.01503, val loss: 0.01540\n",
      "Training epoch: 1032, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1033, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1034, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1035, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 1036, train loss: 0.01522, val loss: 0.01556\n",
      "Training epoch: 1037, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 1038, train loss: 0.01504, val loss: 0.01548\n",
      "Training epoch: 1039, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1040, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1041, train loss: 0.01509, val loss: 0.01548\n",
      "Training epoch: 1042, train loss: 0.01504, val loss: 0.01544\n",
      "Training epoch: 1043, train loss: 0.01504, val loss: 0.01541\n",
      "Training epoch: 1044, train loss: 0.01503, val loss: 0.01543\n",
      "Training epoch: 1045, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1046, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 1047, train loss: 0.01504, val loss: 0.01541\n",
      "Training epoch: 1048, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1049, train loss: 0.01506, val loss: 0.01552\n",
      "Training epoch: 1050, train loss: 0.01509, val loss: 0.01548\n",
      "Training epoch: 1051, train loss: 0.01511, val loss: 0.01546\n",
      "Training epoch: 1052, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1053, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1054, train loss: 0.01502, val loss: 0.01539\n",
      "Training epoch: 1055, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1056, train loss: 0.01506, val loss: 0.01549\n",
      "Training epoch: 1057, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1058, train loss: 0.01503, val loss: 0.01538\n",
      "Training epoch: 1059, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1060, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1061, train loss: 0.01506, val loss: 0.01544\n",
      "Training epoch: 1062, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1063, train loss: 0.01509, val loss: 0.01549\n",
      "Training epoch: 1064, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 1065, train loss: 0.01501, val loss: 0.01540\n",
      "Training epoch: 1066, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1067, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1068, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1069, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1070, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1071, train loss: 0.01508, val loss: 0.01547\n",
      "Training epoch: 1072, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 1073, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 1074, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 1075, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 1076, train loss: 0.01503, val loss: 0.01543\n",
      "Training epoch: 1077, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1078, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1079, train loss: 0.01502, val loss: 0.01540\n",
      "Training epoch: 1080, train loss: 0.01502, val loss: 0.01547\n",
      "Training epoch: 1081, train loss: 0.01505, val loss: 0.01546\n",
      "Training epoch: 1082, train loss: 0.01517, val loss: 0.01564\n",
      "Training epoch: 1083, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1084, train loss: 0.01506, val loss: 0.01549\n",
      "Training epoch: 1085, train loss: 0.01501, val loss: 0.01545\n",
      "Training epoch: 1086, train loss: 0.01508, val loss: 0.01544\n",
      "Training epoch: 1087, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1088, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1089, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 1090, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 1091, train loss: 0.01503, val loss: 0.01541\n",
      "Training epoch: 1092, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 1093, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 1094, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 1095, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1096, train loss: 0.01504, val loss: 0.01549\n",
      "Training epoch: 1097, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 1098, train loss: 0.01502, val loss: 0.01546\n",
      "Training epoch: 1099, train loss: 0.01500, val loss: 0.01540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1100, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1101, train loss: 0.01504, val loss: 0.01549\n",
      "Training epoch: 1102, train loss: 0.01506, val loss: 0.01547\n",
      "Training epoch: 1103, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1104, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1105, train loss: 0.01502, val loss: 0.01546\n",
      "Training epoch: 1106, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 1107, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 1108, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 1109, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1110, train loss: 0.01516, val loss: 0.01549\n",
      "Training epoch: 1111, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1112, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1113, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1114, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1115, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 1116, train loss: 0.01501, val loss: 0.01547\n",
      "Training epoch: 1117, train loss: 0.01509, val loss: 0.01549\n",
      "Training epoch: 1118, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 1119, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1120, train loss: 0.01508, val loss: 0.01546\n",
      "Training epoch: 1121, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 1122, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1123, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1124, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 1125, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1126, train loss: 0.01507, val loss: 0.01546\n",
      "Training epoch: 1127, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 1128, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 1129, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 1130, train loss: 0.01524, val loss: 0.01574\n",
      "Training epoch: 1131, train loss: 0.01529, val loss: 0.01577\n",
      "Training epoch: 1132, train loss: 0.01502, val loss: 0.01538\n",
      "Training epoch: 1133, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 1134, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 1135, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1136, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1137, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 1138, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 1139, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1140, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1141, train loss: 0.01503, val loss: 0.01547\n",
      "Training epoch: 1142, train loss: 0.01502, val loss: 0.01546\n",
      "Training epoch: 1143, train loss: 0.01500, val loss: 0.01538\n",
      "Training epoch: 1144, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1145, train loss: 0.01508, val loss: 0.01545\n",
      "Training epoch: 1146, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 1147, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 1148, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1149, train loss: 0.01506, val loss: 0.01552\n",
      "Training epoch: 1150, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1151, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1152, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1153, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1154, train loss: 0.01510, val loss: 0.01547\n",
      "Training epoch: 1155, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1156, train loss: 0.01503, val loss: 0.01547\n",
      "Training epoch: 1157, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 1158, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 1159, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 1160, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 1161, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1162, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1163, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 1164, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1165, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1166, train loss: 0.01508, val loss: 0.01547\n",
      "Training epoch: 1167, train loss: 0.01505, val loss: 0.01546\n",
      "Training epoch: 1168, train loss: 0.01504, val loss: 0.01550\n",
      "Training epoch: 1169, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 1170, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 1171, train loss: 0.01510, val loss: 0.01546\n",
      "Training epoch: 1172, train loss: 0.01506, val loss: 0.01543\n",
      "Training epoch: 1173, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 1174, train loss: 0.01512, val loss: 0.01552\n",
      "Training epoch: 1175, train loss: 0.01505, val loss: 0.01547\n",
      "Training epoch: 1176, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 1177, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1178, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1179, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 1180, train loss: 0.01507, val loss: 0.01545\n",
      "Training epoch: 1181, train loss: 0.01502, val loss: 0.01541\n",
      "Training epoch: 1182, train loss: 0.01501, val loss: 0.01545\n",
      "Training epoch: 1183, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1184, train loss: 0.01505, val loss: 0.01547\n",
      "Training epoch: 1185, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1186, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1187, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 1188, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1189, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1190, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1191, train loss: 0.01503, val loss: 0.01543\n",
      "Training epoch: 1192, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1193, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 1194, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1195, train loss: 0.01506, val loss: 0.01551\n",
      "Training epoch: 1196, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 1197, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1198, train loss: 0.01507, val loss: 0.01544\n",
      "Training epoch: 1199, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 1200, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1201, train loss: 0.01505, val loss: 0.01550\n",
      "Training epoch: 1202, train loss: 0.01511, val loss: 0.01549\n",
      "Training epoch: 1203, train loss: 0.01505, val loss: 0.01550\n",
      "Training epoch: 1204, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1205, train loss: 0.01503, val loss: 0.01547\n",
      "Training epoch: 1206, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 1207, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1208, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 1209, train loss: 0.01504, val loss: 0.01545\n",
      "Training epoch: 1210, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 1211, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1212, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1213, train loss: 0.01506, val loss: 0.01548\n",
      "Training epoch: 1214, train loss: 0.01508, val loss: 0.01547\n",
      "Training epoch: 1215, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 1216, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1217, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1218, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1219, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 1220, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1221, train loss: 0.01502, val loss: 0.01547\n",
      "Training epoch: 1222, train loss: 0.01501, val loss: 0.01540\n",
      "Training epoch: 1223, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 1224, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1225, train loss: 0.01502, val loss: 0.01539\n",
      "Training epoch: 1226, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 1227, train loss: 0.01505, val loss: 0.01552\n",
      "Training epoch: 1228, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1229, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 1230, train loss: 0.01506, val loss: 0.01548\n",
      "Training epoch: 1231, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1232, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1233, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 1234, train loss: 0.01503, val loss: 0.01543\n",
      "Training epoch: 1235, train loss: 0.01500, val loss: 0.01543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1236, train loss: 0.01505, val loss: 0.01550\n",
      "Training epoch: 1237, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1238, train loss: 0.01511, val loss: 0.01551\n",
      "Training epoch: 1239, train loss: 0.01511, val loss: 0.01550\n",
      "Training epoch: 1240, train loss: 0.01503, val loss: 0.01548\n",
      "Training epoch: 1241, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 1242, train loss: 0.01506, val loss: 0.01550\n",
      "Training epoch: 1243, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1244, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1245, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 1246, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1247, train loss: 0.01505, val loss: 0.01551\n",
      "Training epoch: 1248, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 1249, train loss: 0.01504, val loss: 0.01544\n",
      "Training epoch: 1250, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 1251, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1252, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 1253, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1254, train loss: 0.01506, val loss: 0.01552\n",
      "Training epoch: 1255, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1256, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1257, train loss: 0.01513, val loss: 0.01550\n",
      "Training epoch: 1258, train loss: 0.01499, val loss: 0.01539\n",
      "Training epoch: 1259, train loss: 0.01502, val loss: 0.01547\n",
      "Training epoch: 1260, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1261, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 1262, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1263, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1264, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1265, train loss: 0.01501, val loss: 0.01539\n",
      "Training epoch: 1266, train loss: 0.01503, val loss: 0.01550\n",
      "Training epoch: 1267, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1268, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 1269, train loss: 0.01503, val loss: 0.01541\n",
      "Training epoch: 1270, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1271, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1272, train loss: 0.01506, val loss: 0.01547\n",
      "Training epoch: 1273, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1274, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 1275, train loss: 0.01512, val loss: 0.01549\n",
      "Training epoch: 1276, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1277, train loss: 0.01503, val loss: 0.01550\n",
      "Training epoch: 1278, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 1279, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 1280, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 1281, train loss: 0.01515, val loss: 0.01562\n",
      "Training epoch: 1282, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1283, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1284, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1285, train loss: 0.01502, val loss: 0.01540\n",
      "Training epoch: 1286, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1287, train loss: 0.01506, val loss: 0.01551\n",
      "Training epoch: 1288, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1289, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1290, train loss: 0.01500, val loss: 0.01539\n",
      "Training epoch: 1291, train loss: 0.01503, val loss: 0.01542\n",
      "Training epoch: 1292, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1293, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1294, train loss: 0.01510, val loss: 0.01547\n",
      "Training epoch: 1295, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 1296, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1297, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 1298, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1299, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1300, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 1301, train loss: 0.01500, val loss: 0.01537\n",
      "Training epoch: 1302, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1303, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1304, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1305, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1306, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 1307, train loss: 0.01501, val loss: 0.01540\n",
      "Training epoch: 1308, train loss: 0.01501, val loss: 0.01545\n",
      "Training epoch: 1309, train loss: 0.01502, val loss: 0.01548\n",
      "Training epoch: 1310, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1311, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1312, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1313, train loss: 0.01502, val loss: 0.01540\n",
      "Training epoch: 1314, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1315, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1316, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 1317, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1318, train loss: 0.01501, val loss: 0.01545\n",
      "Training epoch: 1319, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1320, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 1321, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 1322, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1323, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1324, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1325, train loss: 0.01509, val loss: 0.01546\n",
      "Training epoch: 1326, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1327, train loss: 0.01506, val loss: 0.01551\n",
      "Training epoch: 1328, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 1329, train loss: 0.01503, val loss: 0.01543\n",
      "Training epoch: 1330, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1331, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1332, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1333, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1334, train loss: 0.01522, val loss: 0.01573\n",
      "Training epoch: 1335, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1336, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1337, train loss: 0.01529, val loss: 0.01560\n",
      "Training epoch: 1338, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 1339, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1340, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 1341, train loss: 0.01500, val loss: 0.01539\n",
      "Training epoch: 1342, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1343, train loss: 0.01505, val loss: 0.01547\n",
      "Training epoch: 1344, train loss: 0.01510, val loss: 0.01558\n",
      "Training epoch: 1345, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1346, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 1347, train loss: 0.01501, val loss: 0.01538\n",
      "Training epoch: 1348, train loss: 0.01500, val loss: 0.01539\n",
      "Training epoch: 1349, train loss: 0.01503, val loss: 0.01542\n",
      "Training epoch: 1350, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1351, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1352, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 1353, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1354, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1355, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1356, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1357, train loss: 0.01503, val loss: 0.01550\n",
      "Training epoch: 1358, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1359, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 1360, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 1361, train loss: 0.01510, val loss: 0.01558\n",
      "Training epoch: 1362, train loss: 0.01505, val loss: 0.01547\n",
      "Training epoch: 1363, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1364, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 1365, train loss: 0.01499, val loss: 0.01539\n",
      "Training epoch: 1366, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 1367, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 1368, train loss: 0.01502, val loss: 0.01546\n",
      "Training epoch: 1369, train loss: 0.01504, val loss: 0.01548\n",
      "Training epoch: 1370, train loss: 0.01513, val loss: 0.01561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1371, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1372, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1373, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1374, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 1375, train loss: 0.01502, val loss: 0.01541\n",
      "Training epoch: 1376, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1377, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1378, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 1379, train loss: 0.01518, val loss: 0.01569\n",
      "Training epoch: 1380, train loss: 0.01502, val loss: 0.01548\n",
      "Training epoch: 1381, train loss: 0.01509, val loss: 0.01544\n",
      "Training epoch: 1382, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 1383, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1384, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1385, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1386, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1387, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1388, train loss: 0.01499, val loss: 0.01544\n",
      "Training epoch: 1389, train loss: 0.01509, val loss: 0.01549\n",
      "Training epoch: 1390, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1391, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1392, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 1393, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 1394, train loss: 0.01506, val loss: 0.01552\n",
      "Training epoch: 1395, train loss: 0.01501, val loss: 0.01546\n",
      "Training epoch: 1396, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1397, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 1398, train loss: 0.01501, val loss: 0.01539\n",
      "Training epoch: 1399, train loss: 0.01506, val loss: 0.01552\n",
      "Training epoch: 1400, train loss: 0.01517, val loss: 0.01566\n",
      "Training epoch: 1401, train loss: 0.01502, val loss: 0.01539\n",
      "Training epoch: 1402, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 1403, train loss: 0.01502, val loss: 0.01547\n",
      "Training epoch: 1404, train loss: 0.01504, val loss: 0.01539\n",
      "Training epoch: 1405, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 1406, train loss: 0.01506, val loss: 0.01547\n",
      "Training epoch: 1407, train loss: 0.01499, val loss: 0.01545\n",
      "Training epoch: 1408, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1409, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1410, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1411, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1412, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1413, train loss: 0.01504, val loss: 0.01542\n",
      "Training epoch: 1414, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1415, train loss: 0.01506, val loss: 0.01554\n",
      "Training epoch: 1416, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1417, train loss: 0.01516, val loss: 0.01565\n",
      "Training epoch: 1418, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1419, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 1420, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1421, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 1422, train loss: 0.01498, val loss: 0.01538\n",
      "Training epoch: 1423, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1424, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1425, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1426, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 1427, train loss: 0.01509, val loss: 0.01545\n",
      "Training epoch: 1428, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 1429, train loss: 0.01504, val loss: 0.01549\n",
      "Training epoch: 1430, train loss: 0.01501, val loss: 0.01539\n",
      "Training epoch: 1431, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 1432, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 1433, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1434, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 1435, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 1436, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 1437, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 1438, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1439, train loss: 0.01507, val loss: 0.01555\n",
      "Training epoch: 1440, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1441, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 1442, train loss: 0.01501, val loss: 0.01539\n",
      "Training epoch: 1443, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1444, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1445, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 1446, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 1447, train loss: 0.01541, val loss: 0.01576\n",
      "Training epoch: 1448, train loss: 0.01504, val loss: 0.01545\n",
      "Training epoch: 1449, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1450, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1451, train loss: 0.01514, val loss: 0.01563\n",
      "Training epoch: 1452, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1453, train loss: 0.01505, val loss: 0.01543\n",
      "Training epoch: 1454, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 1455, train loss: 0.01504, val loss: 0.01544\n",
      "Training epoch: 1456, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 1457, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1458, train loss: 0.01501, val loss: 0.01545\n",
      "Training epoch: 1459, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 1460, train loss: 0.01502, val loss: 0.01548\n",
      "Training epoch: 1461, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1462, train loss: 0.01507, val loss: 0.01546\n",
      "Training epoch: 1463, train loss: 0.01499, val loss: 0.01539\n",
      "Training epoch: 1464, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 1465, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1466, train loss: 0.01501, val loss: 0.01546\n",
      "Training epoch: 1467, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1468, train loss: 0.01505, val loss: 0.01553\n",
      "Training epoch: 1469, train loss: 0.01510, val loss: 0.01558\n",
      "Training epoch: 1470, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1471, train loss: 0.01504, val loss: 0.01541\n",
      "Training epoch: 1472, train loss: 0.01507, val loss: 0.01544\n",
      "Training epoch: 1473, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1474, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1475, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1476, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1477, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1478, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 1479, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 1480, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 1481, train loss: 0.01507, val loss: 0.01546\n",
      "Training epoch: 1482, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 1483, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1484, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1485, train loss: 0.01514, val loss: 0.01562\n",
      "Training epoch: 1486, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1487, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1488, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 1489, train loss: 0.01501, val loss: 0.01547\n",
      "Training epoch: 1490, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 1491, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1492, train loss: 0.01503, val loss: 0.01542\n",
      "Training epoch: 1493, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 1494, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1495, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1496, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1497, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1498, train loss: 0.01505, val loss: 0.01553\n",
      "Training epoch: 1499, train loss: 0.01503, val loss: 0.01547\n",
      "Training epoch: 1500, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1501, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 1502, train loss: 0.01503, val loss: 0.01543\n",
      "Training epoch: 1503, train loss: 0.01505, val loss: 0.01551\n",
      "Training epoch: 1504, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1505, train loss: 0.01505, val loss: 0.01546\n",
      "Training epoch: 1506, train loss: 0.01511, val loss: 0.01555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1507, train loss: 0.01502, val loss: 0.01540\n",
      "Training epoch: 1508, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 1509, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1510, train loss: 0.01501, val loss: 0.01540\n",
      "Training epoch: 1511, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1512, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1513, train loss: 0.01501, val loss: 0.01546\n",
      "Training epoch: 1514, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1515, train loss: 0.01506, val loss: 0.01544\n",
      "Training epoch: 1516, train loss: 0.01507, val loss: 0.01546\n",
      "Training epoch: 1517, train loss: 0.01505, val loss: 0.01552\n",
      "Training epoch: 1518, train loss: 0.01502, val loss: 0.01546\n",
      "Training epoch: 1519, train loss: 0.01507, val loss: 0.01546\n",
      "Training epoch: 1520, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 1521, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 1522, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1523, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1524, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1525, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 1526, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 1527, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1528, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1529, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 1530, train loss: 0.01502, val loss: 0.01541\n",
      "Training epoch: 1531, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1532, train loss: 0.01500, val loss: 0.01539\n",
      "Training epoch: 1533, train loss: 0.01503, val loss: 0.01548\n",
      "Training epoch: 1534, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 1535, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1536, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1537, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1538, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 1539, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 1540, train loss: 0.01512, val loss: 0.01548\n",
      "Training epoch: 1541, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 1542, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1543, train loss: 0.01498, val loss: 0.01541\n",
      "Training epoch: 1544, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1545, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1546, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 1547, train loss: 0.01504, val loss: 0.01542\n",
      "Training epoch: 1548, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1549, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1550, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 1551, train loss: 0.01498, val loss: 0.01544\n",
      "Training epoch: 1552, train loss: 0.01501, val loss: 0.01538\n",
      "Training epoch: 1553, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1554, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1555, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1556, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1557, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1558, train loss: 0.01502, val loss: 0.01547\n",
      "Training epoch: 1559, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1560, train loss: 0.01500, val loss: 0.01539\n",
      "Training epoch: 1561, train loss: 0.01499, val loss: 0.01544\n",
      "Training epoch: 1562, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 1563, train loss: 0.01506, val loss: 0.01552\n",
      "Training epoch: 1564, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1565, train loss: 0.01502, val loss: 0.01549\n",
      "Training epoch: 1566, train loss: 0.01502, val loss: 0.01539\n",
      "Training epoch: 1567, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 1568, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1569, train loss: 0.01521, val loss: 0.01572\n",
      "Training epoch: 1570, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1571, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 1572, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1573, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1574, train loss: 0.01503, val loss: 0.01549\n",
      "Training epoch: 1575, train loss: 0.01498, val loss: 0.01540\n",
      "Training epoch: 1576, train loss: 0.01499, val loss: 0.01539\n",
      "Training epoch: 1577, train loss: 0.01502, val loss: 0.01546\n",
      "Training epoch: 1578, train loss: 0.01528, val loss: 0.01582\n",
      "Training epoch: 1579, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 1580, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1581, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1582, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1583, train loss: 0.01502, val loss: 0.01547\n",
      "Training epoch: 1584, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1585, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 1586, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1587, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1588, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1589, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 1590, train loss: 0.01503, val loss: 0.01548\n",
      "Training epoch: 1591, train loss: 0.01512, val loss: 0.01552\n",
      "Training epoch: 1592, train loss: 0.01503, val loss: 0.01547\n",
      "Training epoch: 1593, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 1594, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 1595, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1596, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 1597, train loss: 0.01499, val loss: 0.01544\n",
      "Training epoch: 1598, train loss: 0.01502, val loss: 0.01540\n",
      "Training epoch: 1599, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1600, train loss: 0.01502, val loss: 0.01549\n",
      "Training epoch: 1601, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1602, train loss: 0.01503, val loss: 0.01551\n",
      "Training epoch: 1603, train loss: 0.01519, val loss: 0.01567\n",
      "Training epoch: 1604, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 1605, train loss: 0.01507, val loss: 0.01549\n",
      "Training epoch: 1606, train loss: 0.01503, val loss: 0.01550\n",
      "Training epoch: 1607, train loss: 0.01509, val loss: 0.01560\n",
      "Training epoch: 1608, train loss: 0.01499, val loss: 0.01536\n",
      "Training epoch: 1609, train loss: 0.01499, val loss: 0.01549\n",
      "Training epoch: 1610, train loss: 0.01505, val loss: 0.01543\n",
      "Training epoch: 1611, train loss: 0.01506, val loss: 0.01543\n",
      "Training epoch: 1612, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1613, train loss: 0.01500, val loss: 0.01539\n",
      "Training epoch: 1614, train loss: 0.01499, val loss: 0.01544\n",
      "Training epoch: 1615, train loss: 0.01503, val loss: 0.01548\n",
      "Training epoch: 1616, train loss: 0.01502, val loss: 0.01546\n",
      "Training epoch: 1617, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 1618, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 1619, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 1620, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1621, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 1622, train loss: 0.01502, val loss: 0.01548\n",
      "Training epoch: 1623, train loss: 0.01499, val loss: 0.01539\n",
      "Training epoch: 1624, train loss: 0.01504, val loss: 0.01549\n",
      "Training epoch: 1625, train loss: 0.01505, val loss: 0.01553\n",
      "Training epoch: 1626, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1627, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1628, train loss: 0.01505, val loss: 0.01552\n",
      "Training epoch: 1629, train loss: 0.01501, val loss: 0.01545\n",
      "Training epoch: 1630, train loss: 0.01498, val loss: 0.01538\n",
      "Training epoch: 1631, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1632, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1633, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1634, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1635, train loss: 0.01514, val loss: 0.01562\n",
      "Training epoch: 1636, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 1637, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 1638, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1639, train loss: 0.01504, val loss: 0.01542\n",
      "Training epoch: 1640, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 1641, train loss: 0.01512, val loss: 0.01549\n",
      "Training epoch: 1642, train loss: 0.01500, val loss: 0.01543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1643, train loss: 0.01504, val loss: 0.01545\n",
      "Training epoch: 1644, train loss: 0.01504, val loss: 0.01542\n",
      "Training epoch: 1645, train loss: 0.01498, val loss: 0.01543\n",
      "Training epoch: 1646, train loss: 0.01506, val loss: 0.01551\n",
      "Training epoch: 1647, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1648, train loss: 0.01501, val loss: 0.01545\n",
      "Training epoch: 1649, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 1650, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1651, train loss: 0.01507, val loss: 0.01546\n",
      "Training epoch: 1652, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1653, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1654, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1655, train loss: 0.01504, val loss: 0.01548\n",
      "Training epoch: 1656, train loss: 0.01509, val loss: 0.01548\n",
      "Training epoch: 1657, train loss: 0.01507, val loss: 0.01545\n",
      "Training epoch: 1658, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1659, train loss: 0.01509, val loss: 0.01558\n",
      "Training epoch: 1660, train loss: 0.01519, val loss: 0.01567\n",
      "Training epoch: 1661, train loss: 0.01502, val loss: 0.01550\n",
      "Training epoch: 1662, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1663, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1664, train loss: 0.01498, val loss: 0.01542\n",
      "Training epoch: 1665, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 1666, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 1667, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1668, train loss: 0.01505, val loss: 0.01550\n",
      "Training epoch: 1669, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1670, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 1671, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1672, train loss: 0.01507, val loss: 0.01554\n",
      "Training epoch: 1673, train loss: 0.01510, val loss: 0.01558\n",
      "Training epoch: 1674, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 1675, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1676, train loss: 0.01514, val loss: 0.01553\n",
      "Training epoch: 1677, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 1678, train loss: 0.01500, val loss: 0.01539\n",
      "Training epoch: 1679, train loss: 0.01500, val loss: 0.01546\n",
      "Training epoch: 1680, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 1681, train loss: 0.01517, val loss: 0.01566\n",
      "Training epoch: 1682, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 1683, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1684, train loss: 0.01509, val loss: 0.01548\n",
      "Training epoch: 1685, train loss: 0.01499, val loss: 0.01544\n",
      "Training epoch: 1686, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1687, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1688, train loss: 0.01501, val loss: 0.01547\n",
      "Training epoch: 1689, train loss: 0.01498, val loss: 0.01542\n",
      "Training epoch: 1690, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1691, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 1692, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1693, train loss: 0.01499, val loss: 0.01539\n",
      "Training epoch: 1694, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 1695, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 1696, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1697, train loss: 0.01501, val loss: 0.01549\n",
      "Training epoch: 1698, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1699, train loss: 0.01502, val loss: 0.01548\n",
      "Training epoch: 1700, train loss: 0.01507, val loss: 0.01557\n",
      "Training epoch: 1701, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 1702, train loss: 0.01506, val loss: 0.01553\n",
      "Training epoch: 1703, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1704, train loss: 0.01508, val loss: 0.01547\n",
      "Training epoch: 1705, train loss: 0.01498, val loss: 0.01538\n",
      "Training epoch: 1706, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 1707, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1708, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1709, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 1710, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1711, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1712, train loss: 0.01510, val loss: 0.01558\n",
      "Training epoch: 1713, train loss: 0.01521, val loss: 0.01570\n",
      "Training epoch: 1714, train loss: 0.01507, val loss: 0.01549\n",
      "Training epoch: 1715, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 1716, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1717, train loss: 0.01508, val loss: 0.01548\n",
      "Training epoch: 1718, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 1719, train loss: 0.01513, val loss: 0.01546\n",
      "Training epoch: 1720, train loss: 0.01504, val loss: 0.01542\n",
      "Training epoch: 1721, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1722, train loss: 0.01504, val loss: 0.01551\n",
      "Training epoch: 1723, train loss: 0.01498, val loss: 0.01538\n",
      "Training epoch: 1724, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 1725, train loss: 0.01500, val loss: 0.01546\n",
      "Training epoch: 1726, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1727, train loss: 0.01505, val loss: 0.01540\n",
      "Training epoch: 1728, train loss: 0.01507, val loss: 0.01549\n",
      "Training epoch: 1729, train loss: 0.01504, val loss: 0.01544\n",
      "Training epoch: 1730, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 1731, train loss: 0.01506, val loss: 0.01555\n",
      "Training epoch: 1732, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1733, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 1734, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1735, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1736, train loss: 0.01504, val loss: 0.01550\n",
      "Training epoch: 1737, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1738, train loss: 0.01498, val loss: 0.01539\n",
      "Training epoch: 1739, train loss: 0.01507, val loss: 0.01549\n",
      "Training epoch: 1740, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1741, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1742, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1743, train loss: 0.01500, val loss: 0.01538\n",
      "Training epoch: 1744, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1745, train loss: 0.01501, val loss: 0.01547\n",
      "Training epoch: 1746, train loss: 0.01505, val loss: 0.01544\n",
      "Training epoch: 1747, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 1748, train loss: 0.01505, val loss: 0.01554\n",
      "Training epoch: 1749, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1750, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1751, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1752, train loss: 0.01506, val loss: 0.01549\n",
      "Training epoch: 1753, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1754, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1755, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1756, train loss: 0.01506, val loss: 0.01545\n",
      "Training epoch: 1757, train loss: 0.01505, val loss: 0.01549\n",
      "Training epoch: 1758, train loss: 0.01513, val loss: 0.01561\n",
      "Training epoch: 1759, train loss: 0.01501, val loss: 0.01550\n",
      "Training epoch: 1760, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1761, train loss: 0.01510, val loss: 0.01547\n",
      "Training epoch: 1762, train loss: 0.01509, val loss: 0.01549\n",
      "Training epoch: 1763, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 1764, train loss: 0.01516, val loss: 0.01547\n",
      "Training epoch: 1765, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1766, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1767, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 1768, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 1769, train loss: 0.01510, val loss: 0.01561\n",
      "Training epoch: 1770, train loss: 0.01521, val loss: 0.01572\n",
      "Training epoch: 1771, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 1772, train loss: 0.01503, val loss: 0.01549\n",
      "Training epoch: 1773, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1774, train loss: 0.01502, val loss: 0.01539\n",
      "Training epoch: 1775, train loss: 0.01508, val loss: 0.01546\n",
      "Training epoch: 1776, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1777, train loss: 0.01504, val loss: 0.01543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1778, train loss: 0.01506, val loss: 0.01554\n",
      "Training epoch: 1779, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1780, train loss: 0.01520, val loss: 0.01570\n",
      "Training epoch: 1781, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1782, train loss: 0.01510, val loss: 0.01549\n",
      "Training epoch: 1783, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 1784, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1785, train loss: 0.01502, val loss: 0.01546\n",
      "Training epoch: 1786, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1787, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1788, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 1789, train loss: 0.01512, val loss: 0.01549\n",
      "Training epoch: 1790, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 1791, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1792, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 1793, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1794, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1795, train loss: 0.01525, val loss: 0.01575\n",
      "Training epoch: 1796, train loss: 0.01510, val loss: 0.01558\n",
      "Training epoch: 1797, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1798, train loss: 0.01516, val loss: 0.01555\n",
      "Training epoch: 1799, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1800, train loss: 0.01503, val loss: 0.01547\n",
      "Training epoch: 1801, train loss: 0.01507, val loss: 0.01554\n",
      "Training epoch: 1802, train loss: 0.01502, val loss: 0.01550\n",
      "Training epoch: 1803, train loss: 0.01501, val loss: 0.01540\n",
      "Training epoch: 1804, train loss: 0.01515, val loss: 0.01554\n",
      "Training epoch: 1805, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 1806, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 1807, train loss: 0.01503, val loss: 0.01545\n",
      "Training epoch: 1808, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1809, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1810, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 1811, train loss: 0.01507, val loss: 0.01545\n",
      "Training epoch: 1812, train loss: 0.01503, val loss: 0.01549\n",
      "Training epoch: 1813, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1814, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1815, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1816, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1817, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1818, train loss: 0.01501, val loss: 0.01547\n",
      "Training epoch: 1819, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1820, train loss: 0.01501, val loss: 0.01548\n",
      "Training epoch: 1821, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1822, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1823, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 1824, train loss: 0.01503, val loss: 0.01549\n",
      "Training epoch: 1825, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1826, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 1827, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1828, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1829, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 1830, train loss: 0.01504, val loss: 0.01545\n",
      "Training epoch: 1831, train loss: 0.01504, val loss: 0.01545\n",
      "Training epoch: 1832, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 1833, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1834, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1835, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 1836, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1837, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1838, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 1839, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1840, train loss: 0.01501, val loss: 0.01538\n",
      "Training epoch: 1841, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 1842, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 1843, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1844, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1845, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1846, train loss: 0.01528, val loss: 0.01581\n",
      "Training epoch: 1847, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1848, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 1849, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1850, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1851, train loss: 0.01502, val loss: 0.01541\n",
      "Training epoch: 1852, train loss: 0.01508, val loss: 0.01556\n",
      "Training epoch: 1853, train loss: 0.01507, val loss: 0.01555\n",
      "Training epoch: 1854, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 1855, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1856, train loss: 0.01498, val loss: 0.01541\n",
      "Training epoch: 1857, train loss: 0.01501, val loss: 0.01547\n",
      "Training epoch: 1858, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1859, train loss: 0.01500, val loss: 0.01539\n",
      "Training epoch: 1860, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1861, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1862, train loss: 0.01503, val loss: 0.01548\n",
      "Training epoch: 1863, train loss: 0.01501, val loss: 0.01549\n",
      "Training epoch: 1864, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1865, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1866, train loss: 0.01498, val loss: 0.01540\n",
      "Training epoch: 1867, train loss: 0.01498, val loss: 0.01541\n",
      "Training epoch: 1868, train loss: 0.01509, val loss: 0.01557\n",
      "Training epoch: 1869, train loss: 0.01506, val loss: 0.01550\n",
      "Training epoch: 1870, train loss: 0.01501, val loss: 0.01548\n",
      "Training epoch: 1871, train loss: 0.01510, val loss: 0.01550\n",
      "Training epoch: 1872, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1873, train loss: 0.01517, val loss: 0.01569\n",
      "Training epoch: 1874, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 1875, train loss: 0.01506, val loss: 0.01552\n",
      "Training epoch: 1876, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1877, train loss: 0.01503, val loss: 0.01548\n",
      "Training epoch: 1878, train loss: 0.01509, val loss: 0.01557\n",
      "Training epoch: 1879, train loss: 0.01510, val loss: 0.01559\n",
      "Training epoch: 1880, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1881, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1882, train loss: 0.01498, val loss: 0.01542\n",
      "Training epoch: 1883, train loss: 0.01514, val loss: 0.01553\n",
      "Training epoch: 1884, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 1885, train loss: 0.01504, val loss: 0.01547\n",
      "Training epoch: 1886, train loss: 0.01501, val loss: 0.01547\n",
      "Training epoch: 1887, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1888, train loss: 0.01502, val loss: 0.01546\n",
      "Training epoch: 1889, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 1890, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1891, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1892, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1893, train loss: 0.01498, val loss: 0.01543\n",
      "Training epoch: 1894, train loss: 0.01521, val loss: 0.01561\n",
      "Training epoch: 1895, train loss: 0.01510, val loss: 0.01547\n",
      "Training epoch: 1896, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 1897, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 1898, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 1899, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 1900, train loss: 0.01509, val loss: 0.01547\n",
      "Training epoch: 1901, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1902, train loss: 0.01498, val loss: 0.01542\n",
      "Training epoch: 1903, train loss: 0.01504, val loss: 0.01549\n",
      "Training epoch: 1904, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1905, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1906, train loss: 0.01501, val loss: 0.01545\n",
      "Training epoch: 1907, train loss: 0.01498, val loss: 0.01538\n",
      "Training epoch: 1908, train loss: 0.01501, val loss: 0.01549\n",
      "Training epoch: 1909, train loss: 0.01502, val loss: 0.01542\n",
      "Training epoch: 1910, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 1911, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 1912, train loss: 0.01508, val loss: 0.01549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1913, train loss: 0.01498, val loss: 0.01540\n",
      "Training epoch: 1914, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1915, train loss: 0.01498, val loss: 0.01539\n",
      "Training epoch: 1916, train loss: 0.01514, val loss: 0.01562\n",
      "Training epoch: 1917, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 1918, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 1919, train loss: 0.01512, val loss: 0.01551\n",
      "Training epoch: 1920, train loss: 0.01509, val loss: 0.01548\n",
      "Training epoch: 1921, train loss: 0.01511, val loss: 0.01547\n",
      "Training epoch: 1922, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 1923, train loss: 0.01501, val loss: 0.01549\n",
      "Training epoch: 1924, train loss: 0.01501, val loss: 0.01547\n",
      "Training epoch: 1925, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1926, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 1927, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1928, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 1929, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1930, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 1931, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1932, train loss: 0.01516, val loss: 0.01556\n",
      "Training epoch: 1933, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 1934, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1935, train loss: 0.01501, val loss: 0.01547\n",
      "Training epoch: 1936, train loss: 0.01502, val loss: 0.01546\n",
      "Training epoch: 1937, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1938, train loss: 0.01503, val loss: 0.01551\n",
      "Training epoch: 1939, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1940, train loss: 0.01509, val loss: 0.01547\n",
      "Training epoch: 1941, train loss: 0.01497, val loss: 0.01540\n",
      "Training epoch: 1942, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 1943, train loss: 0.01497, val loss: 0.01540\n",
      "Training epoch: 1944, train loss: 0.01498, val loss: 0.01539\n",
      "Training epoch: 1945, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1946, train loss: 0.01505, val loss: 0.01546\n",
      "Training epoch: 1947, train loss: 0.01506, val loss: 0.01553\n",
      "Training epoch: 1948, train loss: 0.01510, val loss: 0.01560\n",
      "Training epoch: 1949, train loss: 0.01534, val loss: 0.01584\n",
      "Training epoch: 1950, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 1951, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 1952, train loss: 0.01497, val loss: 0.01539\n",
      "Training epoch: 1953, train loss: 0.01506, val loss: 0.01548\n",
      "Training epoch: 1954, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1955, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1956, train loss: 0.01499, val loss: 0.01545\n",
      "Training epoch: 1957, train loss: 0.01500, val loss: 0.01539\n",
      "Training epoch: 1958, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 1959, train loss: 0.01513, val loss: 0.01562\n",
      "Training epoch: 1960, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 1961, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1962, train loss: 0.01508, val loss: 0.01556\n",
      "Training epoch: 1963, train loss: 0.01512, val loss: 0.01550\n",
      "Training epoch: 1964, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1965, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 1966, train loss: 0.01506, val loss: 0.01549\n",
      "Training epoch: 1967, train loss: 0.01500, val loss: 0.01547\n",
      "Training epoch: 1968, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 1969, train loss: 0.01502, val loss: 0.01547\n",
      "Training epoch: 1970, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1971, train loss: 0.01508, val loss: 0.01556\n",
      "Training epoch: 1972, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 1973, train loss: 0.01499, val loss: 0.01538\n",
      "Training epoch: 1974, train loss: 0.01511, val loss: 0.01550\n",
      "Training epoch: 1975, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 1976, train loss: 0.01509, val loss: 0.01545\n",
      "Training epoch: 1977, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 1978, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 1979, train loss: 0.01509, val loss: 0.01548\n",
      "Training epoch: 1980, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1981, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1982, train loss: 0.01500, val loss: 0.01547\n",
      "Training epoch: 1983, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 1984, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 1985, train loss: 0.01506, val loss: 0.01546\n",
      "Training epoch: 1986, train loss: 0.01498, val loss: 0.01540\n",
      "Training epoch: 1987, train loss: 0.01505, val loss: 0.01548\n",
      "Training epoch: 1988, train loss: 0.01497, val loss: 0.01539\n",
      "Training epoch: 1989, train loss: 0.01502, val loss: 0.01547\n",
      "Training epoch: 1990, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 1991, train loss: 0.01499, val loss: 0.01544\n",
      "Training epoch: 1992, train loss: 0.01506, val loss: 0.01554\n",
      "Training epoch: 1993, train loss: 0.01509, val loss: 0.01546\n",
      "Training epoch: 1994, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 1995, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 1996, train loss: 0.01500, val loss: 0.01540\n",
      "Training epoch: 1997, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 1998, train loss: 0.01501, val loss: 0.01544\n",
      "Training epoch: 1999, train loss: 0.01501, val loss: 0.01540\n",
      "Training epoch: 2000, train loss: 0.01500, val loss: 0.01547\n",
      "Training epoch: 2001, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 2002, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 2003, train loss: 0.01503, val loss: 0.01551\n",
      "Training epoch: 2004, train loss: 0.01502, val loss: 0.01548\n",
      "Training epoch: 2005, train loss: 0.01500, val loss: 0.01546\n",
      "Training epoch: 2006, train loss: 0.01503, val loss: 0.01551\n",
      "Training epoch: 2007, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 2008, train loss: 0.01499, val loss: 0.01538\n",
      "Training epoch: 2009, train loss: 0.01513, val loss: 0.01550\n",
      "Training epoch: 2010, train loss: 0.01506, val loss: 0.01549\n",
      "Training epoch: 2011, train loss: 0.01506, val loss: 0.01549\n",
      "Training epoch: 2012, train loss: 0.01500, val loss: 0.01542\n",
      "Training epoch: 2013, train loss: 0.01499, val loss: 0.01543\n",
      "Training epoch: 2014, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 2015, train loss: 0.01506, val loss: 0.01544\n",
      "Training epoch: 2016, train loss: 0.01502, val loss: 0.01541\n",
      "Training epoch: 2017, train loss: 0.01499, val loss: 0.01545\n",
      "Training epoch: 2018, train loss: 0.01498, val loss: 0.01539\n",
      "Training epoch: 2019, train loss: 0.01511, val loss: 0.01560\n",
      "Training epoch: 2020, train loss: 0.01503, val loss: 0.01549\n",
      "Training epoch: 2021, train loss: 0.01500, val loss: 0.01539\n",
      "Training epoch: 2022, train loss: 0.01509, val loss: 0.01549\n",
      "Training epoch: 2023, train loss: 0.01501, val loss: 0.01543\n",
      "Training epoch: 2024, train loss: 0.01499, val loss: 0.01542\n",
      "Training epoch: 2025, train loss: 0.01504, val loss: 0.01548\n",
      "Training epoch: 2026, train loss: 0.01497, val loss: 0.01538\n",
      "Training epoch: 2027, train loss: 0.01499, val loss: 0.01546\n",
      "Training epoch: 2028, train loss: 0.01501, val loss: 0.01547\n",
      "Training epoch: 2029, train loss: 0.01498, val loss: 0.01538\n",
      "Training epoch: 2030, train loss: 0.01500, val loss: 0.01545\n",
      "Training epoch: 2031, train loss: 0.01508, val loss: 0.01545\n",
      "Training epoch: 2032, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 2033, train loss: 0.01499, val loss: 0.01545\n",
      "Training epoch: 2034, train loss: 0.01498, val loss: 0.01538\n",
      "Training epoch: 2035, train loss: 0.01503, val loss: 0.01549\n",
      "Training epoch: 2036, train loss: 0.01500, val loss: 0.01543\n",
      "Training epoch: 2037, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 2038, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 2039, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 2040, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 2041, train loss: 0.01511, val loss: 0.01549\n",
      "Training epoch: 2042, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 2043, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2044, train loss: 0.01506, val loss: 0.01550\n",
      "Training epoch: 2045, train loss: 0.01507, val loss: 0.01554\n",
      "Training epoch: 2046, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 2047, train loss: 0.01509, val loss: 0.01556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2048, train loss: 0.01501, val loss: 0.01550\n",
      "Training epoch: 2049, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 2050, train loss: 0.01501, val loss: 0.01542\n",
      "Training epoch: 2051, train loss: 0.01511, val loss: 0.01551\n",
      "Training epoch: 2052, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 2053, train loss: 0.01505, val loss: 0.01545\n",
      "Training epoch: 2054, train loss: 0.01500, val loss: 0.01541\n",
      "Training epoch: 2055, train loss: 0.01507, val loss: 0.01548\n",
      "Training epoch: 2056, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 2057, train loss: 0.01498, val loss: 0.01540\n",
      "Training epoch: 2058, train loss: 0.01497, val loss: 0.01540\n",
      "Training epoch: 2059, train loss: 0.01504, val loss: 0.01543\n",
      "Training epoch: 2060, train loss: 0.01512, val loss: 0.01552\n",
      "Training epoch: 2061, train loss: 0.01509, val loss: 0.01549\n",
      "Training epoch: 2062, train loss: 0.01503, val loss: 0.01544\n",
      "Training epoch: 2063, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2064, train loss: 0.01511, val loss: 0.01551\n",
      "Training epoch: 2065, train loss: 0.01499, val loss: 0.01540\n",
      "Training epoch: 2066, train loss: 0.01499, val loss: 0.01545\n",
      "Training epoch: 2067, train loss: 0.01503, val loss: 0.01548\n",
      "Training epoch: 2068, train loss: 0.01503, val loss: 0.01546\n",
      "Training epoch: 2069, train loss: 0.01503, val loss: 0.01549\n",
      "Training epoch: 2070, train loss: 0.01505, val loss: 0.01547\n",
      "Training epoch: 2071, train loss: 0.01501, val loss: 0.01545\n",
      "Training epoch: 2072, train loss: 0.01501, val loss: 0.01541\n",
      "Training epoch: 2073, train loss: 0.01505, val loss: 0.01543\n",
      "Training epoch: 2074, train loss: 0.01498, val loss: 0.01543\n",
      "Training epoch: 2075, train loss: 0.01503, val loss: 0.01549\n",
      "Training epoch: 2076, train loss: 0.01498, val loss: 0.01541\n",
      "Training epoch: 2077, train loss: 0.01504, val loss: 0.01551\n",
      "Training epoch: 2078, train loss: 0.01521, val loss: 0.01572\n",
      "Training epoch: 2079, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2080, train loss: 0.01498, val loss: 0.01539\n",
      "Training epoch: 2081, train loss: 0.01504, val loss: 0.01549\n",
      "Training epoch: 2082, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 2083, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 2084, train loss: 0.01498, val loss: 0.01542\n",
      "Training epoch: 2085, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 2086, train loss: 0.01505, val loss: 0.01546\n",
      "Training epoch: 2087, train loss: 0.01498, val loss: 0.01539\n",
      "Training epoch: 2088, train loss: 0.01499, val loss: 0.01541\n",
      "Training epoch: 2089, train loss: 0.01509, val loss: 0.01547\n",
      "Training epoch: 2090, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2091, train loss: 0.01498, val loss: 0.01538\n",
      "Training epoch: 2092, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2093, train loss: 0.01504, val loss: 0.01555\n",
      "Training epoch: 2094, train loss: 0.01502, val loss: 0.01544\n",
      "Training epoch: 2095, train loss: 0.01502, val loss: 0.01545\n",
      "Training epoch: 2096, train loss: 0.01504, val loss: 0.01546\n",
      "Training epoch: 2097, train loss: 0.01503, val loss: 0.01539\n",
      "Training epoch: 2098, train loss: 0.01506, val loss: 0.01547\n",
      "Training epoch: 2099, train loss: 0.01502, val loss: 0.01543\n",
      "Training epoch: 2100, train loss: 0.01507, val loss: 0.01547\n",
      "Training epoch: 2101, train loss: 0.01511, val loss: 0.01550\n",
      "Training epoch: 2102, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 2103, train loss: 0.01501, val loss: 0.01540\n",
      "Training epoch: 2104, train loss: 0.01512, val loss: 0.01562\n",
      "Training epoch: 2105, train loss: 0.01514, val loss: 0.01564\n",
      "Training epoch: 2106, train loss: 0.01500, val loss: 0.01544\n",
      "Training epoch: 2107, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 2108, train loss: 0.01504, val loss: 0.01545\n",
      "Training epoch: 2109, train loss: 0.01499, val loss: 0.01546\n",
      "Early stop at epoch 2109, With Testing Error: 0.01546\n",
      "Subnetwork pruning.\n",
      "Fine tuning.\n",
      "Tuning epoch: 1, train loss: 0.01499, val loss: 0.01540\n",
      "Tuning epoch: 2, train loss: 0.01500, val loss: 0.01542\n",
      "Tuning epoch: 3, train loss: 0.01498, val loss: 0.01541\n",
      "Tuning epoch: 4, train loss: 0.01498, val loss: 0.01541\n",
      "Tuning epoch: 5, train loss: 0.01498, val loss: 0.01543\n",
      "Tuning epoch: 6, train loss: 0.01500, val loss: 0.01543\n",
      "Tuning epoch: 7, train loss: 0.01500, val loss: 0.01545\n",
      "Tuning epoch: 8, train loss: 0.01497, val loss: 0.01538\n",
      "Tuning epoch: 9, train loss: 0.01496, val loss: 0.01541\n",
      "Tuning epoch: 10, train loss: 0.01499, val loss: 0.01542\n",
      "Tuning epoch: 11, train loss: 0.01503, val loss: 0.01547\n",
      "Tuning epoch: 12, train loss: 0.01496, val loss: 0.01541\n",
      "Tuning epoch: 13, train loss: 0.01501, val loss: 0.01543\n",
      "Tuning epoch: 14, train loss: 0.01498, val loss: 0.01543\n",
      "Tuning epoch: 15, train loss: 0.01497, val loss: 0.01540\n",
      "Tuning epoch: 16, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 17, train loss: 0.01499, val loss: 0.01544\n",
      "Tuning epoch: 18, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 19, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 20, train loss: 0.01498, val loss: 0.01543\n",
      "Tuning epoch: 21, train loss: 0.01496, val loss: 0.01539\n",
      "Tuning epoch: 22, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 23, train loss: 0.01502, val loss: 0.01544\n",
      "Tuning epoch: 24, train loss: 0.01497, val loss: 0.01541\n",
      "Tuning epoch: 25, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 26, train loss: 0.01496, val loss: 0.01541\n",
      "Tuning epoch: 27, train loss: 0.01496, val loss: 0.01539\n",
      "Tuning epoch: 28, train loss: 0.01499, val loss: 0.01542\n",
      "Tuning epoch: 29, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 30, train loss: 0.01498, val loss: 0.01541\n",
      "Tuning epoch: 31, train loss: 0.01501, val loss: 0.01543\n",
      "Tuning epoch: 32, train loss: 0.01499, val loss: 0.01544\n",
      "Tuning epoch: 33, train loss: 0.01499, val loss: 0.01543\n",
      "Tuning epoch: 34, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 35, train loss: 0.01496, val loss: 0.01542\n",
      "Tuning epoch: 36, train loss: 0.01498, val loss: 0.01541\n",
      "Tuning epoch: 37, train loss: 0.01497, val loss: 0.01539\n",
      "Tuning epoch: 38, train loss: 0.01495, val loss: 0.01539\n",
      "Tuning epoch: 39, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 40, train loss: 0.01496, val loss: 0.01539\n",
      "Tuning epoch: 41, train loss: 0.01502, val loss: 0.01546\n",
      "Tuning epoch: 42, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 43, train loss: 0.01498, val loss: 0.01541\n",
      "Tuning epoch: 44, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 45, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 46, train loss: 0.01497, val loss: 0.01540\n",
      "Tuning epoch: 47, train loss: 0.01506, val loss: 0.01550\n",
      "Tuning epoch: 48, train loss: 0.01496, val loss: 0.01536\n",
      "Tuning epoch: 49, train loss: 0.01498, val loss: 0.01543\n",
      "Tuning epoch: 50, train loss: 0.01495, val loss: 0.01539\n",
      "Tuning epoch: 51, train loss: 0.01496, val loss: 0.01539\n",
      "Tuning epoch: 52, train loss: 0.01501, val loss: 0.01543\n",
      "Tuning epoch: 53, train loss: 0.01497, val loss: 0.01541\n",
      "Tuning epoch: 54, train loss: 0.01497, val loss: 0.01541\n",
      "Tuning epoch: 55, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 56, train loss: 0.01496, val loss: 0.01541\n",
      "Tuning epoch: 57, train loss: 0.01503, val loss: 0.01545\n",
      "Tuning epoch: 58, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 59, train loss: 0.01504, val loss: 0.01548\n",
      "Tuning epoch: 60, train loss: 0.01497, val loss: 0.01540\n",
      "Tuning epoch: 61, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 62, train loss: 0.01507, val loss: 0.01550\n",
      "Tuning epoch: 63, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 64, train loss: 0.01501, val loss: 0.01545\n",
      "Tuning epoch: 65, train loss: 0.01495, val loss: 0.01539\n",
      "Tuning epoch: 66, train loss: 0.01500, val loss: 0.01541\n",
      "Tuning epoch: 67, train loss: 0.01497, val loss: 0.01542\n",
      "Tuning epoch: 68, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 69, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 70, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 71, train loss: 0.01496, val loss: 0.01539\n",
      "Tuning epoch: 72, train loss: 0.01500, val loss: 0.01542\n",
      "Tuning epoch: 73, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 74, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 75, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 76, train loss: 0.01497, val loss: 0.01538\n",
      "Tuning epoch: 77, train loss: 0.01496, val loss: 0.01539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning epoch: 78, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 79, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 80, train loss: 0.01495, val loss: 0.01540\n",
      "Tuning epoch: 81, train loss: 0.01498, val loss: 0.01540\n",
      "Tuning epoch: 82, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 83, train loss: 0.01498, val loss: 0.01540\n",
      "Tuning epoch: 84, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 85, train loss: 0.01496, val loss: 0.01539\n",
      "Tuning epoch: 86, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 87, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 88, train loss: 0.01496, val loss: 0.01541\n",
      "Tuning epoch: 89, train loss: 0.01496, val loss: 0.01537\n",
      "Tuning epoch: 90, train loss: 0.01497, val loss: 0.01543\n",
      "Tuning epoch: 91, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 92, train loss: 0.01497, val loss: 0.01540\n",
      "Tuning epoch: 93, train loss: 0.01494, val loss: 0.01537\n",
      "Tuning epoch: 94, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 95, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 96, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 97, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 98, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 99, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 100, train loss: 0.01497, val loss: 0.01540\n",
      "Tuning epoch: 101, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 102, train loss: 0.01497, val loss: 0.01538\n",
      "Tuning epoch: 103, train loss: 0.01495, val loss: 0.01539\n",
      "Tuning epoch: 104, train loss: 0.01496, val loss: 0.01539\n",
      "Tuning epoch: 105, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 106, train loss: 0.01497, val loss: 0.01541\n",
      "Tuning epoch: 107, train loss: 0.01494, val loss: 0.01537\n",
      "Tuning epoch: 108, train loss: 0.01500, val loss: 0.01542\n",
      "Tuning epoch: 109, train loss: 0.01498, val loss: 0.01543\n",
      "Tuning epoch: 110, train loss: 0.01496, val loss: 0.01537\n",
      "Tuning epoch: 111, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 112, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 113, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 114, train loss: 0.01496, val loss: 0.01539\n",
      "Tuning epoch: 115, train loss: 0.01495, val loss: 0.01536\n",
      "Tuning epoch: 116, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 117, train loss: 0.01495, val loss: 0.01540\n",
      "Tuning epoch: 118, train loss: 0.01498, val loss: 0.01539\n",
      "Tuning epoch: 119, train loss: 0.01499, val loss: 0.01543\n",
      "Tuning epoch: 120, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 121, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 122, train loss: 0.01498, val loss: 0.01539\n",
      "Tuning epoch: 123, train loss: 0.01498, val loss: 0.01541\n",
      "Tuning epoch: 124, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 125, train loss: 0.01497, val loss: 0.01539\n",
      "Tuning epoch: 126, train loss: 0.01494, val loss: 0.01537\n",
      "Tuning epoch: 127, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 128, train loss: 0.01494, val loss: 0.01535\n",
      "Tuning epoch: 129, train loss: 0.01499, val loss: 0.01542\n",
      "Tuning epoch: 130, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 131, train loss: 0.01499, val loss: 0.01541\n",
      "Tuning epoch: 132, train loss: 0.01496, val loss: 0.01540\n",
      "Tuning epoch: 133, train loss: 0.01500, val loss: 0.01542\n",
      "Tuning epoch: 134, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 135, train loss: 0.01504, val loss: 0.01547\n",
      "Tuning epoch: 136, train loss: 0.01500, val loss: 0.01541\n",
      "Tuning epoch: 137, train loss: 0.01512, val loss: 0.01557\n",
      "Tuning epoch: 138, train loss: 0.01494, val loss: 0.01535\n",
      "Tuning epoch: 139, train loss: 0.01500, val loss: 0.01540\n",
      "Tuning epoch: 140, train loss: 0.01495, val loss: 0.01540\n",
      "Tuning epoch: 141, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 142, train loss: 0.01502, val loss: 0.01544\n",
      "Tuning epoch: 143, train loss: 0.01495, val loss: 0.01539\n",
      "Tuning epoch: 144, train loss: 0.01495, val loss: 0.01536\n",
      "Tuning epoch: 145, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 146, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 147, train loss: 0.01499, val loss: 0.01541\n",
      "Tuning epoch: 148, train loss: 0.01507, val loss: 0.01550\n",
      "Tuning epoch: 149, train loss: 0.01496, val loss: 0.01539\n",
      "Tuning epoch: 150, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 151, train loss: 0.01494, val loss: 0.01535\n",
      "Tuning epoch: 152, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 153, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 154, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 155, train loss: 0.01494, val loss: 0.01537\n",
      "Tuning epoch: 156, train loss: 0.01499, val loss: 0.01541\n",
      "Tuning epoch: 157, train loss: 0.01497, val loss: 0.01539\n",
      "Tuning epoch: 158, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 159, train loss: 0.01498, val loss: 0.01542\n",
      "Tuning epoch: 160, train loss: 0.01494, val loss: 0.01535\n",
      "Tuning epoch: 161, train loss: 0.01494, val loss: 0.01537\n",
      "Tuning epoch: 162, train loss: 0.01498, val loss: 0.01540\n",
      "Tuning epoch: 163, train loss: 0.01495, val loss: 0.01535\n",
      "Tuning epoch: 164, train loss: 0.01494, val loss: 0.01538\n",
      "Tuning epoch: 165, train loss: 0.01494, val loss: 0.01534\n",
      "Tuning epoch: 166, train loss: 0.01495, val loss: 0.01539\n",
      "Tuning epoch: 167, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 168, train loss: 0.01496, val loss: 0.01537\n",
      "Tuning epoch: 169, train loss: 0.01495, val loss: 0.01540\n",
      "Tuning epoch: 170, train loss: 0.01498, val loss: 0.01539\n",
      "Tuning epoch: 171, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 172, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 173, train loss: 0.01494, val loss: 0.01534\n",
      "Tuning epoch: 174, train loss: 0.01496, val loss: 0.01539\n",
      "Tuning epoch: 175, train loss: 0.01495, val loss: 0.01536\n",
      "Tuning epoch: 176, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 177, train loss: 0.01494, val loss: 0.01537\n",
      "Tuning epoch: 178, train loss: 0.01500, val loss: 0.01540\n",
      "Tuning epoch: 179, train loss: 0.01494, val loss: 0.01539\n",
      "Tuning epoch: 180, train loss: 0.01493, val loss: 0.01534\n",
      "Tuning epoch: 181, train loss: 0.01493, val loss: 0.01536\n",
      "Tuning epoch: 182, train loss: 0.01498, val loss: 0.01541\n",
      "Tuning epoch: 183, train loss: 0.01497, val loss: 0.01536\n",
      "Tuning epoch: 184, train loss: 0.01504, val loss: 0.01548\n",
      "Tuning epoch: 185, train loss: 0.01496, val loss: 0.01536\n",
      "Tuning epoch: 186, train loss: 0.01498, val loss: 0.01540\n",
      "Tuning epoch: 187, train loss: 0.01494, val loss: 0.01535\n",
      "Tuning epoch: 188, train loss: 0.01494, val loss: 0.01537\n",
      "Tuning epoch: 189, train loss: 0.01497, val loss: 0.01537\n",
      "Tuning epoch: 190, train loss: 0.01495, val loss: 0.01539\n",
      "Tuning epoch: 191, train loss: 0.01496, val loss: 0.01538\n",
      "Tuning epoch: 192, train loss: 0.01495, val loss: 0.01536\n",
      "Tuning epoch: 193, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 194, train loss: 0.01495, val loss: 0.01538\n",
      "Tuning epoch: 195, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 196, train loss: 0.01494, val loss: 0.01536\n",
      "Tuning epoch: 197, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 198, train loss: 0.01497, val loss: 0.01539\n",
      "Tuning epoch: 199, train loss: 0.01495, val loss: 0.01537\n",
      "Tuning epoch: 200, train loss: 0.01494, val loss: 0.01536\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1296 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "model = xNN(input_num = 10, input_dummy_num=0, subnet_num=10, subnet_arch=[10, 6], task=\"Regression\",\n",
    "               activation_func=tf.tanh, batch_size=1000, training_epochs=5000, lr_bp=0.001, \n",
    "               beta_threshold=0.01, tuning_epochs=200, l1_proj=0.001, l1_subnet=0.001, \n",
    "               verbose=True, val_ratio=0.2, early_stop_thres=500)\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99012 1.01791 1.01834]\n"
     ]
    }
   ],
   "source": [
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "\n",
    "sosxnn_mse_stat = np.hstack([np.round(np.mean((scaler_y.inverse_transform(tr_pred) - scaler_y.inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(val_pred) - scaler_y.inverse_transform(model.val_y))**2),5),\\\n",
    "               np.round(np.mean((scaler_y.inverse_transform(pred_test) - scaler_y.inverse_transform(test_y))**2),5)])\n",
    "print(sosxnn_mse_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
