{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "from exnn import xNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(label, pred, scaler):\n",
    "    pred = scaler.inverse_transform(pred.reshape([-1, 1]))\n",
    "    label = scaler.inverse_transform(label.reshape([-1, 1]))\n",
    "    return np.mean((pred - label)**2)\n",
    "\n",
    "def simu_loader(generator, datanum, testnum, noise_sigma):\n",
    "    def wrapper(rand_seed=0):\n",
    "        return generator(datanum, testnum=testnum, noise_sigma=noise_sigma, rand_seed=rand_seed)\n",
    "    return wrapper\n",
    "\n",
    "def data_generator1(datanum, testnum=10000, noise_sigma=1, rand_seed=0):\n",
    "    \n",
    "    corr = 0.5\n",
    "    np.random.seed(rand_seed)\n",
    "    proj_matrix = np.zeros((10, 4))\n",
    "    proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "    proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "    proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "    proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "    u = np.random.uniform(-1, 1, [datanum + testnum, 1])\n",
    "    t = np.sqrt(corr / (1 - corr))\n",
    "    x = np.zeros((datanum + testnum, 10))\n",
    "    for i in range(10):\n",
    "        x[:, i:i + 1] = (np.random.uniform(-1, 1, [datanum + testnum, 1]) + t * u) / (1 + t)\n",
    "\n",
    "    y = np.reshape(2 * np.dot(x, proj_matrix[:, 0]) + 0.2 * np.exp(-4 * np.dot(x, proj_matrix[:, 1])) + \\\n",
    "                   3 * (np.dot(x, proj_matrix[:, 2]))**2 + 2.5 * np.sin(np.pi * np.dot(x, proj_matrix[:, 3])), [-1, 1]) + \\\n",
    "              noise_sigma * np.random.normal(0, 1, [datanum + testnum, 1])\n",
    "    \n",
    "    task_type = \"Regression\"\n",
    "    meta_info = {\"X1\":{\"type\":\"continuous\"},\n",
    "             \"X2\":{\"type\":\"continuous\"},\n",
    "             \"X3\":{\"type\":\"continuous\"},\n",
    "             \"X4\":{\"type\":\"continuous\"},\n",
    "             \"X5\":{\"type\":\"continuous\"},\n",
    "             \"X6\":{\"type\":\"continuous\"},\n",
    "             \"X7\":{\"type\":\"continuous\"},\n",
    "             \"X8\":{\"type\":\"continuous\"},\n",
    "             \"X9\":{\"type\":\"continuous\"},\n",
    "             \"X10\":{\"type\":\"continuous\"},\n",
    "             \"Y\":{\"type\":\"target\"}}\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == \"target\":\n",
    "            sy = MinMaxScaler((-1, 1))\n",
    "            y = sy.fit_transform(y)\n",
    "            meta_info[key][\"scaler\"] = sy\n",
    "        elif item['type'] == \"categorical\":\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(x[:,[i]])\n",
    "            ordinal_feature = enc.transform(x[:,[i]])\n",
    "            x[:,[i]] = ordinal_feature\n",
    "            meta_info[key][\"values\"] = enc.categories_[0].tolist()\n",
    "        else:\n",
    "            sx = MinMaxScaler((-1, 1))\n",
    "            x[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "            meta_info[key][\"scaler\"] = sx\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=testnum, random_state=rand_seed)\n",
    "    return train_x, test_x, train_y, test_y, task_type, meta_info\n",
    "\n",
    "train_x, test_x, train_y, test_y, task_type, meta_info = data_generator1(datanum=10000, testnum=10000, noise_sigma=1, rand_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xnn_repeat(folder, name, data_generator,\n",
    "                  subnet_num=10,\n",
    "                  subnet_arch=[10, 6],\n",
    "                  task=\"Regression\",\n",
    "                  activation_func=tf.tanh,\n",
    "                  lr_bp=0.001,\n",
    "                  l1_proj=0.001,\n",
    "                  l1_subnet=0.001,\n",
    "                  batch_size=1000,\n",
    "                  training_epochs=5000,\n",
    "                  tuning_epochs=500,\n",
    "                  beta_threshold=0.05,\n",
    "                  verbose=False,\n",
    "                  val_ratio=0.2,\n",
    "                  early_stop_thres=1000,\n",
    "                  rand_seed=0):\n",
    "\n",
    "    train_x, test_x, train_y, test_y, task_type, meta_info = data_generator(rand_seed)\n",
    "\n",
    "    input_num = train_x.shape[1]\n",
    "    model = xNN(meta_info=meta_info,\n",
    "                   subnet_num=10,\n",
    "                   subnet_arch=subnet_arch,\n",
    "                   task_type=task_type,\n",
    "                   activation_func=tf.tanh,\n",
    "                   batch_size=min(batch_size, int(train_x.shape[0] * 0.2)),\n",
    "                   training_epochs=training_epochs,\n",
    "                   lr_bp=lr_bp,\n",
    "                   beta_threshold=beta_threshold,\n",
    "                   tuning_epochs=tuning_epochs,\n",
    "                   l1_proj=l1_proj,\n",
    "                   l1_subnet=l1_subnet,\n",
    "                   verbose=verbose,\n",
    "                   val_ratio=val_ratio,\n",
    "                   early_stop_thres=early_stop_thres)\n",
    "    model.fit(train_x, train_y)\n",
    "    model.visualize(folder=folder,\n",
    "                    name=name,\n",
    "                    save_eps=False)\n",
    "\n",
    "    tr_pred = model.predict(model.tr_x)\n",
    "    val_pred = model.predict(model.val_x)\n",
    "    pred_test = model.predict(test_x)\n",
    "\n",
    "    if task_type == \"Regression\":\n",
    "        stat = np.hstack([np.round(mse(model.tr_y, tr_pred, meta_info[\"Y\"][\"scaler\"]), 5),\\\n",
    "                              np.round(mse(model.val_y, val_pred, meta_info[\"Y\"][\"scaler\"]), 5),\\\n",
    "                              np.round(mse(test_y, pred_test, meta_info[\"Y\"][\"scaler\"]), 5)])\n",
    "    elif task_type == \"Classification\":\n",
    "        stat = np.hstack([np.round(auc(model.tr_y, tr_pred), 5),\\\n",
    "                          np.round(auc(model.val_y, val_pred), 5),\\\n",
    "                          np.round(auc(test_y, pred_test), 5)])\n",
    "\n",
    "    res_stat = pd.DataFrame(np.vstack([stat[0], stat[1], stat[2]]).T, columns=['train_metric', \"val_metric\", \"test_metric\"])\n",
    "    res_stat[\"Subnet_Number\"] = min(input_num, 10)\n",
    "    res_stat[\"lr_BP\"] = lr_bp\n",
    "    res_stat[\"L1_Penalty_Proj\"] = l1_proj\n",
    "    res_stat[\"L1_Penalty_Subnet\"] = l1_subnet\n",
    "    res_stat[\"Training_Epochs\"] = training_epochs\n",
    "    return res_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_metric</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>Subnet_Number</th>\n",
       "      <th>lr_BP</th>\n",
       "      <th>L1_Penalty_Proj</th>\n",
       "      <th>L1_Penalty_Subnet</th>\n",
       "      <th>Training_Epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99941</td>\n",
       "      <td>1.02802</td>\n",
       "      <td>0.99131</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00005</td>\n",
       "      <td>1.02963</td>\n",
       "      <td>0.99138</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00077</td>\n",
       "      <td>1.03103</td>\n",
       "      <td>0.99088</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00694</td>\n",
       "      <td>1.03507</td>\n",
       "      <td>0.99288</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00873</td>\n",
       "      <td>1.03597</td>\n",
       "      <td>0.99592</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.01114</td>\n",
       "      <td>1.03796</td>\n",
       "      <td>0.99696</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08260</td>\n",
       "      <td>1.11844</td>\n",
       "      <td>1.06516</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08184</td>\n",
       "      <td>1.12036</td>\n",
       "      <td>1.06764</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08229</td>\n",
       "      <td>1.12094</td>\n",
       "      <td>1.06543</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08266</td>\n",
       "      <td>1.12331</td>\n",
       "      <td>1.06826</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.09456</td>\n",
       "      <td>1.12724</td>\n",
       "      <td>1.06971</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.12611</td>\n",
       "      <td>1.15771</td>\n",
       "      <td>1.10765</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.12714</td>\n",
       "      <td>1.16161</td>\n",
       "      <td>1.10889</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20810</td>\n",
       "      <td>1.23987</td>\n",
       "      <td>1.20099</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20086</td>\n",
       "      <td>1.24026</td>\n",
       "      <td>1.19830</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20246</td>\n",
       "      <td>1.24271</td>\n",
       "      <td>1.19894</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20624</td>\n",
       "      <td>1.24301</td>\n",
       "      <td>1.19840</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20501</td>\n",
       "      <td>1.24319</td>\n",
       "      <td>1.20092</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20471</td>\n",
       "      <td>1.24319</td>\n",
       "      <td>1.19759</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20349</td>\n",
       "      <td>1.24482</td>\n",
       "      <td>1.20041</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20709</td>\n",
       "      <td>1.24554</td>\n",
       "      <td>1.20217</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.21740</td>\n",
       "      <td>1.25516</td>\n",
       "      <td>1.20866</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.22372</td>\n",
       "      <td>2.26434</td>\n",
       "      <td>2.19919</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.21998</td>\n",
       "      <td>2.26515</td>\n",
       "      <td>2.19671</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.21910</td>\n",
       "      <td>2.26897</td>\n",
       "      <td>2.19821</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_metric  val_metric  test_metric  Subnet_Number  lr_BP  \\\n",
       "0       0.99941     1.02802      0.99131             10  0.001   \n",
       "0       1.00005     1.02963      0.99138             10  0.001   \n",
       "0       1.00077     1.03103      0.99088             10  0.001   \n",
       "0       1.00694     1.03507      0.99288             10  0.001   \n",
       "0       1.00873     1.03597      0.99592             10  0.001   \n",
       "0       1.01114     1.03796      0.99696             10  0.001   \n",
       "0       1.08260     1.11844      1.06516             10  0.001   \n",
       "0       1.08184     1.12036      1.06764             10  0.001   \n",
       "0       1.08229     1.12094      1.06543             10  0.001   \n",
       "0       1.08266     1.12331      1.06826             10  0.001   \n",
       "0       1.09456     1.12724      1.06971             10  0.001   \n",
       "0       1.12611     1.15771      1.10765             10  0.001   \n",
       "0       1.12714     1.16161      1.10889             10  0.001   \n",
       "0       1.20810     1.23987      1.20099             10  0.001   \n",
       "0       1.20086     1.24026      1.19830             10  0.001   \n",
       "0       1.20246     1.24271      1.19894             10  0.001   \n",
       "0       1.20624     1.24301      1.19840             10  0.001   \n",
       "0       1.20501     1.24319      1.20092             10  0.001   \n",
       "0       1.20471     1.24319      1.19759             10  0.001   \n",
       "0       1.20349     1.24482      1.20041             10  0.001   \n",
       "0       1.20709     1.24554      1.20217             10  0.001   \n",
       "0       1.21740     1.25516      1.20866             10  0.001   \n",
       "0       2.22372     2.26434      2.19919             10  0.001   \n",
       "0       2.21998     2.26515      2.19671             10  0.001   \n",
       "0       2.21910     2.26897      2.19821             10  0.001   \n",
       "\n",
       "   L1_Penalty_Proj  L1_Penalty_Subnet  Training_Epochs  \n",
       "0         0.000100           0.000100            10000  \n",
       "0         0.000100           0.000316            10000  \n",
       "0         0.000100           0.001000            10000  \n",
       "0         0.000316           0.000100            10000  \n",
       "0         0.000316           0.000316            10000  \n",
       "0         0.000316           0.001000            10000  \n",
       "0         0.003162           0.001000            10000  \n",
       "0         0.003162           0.000100            10000  \n",
       "0         0.001000           0.001000            10000  \n",
       "0         0.003162           0.000316            10000  \n",
       "0         0.001000           0.003162            10000  \n",
       "0         0.001000           0.000100            10000  \n",
       "0         0.001000           0.000316            10000  \n",
       "0         0.010000           0.001000            10000  \n",
       "0         0.000100           0.010000            10000  \n",
       "0         0.000100           0.003162            10000  \n",
       "0         0.010000           0.000316            10000  \n",
       "0         0.000316           0.010000            10000  \n",
       "0         0.000316           0.003162            10000  \n",
       "0         0.001000           0.010000            10000  \n",
       "0         0.010000           0.000100            10000  \n",
       "0         0.010000           0.003162            10000  \n",
       "0         0.003162           0.003162            10000  \n",
       "0         0.003162           0.010000            10000  \n",
       "0         0.010000           0.010000            10000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = Parallel(n_jobs=25)(delayed(xnn_repeat)(folder=\"./results/S1_xnn/\",\n",
    "                      name=str(i + 1).zfill(2) + \"_\" + str(j + 1).zfill(2),\n",
    "                      data_generator=simu_loader(data_generator1, 10000, 10000, 1),\n",
    "                      task=task_type,\n",
    "                      subnet_arch=[10, 6],\n",
    "                      beta_threshold=0.05,\n",
    "                      l1_proj=10**(-2 - i*0.5),\n",
    "                      l1_subnet=10**(-2 - j*0.5),\n",
    "                      training_epochs=10000,\n",
    "                      lr_bp=0.001,\n",
    "                      batch_size=1000,\n",
    "                      early_stop_thres=500,\n",
    "                      tuning_epochs=100,\n",
    "                      rand_seed=0) for i in range(5) for j in range(5) for k in [1])\n",
    "sosxnn_stat_all = pd.concat(cv_results)\n",
    "sosxnn_stat_all.sort_values(\"val_metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_l1_prob = sosxnn_stat_all.sort_values(\"val_metric\").loc[:,\"L1_Penalty_Proj\"].iloc[0]\n",
    "best_l1_subnet = sosxnn_stat_all.sort_values(\"val_metric\").loc[:,\"L1_Penalty_Subnet\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1209 18:42:19.808708 140370578847552 deprecation.py:323] From /home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py:330: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1, train loss: 0.05167, val loss: 0.05170\n",
      "Training epoch: 2, train loss: 0.04446, val loss: 0.04462\n",
      "Training epoch: 3, train loss: 0.04094, val loss: 0.04136\n",
      "Training epoch: 4, train loss: 0.03951, val loss: 0.03995\n",
      "Training epoch: 5, train loss: 0.03852, val loss: 0.03902\n",
      "Training epoch: 6, train loss: 0.03759, val loss: 0.03819\n",
      "Training epoch: 7, train loss: 0.03690, val loss: 0.03754\n",
      "Training epoch: 8, train loss: 0.03621, val loss: 0.03688\n",
      "Training epoch: 9, train loss: 0.03550, val loss: 0.03618\n",
      "Training epoch: 10, train loss: 0.03480, val loss: 0.03549\n",
      "Training epoch: 11, train loss: 0.03410, val loss: 0.03481\n",
      "Training epoch: 12, train loss: 0.03345, val loss: 0.03415\n",
      "Training epoch: 13, train loss: 0.03285, val loss: 0.03356\n",
      "Training epoch: 14, train loss: 0.03235, val loss: 0.03308\n",
      "Training epoch: 15, train loss: 0.03191, val loss: 0.03266\n",
      "Training epoch: 16, train loss: 0.03154, val loss: 0.03228\n",
      "Training epoch: 17, train loss: 0.03118, val loss: 0.03195\n",
      "Training epoch: 18, train loss: 0.03080, val loss: 0.03161\n",
      "Training epoch: 19, train loss: 0.03041, val loss: 0.03121\n",
      "Training epoch: 20, train loss: 0.03000, val loss: 0.03082\n",
      "Training epoch: 21, train loss: 0.02959, val loss: 0.03043\n",
      "Training epoch: 22, train loss: 0.02915, val loss: 0.03002\n",
      "Training epoch: 23, train loss: 0.02875, val loss: 0.02962\n",
      "Training epoch: 24, train loss: 0.02831, val loss: 0.02923\n",
      "Training epoch: 25, train loss: 0.02791, val loss: 0.02886\n",
      "Training epoch: 26, train loss: 0.02751, val loss: 0.02848\n",
      "Training epoch: 27, train loss: 0.02714, val loss: 0.02809\n",
      "Training epoch: 28, train loss: 0.02677, val loss: 0.02777\n",
      "Training epoch: 29, train loss: 0.02642, val loss: 0.02743\n",
      "Training epoch: 30, train loss: 0.02610, val loss: 0.02714\n",
      "Training epoch: 31, train loss: 0.02580, val loss: 0.02684\n",
      "Training epoch: 32, train loss: 0.02543, val loss: 0.02649\n",
      "Training epoch: 33, train loss: 0.02514, val loss: 0.02616\n",
      "Training epoch: 34, train loss: 0.02481, val loss: 0.02591\n",
      "Training epoch: 35, train loss: 0.02441, val loss: 0.02550\n",
      "Training epoch: 36, train loss: 0.02409, val loss: 0.02519\n",
      "Training epoch: 37, train loss: 0.02375, val loss: 0.02484\n",
      "Training epoch: 38, train loss: 0.02347, val loss: 0.02457\n",
      "Training epoch: 39, train loss: 0.02320, val loss: 0.02432\n",
      "Training epoch: 40, train loss: 0.02281, val loss: 0.02393\n",
      "Training epoch: 41, train loss: 0.02257, val loss: 0.02366\n",
      "Training epoch: 42, train loss: 0.02225, val loss: 0.02337\n",
      "Training epoch: 43, train loss: 0.02201, val loss: 0.02314\n",
      "Training epoch: 44, train loss: 0.02176, val loss: 0.02288\n",
      "Training epoch: 45, train loss: 0.02152, val loss: 0.02267\n",
      "Training epoch: 46, train loss: 0.02131, val loss: 0.02246\n",
      "Training epoch: 47, train loss: 0.02117, val loss: 0.02229\n",
      "Training epoch: 48, train loss: 0.02101, val loss: 0.02211\n",
      "Training epoch: 49, train loss: 0.02076, val loss: 0.02189\n",
      "Training epoch: 50, train loss: 0.02078, val loss: 0.02190\n",
      "Training epoch: 51, train loss: 0.02054, val loss: 0.02164\n",
      "Training epoch: 52, train loss: 0.02035, val loss: 0.02143\n",
      "Training epoch: 53, train loss: 0.02026, val loss: 0.02138\n",
      "Training epoch: 54, train loss: 0.02016, val loss: 0.02126\n",
      "Training epoch: 55, train loss: 0.02000, val loss: 0.02112\n",
      "Training epoch: 56, train loss: 0.01992, val loss: 0.02099\n",
      "Training epoch: 57, train loss: 0.01978, val loss: 0.02086\n",
      "Training epoch: 58, train loss: 0.01976, val loss: 0.02080\n",
      "Training epoch: 59, train loss: 0.01982, val loss: 0.02089\n",
      "Training epoch: 60, train loss: 0.01958, val loss: 0.02062\n",
      "Training epoch: 61, train loss: 0.01957, val loss: 0.02063\n",
      "Training epoch: 62, train loss: 0.01935, val loss: 0.02039\n",
      "Training epoch: 63, train loss: 0.01926, val loss: 0.02029\n",
      "Training epoch: 64, train loss: 0.01915, val loss: 0.02019\n",
      "Training epoch: 65, train loss: 0.01909, val loss: 0.02011\n",
      "Training epoch: 66, train loss: 0.01904, val loss: 0.02006\n",
      "Training epoch: 67, train loss: 0.01901, val loss: 0.02002\n",
      "Training epoch: 68, train loss: 0.01886, val loss: 0.01986\n",
      "Training epoch: 69, train loss: 0.01878, val loss: 0.01976\n",
      "Training epoch: 70, train loss: 0.01882, val loss: 0.01981\n",
      "Training epoch: 71, train loss: 0.01863, val loss: 0.01962\n",
      "Training epoch: 72, train loss: 0.01868, val loss: 0.01968\n",
      "Training epoch: 73, train loss: 0.01851, val loss: 0.01949\n",
      "Training epoch: 74, train loss: 0.01844, val loss: 0.01941\n",
      "Training epoch: 75, train loss: 0.01841, val loss: 0.01936\n",
      "Training epoch: 76, train loss: 0.01836, val loss: 0.01932\n",
      "Training epoch: 77, train loss: 0.01825, val loss: 0.01919\n",
      "Training epoch: 78, train loss: 0.01820, val loss: 0.01914\n",
      "Training epoch: 79, train loss: 0.01814, val loss: 0.01906\n",
      "Training epoch: 80, train loss: 0.01816, val loss: 0.01908\n",
      "Training epoch: 81, train loss: 0.01804, val loss: 0.01895\n",
      "Training epoch: 82, train loss: 0.01798, val loss: 0.01891\n",
      "Training epoch: 83, train loss: 0.01795, val loss: 0.01886\n",
      "Training epoch: 84, train loss: 0.01789, val loss: 0.01877\n",
      "Training epoch: 85, train loss: 0.01784, val loss: 0.01874\n",
      "Training epoch: 86, train loss: 0.01794, val loss: 0.01884\n",
      "Training epoch: 87, train loss: 0.01773, val loss: 0.01861\n",
      "Training epoch: 88, train loss: 0.01768, val loss: 0.01855\n",
      "Training epoch: 89, train loss: 0.01764, val loss: 0.01850\n",
      "Training epoch: 90, train loss: 0.01760, val loss: 0.01845\n",
      "Training epoch: 91, train loss: 0.01758, val loss: 0.01844\n",
      "Training epoch: 92, train loss: 0.01755, val loss: 0.01837\n",
      "Training epoch: 93, train loss: 0.01749, val loss: 0.01833\n",
      "Training epoch: 94, train loss: 0.01747, val loss: 0.01827\n",
      "Training epoch: 95, train loss: 0.01745, val loss: 0.01829\n",
      "Training epoch: 96, train loss: 0.01738, val loss: 0.01820\n",
      "Training epoch: 97, train loss: 0.01733, val loss: 0.01813\n",
      "Training epoch: 98, train loss: 0.01733, val loss: 0.01816\n",
      "Training epoch: 99, train loss: 0.01731, val loss: 0.01808\n",
      "Training epoch: 100, train loss: 0.01726, val loss: 0.01806\n",
      "Training epoch: 101, train loss: 0.01724, val loss: 0.01803\n",
      "Training epoch: 102, train loss: 0.01717, val loss: 0.01796\n",
      "Training epoch: 103, train loss: 0.01715, val loss: 0.01793\n",
      "Training epoch: 104, train loss: 0.01716, val loss: 0.01792\n",
      "Training epoch: 105, train loss: 0.01720, val loss: 0.01796\n",
      "Training epoch: 106, train loss: 0.01706, val loss: 0.01782\n",
      "Training epoch: 107, train loss: 0.01709, val loss: 0.01785\n",
      "Training epoch: 108, train loss: 0.01700, val loss: 0.01774\n",
      "Training epoch: 109, train loss: 0.01701, val loss: 0.01772\n",
      "Training epoch: 110, train loss: 0.01695, val loss: 0.01767\n",
      "Training epoch: 111, train loss: 0.01694, val loss: 0.01767\n",
      "Training epoch: 112, train loss: 0.01694, val loss: 0.01766\n",
      "Training epoch: 113, train loss: 0.01695, val loss: 0.01769\n",
      "Training epoch: 114, train loss: 0.01686, val loss: 0.01756\n",
      "Training epoch: 115, train loss: 0.01691, val loss: 0.01761\n",
      "Training epoch: 116, train loss: 0.01681, val loss: 0.01750\n",
      "Training epoch: 117, train loss: 0.01679, val loss: 0.01749\n",
      "Training epoch: 118, train loss: 0.01679, val loss: 0.01746\n",
      "Training epoch: 119, train loss: 0.01675, val loss: 0.01743\n",
      "Training epoch: 120, train loss: 0.01673, val loss: 0.01740\n",
      "Training epoch: 121, train loss: 0.01669, val loss: 0.01737\n",
      "Training epoch: 122, train loss: 0.01668, val loss: 0.01732\n",
      "Training epoch: 123, train loss: 0.01668, val loss: 0.01733\n",
      "Training epoch: 124, train loss: 0.01663, val loss: 0.01729\n",
      "Training epoch: 125, train loss: 0.01661, val loss: 0.01724\n",
      "Training epoch: 126, train loss: 0.01659, val loss: 0.01721\n",
      "Training epoch: 127, train loss: 0.01669, val loss: 0.01734\n",
      "Training epoch: 128, train loss: 0.01657, val loss: 0.01722\n",
      "Training epoch: 129, train loss: 0.01653, val loss: 0.01715\n",
      "Training epoch: 130, train loss: 0.01650, val loss: 0.01711\n",
      "Training epoch: 131, train loss: 0.01648, val loss: 0.01709\n",
      "Training epoch: 132, train loss: 0.01646, val loss: 0.01706\n",
      "Training epoch: 133, train loss: 0.01646, val loss: 0.01707\n",
      "Training epoch: 134, train loss: 0.01644, val loss: 0.01706\n",
      "Training epoch: 135, train loss: 0.01640, val loss: 0.01700\n",
      "Training epoch: 136, train loss: 0.01639, val loss: 0.01699\n",
      "Training epoch: 137, train loss: 0.01638, val loss: 0.01695\n",
      "Training epoch: 138, train loss: 0.01635, val loss: 0.01693\n",
      "Training epoch: 139, train loss: 0.01637, val loss: 0.01696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 140, train loss: 0.01634, val loss: 0.01689\n",
      "Training epoch: 141, train loss: 0.01633, val loss: 0.01687\n",
      "Training epoch: 142, train loss: 0.01629, val loss: 0.01685\n",
      "Training epoch: 143, train loss: 0.01627, val loss: 0.01682\n",
      "Training epoch: 144, train loss: 0.01632, val loss: 0.01686\n",
      "Training epoch: 145, train loss: 0.01626, val loss: 0.01679\n",
      "Training epoch: 146, train loss: 0.01626, val loss: 0.01683\n",
      "Training epoch: 147, train loss: 0.01625, val loss: 0.01681\n",
      "Training epoch: 148, train loss: 0.01624, val loss: 0.01678\n",
      "Training epoch: 149, train loss: 0.01616, val loss: 0.01670\n",
      "Training epoch: 150, train loss: 0.01613, val loss: 0.01665\n",
      "Training epoch: 151, train loss: 0.01612, val loss: 0.01663\n",
      "Training epoch: 152, train loss: 0.01613, val loss: 0.01663\n",
      "Training epoch: 153, train loss: 0.01612, val loss: 0.01663\n",
      "Training epoch: 154, train loss: 0.01606, val loss: 0.01658\n",
      "Training epoch: 155, train loss: 0.01606, val loss: 0.01658\n",
      "Training epoch: 156, train loss: 0.01611, val loss: 0.01661\n",
      "Training epoch: 157, train loss: 0.01607, val loss: 0.01654\n",
      "Training epoch: 158, train loss: 0.01604, val loss: 0.01656\n",
      "Training epoch: 159, train loss: 0.01607, val loss: 0.01656\n",
      "Training epoch: 160, train loss: 0.01604, val loss: 0.01656\n",
      "Training epoch: 161, train loss: 0.01602, val loss: 0.01651\n",
      "Training epoch: 162, train loss: 0.01595, val loss: 0.01644\n",
      "Training epoch: 163, train loss: 0.01594, val loss: 0.01644\n",
      "Training epoch: 164, train loss: 0.01594, val loss: 0.01644\n",
      "Training epoch: 165, train loss: 0.01592, val loss: 0.01642\n",
      "Training epoch: 166, train loss: 0.01610, val loss: 0.01654\n",
      "Training epoch: 167, train loss: 0.01619, val loss: 0.01660\n",
      "Training epoch: 168, train loss: 0.01590, val loss: 0.01637\n",
      "Training epoch: 169, train loss: 0.01586, val loss: 0.01632\n",
      "Training epoch: 170, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 171, train loss: 0.01584, val loss: 0.01632\n",
      "Training epoch: 172, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 173, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 174, train loss: 0.01582, val loss: 0.01629\n",
      "Training epoch: 175, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 176, train loss: 0.01600, val loss: 0.01651\n",
      "Training epoch: 177, train loss: 0.01581, val loss: 0.01628\n",
      "Training epoch: 178, train loss: 0.01579, val loss: 0.01622\n",
      "Training epoch: 179, train loss: 0.01580, val loss: 0.01622\n",
      "Training epoch: 180, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 181, train loss: 0.01580, val loss: 0.01622\n",
      "Training epoch: 182, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 183, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 184, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 185, train loss: 0.01575, val loss: 0.01624\n",
      "Training epoch: 186, train loss: 0.01571, val loss: 0.01615\n",
      "Training epoch: 187, train loss: 0.01578, val loss: 0.01626\n",
      "Training epoch: 188, train loss: 0.01570, val loss: 0.01616\n",
      "Training epoch: 189, train loss: 0.01568, val loss: 0.01613\n",
      "Training epoch: 190, train loss: 0.01572, val loss: 0.01619\n",
      "Training epoch: 191, train loss: 0.01565, val loss: 0.01610\n",
      "Training epoch: 192, train loss: 0.01566, val loss: 0.01608\n",
      "Training epoch: 193, train loss: 0.01564, val loss: 0.01608\n",
      "Training epoch: 194, train loss: 0.01564, val loss: 0.01610\n",
      "Training epoch: 195, train loss: 0.01567, val loss: 0.01613\n",
      "Training epoch: 196, train loss: 0.01564, val loss: 0.01609\n",
      "Training epoch: 197, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 198, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 199, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 200, train loss: 0.01570, val loss: 0.01611\n",
      "Training epoch: 201, train loss: 0.01568, val loss: 0.01609\n",
      "Training epoch: 202, train loss: 0.01566, val loss: 0.01610\n",
      "Training epoch: 203, train loss: 0.01561, val loss: 0.01604\n",
      "Training epoch: 204, train loss: 0.01563, val loss: 0.01608\n",
      "Training epoch: 205, train loss: 0.01558, val loss: 0.01603\n",
      "Training epoch: 206, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 207, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 208, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 209, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 210, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 211, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 212, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 213, train loss: 0.01556, val loss: 0.01601\n",
      "Training epoch: 214, train loss: 0.01562, val loss: 0.01602\n",
      "Training epoch: 215, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 216, train loss: 0.01555, val loss: 0.01599\n",
      "Training epoch: 217, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 218, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 219, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 220, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 221, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 222, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 223, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 224, train loss: 0.01559, val loss: 0.01604\n",
      "Training epoch: 225, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 226, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 227, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 228, train loss: 0.01551, val loss: 0.01595\n",
      "Training epoch: 229, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 230, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 231, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 232, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 233, train loss: 0.01571, val loss: 0.01621\n",
      "Training epoch: 234, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 235, train loss: 0.01550, val loss: 0.01594\n",
      "Training epoch: 236, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 237, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 238, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 239, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 240, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 241, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 242, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 243, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 244, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 245, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 246, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 247, train loss: 0.01551, val loss: 0.01593\n",
      "Training epoch: 248, train loss: 0.01555, val loss: 0.01600\n",
      "Training epoch: 249, train loss: 0.01553, val loss: 0.01596\n",
      "Training epoch: 250, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 251, train loss: 0.01548, val loss: 0.01592\n",
      "Training epoch: 252, train loss: 0.01554, val loss: 0.01599\n",
      "Training epoch: 253, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 254, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 255, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 256, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 257, train loss: 0.01542, val loss: 0.01590\n",
      "Training epoch: 258, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 259, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 260, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 261, train loss: 0.01557, val loss: 0.01607\n",
      "Training epoch: 262, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 263, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 264, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 265, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 266, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 267, train loss: 0.01542, val loss: 0.01590\n",
      "Training epoch: 268, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 269, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 270, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 271, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 272, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 273, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 274, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 275, train loss: 0.01540, val loss: 0.01589\n",
      "Training epoch: 276, train loss: 0.01544, val loss: 0.01594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 277, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 278, train loss: 0.01539, val loss: 0.01588\n",
      "Training epoch: 279, train loss: 0.01545, val loss: 0.01596\n",
      "Training epoch: 280, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 281, train loss: 0.01538, val loss: 0.01585\n",
      "Training epoch: 282, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 283, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 284, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 285, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 286, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 287, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 288, train loss: 0.01563, val loss: 0.01602\n",
      "Training epoch: 289, train loss: 0.01536, val loss: 0.01584\n",
      "Training epoch: 290, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 291, train loss: 0.01544, val loss: 0.01593\n",
      "Training epoch: 292, train loss: 0.01556, val loss: 0.01608\n",
      "Training epoch: 293, train loss: 0.01537, val loss: 0.01585\n",
      "Training epoch: 294, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 295, train loss: 0.01546, val loss: 0.01589\n",
      "Training epoch: 296, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 297, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 298, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 299, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 300, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 301, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 302, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 303, train loss: 0.01542, val loss: 0.01585\n",
      "Training epoch: 304, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 305, train loss: 0.01536, val loss: 0.01584\n",
      "Training epoch: 306, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 307, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 308, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 309, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 310, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 311, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 312, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 313, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 314, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 315, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 316, train loss: 0.01536, val loss: 0.01584\n",
      "Training epoch: 317, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 318, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 319, train loss: 0.01533, val loss: 0.01580\n",
      "Training epoch: 320, train loss: 0.01535, val loss: 0.01583\n",
      "Training epoch: 321, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 322, train loss: 0.01534, val loss: 0.01581\n",
      "Training epoch: 323, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 324, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 325, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 326, train loss: 0.01544, val loss: 0.01594\n",
      "Training epoch: 327, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 328, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 329, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 330, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 331, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 332, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 333, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 334, train loss: 0.01537, val loss: 0.01586\n",
      "Training epoch: 335, train loss: 0.01546, val loss: 0.01596\n",
      "Training epoch: 336, train loss: 0.01540, val loss: 0.01588\n",
      "Training epoch: 337, train loss: 0.01545, val loss: 0.01594\n",
      "Training epoch: 338, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 339, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 340, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 341, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 342, train loss: 0.01533, val loss: 0.01580\n",
      "Training epoch: 343, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 344, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 345, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 346, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 347, train loss: 0.01533, val loss: 0.01581\n",
      "Training epoch: 348, train loss: 0.01544, val loss: 0.01593\n",
      "Training epoch: 349, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 350, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 351, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 352, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 353, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 354, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 355, train loss: 0.01532, val loss: 0.01580\n",
      "Training epoch: 356, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 357, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 358, train loss: 0.01537, val loss: 0.01580\n",
      "Training epoch: 359, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 360, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 361, train loss: 0.01536, val loss: 0.01584\n",
      "Training epoch: 362, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 363, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 364, train loss: 0.01545, val loss: 0.01595\n",
      "Training epoch: 365, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 366, train loss: 0.01531, val loss: 0.01579\n",
      "Training epoch: 367, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 368, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 369, train loss: 0.01528, val loss: 0.01576\n",
      "Training epoch: 370, train loss: 0.01533, val loss: 0.01581\n",
      "Training epoch: 371, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 372, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 373, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 374, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 375, train loss: 0.01528, val loss: 0.01576\n",
      "Training epoch: 376, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 377, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 378, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 379, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 380, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 381, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 382, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 383, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 384, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 385, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 386, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 387, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 388, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 389, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 390, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 391, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 392, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 393, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 394, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 395, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 396, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 397, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 398, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 399, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 400, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 401, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 402, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 403, train loss: 0.01527, val loss: 0.01574\n",
      "Training epoch: 404, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 405, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 406, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 407, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 408, train loss: 0.01544, val loss: 0.01587\n",
      "Training epoch: 409, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 410, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 411, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 412, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 413, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 414, train loss: 0.01527, val loss: 0.01572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 415, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 416, train loss: 0.01534, val loss: 0.01582\n",
      "Training epoch: 417, train loss: 0.01537, val loss: 0.01586\n",
      "Training epoch: 418, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 419, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 420, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 421, train loss: 0.01537, val loss: 0.01585\n",
      "Training epoch: 422, train loss: 0.01534, val loss: 0.01583\n",
      "Training epoch: 423, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 424, train loss: 0.01540, val loss: 0.01588\n",
      "Training epoch: 425, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 426, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 427, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 428, train loss: 0.01531, val loss: 0.01579\n",
      "Training epoch: 429, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 430, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 431, train loss: 0.01533, val loss: 0.01582\n",
      "Training epoch: 432, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 433, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 434, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 435, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 436, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 437, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 438, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 439, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 440, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 441, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 442, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 443, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 444, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 445, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 446, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 447, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 448, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 449, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 450, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 451, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 452, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 453, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 454, train loss: 0.01542, val loss: 0.01591\n",
      "Training epoch: 455, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 456, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 457, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 458, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 459, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 460, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 461, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 462, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 463, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 464, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 465, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 466, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 467, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 468, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 469, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 470, train loss: 0.01540, val loss: 0.01582\n",
      "Training epoch: 471, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 472, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 473, train loss: 0.01549, val loss: 0.01592\n",
      "Training epoch: 474, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 475, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 476, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 477, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 478, train loss: 0.01562, val loss: 0.01614\n",
      "Training epoch: 479, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 480, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 481, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 482, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 483, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 484, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 485, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 486, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 487, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 488, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 489, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 490, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 491, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 492, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 493, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 494, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 495, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 496, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 497, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 498, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 499, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 500, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 501, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 502, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 503, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 504, train loss: 0.01524, val loss: 0.01568\n",
      "Training epoch: 505, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 506, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 507, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 508, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 509, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 510, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 511, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 512, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 513, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 514, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 515, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 516, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 517, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 518, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 519, train loss: 0.01537, val loss: 0.01586\n",
      "Training epoch: 520, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 521, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 522, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 523, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 524, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 525, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 526, train loss: 0.01544, val loss: 0.01593\n",
      "Training epoch: 527, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 528, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 529, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 530, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 531, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 532, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 533, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 534, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 535, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 536, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 537, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 538, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 539, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 540, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 541, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 542, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 543, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 544, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 545, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 546, train loss: 0.01541, val loss: 0.01591\n",
      "Training epoch: 547, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 548, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 549, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 550, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 551, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 552, train loss: 0.01533, val loss: 0.01579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 553, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 554, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 555, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 556, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 557, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 558, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 559, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 560, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 561, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 562, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 563, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 564, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 565, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 566, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 567, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 568, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 569, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 570, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 571, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 572, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 573, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 574, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 575, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 576, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 577, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 578, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 579, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 580, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 581, train loss: 0.01542, val loss: 0.01591\n",
      "Training epoch: 582, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 583, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 584, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 585, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 586, train loss: 0.01524, val loss: 0.01568\n",
      "Training epoch: 587, train loss: 0.01539, val loss: 0.01582\n",
      "Training epoch: 588, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 589, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 590, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 591, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 592, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 593, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 594, train loss: 0.01523, val loss: 0.01571\n",
      "Training epoch: 595, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 596, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 597, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 598, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 599, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 600, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 601, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 602, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 603, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 604, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 605, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 606, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 607, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 608, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 609, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 610, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 611, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 612, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 613, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 614, train loss: 0.01526, val loss: 0.01575\n",
      "Training epoch: 615, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 616, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 617, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 618, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 619, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 620, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 621, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 622, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 623, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 624, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 625, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 626, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 627, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 628, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 629, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 630, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 631, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 632, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 633, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 634, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 635, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 636, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 637, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 638, train loss: 0.01524, val loss: 0.01572\n",
      "Training epoch: 639, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 640, train loss: 0.01538, val loss: 0.01586\n",
      "Training epoch: 641, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 642, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 643, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 644, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 645, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 646, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 647, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 648, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 649, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 650, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 651, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 652, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 653, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 654, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 655, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 656, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 657, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 658, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 659, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 660, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 661, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 662, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 663, train loss: 0.01520, val loss: 0.01568\n",
      "Training epoch: 664, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 665, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 666, train loss: 0.01538, val loss: 0.01587\n",
      "Training epoch: 667, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 668, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 669, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 670, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 671, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 672, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 673, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 674, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 675, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 676, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 677, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 678, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 679, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 680, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 681, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 682, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 683, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 684, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 685, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 686, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 687, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 688, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 689, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 690, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 691, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 692, train loss: 0.01524, val loss: 0.01567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 693, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 694, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 695, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 696, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 697, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 698, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 699, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 700, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 701, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 702, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 703, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 704, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 705, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 706, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 707, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 708, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 709, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 710, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 711, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 712, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 713, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 714, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 715, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 716, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 717, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 718, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 719, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 720, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 721, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 722, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 723, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 724, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 725, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 726, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 727, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 728, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 729, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 730, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 731, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 732, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 733, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 734, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 735, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 736, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 737, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 738, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 739, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 740, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 741, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 742, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 743, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 744, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 745, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 746, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 747, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 748, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 749, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 750, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 751, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 752, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 753, train loss: 0.01518, val loss: 0.01566\n",
      "Training epoch: 754, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 755, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 756, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 757, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 758, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 759, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 760, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 761, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 762, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 763, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 764, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 765, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 766, train loss: 0.01517, val loss: 0.01564\n",
      "Training epoch: 767, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 768, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 769, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 770, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 771, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 772, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 773, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 774, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 775, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 776, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 777, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 778, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 779, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 780, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 781, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 782, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 783, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 784, train loss: 0.01528, val loss: 0.01572\n",
      "Training epoch: 785, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 786, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 787, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 788, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 789, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 790, train loss: 0.01531, val loss: 0.01579\n",
      "Training epoch: 791, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 792, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 793, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 794, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 795, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 796, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 797, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 798, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 799, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 800, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 801, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 802, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 803, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 804, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 805, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 806, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 807, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 808, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 809, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 810, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 811, train loss: 0.01534, val loss: 0.01581\n",
      "Training epoch: 812, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 813, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 814, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 815, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 816, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 817, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 818, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 819, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 820, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 821, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 822, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 823, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 824, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 825, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 826, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 827, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 828, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 829, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 830, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 831, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 832, train loss: 0.01525, val loss: 0.01571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 833, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 834, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 835, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 836, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 837, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 838, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 839, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 840, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 841, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 842, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 843, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 844, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 845, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 846, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 847, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 848, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 849, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 850, train loss: 0.01529, val loss: 0.01577\n",
      "Training epoch: 851, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 852, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 853, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 854, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 855, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 856, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 857, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 858, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 859, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 860, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 861, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 862, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 863, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 864, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 865, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 866, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 867, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 868, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 869, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 870, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 871, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 872, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 873, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 874, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 875, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 876, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 877, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 878, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 879, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 880, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 881, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 882, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 883, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 884, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 885, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 886, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 887, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 888, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 889, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 890, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 891, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 892, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 893, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 894, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 895, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 896, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 897, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 898, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 899, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 900, train loss: 0.01519, val loss: 0.01567\n",
      "Training epoch: 901, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 902, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 903, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 904, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 905, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 906, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 907, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 908, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 909, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 910, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 911, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 912, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 913, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 914, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 915, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 916, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 917, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 918, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 919, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 920, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 921, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 922, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 923, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 924, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 925, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 926, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 927, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 928, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 929, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 930, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 931, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 932, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 933, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 934, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 935, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 936, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 937, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 938, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 939, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 940, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 941, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 942, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 943, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 944, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 945, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 946, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 947, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 948, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 949, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 950, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 951, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 952, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 953, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 954, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 955, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 956, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 957, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 958, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 959, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 960, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 961, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 962, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 963, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 964, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 965, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 966, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 967, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 968, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 969, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 970, train loss: 0.01524, val loss: 0.01568\n",
      "Training epoch: 971, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 972, train loss: 0.01515, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 973, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 974, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 975, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 976, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 977, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 978, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 979, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 980, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 981, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 982, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 983, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 984, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 985, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 986, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 987, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 988, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 989, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 990, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 991, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 992, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 993, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 994, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 995, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 996, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 997, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 998, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 999, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1000, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1001, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1002, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1003, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1004, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1005, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 1006, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1007, train loss: 0.01534, val loss: 0.01582\n",
      "Training epoch: 1008, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 1009, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 1010, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1011, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1012, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 1013, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 1014, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 1015, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1016, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1017, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1018, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1019, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1020, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 1021, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 1022, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1023, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1024, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1025, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1026, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1027, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 1028, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 1029, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1030, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1031, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 1032, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1033, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1034, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1035, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 1036, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1037, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1038, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1039, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1040, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1041, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1042, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1043, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 1044, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1045, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 1046, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1047, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1048, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1049, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1050, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1051, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1052, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 1053, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1054, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1055, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1056, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1057, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1058, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1059, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1060, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 1061, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1062, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1063, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1064, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1065, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 1066, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1067, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1068, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1069, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1070, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 1071, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 1072, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1073, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 1074, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 1075, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1076, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1077, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1078, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1079, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1080, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1081, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1082, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 1083, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1084, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1085, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1086, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1087, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1088, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1089, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1090, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1091, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1092, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1093, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 1094, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 1095, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1096, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1097, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1098, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 1099, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 1100, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1101, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1102, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1103, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 1104, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1105, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 1106, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 1107, train loss: 0.01515, val loss: 0.01559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1108, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1109, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 1110, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1111, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 1112, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1113, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 1114, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1115, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 1116, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1117, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1118, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1119, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1120, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 1121, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1122, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 1123, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1124, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 1125, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1126, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1127, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1128, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1129, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1130, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 1131, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 1132, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 1133, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1134, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1135, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1136, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1137, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1138, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1139, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1140, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1141, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 1142, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 1143, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 1144, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1145, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1146, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1147, train loss: 0.01515, val loss: 0.01562\n",
      "Training epoch: 1148, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1149, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 1150, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1151, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1152, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1153, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 1154, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 1155, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1156, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1157, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 1158, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1159, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1160, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1161, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1162, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 1163, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1164, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1165, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1166, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1167, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1168, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1169, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 1170, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1171, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 1172, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1173, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1174, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1175, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1176, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1177, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1178, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1179, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1180, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 1181, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1182, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 1183, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 1184, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1185, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1186, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1187, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 1188, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1189, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1190, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 1191, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 1192, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 1193, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 1194, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 1195, train loss: 0.01519, val loss: 0.01559\n",
      "Training epoch: 1196, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1197, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 1198, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 1199, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1200, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1201, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1202, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1203, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 1204, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1205, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1206, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 1207, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1208, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 1209, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1210, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1211, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1212, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1213, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1214, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1215, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1216, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 1217, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1218, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1219, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1220, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1221, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1222, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 1223, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 1224, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1225, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1226, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 1227, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 1228, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 1229, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1230, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1231, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1232, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 1233, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 1234, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 1235, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1236, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1237, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1238, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1239, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1240, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1241, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1242, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1243, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1244, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1245, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 1246, train loss: 0.01512, val loss: 0.01556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1247, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1248, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 1249, train loss: 0.01545, val loss: 0.01588\n",
      "Training epoch: 1250, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 1251, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 1252, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1253, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1254, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1255, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1256, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1257, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1258, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1259, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1260, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1261, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1262, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 1263, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1264, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1265, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 1266, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1267, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1268, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1269, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1270, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 1271, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 1272, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1273, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1274, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 1275, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 1276, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1277, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 1278, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1279, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 1280, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 1281, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1282, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1283, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1284, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1285, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1286, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1287, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1288, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1289, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1290, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1291, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1292, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1293, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 1294, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1295, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1296, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 1297, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 1298, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1299, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1300, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 1301, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1302, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1303, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 1304, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1305, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 1306, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1307, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1308, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1309, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1310, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1311, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1312, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1313, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1314, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1315, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 1316, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1317, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 1318, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1319, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 1320, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 1321, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1322, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1323, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 1324, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1325, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1326, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1327, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1328, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1329, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1330, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1331, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 1332, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1333, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1334, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1335, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1336, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1337, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 1338, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1339, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1340, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 1341, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1342, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1343, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1344, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1345, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 1346, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 1347, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 1348, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1349, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1350, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1351, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 1352, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1353, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1354, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1355, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1356, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 1357, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1358, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1359, train loss: 0.01518, val loss: 0.01558\n",
      "Training epoch: 1360, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1361, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1362, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1363, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1364, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1365, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1366, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1367, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1368, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1369, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1370, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1371, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1372, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1373, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1374, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1375, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1376, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1377, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1378, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 1379, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1380, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1381, train loss: 0.01512, val loss: 0.01555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1382, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1383, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 1384, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 1385, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 1386, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1387, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1388, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1389, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1390, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1391, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1392, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1393, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1394, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 1395, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1396, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1397, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1398, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1399, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1400, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1401, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1402, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 1403, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1404, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1405, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1406, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1407, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1408, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1409, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1410, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1411, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1412, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1413, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1414, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 1415, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 1416, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1417, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1418, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1419, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1420, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1421, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1422, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 1423, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1424, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 1425, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1426, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1427, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1428, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1429, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1430, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1431, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 1432, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 1433, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 1434, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 1435, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 1436, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 1437, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1438, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1439, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1440, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1441, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1442, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1443, train loss: 0.01520, val loss: 0.01561\n",
      "Training epoch: 1444, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1445, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1446, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1447, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1448, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1449, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1450, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1451, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1452, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1453, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1454, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1455, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1456, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1457, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1458, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1459, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1460, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1461, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1462, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 1463, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1464, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1465, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 1466, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1467, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1468, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1469, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1470, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1471, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1472, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1473, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1474, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1475, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 1476, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1477, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1478, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 1479, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1480, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1481, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1482, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1483, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1484, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1485, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1486, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1487, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1488, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1489, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1490, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1491, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1492, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1493, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1494, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1495, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1496, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1497, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1498, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1499, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1500, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1501, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1502, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1503, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 1504, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 1505, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1506, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1507, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1508, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1509, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 1510, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1511, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1512, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1513, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1514, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1515, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 1516, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1517, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1518, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1519, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1520, train loss: 0.01512, val loss: 0.01555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1521, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1522, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1523, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1524, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1525, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1526, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1527, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1528, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1529, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1530, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1531, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1532, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1533, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1534, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1535, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1536, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1537, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1538, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1539, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 1540, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1541, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1542, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1543, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1544, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 1545, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1546, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1547, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1548, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1549, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1550, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1551, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1552, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1553, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1554, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1555, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1556, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1557, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 1558, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1559, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1560, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1561, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1562, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1563, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1564, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1565, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 1566, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 1567, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 1568, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1569, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1570, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1571, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 1572, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1573, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1574, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1575, train loss: 0.01518, val loss: 0.01558\n",
      "Training epoch: 1576, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1577, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1578, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 1579, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1580, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1581, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1582, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1583, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1584, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1585, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 1586, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1587, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1588, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1589, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1590, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1591, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1592, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1593, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1594, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1595, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1596, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1597, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1598, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1599, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1600, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1601, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1602, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1603, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1604, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1605, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1606, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1607, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 1608, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 1609, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1610, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 1611, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1612, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1613, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1614, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1615, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1616, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1617, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1618, train loss: 0.01532, val loss: 0.01580\n",
      "Training epoch: 1619, train loss: 0.01541, val loss: 0.01589\n",
      "Training epoch: 1620, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 1621, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 1622, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 1623, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1624, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1625, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1626, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 1627, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1628, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1629, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1630, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1631, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1632, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1633, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1634, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1635, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1636, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1637, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1638, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 1639, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 1640, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1641, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 1642, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1643, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1644, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 1645, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1646, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1647, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1648, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1649, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1650, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1651, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1652, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1653, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1654, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1655, train loss: 0.01531, val loss: 0.01576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1656, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1657, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1658, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1659, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1660, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1661, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1662, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1663, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1664, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 1665, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1666, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 1667, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1668, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1669, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1670, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1671, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1672, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1673, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1674, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1675, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1676, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1677, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1678, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 1679, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1680, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1681, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 1682, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1683, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1684, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1685, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 1686, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1687, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1688, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 1689, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1690, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1691, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 1692, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1693, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1694, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1695, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1696, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1697, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1698, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1699, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1700, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1701, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1702, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1703, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 1704, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1705, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1706, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 1707, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 1708, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1709, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1710, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1711, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 1712, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1713, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 1714, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1715, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1716, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1717, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1718, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1719, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1720, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1721, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 1722, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1723, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 1724, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1725, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1726, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1727, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1728, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1729, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1730, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 1731, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1732, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1733, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1734, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1735, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1736, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1737, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1738, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 1739, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1740, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1741, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1742, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1743, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 1744, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1745, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1746, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1747, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 1748, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1749, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1750, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1751, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1752, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1753, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1754, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1755, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1756, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1757, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1758, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1759, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 1760, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 1761, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 1762, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1763, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1764, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1765, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1766, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1767, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1768, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1769, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1770, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1771, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1772, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1773, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1774, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1775, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 1776, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1777, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1778, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 1779, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1780, train loss: 0.01516, val loss: 0.01564\n",
      "Training epoch: 1781, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 1782, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1783, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1784, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1785, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1786, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1787, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1788, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1789, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1790, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1791, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 1792, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1793, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 1794, train loss: 0.01510, val loss: 0.01552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1795, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1796, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1797, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1798, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1799, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 1800, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1801, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1802, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1803, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1804, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1805, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1806, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1807, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1808, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 1809, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1810, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1811, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1812, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1813, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 1814, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1815, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1816, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1817, train loss: 0.01515, val loss: 0.01554\n",
      "Training epoch: 1818, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1819, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1820, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 1821, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 1822, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1823, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1824, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1825, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1826, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1827, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1828, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1829, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1830, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1831, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1832, train loss: 0.01513, val loss: 0.01553\n",
      "Training epoch: 1833, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1834, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1835, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 1836, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1837, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1838, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1839, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1840, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1841, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1842, train loss: 0.01517, val loss: 0.01557\n",
      "Training epoch: 1843, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1844, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1845, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1846, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1847, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 1848, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1849, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1850, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1851, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1852, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1853, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1854, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1855, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1856, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1857, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1858, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1859, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1860, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1861, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1862, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1863, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1864, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1865, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1866, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1867, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1868, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1869, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1870, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1871, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1872, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1873, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1874, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1875, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 1876, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1877, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1878, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1879, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1880, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1881, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1882, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1883, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1884, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1885, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 1886, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1887, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 1888, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 1889, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 1890, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 1891, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1892, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 1893, train loss: 0.01532, val loss: 0.01580\n",
      "Training epoch: 1894, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1895, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1896, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1897, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1898, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1899, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1900, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1901, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1902, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1903, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1904, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1905, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1906, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1907, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1908, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1909, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1910, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1911, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 1912, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 1913, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1914, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1915, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1916, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 1917, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1918, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1919, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 1920, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1921, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1922, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 1923, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1924, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1925, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1926, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1927, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1928, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1929, train loss: 0.01515, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1930, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1931, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1932, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1933, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1934, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1935, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1936, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 1937, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1938, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1939, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1940, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1941, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1942, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1943, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 1944, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1945, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 1946, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1947, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 1948, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1949, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1950, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1951, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1952, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 1953, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1954, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1955, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1956, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1957, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1958, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1959, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1960, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1961, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 1962, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1963, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1964, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 1965, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1966, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1967, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1968, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1969, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1970, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1971, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1972, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1973, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1974, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1975, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 1976, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1977, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1978, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1979, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1980, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1981, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1982, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1983, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1984, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1985, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1986, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1987, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1988, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1989, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1990, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1991, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1992, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1993, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1994, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 1995, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 1996, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1997, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1998, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1999, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2000, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2001, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2002, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2003, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2004, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2005, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2006, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2007, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2008, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2009, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2010, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2011, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2012, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 2013, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2014, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 2015, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2016, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2017, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2018, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2019, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2020, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2021, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2022, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2023, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 2024, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2025, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2026, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2027, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2028, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2029, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2030, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2031, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2032, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2033, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2034, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2035, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2036, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2037, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2038, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2039, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2040, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2041, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2042, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2043, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2044, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2045, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2046, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2047, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 2048, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2049, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2050, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2051, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2052, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2053, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2054, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 2055, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2056, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2057, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2058, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2059, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2060, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 2061, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2062, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2063, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 2064, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 2065, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2066, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2067, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2068, train loss: 0.01513, val loss: 0.01557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2069, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2070, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2071, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2072, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2073, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2074, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2075, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 2076, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2077, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2078, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2079, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2080, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2081, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2082, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2083, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2084, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2085, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 2086, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 2087, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2088, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2089, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2090, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2091, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2092, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2093, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 2094, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2095, train loss: 0.01517, val loss: 0.01565\n",
      "Training epoch: 2096, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2097, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 2098, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2099, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2100, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2101, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2102, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2103, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2104, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2105, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2106, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2107, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2108, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2109, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 2110, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 2111, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2112, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2113, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2114, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2115, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2116, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2117, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2118, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2119, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2120, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2121, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2122, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 2123, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2124, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2125, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2126, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2127, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2128, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2129, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2130, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2131, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2132, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2133, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2134, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2135, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2136, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2137, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2138, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 2139, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 2140, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 2141, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2142, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2143, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2144, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 2145, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2146, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2147, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2148, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2149, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2150, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 2151, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2152, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2153, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2154, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2155, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2156, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2157, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2158, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 2159, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 2160, train loss: 0.01517, val loss: 0.01564\n",
      "Training epoch: 2161, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2162, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2163, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2164, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 2165, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2166, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2167, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2168, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2169, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2170, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2171, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2172, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2173, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2174, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2175, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2176, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2177, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2178, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2179, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2180, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2181, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2182, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2183, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2184, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2185, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2186, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2187, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 2188, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2189, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 2190, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2191, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2192, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2193, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2194, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 2195, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 2196, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 2197, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2198, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2199, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2200, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2201, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2202, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2203, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2204, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2205, train loss: 0.01511, val loss: 0.01557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2206, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 2207, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2208, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2209, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2210, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 2211, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2212, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2213, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 2214, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2215, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2216, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2217, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2218, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2219, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 2220, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2221, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2222, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2223, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2224, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2225, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2226, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2227, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2228, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2229, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2230, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2231, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 2232, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2233, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2234, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2235, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 2236, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 2237, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2238, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2239, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2240, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2241, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2242, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2243, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2244, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2245, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2246, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2247, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 2248, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2249, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2250, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2251, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 2252, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2253, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2254, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 2255, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2256, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2257, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2258, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2259, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2260, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2261, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 2262, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 2263, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2264, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2265, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2266, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2267, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2268, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2269, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 2270, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 2271, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2272, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 2273, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2274, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2275, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2276, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2277, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2278, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2279, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2280, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2281, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2282, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2283, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2284, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2285, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2286, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2287, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2288, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2289, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2290, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2291, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2292, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2293, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2294, train loss: 0.01518, val loss: 0.01566\n",
      "Training epoch: 2295, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 2296, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2297, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2298, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2299, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2300, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2301, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2302, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2303, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2304, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 2305, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 2306, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2307, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2308, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2309, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2310, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 2311, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 2312, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2313, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2314, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2315, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2316, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 2317, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2318, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 2319, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2320, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2321, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2322, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2323, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2324, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2325, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2326, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2327, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2328, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 2329, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 2330, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 2331, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2332, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2333, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2334, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2335, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2336, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2337, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2338, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 2339, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2340, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 2341, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2342, train loss: 0.01518, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2343, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 2344, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2345, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2346, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2347, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2348, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2349, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2350, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2351, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2352, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2353, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2354, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2355, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2356, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2357, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2358, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 2359, train loss: 0.01527, val loss: 0.01574\n",
      "Training epoch: 2360, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2361, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2362, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2363, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2364, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2365, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2366, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2367, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2368, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2369, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 2370, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 2371, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2372, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2373, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2374, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 2375, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2376, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2377, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2378, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2379, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2380, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 2381, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2382, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2383, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2384, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 2385, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 2386, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2387, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2388, train loss: 0.01527, val loss: 0.01574\n",
      "Training epoch: 2389, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2390, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2391, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2392, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2393, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2394, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2395, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2396, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2397, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 2398, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 2399, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2400, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2401, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2402, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2403, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2404, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2405, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 2406, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2407, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2408, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2409, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2410, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2411, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2412, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2413, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2414, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 2415, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2416, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 2417, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2418, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 2419, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2420, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2421, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2422, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2423, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2424, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2425, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2426, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2427, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2428, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2429, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 2430, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2431, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2432, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 2433, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2434, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2435, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2436, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2437, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 2438, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2439, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2440, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2441, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2442, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 2443, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2444, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2445, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2446, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 2447, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2448, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2449, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2450, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2451, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2452, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2453, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2454, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2455, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2456, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 2457, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2458, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2459, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2460, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2461, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2462, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2463, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2464, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2465, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2466, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 2467, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 2468, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2469, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2470, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2471, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2472, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 2473, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2474, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 2475, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2476, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2477, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2478, train loss: 0.01511, val loss: 0.01551\n",
      "Training epoch: 2479, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2480, train loss: 0.01518, val loss: 0.01561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2481, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2482, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2483, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2484, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2485, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2486, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 2487, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2488, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2489, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2490, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2491, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2492, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2493, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2494, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2495, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2496, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2497, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2498, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2499, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2500, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2501, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2502, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2503, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2504, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2505, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 2506, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 2507, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 2508, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 2509, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2510, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2511, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2512, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2513, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2514, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2515, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2516, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2517, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2518, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2519, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2520, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2521, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2522, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2523, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2524, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2525, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 2526, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2527, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 2528, train loss: 0.01524, val loss: 0.01566\n",
      "Training epoch: 2529, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 2530, train loss: 0.01524, val loss: 0.01566\n",
      "Training epoch: 2531, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2532, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2533, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2534, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2535, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2536, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2537, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 2538, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2539, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2540, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2541, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2542, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 2543, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2544, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2545, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2546, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2547, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2548, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2549, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2550, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 2551, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2552, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2553, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2554, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2555, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2556, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2557, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2558, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2559, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2560, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2561, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2562, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 2563, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 2564, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2565, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2566, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2567, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2568, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2569, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2570, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2571, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2572, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2573, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2574, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2575, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2576, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2577, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 2578, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 2579, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 2580, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2581, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 2582, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2583, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 2584, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2585, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2586, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2587, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2588, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2589, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 2590, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2591, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2592, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2593, train loss: 0.01520, val loss: 0.01561\n",
      "Training epoch: 2594, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 2595, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2596, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2597, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2598, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2599, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2600, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2601, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2602, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2603, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 2604, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2605, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 2606, train loss: 0.01520, val loss: 0.01560\n",
      "Training epoch: 2607, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2608, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2609, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2610, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2611, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 2612, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 2613, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2614, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 2615, train loss: 0.01512, val loss: 0.01557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2616, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2617, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2618, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2619, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2620, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2621, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2622, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2623, train loss: 0.01515, val loss: 0.01562\n",
      "Training epoch: 2624, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2625, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2626, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 2627, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2628, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 2629, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2630, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2631, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2632, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2633, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2634, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2635, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2636, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 2637, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2638, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2639, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2640, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2641, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 2642, train loss: 0.01514, val loss: 0.01562\n",
      "Training epoch: 2643, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2644, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2645, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2646, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2647, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2648, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 2649, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2650, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2651, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2652, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2653, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2654, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2655, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2656, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2657, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2658, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2659, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2660, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2661, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2662, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2663, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2664, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2665, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2666, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2667, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2668, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2669, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2670, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2671, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2672, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2673, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2674, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2675, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2676, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2677, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2678, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2679, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2680, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2681, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 2682, train loss: 0.01519, val loss: 0.01558\n",
      "Training epoch: 2683, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2684, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 2685, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2686, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2687, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2688, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2689, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2690, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2691, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2692, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 2693, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2694, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2695, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2696, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2697, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2698, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2699, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2700, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2701, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2702, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2703, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2704, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2705, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2706, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2707, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 2708, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2709, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2710, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 2711, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2712, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2713, train loss: 0.01517, val loss: 0.01564\n",
      "Training epoch: 2714, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2715, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2716, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2717, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2718, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2719, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2720, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2721, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 2722, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 2723, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2724, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2725, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 2726, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2727, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2728, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2729, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2730, train loss: 0.01511, val loss: 0.01551\n",
      "Training epoch: 2731, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2732, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2733, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2734, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2735, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2736, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2737, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2738, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 2739, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 2740, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 2741, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2742, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2743, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 2744, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2745, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2746, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2747, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 2748, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2749, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 2750, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2751, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 2752, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2753, train loss: 0.01510, val loss: 0.01555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2754, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2755, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2756, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2757, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2758, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2759, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2760, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2761, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2762, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2763, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2764, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2765, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2766, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2767, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2768, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 2769, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2770, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2771, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2772, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 2773, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2774, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2775, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2776, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 2777, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2778, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2779, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2780, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2781, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2782, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2783, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2784, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2785, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2786, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 2787, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2788, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2789, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2790, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2791, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2792, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 2793, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2794, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2795, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2796, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2797, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2798, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2799, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2800, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2801, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2802, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2803, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2804, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2805, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2806, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2807, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 2808, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 2809, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2810, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2811, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2812, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2813, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2814, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2815, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2816, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 2817, train loss: 0.01517, val loss: 0.01565\n",
      "Training epoch: 2818, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2819, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2820, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2821, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2822, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2823, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2824, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2825, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2826, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2827, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2828, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2829, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2830, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2831, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2832, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2833, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2834, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2835, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2836, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2837, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 2838, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2839, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2840, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2841, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 2842, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2843, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2844, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 2845, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2846, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2847, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2848, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2849, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2850, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2851, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2852, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2853, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2854, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 2855, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2856, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2857, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2858, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2859, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2860, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2861, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 2862, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2863, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2864, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2865, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2866, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 2867, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 2868, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2869, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 2870, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2871, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2872, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2873, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2874, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 2875, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2876, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2877, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2878, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2879, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 2880, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 2881, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2882, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 2883, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2884, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2885, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 2886, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2887, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2888, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2889, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 2890, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 2891, train loss: 0.01508, val loss: 0.01553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2892, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2893, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2894, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2895, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2896, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2897, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2898, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2899, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2900, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 2901, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2902, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2903, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2904, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2905, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2906, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2907, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2908, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2909, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2910, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 2911, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2912, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2913, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2914, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2915, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2916, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 2917, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2918, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2919, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2920, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2921, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 2922, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2923, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2924, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2925, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2926, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2927, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2928, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2929, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2930, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2931, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 2932, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2933, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 2934, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2935, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2936, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2937, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 2938, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2939, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2940, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2941, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2942, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2943, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2944, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2945, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 2946, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2947, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2948, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 2949, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2950, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2951, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2952, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 2953, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2954, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2955, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2956, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 2957, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2958, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2959, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2960, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 2961, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2962, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2963, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2964, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 2965, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2966, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2967, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 2968, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 2969, train loss: 0.01523, val loss: 0.01572\n",
      "Training epoch: 2970, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2971, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 2972, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 2973, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2974, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2975, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2976, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2977, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 2978, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 2979, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2980, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2981, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 2982, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2983, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2984, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2985, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2986, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2987, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2988, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2989, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2990, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2991, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2992, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2993, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2994, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2995, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2996, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2997, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 2998, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2999, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 3000, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3001, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3002, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 3003, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3004, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3005, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3006, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3007, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3008, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3009, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3010, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 3011, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 3012, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3013, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 3014, train loss: 0.01534, val loss: 0.01582\n",
      "Training epoch: 3015, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 3016, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 3017, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 3018, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 3019, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3020, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3021, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 3022, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3023, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3024, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 3025, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 3026, train loss: 0.01508, val loss: 0.01553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3027, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3028, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3029, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 3030, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3031, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3032, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3033, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 3034, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3035, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3036, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3037, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3038, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 3039, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3040, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3041, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 3042, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3043, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3044, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 3045, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 3046, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3047, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 3048, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 3049, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3050, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 3051, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3052, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3053, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 3054, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3055, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 3056, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3057, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3058, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3059, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 3060, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 3061, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 3062, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3063, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3064, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3065, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3066, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3067, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 3068, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 3069, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3070, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3071, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3072, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 3073, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3074, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3075, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3076, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3077, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3078, train loss: 0.01516, val loss: 0.01556\n",
      "Training epoch: 3079, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3080, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3081, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 3082, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3083, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3084, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3085, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3086, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3087, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3088, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3089, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3090, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 3091, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 3092, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3093, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 3094, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3095, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3096, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3097, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3098, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3099, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3100, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3101, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3102, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3103, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3104, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 3105, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3106, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 3107, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3108, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 3109, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3110, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 3111, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 3112, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3113, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 3114, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3115, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 3116, train loss: 0.01534, val loss: 0.01581\n",
      "Training epoch: 3117, train loss: 0.01510, val loss: 0.01558\n",
      "Training epoch: 3118, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 3119, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 3120, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 3121, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3122, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 3123, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 3124, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3125, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 3126, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 3127, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 3128, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3129, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 3130, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 3131, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 3132, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 3133, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 3134, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3135, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3136, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 3137, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3138, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 3139, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3140, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3141, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 3142, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3143, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3144, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3145, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3146, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3147, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 3148, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3149, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 3150, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 3151, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 3152, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 3153, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3154, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 3155, train loss: 0.01524, val loss: 0.01566\n",
      "Training epoch: 3156, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 3157, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3158, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3159, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3160, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3161, train loss: 0.01508, val loss: 0.01552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3162, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3163, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 3164, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 3165, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3166, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3167, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3168, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 3169, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 3170, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3171, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3172, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 3173, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 3174, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3175, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3176, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3177, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 3178, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3179, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 3180, train loss: 0.01510, val loss: 0.01559\n",
      "Training epoch: 3181, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3182, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 3183, train loss: 0.01515, val loss: 0.01562\n",
      "Training epoch: 3184, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3185, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 3186, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3187, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 3188, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3189, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3190, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3191, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 3192, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 3193, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3194, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 3195, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 3196, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3197, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3198, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 3199, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3200, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3201, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3202, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 3203, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3204, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3205, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 3206, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 3207, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3208, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3209, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 3210, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3211, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 3212, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3213, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3214, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3215, train loss: 0.01520, val loss: 0.01560\n",
      "Training epoch: 3216, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 3217, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 3218, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 3219, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3220, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 3221, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 3222, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 3223, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3224, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3225, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 3226, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3227, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3228, train loss: 0.01518, val loss: 0.01566\n",
      "Training epoch: 3229, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 3230, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3231, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3232, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3233, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 3234, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3235, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 3236, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3237, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 3238, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 3239, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3240, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 3241, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 3242, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 3243, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3244, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3245, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3246, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3247, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 3248, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3249, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3250, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 3251, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3252, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3253, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 3254, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3255, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3256, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3257, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3258, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3259, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 3260, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 3261, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3262, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 3263, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3264, train loss: 0.01514, val loss: 0.01562\n",
      "Training epoch: 3265, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3266, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3267, train loss: 0.01509, val loss: 0.01558\n",
      "Training epoch: 3268, train loss: 0.01516, val loss: 0.01555\n",
      "Training epoch: 3269, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 3270, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 3271, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3272, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3273, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3274, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3275, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3276, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3277, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 3278, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3279, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 3280, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3281, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3282, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 3283, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3284, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3285, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3286, train loss: 0.01515, val loss: 0.01562\n",
      "Training epoch: 3287, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3288, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3289, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3290, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3291, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 3292, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 3293, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3294, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3295, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3296, train loss: 0.01515, val loss: 0.01563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3297, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3298, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3299, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 3300, train loss: 0.01512, val loss: 0.01552\n",
      "Training epoch: 3301, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 3302, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3303, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 3304, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3305, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3306, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3307, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3308, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3309, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3310, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3311, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3312, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 3313, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 3314, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 3315, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 3316, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 3317, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3318, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3319, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3320, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 3321, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3322, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 3323, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3324, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3325, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3326, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3327, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 3328, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3329, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3330, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 3331, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3332, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 3333, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3334, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 3335, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 3336, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 3337, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 3338, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3339, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3340, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3341, train loss: 0.01510, val loss: 0.01558\n",
      "Training epoch: 3342, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 3343, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3344, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3345, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3346, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3347, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 3348, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3349, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 3350, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 3351, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3352, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 3353, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3354, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 3355, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3356, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3357, train loss: 0.01527, val loss: 0.01575\n",
      "Training epoch: 3358, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 3359, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3360, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 3361, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3362, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3363, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 3364, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3365, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3366, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 3367, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 3368, train loss: 0.01513, val loss: 0.01561\n",
      "Early stop at epoch 3368, With Testing Error: 0.01561\n",
      "Subnetwork pruning.\n",
      "Fine tuning.\n",
      "Tuning epoch: 1, train loss: 0.01542, val loss: 0.01588\n",
      "Tuning epoch: 2, train loss: 0.01592, val loss: 0.01629\n",
      "Tuning epoch: 3, train loss: 0.01508, val loss: 0.01554\n",
      "Tuning epoch: 4, train loss: 0.01517, val loss: 0.01563\n",
      "Tuning epoch: 5, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 6, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 7, train loss: 0.01510, val loss: 0.01555\n",
      "Tuning epoch: 8, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 9, train loss: 0.01510, val loss: 0.01552\n",
      "Tuning epoch: 10, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 11, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 12, train loss: 0.01507, val loss: 0.01550\n",
      "Tuning epoch: 13, train loss: 0.01510, val loss: 0.01554\n",
      "Tuning epoch: 14, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 15, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 16, train loss: 0.01507, val loss: 0.01550\n",
      "Tuning epoch: 17, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 18, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 19, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 20, train loss: 0.01510, val loss: 0.01553\n",
      "Tuning epoch: 21, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 22, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 23, train loss: 0.01516, val loss: 0.01562\n",
      "Tuning epoch: 24, train loss: 0.01509, val loss: 0.01552\n",
      "Tuning epoch: 25, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 26, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 27, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 28, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 29, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 30, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 31, train loss: 0.01515, val loss: 0.01562\n",
      "Tuning epoch: 32, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 33, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 34, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 35, train loss: 0.01509, val loss: 0.01554\n",
      "Tuning epoch: 36, train loss: 0.01509, val loss: 0.01552\n",
      "Tuning epoch: 37, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 38, train loss: 0.01509, val loss: 0.01555\n",
      "Tuning epoch: 39, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 40, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 41, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 42, train loss: 0.01508, val loss: 0.01554\n",
      "Tuning epoch: 43, train loss: 0.01509, val loss: 0.01554\n",
      "Tuning epoch: 44, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 45, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 46, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 47, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 48, train loss: 0.01507, val loss: 0.01553\n",
      "Tuning epoch: 49, train loss: 0.01514, val loss: 0.01558\n",
      "Tuning epoch: 50, train loss: 0.01510, val loss: 0.01554\n",
      "Tuning epoch: 51, train loss: 0.01507, val loss: 0.01553\n",
      "Tuning epoch: 52, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 53, train loss: 0.01511, val loss: 0.01556\n",
      "Tuning epoch: 54, train loss: 0.01514, val loss: 0.01559\n",
      "Tuning epoch: 55, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 56, train loss: 0.01507, val loss: 0.01553\n",
      "Tuning epoch: 57, train loss: 0.01511, val loss: 0.01554\n",
      "Tuning epoch: 58, train loss: 0.01507, val loss: 0.01553\n",
      "Tuning epoch: 59, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 60, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 61, train loss: 0.01515, val loss: 0.01561\n",
      "Tuning epoch: 62, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 63, train loss: 0.01510, val loss: 0.01553\n",
      "Tuning epoch: 64, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 65, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 66, train loss: 0.01510, val loss: 0.01553\n",
      "Tuning epoch: 67, train loss: 0.01508, val loss: 0.01554\n",
      "Tuning epoch: 68, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 69, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 70, train loss: 0.01507, val loss: 0.01552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning epoch: 71, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 72, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 73, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 74, train loss: 0.01508, val loss: 0.01554\n",
      "Tuning epoch: 75, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 76, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 77, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 78, train loss: 0.01509, val loss: 0.01552\n",
      "Tuning epoch: 79, train loss: 0.01508, val loss: 0.01554\n",
      "Tuning epoch: 80, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 81, train loss: 0.01509, val loss: 0.01552\n",
      "Tuning epoch: 82, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 83, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 84, train loss: 0.01509, val loss: 0.01554\n",
      "Tuning epoch: 85, train loss: 0.01532, val loss: 0.01579\n",
      "Tuning epoch: 86, train loss: 0.01509, val loss: 0.01555\n",
      "Tuning epoch: 87, train loss: 0.01529, val loss: 0.01570\n",
      "Tuning epoch: 88, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 89, train loss: 0.01510, val loss: 0.01556\n",
      "Tuning epoch: 90, train loss: 0.01511, val loss: 0.01557\n",
      "Tuning epoch: 91, train loss: 0.01509, val loss: 0.01555\n",
      "Tuning epoch: 92, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 93, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 94, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 95, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 96, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 97, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 98, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 99, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 100, train loss: 0.01508, val loss: 0.01551\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1584 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = xNN(meta_info=meta_info,\n",
    "               subnet_num=10,\n",
    "               subnet_arch=[10, 6],\n",
    "               task_type=task_type,\n",
    "               activation_func=tf.tanh,\n",
    "               batch_size=min(1000, int(train_x.shape[0] * 0.2)),\n",
    "               training_epochs=10000,\n",
    "               lr_bp=0.001,\n",
    "               beta_threshold=0.05,\n",
    "               tuning_epochs=100,\n",
    "               l1_proj=best_l1_prob,\n",
    "               l1_subnet=best_l1_subnet,\n",
    "               verbose=True,\n",
    "               val_ratio=0.2,\n",
    "               early_stop_thres=500)\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"xnn_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99941 1.02802 0.99131]\n"
     ]
    }
   ],
   "source": [
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "\n",
    "mse_stat = np.hstack([np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(tr_pred) - meta_info[\"Y\"][\"scaler\"].inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(val_pred) - meta_info[\"Y\"][\"scaler\"].inverse_transform(model.val_y))**2),5),\\\n",
    "               np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(pred_test) - meta_info[\"Y\"][\"scaler\"].inverse_transform(test_y))**2),5)])\n",
    "print(mse_stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
