{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "from exnn import ExNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simu_loader(generator, datanum, testnum, noise_sigma):\n",
    "    def wrapper(rand_seed=0):\n",
    "        return generator(datanum, testnum=testnum, noise_sigma=noise_sigma, rand_seed=rand_seed)\n",
    "    return wrapper\n",
    "\n",
    "def mse(label, pred, scaler):\n",
    "    pred = scaler.inverse_transform(pred.reshape([-1, 1]))\n",
    "    label = scaler.inverse_transform(label.reshape([-1, 1]))\n",
    "    return np.mean((pred - label)**2)\n",
    "\n",
    "\n",
    "def metric_wrapper(metric, scaler):\n",
    "    def wrapper(label, pred):\n",
    "        return metric(label, pred, scaler=scaler)\n",
    "    return wrapper\n",
    "\n",
    "def data_generator1(datanum, testnum=10000, noise_sigma=1, rand_seed=0):\n",
    "    \n",
    "    corr = 0.5\n",
    "    np.random.seed(rand_seed)\n",
    "    proj_matrix = np.zeros((10, 4))\n",
    "    proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "    proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "    proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "    proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "    u = np.random.uniform(-1, 1, [datanum + testnum, 1])\n",
    "    t = np.sqrt(corr / (1 - corr))\n",
    "    x = np.zeros((datanum + testnum, 10))\n",
    "    for i in range(10):\n",
    "        x[:, i:i + 1] = (np.random.uniform(-1, 1, [datanum + testnum, 1]) + t * u) / (1 + t)\n",
    "\n",
    "    y = np.reshape(2 * np.dot(x, proj_matrix[:, 0]) + 0.2 * np.exp(-4 * np.dot(x, proj_matrix[:, 1])) + \\\n",
    "                   3 * (np.dot(x, proj_matrix[:, 2]))**2 + 2.5 * np.sin(np.pi * np.dot(x, proj_matrix[:, 3])), [-1, 1]) + \\\n",
    "              noise_sigma * np.random.normal(0, 1, [datanum + testnum, 1])\n",
    "    \n",
    "    task_type = \"Regression\"\n",
    "    meta_info = {\"X1\":{\"type\":\"continuous\"},\n",
    "             \"X2\":{\"type\":\"continuous\"},\n",
    "             \"X3\":{\"type\":\"continuous\"},\n",
    "             \"X4\":{\"type\":\"continuous\"},\n",
    "             \"X5\":{\"type\":\"continuous\"},\n",
    "             \"X6\":{\"type\":\"continuous\"},\n",
    "             \"X7\":{\"type\":\"continuous\"},\n",
    "             \"X8\":{\"type\":\"continuous\"},\n",
    "             \"X9\":{\"type\":\"continuous\"},\n",
    "             \"X10\":{\"type\":\"continuous\"},\n",
    "             \"Y\":{\"type\":\"target\"}}\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == \"target\":\n",
    "            sy = MinMaxScaler((-1, 1))\n",
    "            y = sy.fit_transform(y)\n",
    "            meta_info[key][\"scaler\"] = sy\n",
    "        elif item['type'] == \"categorical\":\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(x[:,[i]])\n",
    "            ordinal_feature = enc.transform(x[:,[i]])\n",
    "            x[:,[i]] = ordinal_feature\n",
    "            meta_info[key][\"values\"] = enc.categories_[0].tolist()\n",
    "        else:\n",
    "            sx = MinMaxScaler((-1, 1))\n",
    "            x[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "            meta_info[key][\"scaler\"] = sx\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=testnum, random_state=rand_seed)\n",
    "    return train_x, test_x, train_y, test_y, task_type, meta_info, metric_wrapper(mse, sy)\n",
    "\n",
    "train_x, test_x, train_y, test_y, task_type, meta_info, get_metric = data_generator1(datanum=10000, testnum=10000,\n",
    "                                                                                     noise_sigma=1, rand_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exnn_repeat(folder, name, data_generator,\n",
    "                  subnet_num=10,\n",
    "                  subnet_arch=[10, 6],\n",
    "                  task=\"Regression\",\n",
    "                  activation_func=tf.tanh,\n",
    "                  lr_bp=0.001,\n",
    "                  lr_cl=0.1,\n",
    "                  l1_proj=0.001,\n",
    "                  l1_subnet=0.001,\n",
    "                  smooth_lambda=0.00001,\n",
    "                  batch_size=1000,\n",
    "                  training_epochs=5000,\n",
    "                  tuning_epochs=500,\n",
    "                  beta_threshold=0.05,\n",
    "                  verbose=False,\n",
    "                  val_ratio=0.2,\n",
    "                  early_stop_thres=1000,\n",
    "                  rand_seed=0):\n",
    "\n",
    "    train_x, test_x, train_y, test_y, task_type, meta_info, get_metric = data_generator(rand_seed)\n",
    "\n",
    "    input_num = train_x.shape[1]\n",
    "    model = ExNN(meta_info=meta_info,\n",
    "                   subnet_num=min(10, input_num),\n",
    "                   subnet_arch=subnet_arch,\n",
    "                   task_type=task_type,\n",
    "                   activation_func=tf.tanh,\n",
    "                   batch_size=min(batch_size, int(train_x.shape[0] * 0.2)),\n",
    "                   training_epochs=training_epochs,\n",
    "                   lr_bp=lr_bp,\n",
    "                   lr_cl=lr_cl,\n",
    "                   beta_threshold=beta_threshold,\n",
    "                   tuning_epochs=tuning_epochs,\n",
    "                   l1_proj=l1_proj,\n",
    "                   l1_subnet=l1_subnet,\n",
    "                   smooth_lambda=smooth_lambda,\n",
    "                   verbose=verbose,\n",
    "                   val_ratio=val_ratio,\n",
    "                   early_stop_thres=early_stop_thres)\n",
    "    model.fit(train_x, train_y)\n",
    "    model.visualize(folder=folder,\n",
    "                    name=name,\n",
    "                    save_png=True,\n",
    "                    save_eps=True)\n",
    "\n",
    "    tr_x = train_x[model.tr_idx]\n",
    "    tr_y = train_y[model.tr_idx]\n",
    "    val_x = train_x[model.val_idx]\n",
    "    val_y = train_y[model.val_idx]\n",
    "\n",
    "    pred_train = model.predict(tr_x)\n",
    "    pred_val = model.predict(val_x)\n",
    "    pred_test = model.predict(test_x)\n",
    "\n",
    "    stat = np.hstack([np.round(get_metric(tr_y, pred_train), 5),\\\n",
    "                np.round(get_metric(val_y, pred_val), 5),\\\n",
    "                np.round(get_metric(test_y, pred_test), 5)])\n",
    "    res_stat = pd.DataFrame(stat.reshape([1, -1]), columns=['train_metric', 'val_metric', \"test_metric\"])\n",
    "    res_stat[\"Subnet_Number\"] = min(input_num, 10)\n",
    "    res_stat[\"lr_BP\"] = lr_bp\n",
    "    res_stat[\"lr_CL\"] = lr_cl\n",
    "    res_stat[\"L1_Penalty_Proj\"] = l1_proj\n",
    "    res_stat[\"L1_Penalty_Subnet\"] = l1_subnet\n",
    "    res_stat[\"Smooth_labmda\"] = smooth_lambda\n",
    "    res_stat[\"Training_Epochs\"] = training_epochs\n",
    "    return res_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_metric</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>Subnet_Number</th>\n",
       "      <th>lr_BP</th>\n",
       "      <th>lr_CL</th>\n",
       "      <th>L1_Penalty_Proj</th>\n",
       "      <th>L1_Penalty_Subnet</th>\n",
       "      <th>Smooth_labmda</th>\n",
       "      <th>Training_Epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00967</td>\n",
       "      <td>1.03250</td>\n",
       "      <td>0.99072</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00974</td>\n",
       "      <td>1.03406</td>\n",
       "      <td>0.99062</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.01211</td>\n",
       "      <td>1.03646</td>\n",
       "      <td>0.99345</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.01224</td>\n",
       "      <td>1.04256</td>\n",
       "      <td>1.00040</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.01944</td>\n",
       "      <td>1.04368</td>\n",
       "      <td>1.00114</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.03299</td>\n",
       "      <td>1.06288</td>\n",
       "      <td>1.01930</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.03497</td>\n",
       "      <td>1.06415</td>\n",
       "      <td>1.01893</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.02672</td>\n",
       "      <td>1.06472</td>\n",
       "      <td>1.01462</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.03830</td>\n",
       "      <td>1.07028</td>\n",
       "      <td>1.02324</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.05309</td>\n",
       "      <td>1.07877</td>\n",
       "      <td>1.02796</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.04798</td>\n",
       "      <td>1.08099</td>\n",
       "      <td>1.03936</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.04855</td>\n",
       "      <td>1.08210</td>\n",
       "      <td>1.03294</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.05840</td>\n",
       "      <td>1.08329</td>\n",
       "      <td>1.03335</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.06883</td>\n",
       "      <td>1.10155</td>\n",
       "      <td>1.04572</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.11808</td>\n",
       "      <td>1.14639</td>\n",
       "      <td>1.10058</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13623</td>\n",
       "      <td>1.17733</td>\n",
       "      <td>1.11430</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13927</td>\n",
       "      <td>1.18258</td>\n",
       "      <td>1.11860</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.14840</td>\n",
       "      <td>1.18510</td>\n",
       "      <td>1.12545</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.15495</td>\n",
       "      <td>1.19012</td>\n",
       "      <td>1.13702</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.15690</td>\n",
       "      <td>1.19142</td>\n",
       "      <td>1.12922</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.16452</td>\n",
       "      <td>1.19852</td>\n",
       "      <td>1.14755</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.15942</td>\n",
       "      <td>1.19863</td>\n",
       "      <td>1.13210</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.16154</td>\n",
       "      <td>1.20617</td>\n",
       "      <td>1.13507</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.17340</td>\n",
       "      <td>1.22100</td>\n",
       "      <td>1.14451</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.17855</td>\n",
       "      <td>1.22708</td>\n",
       "      <td>1.14906</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_metric  val_metric  test_metric  Subnet_Number  lr_BP  lr_CL  \\\n",
       "0       1.00967     1.03250      0.99072             10  0.001    0.1   \n",
       "0       1.00974     1.03406      0.99062             10  0.001    0.1   \n",
       "0       1.01211     1.03646      0.99345             10  0.001    0.1   \n",
       "0       1.01224     1.04256      1.00040             10  0.001    0.1   \n",
       "0       1.01944     1.04368      1.00114             10  0.001    0.1   \n",
       "0       1.03299     1.06288      1.01930             10  0.001    0.1   \n",
       "0       1.03497     1.06415      1.01893             10  0.001    0.1   \n",
       "0       1.02672     1.06472      1.01462             10  0.001    0.1   \n",
       "0       1.03830     1.07028      1.02324             10  0.001    0.1   \n",
       "0       1.05309     1.07877      1.02796             10  0.001    0.1   \n",
       "0       1.04798     1.08099      1.03936             10  0.001    0.1   \n",
       "0       1.04855     1.08210      1.03294             10  0.001    0.1   \n",
       "0       1.05840     1.08329      1.03335             10  0.001    0.1   \n",
       "0       1.06883     1.10155      1.04572             10  0.001    0.1   \n",
       "0       1.11808     1.14639      1.10058             10  0.001    0.1   \n",
       "0       1.13623     1.17733      1.11430             10  0.001    0.1   \n",
       "0       1.13927     1.18258      1.11860             10  0.001    0.1   \n",
       "0       1.14840     1.18510      1.12545             10  0.001    0.1   \n",
       "0       1.15495     1.19012      1.13702             10  0.001    0.1   \n",
       "0       1.15690     1.19142      1.12922             10  0.001    0.1   \n",
       "0       1.16452     1.19852      1.14755             10  0.001    0.1   \n",
       "0       1.15942     1.19863      1.13210             10  0.001    0.1   \n",
       "0       1.16154     1.20617      1.13507             10  0.001    0.1   \n",
       "0       1.17340     1.22100      1.14451             10  0.001    0.1   \n",
       "0       1.17855     1.22708      1.14906             10  0.001    0.1   \n",
       "\n",
       "   L1_Penalty_Proj  L1_Penalty_Subnet  Smooth_labmda  Training_Epochs  \n",
       "0         0.000316           0.010000       0.000001            10000  \n",
       "0         0.000100           0.010000       0.000001            10000  \n",
       "0         0.000316           0.003162       0.000001            10000  \n",
       "0         0.000100           0.000100       0.000001            10000  \n",
       "0         0.000100           0.003162       0.000001            10000  \n",
       "0         0.000316           0.001000       0.000001            10000  \n",
       "0         0.000316           0.000316       0.000001            10000  \n",
       "0         0.000100           0.001000       0.000001            10000  \n",
       "0         0.000316           0.000100       0.000001            10000  \n",
       "0         0.001000           0.010000       0.000001            10000  \n",
       "0         0.000100           0.000316       0.000001            10000  \n",
       "0         0.001000           0.000100       0.000001            10000  \n",
       "0         0.001000           0.003162       0.000001            10000  \n",
       "0         0.001000           0.000316       0.000001            10000  \n",
       "0         0.001000           0.001000       0.000001            10000  \n",
       "0         0.003162           0.010000       0.000001            10000  \n",
       "0         0.010000           0.000100       0.000001            10000  \n",
       "0         0.010000           0.003162       0.000001            10000  \n",
       "0         0.003162           0.000100       0.000001            10000  \n",
       "0         0.010000           0.010000       0.000001            10000  \n",
       "0         0.003162           0.001000       0.000001            10000  \n",
       "0         0.010000           0.001000       0.000001            10000  \n",
       "0         0.010000           0.000316       0.000001            10000  \n",
       "0         0.003162           0.000316       0.000001            10000  \n",
       "0         0.003162           0.003162       0.000001            10000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = Parallel(n_jobs=10)(delayed(exnn_repeat)(folder=\"./results/S1_exnn/\",\n",
    "                      name=str(i + 1).zfill(2) + \"_\" + str(j + 1).zfill(2),\n",
    "                      data_generator=simu_loader(data_generator1, 10000, 10000, 1),\n",
    "                      task=task_type,\n",
    "                      subnet_arch=[10, 6],\n",
    "                      beta_threshold=0.05,\n",
    "                      l1_proj=10**(-2 - i*0.5),\n",
    "                      l1_subnet=10**(-2 - j*0.5),\n",
    "                      smooth_lambda=10**(-5 - k),\n",
    "                      training_epochs=10000,\n",
    "                      lr_bp=0.001,\n",
    "                      lr_cl=0.1,\n",
    "                      batch_size=1000,\n",
    "                      early_stop_thres=500,\n",
    "                      tuning_epochs=100,\n",
    "                      rand_seed=0) for i in range(5) for j in range(5) for k in [1])\n",
    "exnn_stat_all = pd.concat(cv_results)\n",
    "exnn_stat_all.sort_values(\"val_metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_l1_prob = exnn_stat_all.sort_values(\"val_metric\").loc[:,\"L1_Penalty_Proj\"].iloc[0]\n",
    "best_l1_subnet = exnn_stat_all.sort_values(\"val_metric\").loc[:,\"L1_Penalty_Subnet\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0304 13:56:58.881133 140488656930624 deprecation.py:323] From /home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/exnn/exnn.py:117: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1, train loss: 0.15244, val loss: 0.15320\n",
      "Training epoch: 2, train loss: 0.13356, val loss: 0.13547\n",
      "Training epoch: 3, train loss: 0.11571, val loss: 0.11754\n",
      "Training epoch: 4, train loss: 0.09905, val loss: 0.10039\n",
      "Training epoch: 5, train loss: 0.08533, val loss: 0.08632\n",
      "Training epoch: 6, train loss: 0.07799, val loss: 0.07813\n",
      "Training epoch: 7, train loss: 0.06854, val loss: 0.06850\n",
      "Training epoch: 8, train loss: 0.06534, val loss: 0.06501\n",
      "Training epoch: 9, train loss: 0.05739, val loss: 0.05764\n",
      "Training epoch: 10, train loss: 0.05733, val loss: 0.05760\n",
      "Training epoch: 11, train loss: 0.04922, val loss: 0.04984\n",
      "Training epoch: 12, train loss: 0.04866, val loss: 0.04918\n",
      "Training epoch: 13, train loss: 0.04823, val loss: 0.04861\n",
      "Training epoch: 14, train loss: 0.04421, val loss: 0.04450\n",
      "Training epoch: 15, train loss: 0.04235, val loss: 0.04262\n",
      "Training epoch: 16, train loss: 0.03889, val loss: 0.03935\n",
      "Training epoch: 17, train loss: 0.03962, val loss: 0.03985\n",
      "Training epoch: 18, train loss: 0.03730, val loss: 0.03750\n",
      "Training epoch: 19, train loss: 0.03683, val loss: 0.03710\n",
      "Training epoch: 20, train loss: 0.03485, val loss: 0.03516\n",
      "Training epoch: 21, train loss: 0.03462, val loss: 0.03501\n",
      "Training epoch: 22, train loss: 0.03284, val loss: 0.03309\n",
      "Training epoch: 23, train loss: 0.03132, val loss: 0.03157\n",
      "Training epoch: 24, train loss: 0.03196, val loss: 0.03216\n",
      "Training epoch: 25, train loss: 0.03026, val loss: 0.03033\n",
      "Training epoch: 26, train loss: 0.03023, val loss: 0.03037\n",
      "Training epoch: 27, train loss: 0.02931, val loss: 0.02945\n",
      "Training epoch: 28, train loss: 0.02923, val loss: 0.02940\n",
      "Training epoch: 29, train loss: 0.02851, val loss: 0.02873\n",
      "Training epoch: 30, train loss: 0.02725, val loss: 0.02740\n",
      "Training epoch: 31, train loss: 0.02749, val loss: 0.02778\n",
      "Training epoch: 32, train loss: 0.02610, val loss: 0.02637\n",
      "Training epoch: 33, train loss: 0.02700, val loss: 0.02716\n",
      "Training epoch: 34, train loss: 0.02575, val loss: 0.02605\n",
      "Training epoch: 35, train loss: 0.02549, val loss: 0.02570\n",
      "Training epoch: 36, train loss: 0.02503, val loss: 0.02540\n",
      "Training epoch: 37, train loss: 0.02480, val loss: 0.02501\n",
      "Training epoch: 38, train loss: 0.02421, val loss: 0.02459\n",
      "Training epoch: 39, train loss: 0.02477, val loss: 0.02512\n",
      "Training epoch: 40, train loss: 0.02419, val loss: 0.02455\n",
      "Training epoch: 41, train loss: 0.02416, val loss: 0.02456\n",
      "Training epoch: 42, train loss: 0.02365, val loss: 0.02400\n",
      "Training epoch: 43, train loss: 0.02539, val loss: 0.02584\n",
      "Training epoch: 44, train loss: 0.02328, val loss: 0.02368\n",
      "Training epoch: 45, train loss: 0.02332, val loss: 0.02367\n",
      "Training epoch: 46, train loss: 0.02307, val loss: 0.02346\n",
      "Training epoch: 47, train loss: 0.02257, val loss: 0.02305\n",
      "Training epoch: 48, train loss: 0.02279, val loss: 0.02318\n",
      "Training epoch: 49, train loss: 0.02313, val loss: 0.02362\n",
      "Training epoch: 50, train loss: 0.02264, val loss: 0.02308\n",
      "Training epoch: 51, train loss: 0.02256, val loss: 0.02299\n",
      "Training epoch: 52, train loss: 0.02263, val loss: 0.02314\n",
      "Training epoch: 53, train loss: 0.02297, val loss: 0.02342\n",
      "Training epoch: 54, train loss: 0.02240, val loss: 0.02285\n",
      "Training epoch: 55, train loss: 0.02202, val loss: 0.02254\n",
      "Training epoch: 56, train loss: 0.02211, val loss: 0.02248\n",
      "Training epoch: 57, train loss: 0.02218, val loss: 0.02263\n",
      "Training epoch: 58, train loss: 0.02215, val loss: 0.02255\n",
      "Training epoch: 59, train loss: 0.02206, val loss: 0.02257\n",
      "Training epoch: 60, train loss: 0.02199, val loss: 0.02238\n",
      "Training epoch: 61, train loss: 0.02174, val loss: 0.02229\n",
      "Training epoch: 62, train loss: 0.02165, val loss: 0.02214\n",
      "Training epoch: 63, train loss: 0.02174, val loss: 0.02223\n",
      "Training epoch: 64, train loss: 0.02154, val loss: 0.02205\n",
      "Training epoch: 65, train loss: 0.02154, val loss: 0.02204\n",
      "Training epoch: 66, train loss: 0.02154, val loss: 0.02206\n",
      "Training epoch: 67, train loss: 0.02145, val loss: 0.02196\n",
      "Training epoch: 68, train loss: 0.02164, val loss: 0.02224\n",
      "Training epoch: 69, train loss: 0.02160, val loss: 0.02219\n",
      "Training epoch: 70, train loss: 0.02217, val loss: 0.02272\n",
      "Training epoch: 71, train loss: 0.02146, val loss: 0.02207\n",
      "Training epoch: 72, train loss: 0.02145, val loss: 0.02186\n",
      "Training epoch: 73, train loss: 0.02202, val loss: 0.02250\n",
      "Training epoch: 74, train loss: 0.02122, val loss: 0.02176\n",
      "Training epoch: 75, train loss: 0.02120, val loss: 0.02172\n",
      "Training epoch: 76, train loss: 0.02206, val loss: 0.02255\n",
      "Training epoch: 77, train loss: 0.02127, val loss: 0.02182\n",
      "Training epoch: 78, train loss: 0.02126, val loss: 0.02184\n",
      "Training epoch: 79, train loss: 0.02108, val loss: 0.02154\n",
      "Training epoch: 80, train loss: 0.02120, val loss: 0.02177\n",
      "Training epoch: 81, train loss: 0.02114, val loss: 0.02174\n",
      "Training epoch: 82, train loss: 0.02117, val loss: 0.02176\n",
      "Training epoch: 83, train loss: 0.02114, val loss: 0.02173\n",
      "Training epoch: 84, train loss: 0.02170, val loss: 0.02221\n",
      "Training epoch: 85, train loss: 0.02092, val loss: 0.02144\n",
      "Training epoch: 86, train loss: 0.02103, val loss: 0.02163\n",
      "Training epoch: 87, train loss: 0.02100, val loss: 0.02156\n",
      "Training epoch: 88, train loss: 0.02145, val loss: 0.02210\n",
      "Training epoch: 89, train loss: 0.02102, val loss: 0.02158\n",
      "Training epoch: 90, train loss: 0.02089, val loss: 0.02145\n",
      "Training epoch: 91, train loss: 0.02094, val loss: 0.02151\n",
      "Training epoch: 92, train loss: 0.02081, val loss: 0.02138\n",
      "Training epoch: 93, train loss: 0.02072, val loss: 0.02123\n",
      "Training epoch: 94, train loss: 0.02090, val loss: 0.02145\n",
      "Training epoch: 95, train loss: 0.02092, val loss: 0.02141\n",
      "Training epoch: 96, train loss: 0.02140, val loss: 0.02189\n",
      "Training epoch: 97, train loss: 0.02057, val loss: 0.02109\n",
      "Training epoch: 98, train loss: 0.02108, val loss: 0.02161\n",
      "Training epoch: 99, train loss: 0.02059, val loss: 0.02115\n",
      "Training epoch: 100, train loss: 0.02070, val loss: 0.02127\n",
      "Training epoch: 101, train loss: 0.02067, val loss: 0.02126\n",
      "Training epoch: 102, train loss: 0.02086, val loss: 0.02148\n",
      "Training epoch: 103, train loss: 0.02145, val loss: 0.02191\n",
      "Training epoch: 104, train loss: 0.02080, val loss: 0.02136\n",
      "Training epoch: 105, train loss: 0.02062, val loss: 0.02125\n",
      "Training epoch: 106, train loss: 0.02092, val loss: 0.02142\n",
      "Training epoch: 107, train loss: 0.02102, val loss: 0.02162\n",
      "Training epoch: 108, train loss: 0.02037, val loss: 0.02092\n",
      "Training epoch: 109, train loss: 0.02064, val loss: 0.02120\n",
      "Training epoch: 110, train loss: 0.02053, val loss: 0.02113\n",
      "Training epoch: 111, train loss: 0.02052, val loss: 0.02114\n",
      "Training epoch: 112, train loss: 0.02042, val loss: 0.02089\n",
      "Training epoch: 113, train loss: 0.02059, val loss: 0.02110\n",
      "Training epoch: 114, train loss: 0.02029, val loss: 0.02081\n",
      "Training epoch: 115, train loss: 0.02048, val loss: 0.02110\n",
      "Training epoch: 116, train loss: 0.02029, val loss: 0.02076\n",
      "Training epoch: 117, train loss: 0.02063, val loss: 0.02115\n",
      "Training epoch: 118, train loss: 0.02039, val loss: 0.02089\n",
      "Training epoch: 119, train loss: 0.02051, val loss: 0.02094\n",
      "Training epoch: 120, train loss: 0.02035, val loss: 0.02088\n",
      "Training epoch: 121, train loss: 0.02076, val loss: 0.02131\n",
      "Training epoch: 122, train loss: 0.02026, val loss: 0.02088\n",
      "Training epoch: 123, train loss: 0.02099, val loss: 0.02142\n",
      "Training epoch: 124, train loss: 0.02057, val loss: 0.02105\n",
      "Training epoch: 125, train loss: 0.02091, val loss: 0.02162\n",
      "Training epoch: 126, train loss: 0.02145, val loss: 0.02222\n",
      "Training epoch: 127, train loss: 0.02041, val loss: 0.02100\n",
      "Training epoch: 128, train loss: 0.02018, val loss: 0.02078\n",
      "Training epoch: 129, train loss: 0.02024, val loss: 0.02071\n",
      "Training epoch: 130, train loss: 0.02055, val loss: 0.02121\n",
      "Training epoch: 131, train loss: 0.02025, val loss: 0.02079\n",
      "Training epoch: 132, train loss: 0.02042, val loss: 0.02101\n",
      "Training epoch: 133, train loss: 0.02001, val loss: 0.02060\n",
      "Training epoch: 134, train loss: 0.02004, val loss: 0.02062\n",
      "Training epoch: 135, train loss: 0.01996, val loss: 0.02054\n",
      "Training epoch: 136, train loss: 0.02024, val loss: 0.02084\n",
      "Training epoch: 137, train loss: 0.02041, val loss: 0.02109\n",
      "Training epoch: 138, train loss: 0.02010, val loss: 0.02069\n",
      "Training epoch: 139, train loss: 0.01994, val loss: 0.02054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 140, train loss: 0.02018, val loss: 0.02060\n",
      "Training epoch: 141, train loss: 0.01988, val loss: 0.02040\n",
      "Training epoch: 142, train loss: 0.02012, val loss: 0.02063\n",
      "Training epoch: 143, train loss: 0.01998, val loss: 0.02047\n",
      "Training epoch: 144, train loss: 0.01998, val loss: 0.02046\n",
      "Training epoch: 145, train loss: 0.02008, val loss: 0.02069\n",
      "Training epoch: 146, train loss: 0.02011, val loss: 0.02068\n",
      "Training epoch: 147, train loss: 0.01996, val loss: 0.02061\n",
      "Training epoch: 148, train loss: 0.02019, val loss: 0.02084\n",
      "Training epoch: 149, train loss: 0.01974, val loss: 0.02031\n",
      "Training epoch: 150, train loss: 0.02098, val loss: 0.02170\n",
      "Training epoch: 151, train loss: 0.02003, val loss: 0.02068\n",
      "Training epoch: 152, train loss: 0.01986, val loss: 0.02037\n",
      "Training epoch: 153, train loss: 0.01966, val loss: 0.02022\n",
      "Training epoch: 154, train loss: 0.02002, val loss: 0.02076\n",
      "Training epoch: 155, train loss: 0.01961, val loss: 0.02020\n",
      "Training epoch: 156, train loss: 0.01987, val loss: 0.02054\n",
      "Training epoch: 157, train loss: 0.01968, val loss: 0.02028\n",
      "Training epoch: 158, train loss: 0.02019, val loss: 0.02092\n",
      "Training epoch: 159, train loss: 0.01962, val loss: 0.02021\n",
      "Training epoch: 160, train loss: 0.01972, val loss: 0.02028\n",
      "Training epoch: 161, train loss: 0.01974, val loss: 0.02040\n",
      "Training epoch: 162, train loss: 0.01990, val loss: 0.02054\n",
      "Training epoch: 163, train loss: 0.01996, val loss: 0.02072\n",
      "Training epoch: 164, train loss: 0.02000, val loss: 0.02070\n",
      "Training epoch: 165, train loss: 0.01988, val loss: 0.02054\n",
      "Training epoch: 166, train loss: 0.01968, val loss: 0.02040\n",
      "Training epoch: 167, train loss: 0.01957, val loss: 0.02007\n",
      "Training epoch: 168, train loss: 0.01949, val loss: 0.02000\n",
      "Training epoch: 169, train loss: 0.01972, val loss: 0.02033\n",
      "Training epoch: 170, train loss: 0.01967, val loss: 0.02020\n",
      "Training epoch: 171, train loss: 0.01957, val loss: 0.02023\n",
      "Training epoch: 172, train loss: 0.01947, val loss: 0.02009\n",
      "Training epoch: 173, train loss: 0.01972, val loss: 0.02014\n",
      "Training epoch: 174, train loss: 0.01948, val loss: 0.02001\n",
      "Training epoch: 175, train loss: 0.02013, val loss: 0.02091\n",
      "Training epoch: 176, train loss: 0.01938, val loss: 0.02003\n",
      "Training epoch: 177, train loss: 0.02036, val loss: 0.02074\n",
      "Training epoch: 178, train loss: 0.01933, val loss: 0.01991\n",
      "Training epoch: 179, train loss: 0.01939, val loss: 0.01991\n",
      "Training epoch: 180, train loss: 0.01922, val loss: 0.01972\n",
      "Training epoch: 181, train loss: 0.01972, val loss: 0.02023\n",
      "Training epoch: 182, train loss: 0.01948, val loss: 0.02001\n",
      "Training epoch: 183, train loss: 0.01964, val loss: 0.02023\n",
      "Training epoch: 184, train loss: 0.01978, val loss: 0.02050\n",
      "Training epoch: 185, train loss: 0.01982, val loss: 0.02062\n",
      "Training epoch: 186, train loss: 0.01926, val loss: 0.01996\n",
      "Training epoch: 187, train loss: 0.02007, val loss: 0.02091\n",
      "Training epoch: 188, train loss: 0.01929, val loss: 0.01998\n",
      "Training epoch: 189, train loss: 0.01932, val loss: 0.01998\n",
      "Training epoch: 190, train loss: 0.01913, val loss: 0.01964\n",
      "Training epoch: 191, train loss: 0.01991, val loss: 0.02041\n",
      "Training epoch: 192, train loss: 0.01933, val loss: 0.01988\n",
      "Training epoch: 193, train loss: 0.01901, val loss: 0.01959\n",
      "Training epoch: 194, train loss: 0.01903, val loss: 0.01958\n",
      "Training epoch: 195, train loss: 0.01925, val loss: 0.01996\n",
      "Training epoch: 196, train loss: 0.01914, val loss: 0.01985\n",
      "Training epoch: 197, train loss: 0.01959, val loss: 0.02005\n",
      "Training epoch: 198, train loss: 0.01886, val loss: 0.01946\n",
      "Training epoch: 199, train loss: 0.01927, val loss: 0.01985\n",
      "Training epoch: 200, train loss: 0.01929, val loss: 0.02008\n",
      "Training epoch: 201, train loss: 0.01976, val loss: 0.02023\n",
      "Training epoch: 202, train loss: 0.01901, val loss: 0.01969\n",
      "Training epoch: 203, train loss: 0.01907, val loss: 0.01973\n",
      "Training epoch: 204, train loss: 0.01920, val loss: 0.01970\n",
      "Training epoch: 205, train loss: 0.01903, val loss: 0.01968\n",
      "Training epoch: 206, train loss: 0.01897, val loss: 0.01953\n",
      "Training epoch: 207, train loss: 0.01878, val loss: 0.01942\n",
      "Training epoch: 208, train loss: 0.01867, val loss: 0.01928\n",
      "Training epoch: 209, train loss: 0.01869, val loss: 0.01938\n",
      "Training epoch: 210, train loss: 0.01878, val loss: 0.01945\n",
      "Training epoch: 211, train loss: 0.01904, val loss: 0.01978\n",
      "Training epoch: 212, train loss: 0.01999, val loss: 0.02091\n",
      "Training epoch: 213, train loss: 0.01870, val loss: 0.01943\n",
      "Training epoch: 214, train loss: 0.01892, val loss: 0.01945\n",
      "Training epoch: 215, train loss: 0.01867, val loss: 0.01928\n",
      "Training epoch: 216, train loss: 0.01831, val loss: 0.01900\n",
      "Training epoch: 217, train loss: 0.01888, val loss: 0.01968\n",
      "Training epoch: 218, train loss: 0.01864, val loss: 0.01926\n",
      "Training epoch: 219, train loss: 0.01846, val loss: 0.01912\n",
      "Training epoch: 220, train loss: 0.01856, val loss: 0.01935\n",
      "Training epoch: 221, train loss: 0.02074, val loss: 0.02182\n",
      "Training epoch: 222, train loss: 0.01877, val loss: 0.01941\n",
      "Training epoch: 223, train loss: 0.01879, val loss: 0.01957\n",
      "Training epoch: 224, train loss: 0.01828, val loss: 0.01898\n",
      "Training epoch: 225, train loss: 0.01828, val loss: 0.01906\n",
      "Training epoch: 226, train loss: 0.01869, val loss: 0.01951\n",
      "Training epoch: 227, train loss: 0.01926, val loss: 0.01994\n",
      "Training epoch: 228, train loss: 0.01818, val loss: 0.01886\n",
      "Training epoch: 229, train loss: 0.01891, val loss: 0.01953\n",
      "Training epoch: 230, train loss: 0.01849, val loss: 0.01915\n",
      "Training epoch: 231, train loss: 0.01834, val loss: 0.01901\n",
      "Training epoch: 232, train loss: 0.01819, val loss: 0.01896\n",
      "Training epoch: 233, train loss: 0.01803, val loss: 0.01880\n",
      "Training epoch: 234, train loss: 0.01962, val loss: 0.02015\n",
      "Training epoch: 235, train loss: 0.01859, val loss: 0.01921\n",
      "Training epoch: 236, train loss: 0.01874, val loss: 0.01950\n",
      "Training epoch: 237, train loss: 0.01864, val loss: 0.01946\n",
      "Training epoch: 238, train loss: 0.01870, val loss: 0.01950\n",
      "Training epoch: 239, train loss: 0.01807, val loss: 0.01888\n",
      "Training epoch: 240, train loss: 0.01803, val loss: 0.01873\n",
      "Training epoch: 241, train loss: 0.01851, val loss: 0.01930\n",
      "Training epoch: 242, train loss: 0.01828, val loss: 0.01918\n",
      "Training epoch: 243, train loss: 0.01801, val loss: 0.01875\n",
      "Training epoch: 244, train loss: 0.01802, val loss: 0.01881\n",
      "Training epoch: 245, train loss: 0.01874, val loss: 0.01973\n",
      "Training epoch: 246, train loss: 0.01789, val loss: 0.01867\n",
      "Training epoch: 247, train loss: 0.01821, val loss: 0.01907\n",
      "Training epoch: 248, train loss: 0.01807, val loss: 0.01891\n",
      "Training epoch: 249, train loss: 0.01801, val loss: 0.01879\n",
      "Training epoch: 250, train loss: 0.01869, val loss: 0.01937\n",
      "Training epoch: 251, train loss: 0.01858, val loss: 0.01940\n",
      "Training epoch: 252, train loss: 0.01776, val loss: 0.01851\n",
      "Training epoch: 253, train loss: 0.01787, val loss: 0.01861\n",
      "Training epoch: 254, train loss: 0.01811, val loss: 0.01892\n",
      "Training epoch: 255, train loss: 0.01805, val loss: 0.01901\n",
      "Training epoch: 256, train loss: 0.01762, val loss: 0.01843\n",
      "Training epoch: 257, train loss: 0.01863, val loss: 0.01954\n",
      "Training epoch: 258, train loss: 0.01775, val loss: 0.01863\n",
      "Training epoch: 259, train loss: 0.01756, val loss: 0.01838\n",
      "Training epoch: 260, train loss: 0.01768, val loss: 0.01846\n",
      "Training epoch: 261, train loss: 0.01771, val loss: 0.01858\n",
      "Training epoch: 262, train loss: 0.01822, val loss: 0.01895\n",
      "Training epoch: 263, train loss: 0.01942, val loss: 0.02052\n",
      "Training epoch: 264, train loss: 0.01771, val loss: 0.01852\n",
      "Training epoch: 265, train loss: 0.01771, val loss: 0.01849\n",
      "Training epoch: 266, train loss: 0.01770, val loss: 0.01862\n",
      "Training epoch: 267, train loss: 0.01771, val loss: 0.01852\n",
      "Training epoch: 268, train loss: 0.01772, val loss: 0.01846\n",
      "Training epoch: 269, train loss: 0.01808, val loss: 0.01890\n",
      "Training epoch: 270, train loss: 0.01789, val loss: 0.01872\n",
      "Training epoch: 271, train loss: 0.01777, val loss: 0.01871\n",
      "Training epoch: 272, train loss: 0.01760, val loss: 0.01836\n",
      "Training epoch: 273, train loss: 0.01755, val loss: 0.01835\n",
      "Training epoch: 274, train loss: 0.01739, val loss: 0.01818\n",
      "Training epoch: 275, train loss: 0.01771, val loss: 0.01852\n",
      "Training epoch: 276, train loss: 0.01825, val loss: 0.01903\n",
      "Training epoch: 277, train loss: 0.01836, val loss: 0.01925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 278, train loss: 0.01781, val loss: 0.01869\n",
      "Training epoch: 279, train loss: 0.01776, val loss: 0.01853\n",
      "Training epoch: 280, train loss: 0.01733, val loss: 0.01815\n",
      "Training epoch: 281, train loss: 0.01773, val loss: 0.01858\n",
      "Training epoch: 282, train loss: 0.01764, val loss: 0.01851\n",
      "Training epoch: 283, train loss: 0.01751, val loss: 0.01829\n",
      "Training epoch: 284, train loss: 0.01771, val loss: 0.01847\n",
      "Training epoch: 285, train loss: 0.01736, val loss: 0.01815\n",
      "Training epoch: 286, train loss: 0.01753, val loss: 0.01848\n",
      "Training epoch: 287, train loss: 0.01818, val loss: 0.01904\n",
      "Training epoch: 288, train loss: 0.01781, val loss: 0.01865\n",
      "Training epoch: 289, train loss: 0.01741, val loss: 0.01822\n",
      "Training epoch: 290, train loss: 0.01774, val loss: 0.01850\n",
      "Training epoch: 291, train loss: 0.01759, val loss: 0.01850\n",
      "Training epoch: 292, train loss: 0.01732, val loss: 0.01824\n",
      "Training epoch: 293, train loss: 0.01723, val loss: 0.01808\n",
      "Training epoch: 294, train loss: 0.01772, val loss: 0.01862\n",
      "Training epoch: 295, train loss: 0.01834, val loss: 0.01937\n",
      "Training epoch: 296, train loss: 0.01724, val loss: 0.01810\n",
      "Training epoch: 297, train loss: 0.01749, val loss: 0.01833\n",
      "Training epoch: 298, train loss: 0.01714, val loss: 0.01799\n",
      "Training epoch: 299, train loss: 0.01811, val loss: 0.01908\n",
      "Training epoch: 300, train loss: 0.01732, val loss: 0.01815\n",
      "Training epoch: 301, train loss: 0.01763, val loss: 0.01846\n",
      "Training epoch: 302, train loss: 0.01864, val loss: 0.01961\n",
      "Training epoch: 303, train loss: 0.01733, val loss: 0.01813\n",
      "Training epoch: 304, train loss: 0.01722, val loss: 0.01810\n",
      "Training epoch: 305, train loss: 0.01797, val loss: 0.01887\n",
      "Training epoch: 306, train loss: 0.01745, val loss: 0.01837\n",
      "Training epoch: 307, train loss: 0.01745, val loss: 0.01824\n",
      "Training epoch: 308, train loss: 0.01728, val loss: 0.01811\n",
      "Training epoch: 309, train loss: 0.01741, val loss: 0.01829\n",
      "Training epoch: 310, train loss: 0.01717, val loss: 0.01802\n",
      "Training epoch: 311, train loss: 0.01740, val loss: 0.01824\n",
      "Training epoch: 312, train loss: 0.01756, val loss: 0.01852\n",
      "Training epoch: 313, train loss: 0.01720, val loss: 0.01793\n",
      "Training epoch: 314, train loss: 0.01724, val loss: 0.01810\n",
      "Training epoch: 315, train loss: 0.01705, val loss: 0.01793\n",
      "Training epoch: 316, train loss: 0.01757, val loss: 0.01844\n",
      "Training epoch: 317, train loss: 0.01723, val loss: 0.01810\n",
      "Training epoch: 318, train loss: 0.01791, val loss: 0.01865\n",
      "Training epoch: 319, train loss: 0.01791, val loss: 0.01874\n",
      "Training epoch: 320, train loss: 0.01755, val loss: 0.01849\n",
      "Training epoch: 321, train loss: 0.01708, val loss: 0.01787\n",
      "Training epoch: 322, train loss: 0.01769, val loss: 0.01851\n",
      "Training epoch: 323, train loss: 0.01766, val loss: 0.01847\n",
      "Training epoch: 324, train loss: 0.01703, val loss: 0.01793\n",
      "Training epoch: 325, train loss: 0.01737, val loss: 0.01832\n",
      "Training epoch: 326, train loss: 0.01711, val loss: 0.01792\n",
      "Training epoch: 327, train loss: 0.01718, val loss: 0.01799\n",
      "Training epoch: 328, train loss: 0.01740, val loss: 0.01820\n",
      "Training epoch: 329, train loss: 0.01697, val loss: 0.01780\n",
      "Training epoch: 330, train loss: 0.01689, val loss: 0.01777\n",
      "Training epoch: 331, train loss: 0.01751, val loss: 0.01836\n",
      "Training epoch: 332, train loss: 0.01754, val loss: 0.01849\n",
      "Training epoch: 333, train loss: 0.02004, val loss: 0.02116\n",
      "Training epoch: 334, train loss: 0.01805, val loss: 0.01876\n",
      "Training epoch: 335, train loss: 0.01819, val loss: 0.01890\n",
      "Training epoch: 336, train loss: 0.01689, val loss: 0.01769\n",
      "Training epoch: 337, train loss: 0.01779, val loss: 0.01867\n",
      "Training epoch: 338, train loss: 0.01813, val loss: 0.01888\n",
      "Training epoch: 339, train loss: 0.01686, val loss: 0.01768\n",
      "Training epoch: 340, train loss: 0.01727, val loss: 0.01810\n",
      "Training epoch: 341, train loss: 0.01968, val loss: 0.02076\n",
      "Training epoch: 342, train loss: 0.01947, val loss: 0.02057\n",
      "Training epoch: 343, train loss: 0.01745, val loss: 0.01836\n",
      "Training epoch: 344, train loss: 0.01768, val loss: 0.01844\n",
      "Training epoch: 345, train loss: 0.01729, val loss: 0.01807\n",
      "Training epoch: 346, train loss: 0.01727, val loss: 0.01819\n",
      "Training epoch: 347, train loss: 0.01754, val loss: 0.01840\n",
      "Training epoch: 348, train loss: 0.01716, val loss: 0.01809\n",
      "Training epoch: 349, train loss: 0.01720, val loss: 0.01813\n",
      "Training epoch: 350, train loss: 0.01684, val loss: 0.01762\n",
      "Training epoch: 351, train loss: 0.01719, val loss: 0.01811\n",
      "Training epoch: 352, train loss: 0.01691, val loss: 0.01777\n",
      "Training epoch: 353, train loss: 0.01730, val loss: 0.01810\n",
      "Training epoch: 354, train loss: 0.01719, val loss: 0.01802\n",
      "Training epoch: 355, train loss: 0.01833, val loss: 0.01913\n",
      "Training epoch: 356, train loss: 0.01691, val loss: 0.01770\n",
      "Training epoch: 357, train loss: 0.01707, val loss: 0.01787\n",
      "Training epoch: 358, train loss: 0.01697, val loss: 0.01782\n",
      "Training epoch: 359, train loss: 0.01730, val loss: 0.01811\n",
      "Training epoch: 360, train loss: 0.01693, val loss: 0.01775\n",
      "Training epoch: 361, train loss: 0.01710, val loss: 0.01797\n",
      "Training epoch: 362, train loss: 0.01704, val loss: 0.01785\n",
      "Training epoch: 363, train loss: 0.01682, val loss: 0.01760\n",
      "Training epoch: 364, train loss: 0.01680, val loss: 0.01763\n",
      "Training epoch: 365, train loss: 0.01692, val loss: 0.01776\n",
      "Training epoch: 366, train loss: 0.01686, val loss: 0.01769\n",
      "Training epoch: 367, train loss: 0.01696, val loss: 0.01786\n",
      "Training epoch: 368, train loss: 0.01681, val loss: 0.01761\n",
      "Training epoch: 369, train loss: 0.01716, val loss: 0.01801\n",
      "Training epoch: 370, train loss: 0.01695, val loss: 0.01774\n",
      "Training epoch: 371, train loss: 0.01679, val loss: 0.01757\n",
      "Training epoch: 372, train loss: 0.01674, val loss: 0.01758\n",
      "Training epoch: 373, train loss: 0.01666, val loss: 0.01743\n",
      "Training epoch: 374, train loss: 0.01694, val loss: 0.01777\n",
      "Training epoch: 375, train loss: 0.01686, val loss: 0.01761\n",
      "Training epoch: 376, train loss: 0.01711, val loss: 0.01785\n",
      "Training epoch: 377, train loss: 0.01681, val loss: 0.01766\n",
      "Training epoch: 378, train loss: 0.01706, val loss: 0.01789\n",
      "Training epoch: 379, train loss: 0.01742, val loss: 0.01834\n",
      "Training epoch: 380, train loss: 0.01699, val loss: 0.01775\n",
      "Training epoch: 381, train loss: 0.01679, val loss: 0.01760\n",
      "Training epoch: 382, train loss: 0.01664, val loss: 0.01743\n",
      "Training epoch: 383, train loss: 0.01750, val loss: 0.01837\n",
      "Training epoch: 384, train loss: 0.01731, val loss: 0.01807\n",
      "Training epoch: 385, train loss: 0.01701, val loss: 0.01782\n",
      "Training epoch: 386, train loss: 0.01754, val loss: 0.01837\n",
      "Training epoch: 387, train loss: 0.01667, val loss: 0.01746\n",
      "Training epoch: 388, train loss: 0.01707, val loss: 0.01793\n",
      "Training epoch: 389, train loss: 0.01665, val loss: 0.01744\n",
      "Training epoch: 390, train loss: 0.01657, val loss: 0.01732\n",
      "Training epoch: 391, train loss: 0.01703, val loss: 0.01784\n",
      "Training epoch: 392, train loss: 0.01675, val loss: 0.01761\n",
      "Training epoch: 393, train loss: 0.01687, val loss: 0.01762\n",
      "Training epoch: 394, train loss: 0.01671, val loss: 0.01751\n",
      "Training epoch: 395, train loss: 0.01673, val loss: 0.01757\n",
      "Training epoch: 396, train loss: 0.01645, val loss: 0.01725\n",
      "Training epoch: 397, train loss: 0.01697, val loss: 0.01775\n",
      "Training epoch: 398, train loss: 0.01692, val loss: 0.01763\n",
      "Training epoch: 399, train loss: 0.01718, val loss: 0.01802\n",
      "Training epoch: 400, train loss: 0.01654, val loss: 0.01738\n",
      "Training epoch: 401, train loss: 0.01647, val loss: 0.01723\n",
      "Training epoch: 402, train loss: 0.01672, val loss: 0.01751\n",
      "Training epoch: 403, train loss: 0.01682, val loss: 0.01755\n",
      "Training epoch: 404, train loss: 0.01643, val loss: 0.01722\n",
      "Training epoch: 405, train loss: 0.01680, val loss: 0.01753\n",
      "Training epoch: 406, train loss: 0.01672, val loss: 0.01754\n",
      "Training epoch: 407, train loss: 0.01662, val loss: 0.01743\n",
      "Training epoch: 408, train loss: 0.01687, val loss: 0.01765\n",
      "Training epoch: 409, train loss: 0.01696, val loss: 0.01777\n",
      "Training epoch: 410, train loss: 0.01674, val loss: 0.01745\n",
      "Training epoch: 411, train loss: 0.01727, val loss: 0.01806\n",
      "Training epoch: 412, train loss: 0.01668, val loss: 0.01750\n",
      "Training epoch: 413, train loss: 0.01690, val loss: 0.01771\n",
      "Training epoch: 414, train loss: 0.01647, val loss: 0.01721\n",
      "Training epoch: 415, train loss: 0.01705, val loss: 0.01784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 416, train loss: 0.01746, val loss: 0.01835\n",
      "Training epoch: 417, train loss: 0.01715, val loss: 0.01802\n",
      "Training epoch: 418, train loss: 0.01653, val loss: 0.01727\n",
      "Training epoch: 419, train loss: 0.01656, val loss: 0.01736\n",
      "Training epoch: 420, train loss: 0.01818, val loss: 0.01887\n",
      "Training epoch: 421, train loss: 0.01637, val loss: 0.01713\n",
      "Training epoch: 422, train loss: 0.01687, val loss: 0.01772\n",
      "Training epoch: 423, train loss: 0.01722, val loss: 0.01808\n",
      "Training epoch: 424, train loss: 0.01689, val loss: 0.01764\n",
      "Training epoch: 425, train loss: 0.01693, val loss: 0.01769\n",
      "Training epoch: 426, train loss: 0.01649, val loss: 0.01728\n",
      "Training epoch: 427, train loss: 0.01637, val loss: 0.01717\n",
      "Training epoch: 428, train loss: 0.01664, val loss: 0.01742\n",
      "Training epoch: 429, train loss: 0.01656, val loss: 0.01738\n",
      "Training epoch: 430, train loss: 0.01748, val loss: 0.01836\n",
      "Training epoch: 431, train loss: 0.01643, val loss: 0.01719\n",
      "Training epoch: 432, train loss: 0.01631, val loss: 0.01709\n",
      "Training epoch: 433, train loss: 0.01633, val loss: 0.01709\n",
      "Training epoch: 434, train loss: 0.01644, val loss: 0.01723\n",
      "Training epoch: 435, train loss: 0.01694, val loss: 0.01771\n",
      "Training epoch: 436, train loss: 0.01636, val loss: 0.01714\n",
      "Training epoch: 437, train loss: 0.01661, val loss: 0.01736\n",
      "Training epoch: 438, train loss: 0.01674, val loss: 0.01742\n",
      "Training epoch: 439, train loss: 0.01667, val loss: 0.01745\n",
      "Training epoch: 440, train loss: 0.01645, val loss: 0.01728\n",
      "Training epoch: 441, train loss: 0.01678, val loss: 0.01764\n",
      "Training epoch: 442, train loss: 0.01638, val loss: 0.01717\n",
      "Training epoch: 443, train loss: 0.01628, val loss: 0.01705\n",
      "Training epoch: 444, train loss: 0.01650, val loss: 0.01730\n",
      "Training epoch: 445, train loss: 0.01673, val loss: 0.01756\n",
      "Training epoch: 446, train loss: 0.01643, val loss: 0.01718\n",
      "Training epoch: 447, train loss: 0.01697, val loss: 0.01788\n",
      "Training epoch: 448, train loss: 0.01638, val loss: 0.01712\n",
      "Training epoch: 449, train loss: 0.01636, val loss: 0.01712\n",
      "Training epoch: 450, train loss: 0.01656, val loss: 0.01736\n",
      "Training epoch: 451, train loss: 0.01659, val loss: 0.01734\n",
      "Training epoch: 452, train loss: 0.01688, val loss: 0.01771\n",
      "Training epoch: 453, train loss: 0.01699, val loss: 0.01779\n",
      "Training epoch: 454, train loss: 0.01738, val loss: 0.01820\n",
      "Training epoch: 455, train loss: 0.01646, val loss: 0.01726\n",
      "Training epoch: 456, train loss: 0.01675, val loss: 0.01756\n",
      "Training epoch: 457, train loss: 0.01688, val loss: 0.01763\n",
      "Training epoch: 458, train loss: 0.01678, val loss: 0.01752\n",
      "Training epoch: 459, train loss: 0.01706, val loss: 0.01781\n",
      "Training epoch: 460, train loss: 0.01678, val loss: 0.01757\n",
      "Training epoch: 461, train loss: 0.01682, val loss: 0.01763\n",
      "Training epoch: 462, train loss: 0.01650, val loss: 0.01729\n",
      "Training epoch: 463, train loss: 0.01625, val loss: 0.01705\n",
      "Training epoch: 464, train loss: 0.01619, val loss: 0.01696\n",
      "Training epoch: 465, train loss: 0.01627, val loss: 0.01699\n",
      "Training epoch: 466, train loss: 0.01641, val loss: 0.01718\n",
      "Training epoch: 467, train loss: 0.01689, val loss: 0.01764\n",
      "Training epoch: 468, train loss: 0.01652, val loss: 0.01730\n",
      "Training epoch: 469, train loss: 0.01685, val loss: 0.01762\n",
      "Training epoch: 470, train loss: 0.01693, val loss: 0.01776\n",
      "Training epoch: 471, train loss: 0.01623, val loss: 0.01707\n",
      "Training epoch: 472, train loss: 0.01687, val loss: 0.01769\n",
      "Training epoch: 473, train loss: 0.01625, val loss: 0.01699\n",
      "Training epoch: 474, train loss: 0.01777, val loss: 0.01853\n",
      "Training epoch: 475, train loss: 0.01663, val loss: 0.01733\n",
      "Training epoch: 476, train loss: 0.01630, val loss: 0.01710\n",
      "Training epoch: 477, train loss: 0.01667, val loss: 0.01746\n",
      "Training epoch: 478, train loss: 0.01656, val loss: 0.01739\n",
      "Training epoch: 479, train loss: 0.01617, val loss: 0.01693\n",
      "Training epoch: 480, train loss: 0.01649, val loss: 0.01726\n",
      "Training epoch: 481, train loss: 0.01610, val loss: 0.01689\n",
      "Training epoch: 482, train loss: 0.01637, val loss: 0.01713\n",
      "Training epoch: 483, train loss: 0.01666, val loss: 0.01744\n",
      "Training epoch: 484, train loss: 0.01618, val loss: 0.01691\n",
      "Training epoch: 485, train loss: 0.01727, val loss: 0.01806\n",
      "Training epoch: 486, train loss: 0.01646, val loss: 0.01726\n",
      "Training epoch: 487, train loss: 0.01622, val loss: 0.01699\n",
      "Training epoch: 488, train loss: 0.01612, val loss: 0.01687\n",
      "Training epoch: 489, train loss: 0.01615, val loss: 0.01695\n",
      "Training epoch: 490, train loss: 0.01726, val loss: 0.01797\n",
      "Training epoch: 491, train loss: 0.01628, val loss: 0.01700\n",
      "Training epoch: 492, train loss: 0.01655, val loss: 0.01731\n",
      "Training epoch: 493, train loss: 0.01641, val loss: 0.01719\n",
      "Training epoch: 494, train loss: 0.01627, val loss: 0.01705\n",
      "Training epoch: 495, train loss: 0.01601, val loss: 0.01674\n",
      "Training epoch: 496, train loss: 0.01627, val loss: 0.01702\n",
      "Training epoch: 497, train loss: 0.01609, val loss: 0.01679\n",
      "Training epoch: 498, train loss: 0.01611, val loss: 0.01685\n",
      "Training epoch: 499, train loss: 0.01610, val loss: 0.01680\n",
      "Training epoch: 500, train loss: 0.01611, val loss: 0.01686\n",
      "Training epoch: 501, train loss: 0.01630, val loss: 0.01702\n",
      "Training epoch: 502, train loss: 0.01595, val loss: 0.01670\n",
      "Training epoch: 503, train loss: 0.01673, val loss: 0.01753\n",
      "Training epoch: 504, train loss: 0.01689, val loss: 0.01760\n",
      "Training epoch: 505, train loss: 0.01704, val loss: 0.01782\n",
      "Training epoch: 506, train loss: 0.01617, val loss: 0.01690\n",
      "Training epoch: 507, train loss: 0.01596, val loss: 0.01667\n",
      "Training epoch: 508, train loss: 0.01685, val loss: 0.01760\n",
      "Training epoch: 509, train loss: 0.01629, val loss: 0.01705\n",
      "Training epoch: 510, train loss: 0.01660, val loss: 0.01739\n",
      "Training epoch: 511, train loss: 0.01627, val loss: 0.01704\n",
      "Training epoch: 512, train loss: 0.01615, val loss: 0.01694\n",
      "Training epoch: 513, train loss: 0.01603, val loss: 0.01679\n",
      "Training epoch: 514, train loss: 0.01591, val loss: 0.01662\n",
      "Training epoch: 515, train loss: 0.01605, val loss: 0.01678\n",
      "Training epoch: 516, train loss: 0.01626, val loss: 0.01703\n",
      "Training epoch: 517, train loss: 0.01611, val loss: 0.01684\n",
      "Training epoch: 518, train loss: 0.01650, val loss: 0.01719\n",
      "Training epoch: 519, train loss: 0.01648, val loss: 0.01725\n",
      "Training epoch: 520, train loss: 0.01617, val loss: 0.01693\n",
      "Training epoch: 521, train loss: 0.01698, val loss: 0.01780\n",
      "Training epoch: 522, train loss: 0.01620, val loss: 0.01693\n",
      "Training epoch: 523, train loss: 0.01591, val loss: 0.01666\n",
      "Training epoch: 524, train loss: 0.01622, val loss: 0.01691\n",
      "Training epoch: 525, train loss: 0.01650, val loss: 0.01731\n",
      "Training epoch: 526, train loss: 0.01591, val loss: 0.01669\n",
      "Training epoch: 527, train loss: 0.01610, val loss: 0.01684\n",
      "Training epoch: 528, train loss: 0.01590, val loss: 0.01663\n",
      "Training epoch: 529, train loss: 0.01605, val loss: 0.01675\n",
      "Training epoch: 530, train loss: 0.01691, val loss: 0.01758\n",
      "Training epoch: 531, train loss: 0.01625, val loss: 0.01698\n",
      "Training epoch: 532, train loss: 0.01628, val loss: 0.01703\n",
      "Training epoch: 533, train loss: 0.01646, val loss: 0.01719\n",
      "Training epoch: 534, train loss: 0.01621, val loss: 0.01689\n",
      "Training epoch: 535, train loss: 0.01623, val loss: 0.01697\n",
      "Training epoch: 536, train loss: 0.01601, val loss: 0.01676\n",
      "Training epoch: 537, train loss: 0.01751, val loss: 0.01822\n",
      "Training epoch: 538, train loss: 0.01634, val loss: 0.01709\n",
      "Training epoch: 539, train loss: 0.01638, val loss: 0.01718\n",
      "Training epoch: 540, train loss: 0.01612, val loss: 0.01684\n",
      "Training epoch: 541, train loss: 0.01618, val loss: 0.01689\n",
      "Training epoch: 542, train loss: 0.01634, val loss: 0.01707\n",
      "Training epoch: 543, train loss: 0.01629, val loss: 0.01699\n",
      "Training epoch: 544, train loss: 0.01741, val loss: 0.01807\n",
      "Training epoch: 545, train loss: 0.01635, val loss: 0.01704\n",
      "Training epoch: 546, train loss: 0.01627, val loss: 0.01698\n",
      "Training epoch: 547, train loss: 0.01617, val loss: 0.01695\n",
      "Training epoch: 548, train loss: 0.01652, val loss: 0.01718\n",
      "Training epoch: 549, train loss: 0.01601, val loss: 0.01672\n",
      "Training epoch: 550, train loss: 0.01587, val loss: 0.01658\n",
      "Training epoch: 551, train loss: 0.01598, val loss: 0.01665\n",
      "Training epoch: 552, train loss: 0.01617, val loss: 0.01684\n",
      "Training epoch: 553, train loss: 0.01593, val loss: 0.01664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 554, train loss: 0.01634, val loss: 0.01712\n",
      "Training epoch: 555, train loss: 0.01587, val loss: 0.01656\n",
      "Training epoch: 556, train loss: 0.01606, val loss: 0.01676\n",
      "Training epoch: 557, train loss: 0.01638, val loss: 0.01713\n",
      "Training epoch: 558, train loss: 0.01607, val loss: 0.01673\n",
      "Training epoch: 559, train loss: 0.01600, val loss: 0.01668\n",
      "Training epoch: 560, train loss: 0.01648, val loss: 0.01723\n",
      "Training epoch: 561, train loss: 0.01659, val loss: 0.01738\n",
      "Training epoch: 562, train loss: 0.01639, val loss: 0.01703\n",
      "Training epoch: 563, train loss: 0.01596, val loss: 0.01667\n",
      "Training epoch: 564, train loss: 0.01623, val loss: 0.01688\n",
      "Training epoch: 565, train loss: 0.01625, val loss: 0.01696\n",
      "Training epoch: 566, train loss: 0.01749, val loss: 0.01827\n",
      "Training epoch: 567, train loss: 0.01643, val loss: 0.01714\n",
      "Training epoch: 568, train loss: 0.01636, val loss: 0.01707\n",
      "Training epoch: 569, train loss: 0.01652, val loss: 0.01728\n",
      "Training epoch: 570, train loss: 0.01623, val loss: 0.01687\n",
      "Training epoch: 571, train loss: 0.01621, val loss: 0.01690\n",
      "Training epoch: 572, train loss: 0.01618, val loss: 0.01689\n",
      "Training epoch: 573, train loss: 0.01591, val loss: 0.01665\n",
      "Training epoch: 574, train loss: 0.01687, val loss: 0.01760\n",
      "Training epoch: 575, train loss: 0.01611, val loss: 0.01685\n",
      "Training epoch: 576, train loss: 0.01592, val loss: 0.01656\n",
      "Training epoch: 577, train loss: 0.01615, val loss: 0.01681\n",
      "Training epoch: 578, train loss: 0.01597, val loss: 0.01668\n",
      "Training epoch: 579, train loss: 0.01600, val loss: 0.01665\n",
      "Training epoch: 580, train loss: 0.01646, val loss: 0.01711\n",
      "Training epoch: 581, train loss: 0.01622, val loss: 0.01692\n",
      "Training epoch: 582, train loss: 0.01633, val loss: 0.01701\n",
      "Training epoch: 583, train loss: 0.01625, val loss: 0.01694\n",
      "Training epoch: 584, train loss: 0.01586, val loss: 0.01656\n",
      "Training epoch: 585, train loss: 0.01651, val loss: 0.01724\n",
      "Training epoch: 586, train loss: 0.01664, val loss: 0.01738\n",
      "Training epoch: 587, train loss: 0.01612, val loss: 0.01679\n",
      "Training epoch: 588, train loss: 0.01615, val loss: 0.01681\n",
      "Training epoch: 589, train loss: 0.01616, val loss: 0.01681\n",
      "Training epoch: 590, train loss: 0.01784, val loss: 0.01863\n",
      "Training epoch: 591, train loss: 0.01639, val loss: 0.01702\n",
      "Training epoch: 592, train loss: 0.01631, val loss: 0.01697\n",
      "Training epoch: 593, train loss: 0.01593, val loss: 0.01659\n",
      "Training epoch: 594, train loss: 0.01597, val loss: 0.01669\n",
      "Training epoch: 595, train loss: 0.01622, val loss: 0.01688\n",
      "Training epoch: 596, train loss: 0.01606, val loss: 0.01678\n",
      "Training epoch: 597, train loss: 0.01637, val loss: 0.01701\n",
      "Training epoch: 598, train loss: 0.01598, val loss: 0.01669\n",
      "Training epoch: 599, train loss: 0.01730, val loss: 0.01807\n",
      "Training epoch: 600, train loss: 0.01609, val loss: 0.01678\n",
      "Training epoch: 601, train loss: 0.01655, val loss: 0.01720\n",
      "Training epoch: 602, train loss: 0.01592, val loss: 0.01658\n",
      "Training epoch: 603, train loss: 0.01603, val loss: 0.01672\n",
      "Training epoch: 604, train loss: 0.01573, val loss: 0.01639\n",
      "Training epoch: 605, train loss: 0.01596, val loss: 0.01663\n",
      "Training epoch: 606, train loss: 0.01625, val loss: 0.01696\n",
      "Training epoch: 607, train loss: 0.01627, val loss: 0.01691\n",
      "Training epoch: 608, train loss: 0.01583, val loss: 0.01649\n",
      "Training epoch: 609, train loss: 0.01652, val loss: 0.01725\n",
      "Training epoch: 610, train loss: 0.01661, val loss: 0.01734\n",
      "Training epoch: 611, train loss: 0.01622, val loss: 0.01693\n",
      "Training epoch: 612, train loss: 0.01655, val loss: 0.01718\n",
      "Training epoch: 613, train loss: 0.01593, val loss: 0.01658\n",
      "Training epoch: 614, train loss: 0.01614, val loss: 0.01678\n",
      "Training epoch: 615, train loss: 0.01585, val loss: 0.01653\n",
      "Training epoch: 616, train loss: 0.01638, val loss: 0.01702\n",
      "Training epoch: 617, train loss: 0.01604, val loss: 0.01666\n",
      "Training epoch: 618, train loss: 0.01633, val loss: 0.01701\n",
      "Training epoch: 619, train loss: 0.01613, val loss: 0.01681\n",
      "Training epoch: 620, train loss: 0.01692, val loss: 0.01767\n",
      "Training epoch: 621, train loss: 0.01631, val loss: 0.01692\n",
      "Training epoch: 622, train loss: 0.01587, val loss: 0.01653\n",
      "Training epoch: 623, train loss: 0.01589, val loss: 0.01656\n",
      "Training epoch: 624, train loss: 0.01620, val loss: 0.01689\n",
      "Training epoch: 625, train loss: 0.01589, val loss: 0.01654\n",
      "Training epoch: 626, train loss: 0.01601, val loss: 0.01665\n",
      "Training epoch: 627, train loss: 0.01644, val loss: 0.01711\n",
      "Training epoch: 628, train loss: 0.01598, val loss: 0.01664\n",
      "Training epoch: 629, train loss: 0.01587, val loss: 0.01656\n",
      "Training epoch: 630, train loss: 0.01593, val loss: 0.01657\n",
      "Training epoch: 631, train loss: 0.01608, val loss: 0.01672\n",
      "Training epoch: 632, train loss: 0.01651, val loss: 0.01723\n",
      "Training epoch: 633, train loss: 0.01625, val loss: 0.01686\n",
      "Training epoch: 634, train loss: 0.01575, val loss: 0.01639\n",
      "Training epoch: 635, train loss: 0.01679, val loss: 0.01739\n",
      "Training epoch: 636, train loss: 0.01623, val loss: 0.01694\n",
      "Training epoch: 637, train loss: 0.01659, val loss: 0.01723\n",
      "Training epoch: 638, train loss: 0.01645, val loss: 0.01710\n",
      "Training epoch: 639, train loss: 0.01613, val loss: 0.01680\n",
      "Training epoch: 640, train loss: 0.01610, val loss: 0.01671\n",
      "Training epoch: 641, train loss: 0.01614, val loss: 0.01682\n",
      "Training epoch: 642, train loss: 0.01583, val loss: 0.01648\n",
      "Training epoch: 643, train loss: 0.01604, val loss: 0.01669\n",
      "Training epoch: 644, train loss: 0.01586, val loss: 0.01648\n",
      "Training epoch: 645, train loss: 0.01594, val loss: 0.01665\n",
      "Training epoch: 646, train loss: 0.01603, val loss: 0.01666\n",
      "Training epoch: 647, train loss: 0.01592, val loss: 0.01659\n",
      "Training epoch: 648, train loss: 0.01579, val loss: 0.01643\n",
      "Training epoch: 649, train loss: 0.01621, val loss: 0.01691\n",
      "Training epoch: 650, train loss: 0.01765, val loss: 0.01845\n",
      "Training epoch: 651, train loss: 0.01603, val loss: 0.01664\n",
      "Training epoch: 652, train loss: 0.01639, val loss: 0.01705\n",
      "Training epoch: 653, train loss: 0.01606, val loss: 0.01670\n",
      "Training epoch: 654, train loss: 0.01609, val loss: 0.01676\n",
      "Training epoch: 655, train loss: 0.01656, val loss: 0.01718\n",
      "Training epoch: 656, train loss: 0.01650, val loss: 0.01709\n",
      "Training epoch: 657, train loss: 0.01583, val loss: 0.01651\n",
      "Training epoch: 658, train loss: 0.01615, val loss: 0.01686\n",
      "Training epoch: 659, train loss: 0.01588, val loss: 0.01653\n",
      "Training epoch: 660, train loss: 0.01612, val loss: 0.01674\n",
      "Training epoch: 661, train loss: 0.01627, val loss: 0.01698\n",
      "Training epoch: 662, train loss: 0.01619, val loss: 0.01687\n",
      "Training epoch: 663, train loss: 0.01592, val loss: 0.01656\n",
      "Training epoch: 664, train loss: 0.01599, val loss: 0.01663\n",
      "Training epoch: 665, train loss: 0.01623, val loss: 0.01684\n",
      "Training epoch: 666, train loss: 0.01598, val loss: 0.01665\n",
      "Training epoch: 667, train loss: 0.01621, val loss: 0.01691\n",
      "Training epoch: 668, train loss: 0.01612, val loss: 0.01678\n",
      "Training epoch: 669, train loss: 0.01587, val loss: 0.01651\n",
      "Training epoch: 670, train loss: 0.01672, val loss: 0.01746\n",
      "Training epoch: 671, train loss: 0.01697, val loss: 0.01771\n",
      "Training epoch: 672, train loss: 0.01610, val loss: 0.01673\n",
      "Training epoch: 673, train loss: 0.01600, val loss: 0.01664\n",
      "Training epoch: 674, train loss: 0.01591, val loss: 0.01661\n",
      "Training epoch: 675, train loss: 0.01579, val loss: 0.01644\n",
      "Training epoch: 676, train loss: 0.01631, val loss: 0.01690\n",
      "Training epoch: 677, train loss: 0.01603, val loss: 0.01665\n",
      "Training epoch: 678, train loss: 0.01589, val loss: 0.01653\n",
      "Training epoch: 679, train loss: 0.01591, val loss: 0.01652\n",
      "Training epoch: 680, train loss: 0.01603, val loss: 0.01668\n",
      "Training epoch: 681, train loss: 0.01586, val loss: 0.01649\n",
      "Training epoch: 682, train loss: 0.01607, val loss: 0.01675\n",
      "Training epoch: 683, train loss: 0.01627, val loss: 0.01695\n",
      "Training epoch: 684, train loss: 0.01625, val loss: 0.01686\n",
      "Training epoch: 685, train loss: 0.01583, val loss: 0.01646\n",
      "Training epoch: 686, train loss: 0.01576, val loss: 0.01639\n",
      "Training epoch: 687, train loss: 0.01607, val loss: 0.01671\n",
      "Training epoch: 688, train loss: 0.01767, val loss: 0.01833\n",
      "Training epoch: 689, train loss: 0.01612, val loss: 0.01679\n",
      "Training epoch: 690, train loss: 0.01624, val loss: 0.01685\n",
      "Training epoch: 691, train loss: 0.01581, val loss: 0.01642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 692, train loss: 0.01579, val loss: 0.01646\n",
      "Training epoch: 693, train loss: 0.01624, val loss: 0.01687\n",
      "Training epoch: 694, train loss: 0.01620, val loss: 0.01687\n",
      "Training epoch: 695, train loss: 0.01605, val loss: 0.01670\n",
      "Training epoch: 696, train loss: 0.01592, val loss: 0.01655\n",
      "Training epoch: 697, train loss: 0.01622, val loss: 0.01689\n",
      "Training epoch: 698, train loss: 0.01595, val loss: 0.01661\n",
      "Training epoch: 699, train loss: 0.01612, val loss: 0.01676\n",
      "Training epoch: 700, train loss: 0.01582, val loss: 0.01651\n",
      "Training epoch: 701, train loss: 0.01593, val loss: 0.01655\n",
      "Training epoch: 702, train loss: 0.01581, val loss: 0.01640\n",
      "Training epoch: 703, train loss: 0.01613, val loss: 0.01675\n",
      "Training epoch: 704, train loss: 0.01591, val loss: 0.01654\n",
      "Training epoch: 705, train loss: 0.01585, val loss: 0.01646\n",
      "Training epoch: 706, train loss: 0.01639, val loss: 0.01695\n",
      "Training epoch: 707, train loss: 0.01654, val loss: 0.01718\n",
      "Training epoch: 708, train loss: 0.01631, val loss: 0.01696\n",
      "Training epoch: 709, train loss: 0.01623, val loss: 0.01691\n",
      "Training epoch: 710, train loss: 0.01598, val loss: 0.01662\n",
      "Training epoch: 711, train loss: 0.01645, val loss: 0.01716\n",
      "Training epoch: 712, train loss: 0.01666, val loss: 0.01738\n",
      "Training epoch: 713, train loss: 0.01628, val loss: 0.01689\n",
      "Training epoch: 714, train loss: 0.01599, val loss: 0.01657\n",
      "Training epoch: 715, train loss: 0.01615, val loss: 0.01679\n",
      "Training epoch: 716, train loss: 0.01600, val loss: 0.01666\n",
      "Training epoch: 717, train loss: 0.01628, val loss: 0.01696\n",
      "Training epoch: 718, train loss: 0.01633, val loss: 0.01702\n",
      "Training epoch: 719, train loss: 0.01623, val loss: 0.01691\n",
      "Training epoch: 720, train loss: 0.01596, val loss: 0.01658\n",
      "Training epoch: 721, train loss: 0.01620, val loss: 0.01688\n",
      "Training epoch: 722, train loss: 0.01577, val loss: 0.01638\n",
      "Training epoch: 723, train loss: 0.01594, val loss: 0.01655\n",
      "Training epoch: 724, train loss: 0.01647, val loss: 0.01714\n",
      "Training epoch: 725, train loss: 0.01592, val loss: 0.01655\n",
      "Training epoch: 726, train loss: 0.01598, val loss: 0.01658\n",
      "Training epoch: 727, train loss: 0.01692, val loss: 0.01750\n",
      "Training epoch: 728, train loss: 0.01611, val loss: 0.01669\n",
      "Training epoch: 729, train loss: 0.01591, val loss: 0.01657\n",
      "Training epoch: 730, train loss: 0.01598, val loss: 0.01664\n",
      "Training epoch: 731, train loss: 0.01650, val loss: 0.01711\n",
      "Training epoch: 732, train loss: 0.01599, val loss: 0.01660\n",
      "Training epoch: 733, train loss: 0.01593, val loss: 0.01656\n",
      "Training epoch: 734, train loss: 0.01582, val loss: 0.01644\n",
      "Training epoch: 735, train loss: 0.01618, val loss: 0.01683\n",
      "Training epoch: 736, train loss: 0.01596, val loss: 0.01660\n",
      "Training epoch: 737, train loss: 0.01578, val loss: 0.01638\n",
      "Training epoch: 738, train loss: 0.01583, val loss: 0.01643\n",
      "Training epoch: 739, train loss: 0.01657, val loss: 0.01727\n",
      "Training epoch: 740, train loss: 0.01669, val loss: 0.01725\n",
      "Training epoch: 741, train loss: 0.01563, val loss: 0.01624\n",
      "Training epoch: 742, train loss: 0.01588, val loss: 0.01650\n",
      "Training epoch: 743, train loss: 0.01609, val loss: 0.01668\n",
      "Training epoch: 744, train loss: 0.01588, val loss: 0.01650\n",
      "Training epoch: 745, train loss: 0.01674, val loss: 0.01743\n",
      "Training epoch: 746, train loss: 0.01620, val loss: 0.01684\n",
      "Training epoch: 747, train loss: 0.01636, val loss: 0.01691\n",
      "Training epoch: 748, train loss: 0.01616, val loss: 0.01684\n",
      "Training epoch: 749, train loss: 0.01614, val loss: 0.01678\n",
      "Training epoch: 750, train loss: 0.01566, val loss: 0.01627\n",
      "Training epoch: 751, train loss: 0.01592, val loss: 0.01653\n",
      "Training epoch: 752, train loss: 0.01630, val loss: 0.01687\n",
      "Training epoch: 753, train loss: 0.01582, val loss: 0.01643\n",
      "Training epoch: 754, train loss: 0.01602, val loss: 0.01662\n",
      "Training epoch: 755, train loss: 0.01647, val loss: 0.01717\n",
      "Training epoch: 756, train loss: 0.01571, val loss: 0.01629\n",
      "Training epoch: 757, train loss: 0.01602, val loss: 0.01663\n",
      "Training epoch: 758, train loss: 0.01583, val loss: 0.01646\n",
      "Training epoch: 759, train loss: 0.01603, val loss: 0.01662\n",
      "Training epoch: 760, train loss: 0.01609, val loss: 0.01674\n",
      "Training epoch: 761, train loss: 0.01684, val loss: 0.01754\n",
      "Training epoch: 762, train loss: 0.01729, val loss: 0.01806\n",
      "Training epoch: 763, train loss: 0.01601, val loss: 0.01657\n",
      "Training epoch: 764, train loss: 0.01616, val loss: 0.01672\n",
      "Training epoch: 765, train loss: 0.01657, val loss: 0.01719\n",
      "Training epoch: 766, train loss: 0.01587, val loss: 0.01651\n",
      "Training epoch: 767, train loss: 0.01618, val loss: 0.01682\n",
      "Training epoch: 768, train loss: 0.01591, val loss: 0.01653\n",
      "Training epoch: 769, train loss: 0.01581, val loss: 0.01638\n",
      "Training epoch: 770, train loss: 0.01621, val loss: 0.01682\n",
      "Training epoch: 771, train loss: 0.01624, val loss: 0.01683\n",
      "Training epoch: 772, train loss: 0.01598, val loss: 0.01659\n",
      "Training epoch: 773, train loss: 0.01607, val loss: 0.01666\n",
      "Training epoch: 774, train loss: 0.01602, val loss: 0.01665\n",
      "Training epoch: 775, train loss: 0.01572, val loss: 0.01636\n",
      "Training epoch: 776, train loss: 0.01642, val loss: 0.01704\n",
      "Training epoch: 777, train loss: 0.01625, val loss: 0.01685\n",
      "Training epoch: 778, train loss: 0.01635, val loss: 0.01698\n",
      "Training epoch: 779, train loss: 0.01572, val loss: 0.01632\n",
      "Training epoch: 780, train loss: 0.01613, val loss: 0.01680\n",
      "Training epoch: 781, train loss: 0.01596, val loss: 0.01654\n",
      "Training epoch: 782, train loss: 0.01574, val loss: 0.01633\n",
      "Training epoch: 783, train loss: 0.01583, val loss: 0.01639\n",
      "Training epoch: 784, train loss: 0.01580, val loss: 0.01638\n",
      "Training epoch: 785, train loss: 0.01576, val loss: 0.01642\n",
      "Training epoch: 786, train loss: 0.01622, val loss: 0.01686\n",
      "Training epoch: 787, train loss: 0.01578, val loss: 0.01640\n",
      "Training epoch: 788, train loss: 0.01615, val loss: 0.01668\n",
      "Training epoch: 789, train loss: 0.01602, val loss: 0.01660\n",
      "Training epoch: 790, train loss: 0.01588, val loss: 0.01651\n",
      "Training epoch: 791, train loss: 0.01590, val loss: 0.01649\n",
      "Training epoch: 792, train loss: 0.01593, val loss: 0.01653\n",
      "Training epoch: 793, train loss: 0.01627, val loss: 0.01690\n",
      "Training epoch: 794, train loss: 0.01585, val loss: 0.01647\n",
      "Training epoch: 795, train loss: 0.01564, val loss: 0.01624\n",
      "Training epoch: 796, train loss: 0.01576, val loss: 0.01634\n",
      "Training epoch: 797, train loss: 0.01569, val loss: 0.01627\n",
      "Training epoch: 798, train loss: 0.01714, val loss: 0.01780\n",
      "Training epoch: 799, train loss: 0.01571, val loss: 0.01633\n",
      "Training epoch: 800, train loss: 0.01621, val loss: 0.01676\n",
      "Training epoch: 801, train loss: 0.01591, val loss: 0.01655\n",
      "Training epoch: 802, train loss: 0.01639, val loss: 0.01692\n",
      "Training epoch: 803, train loss: 0.01584, val loss: 0.01642\n",
      "Training epoch: 804, train loss: 0.01607, val loss: 0.01668\n",
      "Training epoch: 805, train loss: 0.01637, val loss: 0.01694\n",
      "Training epoch: 806, train loss: 0.01659, val loss: 0.01720\n",
      "Training epoch: 807, train loss: 0.01644, val loss: 0.01706\n",
      "Training epoch: 808, train loss: 0.01582, val loss: 0.01642\n",
      "Training epoch: 809, train loss: 0.01649, val loss: 0.01713\n",
      "Training epoch: 810, train loss: 0.01597, val loss: 0.01654\n",
      "Training epoch: 811, train loss: 0.01575, val loss: 0.01634\n",
      "Training epoch: 812, train loss: 0.01568, val loss: 0.01629\n",
      "Training epoch: 813, train loss: 0.01622, val loss: 0.01682\n",
      "Training epoch: 814, train loss: 0.01582, val loss: 0.01640\n",
      "Training epoch: 815, train loss: 0.01584, val loss: 0.01642\n",
      "Training epoch: 816, train loss: 0.01587, val loss: 0.01649\n",
      "Training epoch: 817, train loss: 0.01622, val loss: 0.01679\n",
      "Training epoch: 818, train loss: 0.01627, val loss: 0.01690\n",
      "Training epoch: 819, train loss: 0.01627, val loss: 0.01691\n",
      "Training epoch: 820, train loss: 0.01631, val loss: 0.01690\n",
      "Training epoch: 821, train loss: 0.01606, val loss: 0.01669\n",
      "Training epoch: 822, train loss: 0.01604, val loss: 0.01662\n",
      "Training epoch: 823, train loss: 0.01690, val loss: 0.01756\n",
      "Training epoch: 824, train loss: 0.01593, val loss: 0.01652\n",
      "Training epoch: 825, train loss: 0.01622, val loss: 0.01677\n",
      "Training epoch: 826, train loss: 0.01682, val loss: 0.01749\n",
      "Training epoch: 827, train loss: 0.01668, val loss: 0.01735\n",
      "Training epoch: 828, train loss: 0.01587, val loss: 0.01641\n",
      "Training epoch: 829, train loss: 0.01616, val loss: 0.01670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 830, train loss: 0.01608, val loss: 0.01668\n",
      "Training epoch: 831, train loss: 0.01573, val loss: 0.01630\n",
      "Training epoch: 832, train loss: 0.01592, val loss: 0.01651\n",
      "Training epoch: 833, train loss: 0.01589, val loss: 0.01653\n",
      "Training epoch: 834, train loss: 0.01569, val loss: 0.01626\n",
      "Training epoch: 835, train loss: 0.01595, val loss: 0.01658\n",
      "Training epoch: 836, train loss: 0.01598, val loss: 0.01657\n",
      "Training epoch: 837, train loss: 0.01605, val loss: 0.01657\n",
      "Training epoch: 838, train loss: 0.01609, val loss: 0.01665\n",
      "Training epoch: 839, train loss: 0.01580, val loss: 0.01640\n",
      "Training epoch: 840, train loss: 0.01608, val loss: 0.01671\n",
      "Training epoch: 841, train loss: 0.01615, val loss: 0.01672\n",
      "Training epoch: 842, train loss: 0.01581, val loss: 0.01636\n",
      "Training epoch: 843, train loss: 0.01576, val loss: 0.01638\n",
      "Training epoch: 844, train loss: 0.01609, val loss: 0.01675\n",
      "Training epoch: 845, train loss: 0.01565, val loss: 0.01621\n",
      "Training epoch: 846, train loss: 0.01580, val loss: 0.01635\n",
      "Training epoch: 847, train loss: 0.01588, val loss: 0.01648\n",
      "Training epoch: 848, train loss: 0.01630, val loss: 0.01694\n",
      "Training epoch: 849, train loss: 0.01607, val loss: 0.01674\n",
      "Training epoch: 850, train loss: 0.01574, val loss: 0.01633\n",
      "Training epoch: 851, train loss: 0.01724, val loss: 0.01783\n",
      "Training epoch: 852, train loss: 0.01615, val loss: 0.01667\n",
      "Training epoch: 853, train loss: 0.01608, val loss: 0.01669\n",
      "Training epoch: 854, train loss: 0.01575, val loss: 0.01634\n",
      "Training epoch: 855, train loss: 0.01615, val loss: 0.01678\n",
      "Training epoch: 856, train loss: 0.01623, val loss: 0.01680\n",
      "Training epoch: 857, train loss: 0.01605, val loss: 0.01661\n",
      "Training epoch: 858, train loss: 0.01595, val loss: 0.01655\n",
      "Training epoch: 859, train loss: 0.01589, val loss: 0.01652\n",
      "Training epoch: 860, train loss: 0.01589, val loss: 0.01647\n",
      "Training epoch: 861, train loss: 0.01569, val loss: 0.01624\n",
      "Training epoch: 862, train loss: 0.01583, val loss: 0.01643\n",
      "Training epoch: 863, train loss: 0.01584, val loss: 0.01644\n",
      "Training epoch: 864, train loss: 0.01599, val loss: 0.01660\n",
      "Training epoch: 865, train loss: 0.01595, val loss: 0.01656\n",
      "Training epoch: 866, train loss: 0.01607, val loss: 0.01668\n",
      "Training epoch: 867, train loss: 0.01583, val loss: 0.01641\n",
      "Training epoch: 868, train loss: 0.01575, val loss: 0.01633\n",
      "Training epoch: 869, train loss: 0.01595, val loss: 0.01653\n",
      "Training epoch: 870, train loss: 0.01590, val loss: 0.01649\n",
      "Training epoch: 871, train loss: 0.01565, val loss: 0.01620\n",
      "Training epoch: 872, train loss: 0.01595, val loss: 0.01653\n",
      "Training epoch: 873, train loss: 0.01591, val loss: 0.01649\n",
      "Training epoch: 874, train loss: 0.01573, val loss: 0.01631\n",
      "Training epoch: 875, train loss: 0.01605, val loss: 0.01664\n",
      "Training epoch: 876, train loss: 0.01589, val loss: 0.01648\n",
      "Training epoch: 877, train loss: 0.01562, val loss: 0.01618\n",
      "Training epoch: 878, train loss: 0.01574, val loss: 0.01630\n",
      "Training epoch: 879, train loss: 0.01590, val loss: 0.01648\n",
      "Training epoch: 880, train loss: 0.01614, val loss: 0.01680\n",
      "Training epoch: 881, train loss: 0.01581, val loss: 0.01635\n",
      "Training epoch: 882, train loss: 0.01618, val loss: 0.01677\n",
      "Training epoch: 883, train loss: 0.01590, val loss: 0.01648\n",
      "Training epoch: 884, train loss: 0.01621, val loss: 0.01685\n",
      "Training epoch: 885, train loss: 0.01614, val loss: 0.01673\n",
      "Training epoch: 886, train loss: 0.01611, val loss: 0.01675\n",
      "Training epoch: 887, train loss: 0.01569, val loss: 0.01626\n",
      "Training epoch: 888, train loss: 0.01576, val loss: 0.01633\n",
      "Training epoch: 889, train loss: 0.01620, val loss: 0.01676\n",
      "Training epoch: 890, train loss: 0.01592, val loss: 0.01653\n",
      "Training epoch: 891, train loss: 0.01604, val loss: 0.01664\n",
      "Training epoch: 892, train loss: 0.01616, val loss: 0.01666\n",
      "Training epoch: 893, train loss: 0.01627, val loss: 0.01685\n",
      "Training epoch: 894, train loss: 0.01583, val loss: 0.01642\n",
      "Training epoch: 895, train loss: 0.01592, val loss: 0.01645\n",
      "Training epoch: 896, train loss: 0.01629, val loss: 0.01692\n",
      "Training epoch: 897, train loss: 0.01617, val loss: 0.01682\n",
      "Training epoch: 898, train loss: 0.01632, val loss: 0.01682\n",
      "Training epoch: 899, train loss: 0.01575, val loss: 0.01629\n",
      "Training epoch: 900, train loss: 0.01575, val loss: 0.01633\n",
      "Training epoch: 901, train loss: 0.01598, val loss: 0.01657\n",
      "Training epoch: 902, train loss: 0.01607, val loss: 0.01663\n",
      "Training epoch: 903, train loss: 0.01692, val loss: 0.01745\n",
      "Training epoch: 904, train loss: 0.01586, val loss: 0.01643\n",
      "Training epoch: 905, train loss: 0.01595, val loss: 0.01652\n",
      "Training epoch: 906, train loss: 0.01580, val loss: 0.01637\n",
      "Training epoch: 907, train loss: 0.01578, val loss: 0.01637\n",
      "Training epoch: 908, train loss: 0.01587, val loss: 0.01644\n",
      "Training epoch: 909, train loss: 0.01583, val loss: 0.01639\n",
      "Training epoch: 910, train loss: 0.01725, val loss: 0.01773\n",
      "Training epoch: 911, train loss: 0.01624, val loss: 0.01680\n",
      "Training epoch: 912, train loss: 0.01612, val loss: 0.01668\n",
      "Training epoch: 913, train loss: 0.01613, val loss: 0.01671\n",
      "Training epoch: 914, train loss: 0.01574, val loss: 0.01633\n",
      "Training epoch: 915, train loss: 0.01569, val loss: 0.01629\n",
      "Training epoch: 916, train loss: 0.01579, val loss: 0.01636\n",
      "Training epoch: 917, train loss: 0.01636, val loss: 0.01698\n",
      "Training epoch: 918, train loss: 0.01575, val loss: 0.01630\n",
      "Training epoch: 919, train loss: 0.01564, val loss: 0.01622\n",
      "Training epoch: 920, train loss: 0.01565, val loss: 0.01625\n",
      "Training epoch: 921, train loss: 0.01650, val loss: 0.01700\n",
      "Training epoch: 922, train loss: 0.01584, val loss: 0.01642\n",
      "Training epoch: 923, train loss: 0.01585, val loss: 0.01646\n",
      "Training epoch: 924, train loss: 0.01604, val loss: 0.01663\n",
      "Training epoch: 925, train loss: 0.01606, val loss: 0.01664\n",
      "Training epoch: 926, train loss: 0.01643, val loss: 0.01694\n",
      "Training epoch: 927, train loss: 0.01617, val loss: 0.01669\n",
      "Training epoch: 928, train loss: 0.01591, val loss: 0.01645\n",
      "Training epoch: 929, train loss: 0.01589, val loss: 0.01646\n",
      "Training epoch: 930, train loss: 0.01627, val loss: 0.01682\n",
      "Training epoch: 931, train loss: 0.01584, val loss: 0.01641\n",
      "Training epoch: 932, train loss: 0.01613, val loss: 0.01669\n",
      "Training epoch: 933, train loss: 0.01596, val loss: 0.01652\n",
      "Training epoch: 934, train loss: 0.01568, val loss: 0.01625\n",
      "Training epoch: 935, train loss: 0.01584, val loss: 0.01642\n",
      "Training epoch: 936, train loss: 0.01582, val loss: 0.01638\n",
      "Training epoch: 937, train loss: 0.01571, val loss: 0.01630\n",
      "Training epoch: 938, train loss: 0.01604, val loss: 0.01660\n",
      "Training epoch: 939, train loss: 0.01609, val loss: 0.01669\n",
      "Training epoch: 940, train loss: 0.01616, val loss: 0.01668\n",
      "Training epoch: 941, train loss: 0.01576, val loss: 0.01630\n",
      "Training epoch: 942, train loss: 0.01633, val loss: 0.01685\n",
      "Training epoch: 943, train loss: 0.01580, val loss: 0.01634\n",
      "Training epoch: 944, train loss: 0.01623, val loss: 0.01683\n",
      "Training epoch: 945, train loss: 0.01583, val loss: 0.01641\n",
      "Training epoch: 946, train loss: 0.01585, val loss: 0.01646\n",
      "Training epoch: 947, train loss: 0.01584, val loss: 0.01643\n",
      "Training epoch: 948, train loss: 0.01587, val loss: 0.01647\n",
      "Training epoch: 949, train loss: 0.01590, val loss: 0.01651\n",
      "Training epoch: 950, train loss: 0.01575, val loss: 0.01631\n",
      "Training epoch: 951, train loss: 0.01594, val loss: 0.01655\n",
      "Training epoch: 952, train loss: 0.01568, val loss: 0.01621\n",
      "Training epoch: 953, train loss: 0.01592, val loss: 0.01648\n",
      "Training epoch: 954, train loss: 0.01567, val loss: 0.01621\n",
      "Training epoch: 955, train loss: 0.01607, val loss: 0.01663\n",
      "Training epoch: 956, train loss: 0.01584, val loss: 0.01641\n",
      "Training epoch: 957, train loss: 0.01597, val loss: 0.01653\n",
      "Training epoch: 958, train loss: 0.01602, val loss: 0.01655\n",
      "Training epoch: 959, train loss: 0.01603, val loss: 0.01654\n",
      "Training epoch: 960, train loss: 0.01644, val loss: 0.01694\n",
      "Training epoch: 961, train loss: 0.01575, val loss: 0.01635\n",
      "Training epoch: 962, train loss: 0.01578, val loss: 0.01636\n",
      "Training epoch: 963, train loss: 0.01577, val loss: 0.01631\n",
      "Training epoch: 964, train loss: 0.01573, val loss: 0.01624\n",
      "Training epoch: 965, train loss: 0.01563, val loss: 0.01621\n",
      "Training epoch: 966, train loss: 0.01568, val loss: 0.01622\n",
      "Training epoch: 967, train loss: 0.01586, val loss: 0.01642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 968, train loss: 0.01583, val loss: 0.01637\n",
      "Training epoch: 969, train loss: 0.01605, val loss: 0.01662\n",
      "Training epoch: 970, train loss: 0.01582, val loss: 0.01642\n",
      "Training epoch: 971, train loss: 0.01573, val loss: 0.01627\n",
      "Training epoch: 972, train loss: 0.01595, val loss: 0.01653\n",
      "Training epoch: 973, train loss: 0.01591, val loss: 0.01647\n",
      "Training epoch: 974, train loss: 0.01595, val loss: 0.01654\n",
      "Training epoch: 975, train loss: 0.01639, val loss: 0.01687\n",
      "Training epoch: 976, train loss: 0.01603, val loss: 0.01655\n",
      "Training epoch: 977, train loss: 0.01631, val loss: 0.01689\n",
      "Training epoch: 978, train loss: 0.01619, val loss: 0.01678\n",
      "Training epoch: 979, train loss: 0.01558, val loss: 0.01616\n",
      "Training epoch: 980, train loss: 0.01592, val loss: 0.01652\n",
      "Training epoch: 981, train loss: 0.01688, val loss: 0.01755\n",
      "Training epoch: 982, train loss: 0.01579, val loss: 0.01629\n",
      "Training epoch: 983, train loss: 0.01581, val loss: 0.01632\n",
      "Training epoch: 984, train loss: 0.01608, val loss: 0.01661\n",
      "Training epoch: 985, train loss: 0.01589, val loss: 0.01640\n",
      "Training epoch: 986, train loss: 0.01576, val loss: 0.01633\n",
      "Training epoch: 987, train loss: 0.01599, val loss: 0.01657\n",
      "Training epoch: 988, train loss: 0.01580, val loss: 0.01636\n",
      "Training epoch: 989, train loss: 0.01566, val loss: 0.01625\n",
      "Training epoch: 990, train loss: 0.01565, val loss: 0.01623\n",
      "Training epoch: 991, train loss: 0.01567, val loss: 0.01619\n",
      "Training epoch: 992, train loss: 0.01589, val loss: 0.01642\n",
      "Training epoch: 993, train loss: 0.01590, val loss: 0.01650\n",
      "Training epoch: 994, train loss: 0.01576, val loss: 0.01628\n",
      "Training epoch: 995, train loss: 0.01587, val loss: 0.01646\n",
      "Training epoch: 996, train loss: 0.01588, val loss: 0.01642\n",
      "Training epoch: 997, train loss: 0.01588, val loss: 0.01640\n",
      "Training epoch: 998, train loss: 0.01588, val loss: 0.01644\n",
      "Training epoch: 999, train loss: 0.01573, val loss: 0.01629\n",
      "Training epoch: 1000, train loss: 0.01727, val loss: 0.01796\n",
      "Training epoch: 1001, train loss: 0.01600, val loss: 0.01656\n",
      "Training epoch: 1002, train loss: 0.01607, val loss: 0.01655\n",
      "Training epoch: 1003, train loss: 0.01703, val loss: 0.01749\n",
      "Training epoch: 1004, train loss: 0.01598, val loss: 0.01655\n",
      "Training epoch: 1005, train loss: 0.01588, val loss: 0.01647\n",
      "Training epoch: 1006, train loss: 0.01737, val loss: 0.01809\n",
      "Training epoch: 1007, train loss: 0.01643, val loss: 0.01708\n",
      "Training epoch: 1008, train loss: 0.01570, val loss: 0.01621\n",
      "Training epoch: 1009, train loss: 0.01578, val loss: 0.01636\n",
      "Training epoch: 1010, train loss: 0.01595, val loss: 0.01653\n",
      "Training epoch: 1011, train loss: 0.01581, val loss: 0.01641\n",
      "Training epoch: 1012, train loss: 0.01581, val loss: 0.01635\n",
      "Training epoch: 1013, train loss: 0.01591, val loss: 0.01642\n",
      "Training epoch: 1014, train loss: 0.01618, val loss: 0.01675\n",
      "Training epoch: 1015, train loss: 0.01604, val loss: 0.01659\n",
      "Training epoch: 1016, train loss: 0.01605, val loss: 0.01660\n",
      "Training epoch: 1017, train loss: 0.01630, val loss: 0.01688\n",
      "Training epoch: 1018, train loss: 0.01573, val loss: 0.01629\n",
      "Training epoch: 1019, train loss: 0.01590, val loss: 0.01646\n",
      "Training epoch: 1020, train loss: 0.01564, val loss: 0.01620\n",
      "Training epoch: 1021, train loss: 0.01598, val loss: 0.01653\n",
      "Training epoch: 1022, train loss: 0.01590, val loss: 0.01642\n",
      "Training epoch: 1023, train loss: 0.01571, val loss: 0.01625\n",
      "Training epoch: 1024, train loss: 0.01586, val loss: 0.01645\n",
      "Training epoch: 1025, train loss: 0.01608, val loss: 0.01672\n",
      "Training epoch: 1026, train loss: 0.01578, val loss: 0.01637\n",
      "Training epoch: 1027, train loss: 0.01567, val loss: 0.01619\n",
      "Training epoch: 1028, train loss: 0.01581, val loss: 0.01639\n",
      "Training epoch: 1029, train loss: 0.01602, val loss: 0.01658\n",
      "Training epoch: 1030, train loss: 0.01614, val loss: 0.01671\n",
      "Training epoch: 1031, train loss: 0.01566, val loss: 0.01618\n",
      "Training epoch: 1032, train loss: 0.01591, val loss: 0.01644\n",
      "Training epoch: 1033, train loss: 0.01565, val loss: 0.01618\n",
      "Training epoch: 1034, train loss: 0.01570, val loss: 0.01621\n",
      "Training epoch: 1035, train loss: 0.01585, val loss: 0.01634\n",
      "Training epoch: 1036, train loss: 0.01621, val loss: 0.01678\n",
      "Training epoch: 1037, train loss: 0.01588, val loss: 0.01640\n",
      "Training epoch: 1038, train loss: 0.01602, val loss: 0.01656\n",
      "Training epoch: 1039, train loss: 0.01574, val loss: 0.01632\n",
      "Training epoch: 1040, train loss: 0.01605, val loss: 0.01660\n",
      "Training epoch: 1041, train loss: 0.01604, val loss: 0.01658\n",
      "Training epoch: 1042, train loss: 0.01705, val loss: 0.01755\n",
      "Training epoch: 1043, train loss: 0.01571, val loss: 0.01622\n",
      "Training epoch: 1044, train loss: 0.01596, val loss: 0.01649\n",
      "Training epoch: 1045, train loss: 0.01572, val loss: 0.01626\n",
      "Training epoch: 1046, train loss: 0.01583, val loss: 0.01642\n",
      "Training epoch: 1047, train loss: 0.01563, val loss: 0.01621\n",
      "Training epoch: 1048, train loss: 0.01590, val loss: 0.01645\n",
      "Training epoch: 1049, train loss: 0.01619, val loss: 0.01676\n",
      "Training epoch: 1050, train loss: 0.01605, val loss: 0.01665\n",
      "Training epoch: 1051, train loss: 0.01636, val loss: 0.01698\n",
      "Training epoch: 1052, train loss: 0.01576, val loss: 0.01628\n",
      "Training epoch: 1053, train loss: 0.01581, val loss: 0.01636\n",
      "Training epoch: 1054, train loss: 0.01607, val loss: 0.01665\n",
      "Training epoch: 1055, train loss: 0.01579, val loss: 0.01634\n",
      "Training epoch: 1056, train loss: 0.01622, val loss: 0.01682\n",
      "Training epoch: 1057, train loss: 0.01609, val loss: 0.01670\n",
      "Training epoch: 1058, train loss: 0.01581, val loss: 0.01637\n",
      "Training epoch: 1059, train loss: 0.01585, val loss: 0.01639\n",
      "Training epoch: 1060, train loss: 0.01582, val loss: 0.01643\n",
      "Training epoch: 1061, train loss: 0.01574, val loss: 0.01631\n",
      "Training epoch: 1062, train loss: 0.01587, val loss: 0.01644\n",
      "Training epoch: 1063, train loss: 0.01565, val loss: 0.01621\n",
      "Training epoch: 1064, train loss: 0.01588, val loss: 0.01646\n",
      "Training epoch: 1065, train loss: 0.01629, val loss: 0.01683\n",
      "Training epoch: 1066, train loss: 0.01596, val loss: 0.01651\n",
      "Training epoch: 1067, train loss: 0.01608, val loss: 0.01662\n",
      "Training epoch: 1068, train loss: 0.01628, val loss: 0.01683\n",
      "Training epoch: 1069, train loss: 0.01587, val loss: 0.01645\n",
      "Training epoch: 1070, train loss: 0.01584, val loss: 0.01635\n",
      "Training epoch: 1071, train loss: 0.01587, val loss: 0.01640\n",
      "Training epoch: 1072, train loss: 0.01630, val loss: 0.01679\n",
      "Training epoch: 1073, train loss: 0.01599, val loss: 0.01651\n",
      "Training epoch: 1074, train loss: 0.01588, val loss: 0.01637\n",
      "Training epoch: 1075, train loss: 0.01630, val loss: 0.01685\n",
      "Training epoch: 1076, train loss: 0.01653, val loss: 0.01707\n",
      "Training epoch: 1077, train loss: 0.01607, val loss: 0.01657\n",
      "Training epoch: 1078, train loss: 0.01568, val loss: 0.01624\n",
      "Training epoch: 1079, train loss: 0.01596, val loss: 0.01653\n",
      "Training epoch: 1080, train loss: 0.01571, val loss: 0.01626\n",
      "Training epoch: 1081, train loss: 0.01571, val loss: 0.01625\n",
      "Training epoch: 1082, train loss: 0.01568, val loss: 0.01621\n",
      "Training epoch: 1083, train loss: 0.01590, val loss: 0.01638\n",
      "Training epoch: 1084, train loss: 0.01573, val loss: 0.01627\n",
      "Training epoch: 1085, train loss: 0.01580, val loss: 0.01640\n",
      "Training epoch: 1086, train loss: 0.01591, val loss: 0.01646\n",
      "Training epoch: 1087, train loss: 0.01559, val loss: 0.01613\n",
      "Training epoch: 1088, train loss: 0.01575, val loss: 0.01629\n",
      "Training epoch: 1089, train loss: 0.01569, val loss: 0.01624\n",
      "Training epoch: 1090, train loss: 0.01607, val loss: 0.01664\n",
      "Training epoch: 1091, train loss: 0.01575, val loss: 0.01629\n",
      "Training epoch: 1092, train loss: 0.01572, val loss: 0.01628\n",
      "Training epoch: 1093, train loss: 0.01613, val loss: 0.01675\n",
      "Training epoch: 1094, train loss: 0.01642, val loss: 0.01703\n",
      "Training epoch: 1095, train loss: 0.01592, val loss: 0.01645\n",
      "Training epoch: 1096, train loss: 0.01664, val loss: 0.01715\n",
      "Training epoch: 1097, train loss: 0.01600, val loss: 0.01656\n",
      "Training epoch: 1098, train loss: 0.01644, val loss: 0.01705\n",
      "Training epoch: 1099, train loss: 0.01583, val loss: 0.01642\n",
      "Training epoch: 1100, train loss: 0.01570, val loss: 0.01624\n",
      "Training epoch: 1101, train loss: 0.01601, val loss: 0.01655\n",
      "Training epoch: 1102, train loss: 0.01614, val loss: 0.01664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1103, train loss: 0.01601, val loss: 0.01651\n",
      "Training epoch: 1104, train loss: 0.01656, val loss: 0.01709\n",
      "Training epoch: 1105, train loss: 0.01599, val loss: 0.01650\n",
      "Training epoch: 1106, train loss: 0.01588, val loss: 0.01642\n",
      "Training epoch: 1107, train loss: 0.01610, val loss: 0.01665\n",
      "Training epoch: 1108, train loss: 0.01575, val loss: 0.01630\n",
      "Training epoch: 1109, train loss: 0.01615, val loss: 0.01668\n",
      "Training epoch: 1110, train loss: 0.01598, val loss: 0.01649\n",
      "Training epoch: 1111, train loss: 0.01568, val loss: 0.01622\n",
      "Training epoch: 1112, train loss: 0.01581, val loss: 0.01635\n",
      "Training epoch: 1113, train loss: 0.01599, val loss: 0.01653\n",
      "Training epoch: 1114, train loss: 0.01572, val loss: 0.01623\n",
      "Training epoch: 1115, train loss: 0.01607, val loss: 0.01665\n",
      "Training epoch: 1116, train loss: 0.01577, val loss: 0.01629\n",
      "Training epoch: 1117, train loss: 0.01588, val loss: 0.01639\n",
      "Training epoch: 1118, train loss: 0.01586, val loss: 0.01641\n",
      "Training epoch: 1119, train loss: 0.01587, val loss: 0.01645\n",
      "Training epoch: 1120, train loss: 0.01562, val loss: 0.01617\n",
      "Training epoch: 1121, train loss: 0.01567, val loss: 0.01622\n",
      "Training epoch: 1122, train loss: 0.01586, val loss: 0.01642\n",
      "Training epoch: 1123, train loss: 0.01563, val loss: 0.01617\n",
      "Training epoch: 1124, train loss: 0.01595, val loss: 0.01650\n",
      "Training epoch: 1125, train loss: 0.01608, val loss: 0.01667\n",
      "Training epoch: 1126, train loss: 0.01585, val loss: 0.01638\n",
      "Training epoch: 1127, train loss: 0.01619, val loss: 0.01668\n",
      "Training epoch: 1128, train loss: 0.01577, val loss: 0.01629\n",
      "Training epoch: 1129, train loss: 0.01617, val loss: 0.01674\n",
      "Training epoch: 1130, train loss: 0.01678, val loss: 0.01739\n",
      "Training epoch: 1131, train loss: 0.01652, val loss: 0.01712\n",
      "Training epoch: 1132, train loss: 0.01579, val loss: 0.01635\n",
      "Training epoch: 1133, train loss: 0.01592, val loss: 0.01645\n",
      "Training epoch: 1134, train loss: 0.01569, val loss: 0.01621\n",
      "Training epoch: 1135, train loss: 0.01596, val loss: 0.01649\n",
      "Training epoch: 1136, train loss: 0.01578, val loss: 0.01634\n",
      "Training epoch: 1137, train loss: 0.01563, val loss: 0.01615\n",
      "Training epoch: 1138, train loss: 0.01583, val loss: 0.01637\n",
      "Training epoch: 1139, train loss: 0.01571, val loss: 0.01622\n",
      "Training epoch: 1140, train loss: 0.01583, val loss: 0.01639\n",
      "Training epoch: 1141, train loss: 0.01587, val loss: 0.01646\n",
      "Training epoch: 1142, train loss: 0.01613, val loss: 0.01659\n",
      "Training epoch: 1143, train loss: 0.01568, val loss: 0.01618\n",
      "Training epoch: 1144, train loss: 0.01657, val loss: 0.01705\n",
      "Training epoch: 1145, train loss: 0.01596, val loss: 0.01652\n",
      "Training epoch: 1146, train loss: 0.01569, val loss: 0.01622\n",
      "Training epoch: 1147, train loss: 0.01565, val loss: 0.01622\n",
      "Training epoch: 1148, train loss: 0.01605, val loss: 0.01659\n",
      "Training epoch: 1149, train loss: 0.01612, val loss: 0.01672\n",
      "Training epoch: 1150, train loss: 0.01604, val loss: 0.01663\n",
      "Training epoch: 1151, train loss: 0.01561, val loss: 0.01612\n",
      "Training epoch: 1152, train loss: 0.01578, val loss: 0.01632\n",
      "Training epoch: 1153, train loss: 0.01562, val loss: 0.01616\n",
      "Training epoch: 1154, train loss: 0.01576, val loss: 0.01628\n",
      "Training epoch: 1155, train loss: 0.01609, val loss: 0.01665\n",
      "Training epoch: 1156, train loss: 0.01567, val loss: 0.01621\n",
      "Training epoch: 1157, train loss: 0.01585, val loss: 0.01635\n",
      "Training epoch: 1158, train loss: 0.01574, val loss: 0.01628\n",
      "Training epoch: 1159, train loss: 0.01660, val loss: 0.01713\n",
      "Training epoch: 1160, train loss: 0.01597, val loss: 0.01653\n",
      "Training epoch: 1161, train loss: 0.01576, val loss: 0.01625\n",
      "Training epoch: 1162, train loss: 0.01586, val loss: 0.01638\n",
      "Training epoch: 1163, train loss: 0.01687, val loss: 0.01747\n",
      "Training epoch: 1164, train loss: 0.01579, val loss: 0.01632\n",
      "Training epoch: 1165, train loss: 0.01648, val loss: 0.01703\n",
      "Training epoch: 1166, train loss: 0.01606, val loss: 0.01654\n",
      "Training epoch: 1167, train loss: 0.01611, val loss: 0.01657\n",
      "Training epoch: 1168, train loss: 0.01630, val loss: 0.01681\n",
      "Training epoch: 1169, train loss: 0.01825, val loss: 0.01870\n",
      "Training epoch: 1170, train loss: 0.01645, val loss: 0.01699\n",
      "Training epoch: 1171, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 1172, train loss: 0.01613, val loss: 0.01663\n",
      "Training epoch: 1173, train loss: 0.01594, val loss: 0.01646\n",
      "Training epoch: 1174, train loss: 0.01584, val loss: 0.01636\n",
      "Training epoch: 1175, train loss: 0.01578, val loss: 0.01627\n",
      "Training epoch: 1176, train loss: 0.01570, val loss: 0.01622\n",
      "Training epoch: 1177, train loss: 0.01563, val loss: 0.01615\n",
      "Training epoch: 1178, train loss: 0.01579, val loss: 0.01635\n",
      "Training epoch: 1179, train loss: 0.01603, val loss: 0.01654\n",
      "Training epoch: 1180, train loss: 0.01562, val loss: 0.01614\n",
      "Training epoch: 1181, train loss: 0.01568, val loss: 0.01619\n",
      "Training epoch: 1182, train loss: 0.01649, val loss: 0.01697\n",
      "Training epoch: 1183, train loss: 0.01567, val loss: 0.01620\n",
      "Training epoch: 1184, train loss: 0.01568, val loss: 0.01619\n",
      "Training epoch: 1185, train loss: 0.01600, val loss: 0.01658\n",
      "Training epoch: 1186, train loss: 0.01576, val loss: 0.01631\n",
      "Training epoch: 1187, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1188, train loss: 0.01574, val loss: 0.01626\n",
      "Training epoch: 1189, train loss: 0.01567, val loss: 0.01620\n",
      "Training epoch: 1190, train loss: 0.01572, val loss: 0.01621\n",
      "Training epoch: 1191, train loss: 0.01589, val loss: 0.01640\n",
      "Training epoch: 1192, train loss: 0.01567, val loss: 0.01617\n",
      "Training epoch: 1193, train loss: 0.01602, val loss: 0.01653\n",
      "Training epoch: 1194, train loss: 0.01653, val loss: 0.01703\n",
      "Training epoch: 1195, train loss: 0.01575, val loss: 0.01622\n",
      "Training epoch: 1196, train loss: 0.01565, val loss: 0.01617\n",
      "Training epoch: 1197, train loss: 0.01612, val loss: 0.01669\n",
      "Training epoch: 1198, train loss: 0.01584, val loss: 0.01636\n",
      "Training epoch: 1199, train loss: 0.01657, val loss: 0.01704\n",
      "Training epoch: 1200, train loss: 0.01571, val loss: 0.01621\n",
      "Training epoch: 1201, train loss: 0.01569, val loss: 0.01620\n",
      "Training epoch: 1202, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1203, train loss: 0.01584, val loss: 0.01639\n",
      "Training epoch: 1204, train loss: 0.01609, val loss: 0.01656\n",
      "Training epoch: 1205, train loss: 0.01626, val loss: 0.01670\n",
      "Training epoch: 1206, train loss: 0.01562, val loss: 0.01614\n",
      "Training epoch: 1207, train loss: 0.01585, val loss: 0.01635\n",
      "Training epoch: 1208, train loss: 0.01643, val loss: 0.01698\n",
      "Training epoch: 1209, train loss: 0.01607, val loss: 0.01657\n",
      "Training epoch: 1210, train loss: 0.01581, val loss: 0.01634\n",
      "Training epoch: 1211, train loss: 0.01598, val loss: 0.01648\n",
      "Training epoch: 1212, train loss: 0.01571, val loss: 0.01624\n",
      "Training epoch: 1213, train loss: 0.01579, val loss: 0.01631\n",
      "Training epoch: 1214, train loss: 0.01568, val loss: 0.01619\n",
      "Training epoch: 1215, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 1216, train loss: 0.01565, val loss: 0.01618\n",
      "Training epoch: 1217, train loss: 0.01582, val loss: 0.01637\n",
      "Training epoch: 1218, train loss: 0.01593, val loss: 0.01647\n",
      "Training epoch: 1219, train loss: 0.01588, val loss: 0.01636\n",
      "Training epoch: 1220, train loss: 0.01574, val loss: 0.01622\n",
      "Training epoch: 1221, train loss: 0.01604, val loss: 0.01657\n",
      "Training epoch: 1222, train loss: 0.01568, val loss: 0.01620\n",
      "Training epoch: 1223, train loss: 0.01614, val loss: 0.01659\n",
      "Training epoch: 1224, train loss: 0.01586, val loss: 0.01639\n",
      "Training epoch: 1225, train loss: 0.01593, val loss: 0.01646\n",
      "Training epoch: 1226, train loss: 0.01617, val loss: 0.01670\n",
      "Training epoch: 1227, train loss: 0.01567, val loss: 0.01618\n",
      "Training epoch: 1228, train loss: 0.01581, val loss: 0.01637\n",
      "Training epoch: 1229, train loss: 0.01645, val loss: 0.01700\n",
      "Training epoch: 1230, train loss: 0.01556, val loss: 0.01607\n",
      "Training epoch: 1231, train loss: 0.01585, val loss: 0.01639\n",
      "Training epoch: 1232, train loss: 0.01624, val loss: 0.01682\n",
      "Training epoch: 1233, train loss: 0.01589, val loss: 0.01638\n",
      "Training epoch: 1234, train loss: 0.01597, val loss: 0.01656\n",
      "Training epoch: 1235, train loss: 0.01600, val loss: 0.01646\n",
      "Training epoch: 1236, train loss: 0.01601, val loss: 0.01651\n",
      "Training epoch: 1237, train loss: 0.01596, val loss: 0.01648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1238, train loss: 0.01623, val loss: 0.01678\n",
      "Training epoch: 1239, train loss: 0.01573, val loss: 0.01629\n",
      "Training epoch: 1240, train loss: 0.01612, val loss: 0.01670\n",
      "Training epoch: 1241, train loss: 0.01642, val loss: 0.01685\n",
      "Training epoch: 1242, train loss: 0.01611, val loss: 0.01657\n",
      "Training epoch: 1243, train loss: 0.01559, val loss: 0.01610\n",
      "Training epoch: 1244, train loss: 0.01588, val loss: 0.01642\n",
      "Training epoch: 1245, train loss: 0.01615, val loss: 0.01672\n",
      "Training epoch: 1246, train loss: 0.01602, val loss: 0.01657\n",
      "Training epoch: 1247, train loss: 0.01646, val loss: 0.01691\n",
      "Training epoch: 1248, train loss: 0.01583, val loss: 0.01635\n",
      "Training epoch: 1249, train loss: 0.01587, val loss: 0.01640\n",
      "Training epoch: 1250, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 1251, train loss: 0.01570, val loss: 0.01622\n",
      "Training epoch: 1252, train loss: 0.01596, val loss: 0.01654\n",
      "Training epoch: 1253, train loss: 0.01562, val loss: 0.01615\n",
      "Training epoch: 1254, train loss: 0.01604, val loss: 0.01654\n",
      "Training epoch: 1255, train loss: 0.01577, val loss: 0.01625\n",
      "Training epoch: 1256, train loss: 0.01571, val loss: 0.01624\n",
      "Training epoch: 1257, train loss: 0.01559, val loss: 0.01608\n",
      "Training epoch: 1258, train loss: 0.01561, val loss: 0.01613\n",
      "Training epoch: 1259, train loss: 0.01614, val loss: 0.01662\n",
      "Training epoch: 1260, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 1261, train loss: 0.01586, val loss: 0.01633\n",
      "Training epoch: 1262, train loss: 0.01573, val loss: 0.01625\n",
      "Training epoch: 1263, train loss: 0.01610, val loss: 0.01660\n",
      "Training epoch: 1264, train loss: 0.01624, val loss: 0.01676\n",
      "Training epoch: 1265, train loss: 0.01605, val loss: 0.01660\n",
      "Training epoch: 1266, train loss: 0.01589, val loss: 0.01642\n",
      "Training epoch: 1267, train loss: 0.01565, val loss: 0.01615\n",
      "Training epoch: 1268, train loss: 0.01577, val loss: 0.01630\n",
      "Training epoch: 1269, train loss: 0.01567, val loss: 0.01621\n",
      "Training epoch: 1270, train loss: 0.01602, val loss: 0.01653\n",
      "Training epoch: 1271, train loss: 0.01613, val loss: 0.01670\n",
      "Training epoch: 1272, train loss: 0.01567, val loss: 0.01617\n",
      "Training epoch: 1273, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 1274, train loss: 0.01570, val loss: 0.01621\n",
      "Training epoch: 1275, train loss: 0.01587, val loss: 0.01635\n",
      "Training epoch: 1276, train loss: 0.01575, val loss: 0.01623\n",
      "Training epoch: 1277, train loss: 0.01588, val loss: 0.01643\n",
      "Training epoch: 1278, train loss: 0.01567, val loss: 0.01620\n",
      "Training epoch: 1279, train loss: 0.01578, val loss: 0.01633\n",
      "Training epoch: 1280, train loss: 0.01610, val loss: 0.01657\n",
      "Training epoch: 1281, train loss: 0.01590, val loss: 0.01636\n",
      "Training epoch: 1282, train loss: 0.01606, val loss: 0.01661\n",
      "Training epoch: 1283, train loss: 0.01568, val loss: 0.01618\n",
      "Training epoch: 1284, train loss: 0.01624, val loss: 0.01678\n",
      "Training epoch: 1285, train loss: 0.01584, val loss: 0.01637\n",
      "Training epoch: 1286, train loss: 0.01583, val loss: 0.01636\n",
      "Training epoch: 1287, train loss: 0.01566, val loss: 0.01618\n",
      "Training epoch: 1288, train loss: 0.01564, val loss: 0.01612\n",
      "Training epoch: 1289, train loss: 0.01561, val loss: 0.01612\n",
      "Training epoch: 1290, train loss: 0.01610, val loss: 0.01664\n",
      "Training epoch: 1291, train loss: 0.01588, val loss: 0.01635\n",
      "Training epoch: 1292, train loss: 0.01583, val loss: 0.01636\n",
      "Training epoch: 1293, train loss: 0.01592, val loss: 0.01643\n",
      "Training epoch: 1294, train loss: 0.01586, val loss: 0.01639\n",
      "Training epoch: 1295, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 1296, train loss: 0.01578, val loss: 0.01633\n",
      "Training epoch: 1297, train loss: 0.01627, val loss: 0.01676\n",
      "Training epoch: 1298, train loss: 0.01584, val loss: 0.01637\n",
      "Training epoch: 1299, train loss: 0.01584, val loss: 0.01634\n",
      "Training epoch: 1300, train loss: 0.01582, val loss: 0.01635\n",
      "Training epoch: 1301, train loss: 0.01618, val loss: 0.01675\n",
      "Training epoch: 1302, train loss: 0.01559, val loss: 0.01608\n",
      "Training epoch: 1303, train loss: 0.01575, val loss: 0.01625\n",
      "Training epoch: 1304, train loss: 0.01595, val loss: 0.01648\n",
      "Training epoch: 1305, train loss: 0.01584, val loss: 0.01640\n",
      "Training epoch: 1306, train loss: 0.01565, val loss: 0.01617\n",
      "Training epoch: 1307, train loss: 0.01590, val loss: 0.01641\n",
      "Training epoch: 1308, train loss: 0.01579, val loss: 0.01628\n",
      "Training epoch: 1309, train loss: 0.01564, val loss: 0.01613\n",
      "Training epoch: 1310, train loss: 0.01580, val loss: 0.01630\n",
      "Training epoch: 1311, train loss: 0.01574, val loss: 0.01624\n",
      "Training epoch: 1312, train loss: 0.01591, val loss: 0.01640\n",
      "Training epoch: 1313, train loss: 0.01648, val loss: 0.01706\n",
      "Training epoch: 1314, train loss: 0.01604, val loss: 0.01662\n",
      "Training epoch: 1315, train loss: 0.01568, val loss: 0.01619\n",
      "Training epoch: 1316, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1317, train loss: 0.01569, val loss: 0.01620\n",
      "Training epoch: 1318, train loss: 0.01574, val loss: 0.01623\n",
      "Training epoch: 1319, train loss: 0.01583, val loss: 0.01638\n",
      "Training epoch: 1320, train loss: 0.01565, val loss: 0.01614\n",
      "Training epoch: 1321, train loss: 0.01587, val loss: 0.01640\n",
      "Training epoch: 1322, train loss: 0.01579, val loss: 0.01628\n",
      "Training epoch: 1323, train loss: 0.01599, val loss: 0.01646\n",
      "Training epoch: 1324, train loss: 0.01571, val loss: 0.01620\n",
      "Training epoch: 1325, train loss: 0.01604, val loss: 0.01659\n",
      "Training epoch: 1326, train loss: 0.01568, val loss: 0.01622\n",
      "Training epoch: 1327, train loss: 0.01602, val loss: 0.01656\n",
      "Training epoch: 1328, train loss: 0.01601, val loss: 0.01655\n",
      "Training epoch: 1329, train loss: 0.01608, val loss: 0.01656\n",
      "Training epoch: 1330, train loss: 0.01570, val loss: 0.01620\n",
      "Training epoch: 1331, train loss: 0.01581, val loss: 0.01629\n",
      "Training epoch: 1332, train loss: 0.01613, val loss: 0.01665\n",
      "Training epoch: 1333, train loss: 0.01582, val loss: 0.01635\n",
      "Training epoch: 1334, train loss: 0.01580, val loss: 0.01633\n",
      "Training epoch: 1335, train loss: 0.01594, val loss: 0.01643\n",
      "Training epoch: 1336, train loss: 0.01583, val loss: 0.01628\n",
      "Training epoch: 1337, train loss: 0.01558, val loss: 0.01609\n",
      "Training epoch: 1338, train loss: 0.01577, val loss: 0.01628\n",
      "Training epoch: 1339, train loss: 0.01588, val loss: 0.01641\n",
      "Training epoch: 1340, train loss: 0.01577, val loss: 0.01629\n",
      "Training epoch: 1341, train loss: 0.01568, val loss: 0.01615\n",
      "Training epoch: 1342, train loss: 0.01590, val loss: 0.01640\n",
      "Training epoch: 1343, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1344, train loss: 0.01574, val loss: 0.01624\n",
      "Training epoch: 1345, train loss: 0.01572, val loss: 0.01622\n",
      "Training epoch: 1346, train loss: 0.01576, val loss: 0.01629\n",
      "Training epoch: 1347, train loss: 0.01567, val loss: 0.01617\n",
      "Training epoch: 1348, train loss: 0.01568, val loss: 0.01618\n",
      "Training epoch: 1349, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 1350, train loss: 0.01563, val loss: 0.01610\n",
      "Training epoch: 1351, train loss: 0.01583, val loss: 0.01634\n",
      "Training epoch: 1352, train loss: 0.01582, val loss: 0.01634\n",
      "Training epoch: 1353, train loss: 0.01678, val loss: 0.01732\n",
      "Training epoch: 1354, train loss: 0.01683, val loss: 0.01741\n",
      "Training epoch: 1355, train loss: 0.01599, val loss: 0.01653\n",
      "Training epoch: 1356, train loss: 0.01613, val loss: 0.01667\n",
      "Training epoch: 1357, train loss: 0.01616, val loss: 0.01670\n",
      "Training epoch: 1358, train loss: 0.01608, val loss: 0.01662\n",
      "Training epoch: 1359, train loss: 0.01599, val loss: 0.01652\n",
      "Training epoch: 1360, train loss: 0.01574, val loss: 0.01626\n",
      "Training epoch: 1361, train loss: 0.01603, val loss: 0.01654\n",
      "Training epoch: 1362, train loss: 0.01584, val loss: 0.01633\n",
      "Training epoch: 1363, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 1364, train loss: 0.01627, val loss: 0.01683\n",
      "Training epoch: 1365, train loss: 0.01573, val loss: 0.01626\n",
      "Training epoch: 1366, train loss: 0.01585, val loss: 0.01638\n",
      "Training epoch: 1367, train loss: 0.01597, val loss: 0.01651\n",
      "Training epoch: 1368, train loss: 0.01593, val loss: 0.01646\n",
      "Training epoch: 1369, train loss: 0.01577, val loss: 0.01627\n",
      "Training epoch: 1370, train loss: 0.01567, val loss: 0.01615\n",
      "Training epoch: 1371, train loss: 0.01597, val loss: 0.01646\n",
      "Training epoch: 1372, train loss: 0.01582, val loss: 0.01628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1373, train loss: 0.01589, val loss: 0.01637\n",
      "Training epoch: 1374, train loss: 0.01604, val loss: 0.01656\n",
      "Training epoch: 1375, train loss: 0.01573, val loss: 0.01625\n",
      "Training epoch: 1376, train loss: 0.01569, val loss: 0.01619\n",
      "Training epoch: 1377, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1378, train loss: 0.01613, val loss: 0.01670\n",
      "Training epoch: 1379, train loss: 0.01586, val loss: 0.01638\n",
      "Training epoch: 1380, train loss: 0.01571, val loss: 0.01622\n",
      "Training epoch: 1381, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1382, train loss: 0.01574, val loss: 0.01625\n",
      "Training epoch: 1383, train loss: 0.01614, val loss: 0.01670\n",
      "Training epoch: 1384, train loss: 0.01578, val loss: 0.01628\n",
      "Training epoch: 1385, train loss: 0.01624, val loss: 0.01676\n",
      "Training epoch: 1386, train loss: 0.01596, val loss: 0.01651\n",
      "Training epoch: 1387, train loss: 0.01582, val loss: 0.01631\n",
      "Training epoch: 1388, train loss: 0.01581, val loss: 0.01627\n",
      "Training epoch: 1389, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 1390, train loss: 0.01610, val loss: 0.01662\n",
      "Training epoch: 1391, train loss: 0.01577, val loss: 0.01630\n",
      "Training epoch: 1392, train loss: 0.01595, val loss: 0.01649\n",
      "Training epoch: 1393, train loss: 0.01603, val loss: 0.01658\n",
      "Training epoch: 1394, train loss: 0.01584, val loss: 0.01634\n",
      "Training epoch: 1395, train loss: 0.01586, val loss: 0.01638\n",
      "Training epoch: 1396, train loss: 0.01605, val loss: 0.01657\n",
      "Training epoch: 1397, train loss: 0.01599, val loss: 0.01651\n",
      "Training epoch: 1398, train loss: 0.01576, val loss: 0.01625\n",
      "Training epoch: 1399, train loss: 0.01617, val loss: 0.01675\n",
      "Training epoch: 1400, train loss: 0.01569, val loss: 0.01620\n",
      "Training epoch: 1401, train loss: 0.01574, val loss: 0.01623\n",
      "Training epoch: 1402, train loss: 0.01580, val loss: 0.01630\n",
      "Training epoch: 1403, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1404, train loss: 0.01562, val loss: 0.01613\n",
      "Training epoch: 1405, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 1406, train loss: 0.01579, val loss: 0.01628\n",
      "Training epoch: 1407, train loss: 0.01564, val loss: 0.01614\n",
      "Training epoch: 1408, train loss: 0.01644, val loss: 0.01695\n",
      "Training epoch: 1409, train loss: 0.01605, val loss: 0.01649\n",
      "Training epoch: 1410, train loss: 0.01573, val loss: 0.01620\n",
      "Training epoch: 1411, train loss: 0.01583, val loss: 0.01628\n",
      "Training epoch: 1412, train loss: 0.01555, val loss: 0.01606\n",
      "Training epoch: 1413, train loss: 0.01601, val loss: 0.01655\n",
      "Training epoch: 1414, train loss: 0.01590, val loss: 0.01642\n",
      "Training epoch: 1415, train loss: 0.01613, val loss: 0.01665\n",
      "Training epoch: 1416, train loss: 0.01598, val loss: 0.01643\n",
      "Training epoch: 1417, train loss: 0.01600, val loss: 0.01648\n",
      "Training epoch: 1418, train loss: 0.01653, val loss: 0.01702\n",
      "Training epoch: 1419, train loss: 0.01570, val loss: 0.01618\n",
      "Training epoch: 1420, train loss: 0.01591, val loss: 0.01643\n",
      "Training epoch: 1421, train loss: 0.01575, val loss: 0.01620\n",
      "Training epoch: 1422, train loss: 0.01564, val loss: 0.01615\n",
      "Training epoch: 1423, train loss: 0.01561, val loss: 0.01615\n",
      "Training epoch: 1424, train loss: 0.01561, val loss: 0.01613\n",
      "Training epoch: 1425, train loss: 0.01580, val loss: 0.01632\n",
      "Training epoch: 1426, train loss: 0.01604, val loss: 0.01657\n",
      "Training epoch: 1427, train loss: 0.01599, val loss: 0.01653\n",
      "Training epoch: 1428, train loss: 0.01629, val loss: 0.01676\n",
      "Training epoch: 1429, train loss: 0.01619, val loss: 0.01667\n",
      "Training epoch: 1430, train loss: 0.01620, val loss: 0.01672\n",
      "Training epoch: 1431, train loss: 0.01611, val loss: 0.01659\n",
      "Training epoch: 1432, train loss: 0.01560, val loss: 0.01611\n",
      "Training epoch: 1433, train loss: 0.01593, val loss: 0.01645\n",
      "Training epoch: 1434, train loss: 0.01557, val loss: 0.01607\n",
      "Training epoch: 1435, train loss: 0.01613, val loss: 0.01660\n",
      "Training epoch: 1436, train loss: 0.01622, val loss: 0.01674\n",
      "Training epoch: 1437, train loss: 0.01600, val loss: 0.01653\n",
      "Training epoch: 1438, train loss: 0.01588, val loss: 0.01639\n",
      "Training epoch: 1439, train loss: 0.01578, val loss: 0.01629\n",
      "Training epoch: 1440, train loss: 0.01578, val loss: 0.01628\n",
      "Training epoch: 1441, train loss: 0.01583, val loss: 0.01634\n",
      "Training epoch: 1442, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 1443, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 1444, train loss: 0.01615, val loss: 0.01667\n",
      "Training epoch: 1445, train loss: 0.01583, val loss: 0.01634\n",
      "Training epoch: 1446, train loss: 0.01626, val loss: 0.01673\n",
      "Training epoch: 1447, train loss: 0.01599, val loss: 0.01643\n",
      "Training epoch: 1448, train loss: 0.01629, val loss: 0.01675\n",
      "Training epoch: 1449, train loss: 0.01577, val loss: 0.01627\n",
      "Training epoch: 1450, train loss: 0.01725, val loss: 0.01778\n",
      "Training epoch: 1451, train loss: 0.01660, val loss: 0.01712\n",
      "Training epoch: 1452, train loss: 0.01581, val loss: 0.01629\n",
      "Training epoch: 1453, train loss: 0.01571, val loss: 0.01618\n",
      "Training epoch: 1454, train loss: 0.01600, val loss: 0.01649\n",
      "Training epoch: 1455, train loss: 0.01586, val loss: 0.01631\n",
      "Training epoch: 1456, train loss: 0.01560, val loss: 0.01610\n",
      "Training epoch: 1457, train loss: 0.01570, val loss: 0.01618\n",
      "Training epoch: 1458, train loss: 0.01571, val loss: 0.01620\n",
      "Training epoch: 1459, train loss: 0.01652, val loss: 0.01699\n",
      "Training epoch: 1460, train loss: 0.01627, val loss: 0.01678\n",
      "Training epoch: 1461, train loss: 0.01587, val loss: 0.01634\n",
      "Training epoch: 1462, train loss: 0.01585, val loss: 0.01631\n",
      "Training epoch: 1463, train loss: 0.01580, val loss: 0.01633\n",
      "Training epoch: 1464, train loss: 0.01643, val loss: 0.01689\n",
      "Training epoch: 1465, train loss: 0.01612, val loss: 0.01664\n",
      "Training epoch: 1466, train loss: 0.01655, val loss: 0.01709\n",
      "Training epoch: 1467, train loss: 0.01572, val loss: 0.01619\n",
      "Training epoch: 1468, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1469, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 1470, train loss: 0.01569, val loss: 0.01616\n",
      "Training epoch: 1471, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1472, train loss: 0.01598, val loss: 0.01649\n",
      "Training epoch: 1473, train loss: 0.01572, val loss: 0.01622\n",
      "Training epoch: 1474, train loss: 0.01662, val loss: 0.01714\n",
      "Training epoch: 1475, train loss: 0.01633, val loss: 0.01686\n",
      "Training epoch: 1476, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 1477, train loss: 0.01570, val loss: 0.01620\n",
      "Training epoch: 1478, train loss: 0.01556, val loss: 0.01601\n",
      "Training epoch: 1479, train loss: 0.01602, val loss: 0.01647\n",
      "Training epoch: 1480, train loss: 0.01594, val loss: 0.01642\n",
      "Training epoch: 1481, train loss: 0.01619, val loss: 0.01675\n",
      "Training epoch: 1482, train loss: 0.01569, val loss: 0.01621\n",
      "Training epoch: 1483, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1484, train loss: 0.01595, val loss: 0.01643\n",
      "Training epoch: 1485, train loss: 0.01564, val loss: 0.01610\n",
      "Training epoch: 1486, train loss: 0.01652, val loss: 0.01706\n",
      "Training epoch: 1487, train loss: 0.01584, val loss: 0.01634\n",
      "Training epoch: 1488, train loss: 0.01595, val loss: 0.01648\n",
      "Training epoch: 1489, train loss: 0.01653, val loss: 0.01709\n",
      "Training epoch: 1490, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 1491, train loss: 0.01605, val loss: 0.01649\n",
      "Training epoch: 1492, train loss: 0.01559, val loss: 0.01608\n",
      "Training epoch: 1493, train loss: 0.01588, val loss: 0.01638\n",
      "Training epoch: 1494, train loss: 0.01572, val loss: 0.01618\n",
      "Training epoch: 1495, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 1496, train loss: 0.01592, val loss: 0.01643\n",
      "Training epoch: 1497, train loss: 0.01601, val loss: 0.01648\n",
      "Training epoch: 1498, train loss: 0.01604, val loss: 0.01649\n",
      "Training epoch: 1499, train loss: 0.01600, val loss: 0.01651\n",
      "Training epoch: 1500, train loss: 0.01604, val loss: 0.01656\n",
      "Training epoch: 1501, train loss: 0.01567, val loss: 0.01616\n",
      "Training epoch: 1502, train loss: 0.01595, val loss: 0.01643\n",
      "Training epoch: 1503, train loss: 0.01576, val loss: 0.01625\n",
      "Training epoch: 1504, train loss: 0.01574, val loss: 0.01623\n",
      "Training epoch: 1505, train loss: 0.01590, val loss: 0.01638\n",
      "Training epoch: 1506, train loss: 0.01575, val loss: 0.01624\n",
      "Training epoch: 1507, train loss: 0.01595, val loss: 0.01644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1508, train loss: 0.01567, val loss: 0.01618\n",
      "Training epoch: 1509, train loss: 0.01568, val loss: 0.01618\n",
      "Training epoch: 1510, train loss: 0.01575, val loss: 0.01621\n",
      "Training epoch: 1511, train loss: 0.01567, val loss: 0.01616\n",
      "Training epoch: 1512, train loss: 0.01609, val loss: 0.01656\n",
      "Training epoch: 1513, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1514, train loss: 0.01582, val loss: 0.01630\n",
      "Training epoch: 1515, train loss: 0.01623, val loss: 0.01676\n",
      "Training epoch: 1516, train loss: 0.01577, val loss: 0.01626\n",
      "Training epoch: 1517, train loss: 0.01563, val loss: 0.01609\n",
      "Training epoch: 1518, train loss: 0.01603, val loss: 0.01652\n",
      "Training epoch: 1519, train loss: 0.01575, val loss: 0.01623\n",
      "Training epoch: 1520, train loss: 0.01582, val loss: 0.01629\n",
      "Training epoch: 1521, train loss: 0.01576, val loss: 0.01626\n",
      "Training epoch: 1522, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 1523, train loss: 0.01597, val loss: 0.01638\n",
      "Training epoch: 1524, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1525, train loss: 0.01604, val loss: 0.01653\n",
      "Training epoch: 1526, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 1527, train loss: 0.01634, val loss: 0.01677\n",
      "Training epoch: 1528, train loss: 0.01582, val loss: 0.01631\n",
      "Training epoch: 1529, train loss: 0.01601, val loss: 0.01649\n",
      "Training epoch: 1530, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1531, train loss: 0.01575, val loss: 0.01624\n",
      "Training epoch: 1532, train loss: 0.01604, val loss: 0.01654\n",
      "Training epoch: 1533, train loss: 0.01592, val loss: 0.01641\n",
      "Training epoch: 1534, train loss: 0.01619, val loss: 0.01669\n",
      "Training epoch: 1535, train loss: 0.01595, val loss: 0.01642\n",
      "Training epoch: 1536, train loss: 0.01579, val loss: 0.01630\n",
      "Training epoch: 1537, train loss: 0.01571, val loss: 0.01615\n",
      "Training epoch: 1538, train loss: 0.01595, val loss: 0.01644\n",
      "Training epoch: 1539, train loss: 0.01612, val loss: 0.01659\n",
      "Training epoch: 1540, train loss: 0.01580, val loss: 0.01628\n",
      "Training epoch: 1541, train loss: 0.01643, val loss: 0.01697\n",
      "Training epoch: 1542, train loss: 0.01567, val loss: 0.01616\n",
      "Training epoch: 1543, train loss: 0.01561, val loss: 0.01606\n",
      "Training epoch: 1544, train loss: 0.01597, val loss: 0.01645\n",
      "Training epoch: 1545, train loss: 0.01604, val loss: 0.01656\n",
      "Training epoch: 1546, train loss: 0.01787, val loss: 0.01842\n",
      "Training epoch: 1547, train loss: 0.01581, val loss: 0.01630\n",
      "Training epoch: 1548, train loss: 0.01571, val loss: 0.01617\n",
      "Training epoch: 1549, train loss: 0.01590, val loss: 0.01642\n",
      "Training epoch: 1550, train loss: 0.01576, val loss: 0.01628\n",
      "Training epoch: 1551, train loss: 0.01585, val loss: 0.01634\n",
      "Training epoch: 1552, train loss: 0.01599, val loss: 0.01651\n",
      "Training epoch: 1553, train loss: 0.01594, val loss: 0.01638\n",
      "Training epoch: 1554, train loss: 0.01582, val loss: 0.01631\n",
      "Training epoch: 1555, train loss: 0.01581, val loss: 0.01627\n",
      "Training epoch: 1556, train loss: 0.01588, val loss: 0.01637\n",
      "Training epoch: 1557, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1558, train loss: 0.01610, val loss: 0.01653\n",
      "Training epoch: 1559, train loss: 0.01571, val loss: 0.01617\n",
      "Training epoch: 1560, train loss: 0.01566, val loss: 0.01616\n",
      "Training epoch: 1561, train loss: 0.01608, val loss: 0.01660\n",
      "Training epoch: 1562, train loss: 0.01595, val loss: 0.01644\n",
      "Training epoch: 1563, train loss: 0.01690, val loss: 0.01744\n",
      "Training epoch: 1564, train loss: 0.01607, val loss: 0.01658\n",
      "Training epoch: 1565, train loss: 0.01567, val loss: 0.01617\n",
      "Training epoch: 1566, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 1567, train loss: 0.01564, val loss: 0.01612\n",
      "Training epoch: 1568, train loss: 0.01601, val loss: 0.01647\n",
      "Training epoch: 1569, train loss: 0.01590, val loss: 0.01638\n",
      "Training epoch: 1570, train loss: 0.01579, val loss: 0.01624\n",
      "Training epoch: 1571, train loss: 0.01588, val loss: 0.01635\n",
      "Training epoch: 1572, train loss: 0.01587, val loss: 0.01637\n",
      "Training epoch: 1573, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 1574, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1575, train loss: 0.01584, val loss: 0.01635\n",
      "Training epoch: 1576, train loss: 0.01654, val loss: 0.01705\n",
      "Training epoch: 1577, train loss: 0.01585, val loss: 0.01636\n",
      "Training epoch: 1578, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 1579, train loss: 0.01582, val loss: 0.01629\n",
      "Training epoch: 1580, train loss: 0.01591, val loss: 0.01641\n",
      "Training epoch: 1581, train loss: 0.01590, val loss: 0.01637\n",
      "Training epoch: 1582, train loss: 0.01576, val loss: 0.01623\n",
      "Training epoch: 1583, train loss: 0.01568, val loss: 0.01614\n",
      "Training epoch: 1584, train loss: 0.01613, val loss: 0.01664\n",
      "Training epoch: 1585, train loss: 0.01598, val loss: 0.01648\n",
      "Training epoch: 1586, train loss: 0.01583, val loss: 0.01629\n",
      "Training epoch: 1587, train loss: 0.01587, val loss: 0.01637\n",
      "Training epoch: 1588, train loss: 0.01560, val loss: 0.01610\n",
      "Training epoch: 1589, train loss: 0.01561, val loss: 0.01606\n",
      "Training epoch: 1590, train loss: 0.01576, val loss: 0.01623\n",
      "Training epoch: 1591, train loss: 0.01610, val loss: 0.01659\n",
      "Training epoch: 1592, train loss: 0.01564, val loss: 0.01615\n",
      "Training epoch: 1593, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 1594, train loss: 0.01625, val loss: 0.01670\n",
      "Training epoch: 1595, train loss: 0.01682, val loss: 0.01731\n",
      "Training epoch: 1596, train loss: 0.01668, val loss: 0.01718\n",
      "Training epoch: 1597, train loss: 0.01582, val loss: 0.01629\n",
      "Training epoch: 1598, train loss: 0.01575, val loss: 0.01623\n",
      "Training epoch: 1599, train loss: 0.01581, val loss: 0.01624\n",
      "Training epoch: 1600, train loss: 0.01599, val loss: 0.01648\n",
      "Training epoch: 1601, train loss: 0.01563, val loss: 0.01613\n",
      "Training epoch: 1602, train loss: 0.01585, val loss: 0.01634\n",
      "Training epoch: 1603, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 1604, train loss: 0.01628, val loss: 0.01679\n",
      "Training epoch: 1605, train loss: 0.01607, val loss: 0.01660\n",
      "Training epoch: 1606, train loss: 0.01561, val loss: 0.01605\n",
      "Training epoch: 1607, train loss: 0.01568, val loss: 0.01613\n",
      "Training epoch: 1608, train loss: 0.01603, val loss: 0.01650\n",
      "Training epoch: 1609, train loss: 0.01579, val loss: 0.01628\n",
      "Training epoch: 1610, train loss: 0.01588, val loss: 0.01634\n",
      "Training epoch: 1611, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 1612, train loss: 0.01559, val loss: 0.01602\n",
      "Training epoch: 1613, train loss: 0.01592, val loss: 0.01639\n",
      "Training epoch: 1614, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 1615, train loss: 0.01577, val loss: 0.01624\n",
      "Training epoch: 1616, train loss: 0.01610, val loss: 0.01661\n",
      "Training epoch: 1617, train loss: 0.01595, val loss: 0.01642\n",
      "Training epoch: 1618, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 1619, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 1620, train loss: 0.01587, val loss: 0.01635\n",
      "Training epoch: 1621, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 1622, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 1623, train loss: 0.01614, val loss: 0.01664\n",
      "Training epoch: 1624, train loss: 0.01563, val loss: 0.01610\n",
      "Training epoch: 1625, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 1626, train loss: 0.01596, val loss: 0.01643\n",
      "Training epoch: 1627, train loss: 0.01572, val loss: 0.01622\n",
      "Training epoch: 1628, train loss: 0.01563, val loss: 0.01610\n",
      "Training epoch: 1629, train loss: 0.01574, val loss: 0.01618\n",
      "Training epoch: 1630, train loss: 0.01603, val loss: 0.01652\n",
      "Training epoch: 1631, train loss: 0.01572, val loss: 0.01620\n",
      "Training epoch: 1632, train loss: 0.01595, val loss: 0.01646\n",
      "Training epoch: 1633, train loss: 0.01568, val loss: 0.01615\n",
      "Training epoch: 1634, train loss: 0.01645, val loss: 0.01694\n",
      "Training epoch: 1635, train loss: 0.01594, val loss: 0.01643\n",
      "Training epoch: 1636, train loss: 0.01580, val loss: 0.01631\n",
      "Training epoch: 1637, train loss: 0.01597, val loss: 0.01650\n",
      "Training epoch: 1638, train loss: 0.01587, val loss: 0.01634\n",
      "Training epoch: 1639, train loss: 0.01568, val loss: 0.01615\n",
      "Training epoch: 1640, train loss: 0.01565, val loss: 0.01614\n",
      "Training epoch: 1641, train loss: 0.01567, val loss: 0.01615\n",
      "Training epoch: 1642, train loss: 0.01567, val loss: 0.01609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1643, train loss: 0.01577, val loss: 0.01622\n",
      "Training epoch: 1644, train loss: 0.01589, val loss: 0.01637\n",
      "Training epoch: 1645, train loss: 0.01672, val loss: 0.01722\n",
      "Training epoch: 1646, train loss: 0.01631, val loss: 0.01680\n",
      "Training epoch: 1647, train loss: 0.01621, val loss: 0.01667\n",
      "Training epoch: 1648, train loss: 0.01568, val loss: 0.01613\n",
      "Training epoch: 1649, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1650, train loss: 0.01623, val loss: 0.01671\n",
      "Training epoch: 1651, train loss: 0.01612, val loss: 0.01660\n",
      "Training epoch: 1652, train loss: 0.01577, val loss: 0.01624\n",
      "Training epoch: 1653, train loss: 0.01666, val loss: 0.01704\n",
      "Training epoch: 1654, train loss: 0.01604, val loss: 0.01649\n",
      "Training epoch: 1655, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 1656, train loss: 0.01584, val loss: 0.01632\n",
      "Training epoch: 1657, train loss: 0.01584, val loss: 0.01634\n",
      "Training epoch: 1658, train loss: 0.01567, val loss: 0.01614\n",
      "Training epoch: 1659, train loss: 0.01570, val loss: 0.01618\n",
      "Training epoch: 1660, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 1661, train loss: 0.01580, val loss: 0.01628\n",
      "Training epoch: 1662, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 1663, train loss: 0.01594, val loss: 0.01641\n",
      "Training epoch: 1664, train loss: 0.01581, val loss: 0.01630\n",
      "Training epoch: 1665, train loss: 0.01596, val loss: 0.01644\n",
      "Training epoch: 1666, train loss: 0.01622, val loss: 0.01671\n",
      "Training epoch: 1667, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 1668, train loss: 0.01598, val loss: 0.01649\n",
      "Training epoch: 1669, train loss: 0.01577, val loss: 0.01623\n",
      "Training epoch: 1670, train loss: 0.01584, val loss: 0.01630\n",
      "Training epoch: 1671, train loss: 0.01575, val loss: 0.01622\n",
      "Training epoch: 1672, train loss: 0.01615, val loss: 0.01664\n",
      "Training epoch: 1673, train loss: 0.01580, val loss: 0.01627\n",
      "Training epoch: 1674, train loss: 0.01555, val loss: 0.01600\n",
      "Training epoch: 1675, train loss: 0.01640, val loss: 0.01690\n",
      "Training epoch: 1676, train loss: 0.01567, val loss: 0.01614\n",
      "Training epoch: 1677, train loss: 0.01579, val loss: 0.01627\n",
      "Training epoch: 1678, train loss: 0.01585, val loss: 0.01631\n",
      "Training epoch: 1679, train loss: 0.01595, val loss: 0.01643\n",
      "Training epoch: 1680, train loss: 0.01606, val loss: 0.01654\n",
      "Training epoch: 1681, train loss: 0.01633, val loss: 0.01679\n",
      "Training epoch: 1682, train loss: 0.01647, val loss: 0.01691\n",
      "Training epoch: 1683, train loss: 0.01595, val loss: 0.01642\n",
      "Training epoch: 1684, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 1685, train loss: 0.01570, val loss: 0.01618\n",
      "Training epoch: 1686, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1687, train loss: 0.01562, val loss: 0.01607\n",
      "Training epoch: 1688, train loss: 0.01577, val loss: 0.01624\n",
      "Training epoch: 1689, train loss: 0.01564, val loss: 0.01608\n",
      "Training epoch: 1690, train loss: 0.01632, val loss: 0.01680\n",
      "Training epoch: 1691, train loss: 0.01581, val loss: 0.01625\n",
      "Training epoch: 1692, train loss: 0.01579, val loss: 0.01623\n",
      "Training epoch: 1693, train loss: 0.01584, val loss: 0.01630\n",
      "Training epoch: 1694, train loss: 0.01569, val loss: 0.01614\n",
      "Training epoch: 1695, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 1696, train loss: 0.01602, val loss: 0.01651\n",
      "Training epoch: 1697, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 1698, train loss: 0.01567, val loss: 0.01617\n",
      "Training epoch: 1699, train loss: 0.01619, val loss: 0.01672\n",
      "Training epoch: 1700, train loss: 0.01601, val loss: 0.01647\n",
      "Training epoch: 1701, train loss: 0.01601, val loss: 0.01647\n",
      "Training epoch: 1702, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 1703, train loss: 0.01577, val loss: 0.01627\n",
      "Training epoch: 1704, train loss: 0.01634, val loss: 0.01688\n",
      "Training epoch: 1705, train loss: 0.01635, val loss: 0.01688\n",
      "Training epoch: 1706, train loss: 0.01572, val loss: 0.01620\n",
      "Training epoch: 1707, train loss: 0.01619, val loss: 0.01668\n",
      "Training epoch: 1708, train loss: 0.01618, val loss: 0.01666\n",
      "Training epoch: 1709, train loss: 0.01573, val loss: 0.01619\n",
      "Training epoch: 1710, train loss: 0.01621, val loss: 0.01671\n",
      "Training epoch: 1711, train loss: 0.01644, val loss: 0.01692\n",
      "Training epoch: 1712, train loss: 0.01562, val loss: 0.01610\n",
      "Training epoch: 1713, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1714, train loss: 0.01581, val loss: 0.01630\n",
      "Training epoch: 1715, train loss: 0.01590, val loss: 0.01636\n",
      "Training epoch: 1716, train loss: 0.01590, val loss: 0.01636\n",
      "Training epoch: 1717, train loss: 0.01565, val loss: 0.01613\n",
      "Training epoch: 1718, train loss: 0.01559, val loss: 0.01604\n",
      "Training epoch: 1719, train loss: 0.01572, val loss: 0.01619\n",
      "Training epoch: 1720, train loss: 0.01555, val loss: 0.01600\n",
      "Training epoch: 1721, train loss: 0.01567, val loss: 0.01612\n",
      "Training epoch: 1722, train loss: 0.01590, val loss: 0.01637\n",
      "Training epoch: 1723, train loss: 0.01567, val loss: 0.01612\n",
      "Training epoch: 1724, train loss: 0.01583, val loss: 0.01632\n",
      "Training epoch: 1725, train loss: 0.01590, val loss: 0.01639\n",
      "Training epoch: 1726, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1727, train loss: 0.01564, val loss: 0.01608\n",
      "Training epoch: 1728, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1729, train loss: 0.01582, val loss: 0.01629\n",
      "Training epoch: 1730, train loss: 0.01569, val loss: 0.01616\n",
      "Training epoch: 1731, train loss: 0.01568, val loss: 0.01613\n",
      "Training epoch: 1732, train loss: 0.01562, val loss: 0.01608\n",
      "Training epoch: 1733, train loss: 0.01566, val loss: 0.01610\n",
      "Training epoch: 1734, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1735, train loss: 0.01566, val loss: 0.01612\n",
      "Training epoch: 1736, train loss: 0.01628, val loss: 0.01680\n",
      "Training epoch: 1737, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1738, train loss: 0.01610, val loss: 0.01656\n",
      "Training epoch: 1739, train loss: 0.01574, val loss: 0.01618\n",
      "Training epoch: 1740, train loss: 0.01563, val loss: 0.01607\n",
      "Training epoch: 1741, train loss: 0.01564, val loss: 0.01609\n",
      "Training epoch: 1742, train loss: 0.01561, val loss: 0.01608\n",
      "Training epoch: 1743, train loss: 0.01566, val loss: 0.01611\n",
      "Training epoch: 1744, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1745, train loss: 0.01592, val loss: 0.01637\n",
      "Training epoch: 1746, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1747, train loss: 0.01639, val loss: 0.01689\n",
      "Training epoch: 1748, train loss: 0.01563, val loss: 0.01609\n",
      "Training epoch: 1749, train loss: 0.01692, val loss: 0.01729\n",
      "Training epoch: 1750, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 1751, train loss: 0.01568, val loss: 0.01613\n",
      "Training epoch: 1752, train loss: 0.01607, val loss: 0.01653\n",
      "Training epoch: 1753, train loss: 0.01627, val loss: 0.01670\n",
      "Training epoch: 1754, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 1755, train loss: 0.01588, val loss: 0.01634\n",
      "Training epoch: 1756, train loss: 0.01606, val loss: 0.01654\n",
      "Training epoch: 1757, train loss: 0.01590, val loss: 0.01635\n",
      "Training epoch: 1758, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 1759, train loss: 0.01565, val loss: 0.01611\n",
      "Training epoch: 1760, train loss: 0.01561, val loss: 0.01605\n",
      "Training epoch: 1761, train loss: 0.01585, val loss: 0.01632\n",
      "Training epoch: 1762, train loss: 0.01614, val loss: 0.01662\n",
      "Training epoch: 1763, train loss: 0.01596, val loss: 0.01644\n",
      "Training epoch: 1764, train loss: 0.01596, val loss: 0.01642\n",
      "Training epoch: 1765, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 1766, train loss: 0.01564, val loss: 0.01608\n",
      "Training epoch: 1767, train loss: 0.01606, val loss: 0.01653\n",
      "Training epoch: 1768, train loss: 0.01557, val loss: 0.01600\n",
      "Training epoch: 1769, train loss: 0.01579, val loss: 0.01627\n",
      "Training epoch: 1770, train loss: 0.01572, val loss: 0.01620\n",
      "Training epoch: 1771, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 1772, train loss: 0.01590, val loss: 0.01635\n",
      "Training epoch: 1773, train loss: 0.01559, val loss: 0.01603\n",
      "Training epoch: 1774, train loss: 0.01559, val loss: 0.01608\n",
      "Training epoch: 1775, train loss: 0.01573, val loss: 0.01620\n",
      "Training epoch: 1776, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1777, train loss: 0.01560, val loss: 0.01606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1778, train loss: 0.01569, val loss: 0.01615\n",
      "Training epoch: 1779, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1780, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 1781, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 1782, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1783, train loss: 0.01574, val loss: 0.01618\n",
      "Training epoch: 1784, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 1785, train loss: 0.01600, val loss: 0.01642\n",
      "Training epoch: 1786, train loss: 0.01578, val loss: 0.01625\n",
      "Training epoch: 1787, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 1788, train loss: 0.01580, val loss: 0.01626\n",
      "Training epoch: 1789, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1790, train loss: 0.01602, val loss: 0.01654\n",
      "Training epoch: 1791, train loss: 0.01598, val loss: 0.01646\n",
      "Training epoch: 1792, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 1793, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 1794, train loss: 0.01578, val loss: 0.01620\n",
      "Training epoch: 1795, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 1796, train loss: 0.01574, val loss: 0.01619\n",
      "Training epoch: 1797, train loss: 0.01566, val loss: 0.01612\n",
      "Training epoch: 1798, train loss: 0.01569, val loss: 0.01614\n",
      "Training epoch: 1799, train loss: 0.01624, val loss: 0.01673\n",
      "Training epoch: 1800, train loss: 0.01615, val loss: 0.01662\n",
      "Training epoch: 1801, train loss: 0.01560, val loss: 0.01605\n",
      "Training epoch: 1802, train loss: 0.01602, val loss: 0.01646\n",
      "Training epoch: 1803, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 1804, train loss: 0.01561, val loss: 0.01605\n",
      "Training epoch: 1805, train loss: 0.01563, val loss: 0.01609\n",
      "Training epoch: 1806, train loss: 0.01578, val loss: 0.01627\n",
      "Training epoch: 1807, train loss: 0.01568, val loss: 0.01613\n",
      "Training epoch: 1808, train loss: 0.01585, val loss: 0.01630\n",
      "Training epoch: 1809, train loss: 0.01571, val loss: 0.01617\n",
      "Training epoch: 1810, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 1811, train loss: 0.01572, val loss: 0.01618\n",
      "Training epoch: 1812, train loss: 0.01578, val loss: 0.01625\n",
      "Training epoch: 1813, train loss: 0.01585, val loss: 0.01629\n",
      "Training epoch: 1814, train loss: 0.01598, val loss: 0.01645\n",
      "Training epoch: 1815, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 1816, train loss: 0.01548, val loss: 0.01591\n",
      "Training epoch: 1817, train loss: 0.01589, val loss: 0.01633\n",
      "Training epoch: 1818, train loss: 0.01609, val loss: 0.01656\n",
      "Training epoch: 1819, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1820, train loss: 0.01577, val loss: 0.01626\n",
      "Training epoch: 1821, train loss: 0.01564, val loss: 0.01608\n",
      "Training epoch: 1822, train loss: 0.01656, val loss: 0.01699\n",
      "Training epoch: 1823, train loss: 0.01604, val loss: 0.01649\n",
      "Training epoch: 1824, train loss: 0.01567, val loss: 0.01609\n",
      "Training epoch: 1825, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 1826, train loss: 0.01620, val loss: 0.01669\n",
      "Training epoch: 1827, train loss: 0.01572, val loss: 0.01619\n",
      "Training epoch: 1828, train loss: 0.01551, val loss: 0.01595\n",
      "Training epoch: 1829, train loss: 0.01569, val loss: 0.01616\n",
      "Training epoch: 1830, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 1831, train loss: 0.01628, val loss: 0.01677\n",
      "Training epoch: 1832, train loss: 0.01577, val loss: 0.01622\n",
      "Training epoch: 1833, train loss: 0.01551, val loss: 0.01592\n",
      "Training epoch: 1834, train loss: 0.01566, val loss: 0.01613\n",
      "Training epoch: 1835, train loss: 0.01592, val loss: 0.01637\n",
      "Training epoch: 1836, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 1837, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 1838, train loss: 0.01582, val loss: 0.01628\n",
      "Training epoch: 1839, train loss: 0.01581, val loss: 0.01625\n",
      "Training epoch: 1840, train loss: 0.01573, val loss: 0.01619\n",
      "Training epoch: 1841, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 1842, train loss: 0.01565, val loss: 0.01608\n",
      "Training epoch: 1843, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 1844, train loss: 0.01596, val loss: 0.01639\n",
      "Training epoch: 1845, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 1846, train loss: 0.01631, val loss: 0.01680\n",
      "Training epoch: 1847, train loss: 0.01577, val loss: 0.01620\n",
      "Training epoch: 1848, train loss: 0.01568, val loss: 0.01613\n",
      "Training epoch: 1849, train loss: 0.01578, val loss: 0.01621\n",
      "Training epoch: 1850, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 1851, train loss: 0.01594, val loss: 0.01640\n",
      "Training epoch: 1852, train loss: 0.01594, val loss: 0.01639\n",
      "Training epoch: 1853, train loss: 0.01572, val loss: 0.01618\n",
      "Training epoch: 1854, train loss: 0.01569, val loss: 0.01614\n",
      "Training epoch: 1855, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1856, train loss: 0.01631, val loss: 0.01679\n",
      "Training epoch: 1857, train loss: 0.01571, val loss: 0.01617\n",
      "Training epoch: 1858, train loss: 0.01583, val loss: 0.01627\n",
      "Training epoch: 1859, train loss: 0.01577, val loss: 0.01624\n",
      "Training epoch: 1860, train loss: 0.01552, val loss: 0.01595\n",
      "Training epoch: 1861, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 1862, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 1863, train loss: 0.01603, val loss: 0.01650\n",
      "Training epoch: 1864, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1865, train loss: 0.01582, val loss: 0.01627\n",
      "Training epoch: 1866, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 1867, train loss: 0.01572, val loss: 0.01621\n",
      "Training epoch: 1868, train loss: 0.01570, val loss: 0.01613\n",
      "Training epoch: 1869, train loss: 0.01564, val loss: 0.01605\n",
      "Training epoch: 1870, train loss: 0.01608, val loss: 0.01653\n",
      "Training epoch: 1871, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 1872, train loss: 0.01627, val loss: 0.01669\n",
      "Training epoch: 1873, train loss: 0.01564, val loss: 0.01609\n",
      "Training epoch: 1874, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 1875, train loss: 0.01572, val loss: 0.01618\n",
      "Training epoch: 1876, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1877, train loss: 0.01608, val loss: 0.01654\n",
      "Training epoch: 1878, train loss: 0.01554, val loss: 0.01599\n",
      "Training epoch: 1879, train loss: 0.01566, val loss: 0.01612\n",
      "Training epoch: 1880, train loss: 0.01567, val loss: 0.01613\n",
      "Training epoch: 1881, train loss: 0.01558, val loss: 0.01601\n",
      "Training epoch: 1882, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 1883, train loss: 0.01559, val loss: 0.01601\n",
      "Training epoch: 1884, train loss: 0.01632, val loss: 0.01677\n",
      "Training epoch: 1885, train loss: 0.01563, val loss: 0.01609\n",
      "Training epoch: 1886, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 1887, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 1888, train loss: 0.01576, val loss: 0.01621\n",
      "Training epoch: 1889, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 1890, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 1891, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1892, train loss: 0.01573, val loss: 0.01616\n",
      "Training epoch: 1893, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 1894, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1895, train loss: 0.01579, val loss: 0.01623\n",
      "Training epoch: 1896, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 1897, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 1898, train loss: 0.01585, val loss: 0.01630\n",
      "Training epoch: 1899, train loss: 0.01583, val loss: 0.01627\n",
      "Training epoch: 1900, train loss: 0.01601, val loss: 0.01640\n",
      "Training epoch: 1901, train loss: 0.01573, val loss: 0.01618\n",
      "Training epoch: 1902, train loss: 0.01556, val loss: 0.01598\n",
      "Training epoch: 1903, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 1904, train loss: 0.01571, val loss: 0.01615\n",
      "Training epoch: 1905, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 1906, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 1907, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 1908, train loss: 0.01557, val loss: 0.01603\n",
      "Training epoch: 1909, train loss: 0.01587, val loss: 0.01630\n",
      "Training epoch: 1910, train loss: 0.01593, val loss: 0.01639\n",
      "Training epoch: 1911, train loss: 0.01571, val loss: 0.01617\n",
      "Training epoch: 1912, train loss: 0.01549, val loss: 0.01594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1913, train loss: 0.01569, val loss: 0.01612\n",
      "Training epoch: 1914, train loss: 0.01596, val loss: 0.01644\n",
      "Training epoch: 1915, train loss: 0.01562, val loss: 0.01606\n",
      "Training epoch: 1916, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1917, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 1918, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 1919, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 1920, train loss: 0.01563, val loss: 0.01610\n",
      "Training epoch: 1921, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1922, train loss: 0.01572, val loss: 0.01615\n",
      "Training epoch: 1923, train loss: 0.01541, val loss: 0.01583\n",
      "Training epoch: 1924, train loss: 0.01577, val loss: 0.01621\n",
      "Training epoch: 1925, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 1926, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1927, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 1928, train loss: 0.01606, val loss: 0.01651\n",
      "Training epoch: 1929, train loss: 0.01669, val loss: 0.01716\n",
      "Training epoch: 1930, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 1931, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 1932, train loss: 0.01633, val loss: 0.01681\n",
      "Training epoch: 1933, train loss: 0.01619, val loss: 0.01666\n",
      "Training epoch: 1934, train loss: 0.01581, val loss: 0.01624\n",
      "Training epoch: 1935, train loss: 0.01618, val loss: 0.01662\n",
      "Training epoch: 1936, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 1937, train loss: 0.01556, val loss: 0.01601\n",
      "Training epoch: 1938, train loss: 0.01562, val loss: 0.01604\n",
      "Training epoch: 1939, train loss: 0.01584, val loss: 0.01624\n",
      "Training epoch: 1940, train loss: 0.01635, val loss: 0.01676\n",
      "Training epoch: 1941, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 1942, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 1943, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 1944, train loss: 0.01579, val loss: 0.01623\n",
      "Training epoch: 1945, train loss: 0.01609, val loss: 0.01656\n",
      "Training epoch: 1946, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 1947, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 1948, train loss: 0.01603, val loss: 0.01646\n",
      "Training epoch: 1949, train loss: 0.01569, val loss: 0.01615\n",
      "Training epoch: 1950, train loss: 0.01630, val loss: 0.01674\n",
      "Training epoch: 1951, train loss: 0.01576, val loss: 0.01619\n",
      "Training epoch: 1952, train loss: 0.01560, val loss: 0.01605\n",
      "Training epoch: 1953, train loss: 0.01612, val loss: 0.01655\n",
      "Training epoch: 1954, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 1955, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 1956, train loss: 0.01576, val loss: 0.01620\n",
      "Training epoch: 1957, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 1958, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 1959, train loss: 0.01658, val loss: 0.01704\n",
      "Training epoch: 1960, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 1961, train loss: 0.01569, val loss: 0.01614\n",
      "Training epoch: 1962, train loss: 0.01571, val loss: 0.01614\n",
      "Training epoch: 1963, train loss: 0.01551, val loss: 0.01593\n",
      "Training epoch: 1964, train loss: 0.01564, val loss: 0.01610\n",
      "Training epoch: 1965, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 1966, train loss: 0.01568, val loss: 0.01610\n",
      "Training epoch: 1967, train loss: 0.01565, val loss: 0.01609\n",
      "Training epoch: 1968, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 1969, train loss: 0.01600, val loss: 0.01644\n",
      "Training epoch: 1970, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 1971, train loss: 0.01571, val loss: 0.01612\n",
      "Training epoch: 1972, train loss: 0.01565, val loss: 0.01610\n",
      "Training epoch: 1973, train loss: 0.01579, val loss: 0.01620\n",
      "Training epoch: 1974, train loss: 0.01563, val loss: 0.01607\n",
      "Training epoch: 1975, train loss: 0.01556, val loss: 0.01601\n",
      "Training epoch: 1976, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 1977, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 1978, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 1979, train loss: 0.01566, val loss: 0.01606\n",
      "Training epoch: 1980, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 1981, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1982, train loss: 0.01565, val loss: 0.01608\n",
      "Training epoch: 1983, train loss: 0.01563, val loss: 0.01604\n",
      "Training epoch: 1984, train loss: 0.01574, val loss: 0.01611\n",
      "Training epoch: 1985, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 1986, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 1987, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 1988, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 1989, train loss: 0.01582, val loss: 0.01625\n",
      "Training epoch: 1990, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 1991, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 1992, train loss: 0.01549, val loss: 0.01591\n",
      "Training epoch: 1993, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 1994, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 1995, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 1996, train loss: 0.01585, val loss: 0.01627\n",
      "Training epoch: 1997, train loss: 0.01666, val loss: 0.01712\n",
      "Training epoch: 1998, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 1999, train loss: 0.01617, val loss: 0.01661\n",
      "Training epoch: 2000, train loss: 0.01562, val loss: 0.01607\n",
      "Training epoch: 2001, train loss: 0.01614, val loss: 0.01661\n",
      "Training epoch: 2002, train loss: 0.01595, val loss: 0.01637\n",
      "Training epoch: 2003, train loss: 0.01561, val loss: 0.01608\n",
      "Training epoch: 2004, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 2005, train loss: 0.01615, val loss: 0.01658\n",
      "Training epoch: 2006, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 2007, train loss: 0.01566, val loss: 0.01610\n",
      "Training epoch: 2008, train loss: 0.01561, val loss: 0.01604\n",
      "Training epoch: 2009, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2010, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 2011, train loss: 0.01558, val loss: 0.01601\n",
      "Training epoch: 2012, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 2013, train loss: 0.01557, val loss: 0.01600\n",
      "Training epoch: 2014, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 2015, train loss: 0.01557, val loss: 0.01600\n",
      "Training epoch: 2016, train loss: 0.01550, val loss: 0.01594\n",
      "Training epoch: 2017, train loss: 0.01567, val loss: 0.01613\n",
      "Training epoch: 2018, train loss: 0.01556, val loss: 0.01599\n",
      "Training epoch: 2019, train loss: 0.01565, val loss: 0.01609\n",
      "Training epoch: 2020, train loss: 0.01625, val loss: 0.01669\n",
      "Training epoch: 2021, train loss: 0.01561, val loss: 0.01605\n",
      "Training epoch: 2022, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 2023, train loss: 0.01587, val loss: 0.01634\n",
      "Training epoch: 2024, train loss: 0.01600, val loss: 0.01648\n",
      "Training epoch: 2025, train loss: 0.01553, val loss: 0.01597\n",
      "Training epoch: 2026, train loss: 0.01572, val loss: 0.01618\n",
      "Training epoch: 2027, train loss: 0.01596, val loss: 0.01635\n",
      "Training epoch: 2028, train loss: 0.01568, val loss: 0.01610\n",
      "Training epoch: 2029, train loss: 0.01558, val loss: 0.01601\n",
      "Training epoch: 2030, train loss: 0.01645, val loss: 0.01691\n",
      "Training epoch: 2031, train loss: 0.01583, val loss: 0.01628\n",
      "Training epoch: 2032, train loss: 0.01599, val loss: 0.01643\n",
      "Training epoch: 2033, train loss: 0.01575, val loss: 0.01619\n",
      "Training epoch: 2034, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 2035, train loss: 0.01576, val loss: 0.01619\n",
      "Training epoch: 2036, train loss: 0.01605, val loss: 0.01648\n",
      "Training epoch: 2037, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2038, train loss: 0.01583, val loss: 0.01627\n",
      "Training epoch: 2039, train loss: 0.01572, val loss: 0.01614\n",
      "Training epoch: 2040, train loss: 0.01576, val loss: 0.01621\n",
      "Training epoch: 2041, train loss: 0.01574, val loss: 0.01619\n",
      "Training epoch: 2042, train loss: 0.01601, val loss: 0.01646\n",
      "Training epoch: 2043, train loss: 0.01569, val loss: 0.01612\n",
      "Training epoch: 2044, train loss: 0.01582, val loss: 0.01628\n",
      "Training epoch: 2045, train loss: 0.01576, val loss: 0.01618\n",
      "Training epoch: 2046, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2047, train loss: 0.01552, val loss: 0.01594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2048, train loss: 0.01573, val loss: 0.01615\n",
      "Training epoch: 2049, train loss: 0.01542, val loss: 0.01586\n",
      "Training epoch: 2050, train loss: 0.01632, val loss: 0.01677\n",
      "Training epoch: 2051, train loss: 0.01576, val loss: 0.01618\n",
      "Training epoch: 2052, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 2053, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 2054, train loss: 0.01548, val loss: 0.01592\n",
      "Training epoch: 2055, train loss: 0.01565, val loss: 0.01609\n",
      "Training epoch: 2056, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2057, train loss: 0.01554, val loss: 0.01596\n",
      "Training epoch: 2058, train loss: 0.01566, val loss: 0.01609\n",
      "Training epoch: 2059, train loss: 0.01596, val loss: 0.01641\n",
      "Training epoch: 2060, train loss: 0.01597, val loss: 0.01636\n",
      "Training epoch: 2061, train loss: 0.01565, val loss: 0.01608\n",
      "Training epoch: 2062, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 2063, train loss: 0.01567, val loss: 0.01609\n",
      "Training epoch: 2064, train loss: 0.01565, val loss: 0.01606\n",
      "Training epoch: 2065, train loss: 0.01574, val loss: 0.01615\n",
      "Training epoch: 2066, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2067, train loss: 0.01551, val loss: 0.01592\n",
      "Training epoch: 2068, train loss: 0.01581, val loss: 0.01624\n",
      "Training epoch: 2069, train loss: 0.01550, val loss: 0.01594\n",
      "Training epoch: 2070, train loss: 0.01559, val loss: 0.01602\n",
      "Training epoch: 2071, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 2072, train loss: 0.01548, val loss: 0.01592\n",
      "Training epoch: 2073, train loss: 0.01560, val loss: 0.01604\n",
      "Training epoch: 2074, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 2075, train loss: 0.01551, val loss: 0.01595\n",
      "Training epoch: 2076, train loss: 0.01577, val loss: 0.01617\n",
      "Training epoch: 2077, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 2078, train loss: 0.01601, val loss: 0.01643\n",
      "Training epoch: 2079, train loss: 0.01577, val loss: 0.01624\n",
      "Training epoch: 2080, train loss: 0.01584, val loss: 0.01630\n",
      "Training epoch: 2081, train loss: 0.01609, val loss: 0.01656\n",
      "Training epoch: 2082, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2083, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 2084, train loss: 0.01563, val loss: 0.01607\n",
      "Training epoch: 2085, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 2086, train loss: 0.01601, val loss: 0.01649\n",
      "Training epoch: 2087, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 2088, train loss: 0.01599, val loss: 0.01640\n",
      "Training epoch: 2089, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 2090, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 2091, train loss: 0.01568, val loss: 0.01612\n",
      "Training epoch: 2092, train loss: 0.01581, val loss: 0.01625\n",
      "Training epoch: 2093, train loss: 0.01599, val loss: 0.01644\n",
      "Training epoch: 2094, train loss: 0.01546, val loss: 0.01590\n",
      "Training epoch: 2095, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2096, train loss: 0.01556, val loss: 0.01597\n",
      "Training epoch: 2097, train loss: 0.01561, val loss: 0.01604\n",
      "Training epoch: 2098, train loss: 0.01581, val loss: 0.01622\n",
      "Training epoch: 2099, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 2100, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 2101, train loss: 0.01567, val loss: 0.01607\n",
      "Training epoch: 2102, train loss: 0.01598, val loss: 0.01644\n",
      "Training epoch: 2103, train loss: 0.01549, val loss: 0.01592\n",
      "Training epoch: 2104, train loss: 0.01563, val loss: 0.01603\n",
      "Training epoch: 2105, train loss: 0.01603, val loss: 0.01648\n",
      "Training epoch: 2106, train loss: 0.01555, val loss: 0.01598\n",
      "Training epoch: 2107, train loss: 0.01572, val loss: 0.01616\n",
      "Training epoch: 2108, train loss: 0.01577, val loss: 0.01617\n",
      "Training epoch: 2109, train loss: 0.01681, val loss: 0.01726\n",
      "Training epoch: 2110, train loss: 0.01569, val loss: 0.01610\n",
      "Training epoch: 2111, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 2112, train loss: 0.01592, val loss: 0.01632\n",
      "Training epoch: 2113, train loss: 0.01590, val loss: 0.01636\n",
      "Training epoch: 2114, train loss: 0.01564, val loss: 0.01606\n",
      "Training epoch: 2115, train loss: 0.01600, val loss: 0.01643\n",
      "Training epoch: 2116, train loss: 0.01613, val loss: 0.01654\n",
      "Training epoch: 2117, train loss: 0.01593, val loss: 0.01637\n",
      "Training epoch: 2118, train loss: 0.01578, val loss: 0.01617\n",
      "Training epoch: 2119, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2120, train loss: 0.01648, val loss: 0.01693\n",
      "Training epoch: 2121, train loss: 0.01569, val loss: 0.01611\n",
      "Training epoch: 2122, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 2123, train loss: 0.01551, val loss: 0.01592\n",
      "Training epoch: 2124, train loss: 0.01567, val loss: 0.01611\n",
      "Training epoch: 2125, train loss: 0.01585, val loss: 0.01627\n",
      "Training epoch: 2126, train loss: 0.01596, val loss: 0.01641\n",
      "Training epoch: 2127, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2128, train loss: 0.01560, val loss: 0.01601\n",
      "Training epoch: 2129, train loss: 0.01545, val loss: 0.01587\n",
      "Training epoch: 2130, train loss: 0.01597, val loss: 0.01642\n",
      "Training epoch: 2131, train loss: 0.01568, val loss: 0.01612\n",
      "Training epoch: 2132, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2133, train loss: 0.01567, val loss: 0.01608\n",
      "Training epoch: 2134, train loss: 0.01566, val loss: 0.01609\n",
      "Training epoch: 2135, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 2136, train loss: 0.01578, val loss: 0.01622\n",
      "Training epoch: 2137, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 2138, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 2139, train loss: 0.01588, val loss: 0.01630\n",
      "Training epoch: 2140, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2141, train loss: 0.01579, val loss: 0.01618\n",
      "Training epoch: 2142, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 2143, train loss: 0.01574, val loss: 0.01614\n",
      "Training epoch: 2144, train loss: 0.01591, val loss: 0.01633\n",
      "Training epoch: 2145, train loss: 0.01552, val loss: 0.01595\n",
      "Training epoch: 2146, train loss: 0.01577, val loss: 0.01620\n",
      "Training epoch: 2147, train loss: 0.01559, val loss: 0.01597\n",
      "Training epoch: 2148, train loss: 0.01638, val loss: 0.01682\n",
      "Training epoch: 2149, train loss: 0.01556, val loss: 0.01595\n",
      "Training epoch: 2150, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2151, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 2152, train loss: 0.01541, val loss: 0.01585\n",
      "Training epoch: 2153, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 2154, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2155, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 2156, train loss: 0.01561, val loss: 0.01605\n",
      "Training epoch: 2157, train loss: 0.01575, val loss: 0.01619\n",
      "Training epoch: 2158, train loss: 0.01637, val loss: 0.01683\n",
      "Training epoch: 2159, train loss: 0.01581, val loss: 0.01623\n",
      "Training epoch: 2160, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 2161, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 2162, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 2163, train loss: 0.01569, val loss: 0.01610\n",
      "Training epoch: 2164, train loss: 0.01624, val loss: 0.01670\n",
      "Training epoch: 2165, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 2166, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 2167, train loss: 0.01595, val loss: 0.01641\n",
      "Training epoch: 2168, train loss: 0.01552, val loss: 0.01595\n",
      "Training epoch: 2169, train loss: 0.01573, val loss: 0.01616\n",
      "Training epoch: 2170, train loss: 0.01560, val loss: 0.01599\n",
      "Training epoch: 2171, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 2172, train loss: 0.01571, val loss: 0.01613\n",
      "Training epoch: 2173, train loss: 0.01552, val loss: 0.01594\n",
      "Training epoch: 2174, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 2175, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 2176, train loss: 0.01571, val loss: 0.01613\n",
      "Training epoch: 2177, train loss: 0.01618, val loss: 0.01663\n",
      "Training epoch: 2178, train loss: 0.01628, val loss: 0.01672\n",
      "Training epoch: 2179, train loss: 0.01608, val loss: 0.01653\n",
      "Training epoch: 2180, train loss: 0.01553, val loss: 0.01597\n",
      "Training epoch: 2181, train loss: 0.01553, val loss: 0.01596\n",
      "Training epoch: 2182, train loss: 0.01551, val loss: 0.01593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2183, train loss: 0.01550, val loss: 0.01594\n",
      "Training epoch: 2184, train loss: 0.01563, val loss: 0.01604\n",
      "Training epoch: 2185, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 2186, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2187, train loss: 0.01534, val loss: 0.01576\n",
      "Training epoch: 2188, train loss: 0.01568, val loss: 0.01613\n",
      "Training epoch: 2189, train loss: 0.01565, val loss: 0.01606\n",
      "Training epoch: 2190, train loss: 0.01580, val loss: 0.01622\n",
      "Training epoch: 2191, train loss: 0.01565, val loss: 0.01608\n",
      "Training epoch: 2192, train loss: 0.01567, val loss: 0.01604\n",
      "Training epoch: 2193, train loss: 0.01590, val loss: 0.01633\n",
      "Training epoch: 2194, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 2195, train loss: 0.01598, val loss: 0.01641\n",
      "Training epoch: 2196, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 2197, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 2198, train loss: 0.01542, val loss: 0.01583\n",
      "Training epoch: 2199, train loss: 0.01547, val loss: 0.01590\n",
      "Training epoch: 2200, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 2201, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 2202, train loss: 0.01558, val loss: 0.01599\n",
      "Training epoch: 2203, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 2204, train loss: 0.01560, val loss: 0.01599\n",
      "Training epoch: 2205, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 2206, train loss: 0.01580, val loss: 0.01625\n",
      "Training epoch: 2207, train loss: 0.01597, val loss: 0.01639\n",
      "Training epoch: 2208, train loss: 0.01563, val loss: 0.01604\n",
      "Training epoch: 2209, train loss: 0.01587, val loss: 0.01630\n",
      "Training epoch: 2210, train loss: 0.01548, val loss: 0.01592\n",
      "Training epoch: 2211, train loss: 0.01581, val loss: 0.01623\n",
      "Training epoch: 2212, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 2213, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 2214, train loss: 0.01566, val loss: 0.01608\n",
      "Training epoch: 2215, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 2216, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 2217, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2218, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 2219, train loss: 0.01568, val loss: 0.01611\n",
      "Training epoch: 2220, train loss: 0.01626, val loss: 0.01671\n",
      "Training epoch: 2221, train loss: 0.01566, val loss: 0.01611\n",
      "Training epoch: 2222, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 2223, train loss: 0.01587, val loss: 0.01628\n",
      "Training epoch: 2224, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2225, train loss: 0.01570, val loss: 0.01614\n",
      "Training epoch: 2226, train loss: 0.01577, val loss: 0.01619\n",
      "Training epoch: 2227, train loss: 0.01550, val loss: 0.01589\n",
      "Training epoch: 2228, train loss: 0.01572, val loss: 0.01615\n",
      "Training epoch: 2229, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 2230, train loss: 0.01548, val loss: 0.01592\n",
      "Training epoch: 2231, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 2232, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 2233, train loss: 0.01567, val loss: 0.01609\n",
      "Training epoch: 2234, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2235, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 2236, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 2237, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2238, train loss: 0.01553, val loss: 0.01593\n",
      "Training epoch: 2239, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 2240, train loss: 0.01577, val loss: 0.01616\n",
      "Training epoch: 2241, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 2242, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 2243, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 2244, train loss: 0.01640, val loss: 0.01682\n",
      "Training epoch: 2245, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 2246, train loss: 0.01545, val loss: 0.01587\n",
      "Training epoch: 2247, train loss: 0.01585, val loss: 0.01623\n",
      "Training epoch: 2248, train loss: 0.01568, val loss: 0.01611\n",
      "Training epoch: 2249, train loss: 0.01558, val loss: 0.01601\n",
      "Training epoch: 2250, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 2251, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 2252, train loss: 0.01616, val loss: 0.01658\n",
      "Training epoch: 2253, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 2254, train loss: 0.01598, val loss: 0.01644\n",
      "Training epoch: 2255, train loss: 0.01576, val loss: 0.01620\n",
      "Training epoch: 2256, train loss: 0.01577, val loss: 0.01621\n",
      "Training epoch: 2257, train loss: 0.01587, val loss: 0.01628\n",
      "Training epoch: 2258, train loss: 0.01559, val loss: 0.01601\n",
      "Training epoch: 2259, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 2260, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 2261, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 2262, train loss: 0.01574, val loss: 0.01616\n",
      "Training epoch: 2263, train loss: 0.01565, val loss: 0.01609\n",
      "Training epoch: 2264, train loss: 0.01606, val loss: 0.01647\n",
      "Training epoch: 2265, train loss: 0.01587, val loss: 0.01629\n",
      "Training epoch: 2266, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 2267, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 2268, train loss: 0.01574, val loss: 0.01613\n",
      "Training epoch: 2269, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 2270, train loss: 0.01568, val loss: 0.01607\n",
      "Training epoch: 2271, train loss: 0.01575, val loss: 0.01614\n",
      "Training epoch: 2272, train loss: 0.01563, val loss: 0.01604\n",
      "Training epoch: 2273, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 2274, train loss: 0.01580, val loss: 0.01622\n",
      "Training epoch: 2275, train loss: 0.01608, val loss: 0.01653\n",
      "Training epoch: 2276, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2277, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 2278, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 2279, train loss: 0.01556, val loss: 0.01599\n",
      "Training epoch: 2280, train loss: 0.01582, val loss: 0.01624\n",
      "Training epoch: 2281, train loss: 0.01574, val loss: 0.01614\n",
      "Training epoch: 2282, train loss: 0.01566, val loss: 0.01605\n",
      "Training epoch: 2283, train loss: 0.01541, val loss: 0.01583\n",
      "Training epoch: 2284, train loss: 0.01554, val loss: 0.01596\n",
      "Training epoch: 2285, train loss: 0.01567, val loss: 0.01611\n",
      "Training epoch: 2286, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 2287, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 2288, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 2289, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 2290, train loss: 0.01552, val loss: 0.01594\n",
      "Training epoch: 2291, train loss: 0.01557, val loss: 0.01594\n",
      "Training epoch: 2292, train loss: 0.01575, val loss: 0.01615\n",
      "Training epoch: 2293, train loss: 0.01555, val loss: 0.01596\n",
      "Training epoch: 2294, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 2295, train loss: 0.01586, val loss: 0.01629\n",
      "Training epoch: 2296, train loss: 0.01579, val loss: 0.01620\n",
      "Training epoch: 2297, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2298, train loss: 0.01563, val loss: 0.01603\n",
      "Training epoch: 2299, train loss: 0.01598, val loss: 0.01641\n",
      "Training epoch: 2300, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 2301, train loss: 0.01577, val loss: 0.01620\n",
      "Training epoch: 2302, train loss: 0.01557, val loss: 0.01600\n",
      "Training epoch: 2303, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 2304, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2305, train loss: 0.01549, val loss: 0.01591\n",
      "Training epoch: 2306, train loss: 0.01557, val loss: 0.01601\n",
      "Training epoch: 2307, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 2308, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 2309, train loss: 0.01585, val loss: 0.01625\n",
      "Training epoch: 2310, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 2311, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 2312, train loss: 0.01565, val loss: 0.01608\n",
      "Training epoch: 2313, train loss: 0.01566, val loss: 0.01610\n",
      "Training epoch: 2314, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 2315, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2316, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2317, train loss: 0.01551, val loss: 0.01593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2318, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2319, train loss: 0.01557, val loss: 0.01599\n",
      "Training epoch: 2320, train loss: 0.01599, val loss: 0.01638\n",
      "Training epoch: 2321, train loss: 0.01574, val loss: 0.01615\n",
      "Training epoch: 2322, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 2323, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 2324, train loss: 0.01557, val loss: 0.01598\n",
      "Training epoch: 2325, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 2326, train loss: 0.01592, val loss: 0.01632\n",
      "Training epoch: 2327, train loss: 0.01564, val loss: 0.01606\n",
      "Training epoch: 2328, train loss: 0.01604, val loss: 0.01645\n",
      "Training epoch: 2329, train loss: 0.01577, val loss: 0.01619\n",
      "Training epoch: 2330, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 2331, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 2332, train loss: 0.01644, val loss: 0.01687\n",
      "Training epoch: 2333, train loss: 0.01574, val loss: 0.01614\n",
      "Training epoch: 2334, train loss: 0.01599, val loss: 0.01636\n",
      "Training epoch: 2335, train loss: 0.01598, val loss: 0.01638\n",
      "Training epoch: 2336, train loss: 0.01571, val loss: 0.01607\n",
      "Training epoch: 2337, train loss: 0.01565, val loss: 0.01609\n",
      "Training epoch: 2338, train loss: 0.01563, val loss: 0.01603\n",
      "Training epoch: 2339, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2340, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 2341, train loss: 0.01596, val loss: 0.01635\n",
      "Training epoch: 2342, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 2343, train loss: 0.01564, val loss: 0.01607\n",
      "Training epoch: 2344, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2345, train loss: 0.01559, val loss: 0.01602\n",
      "Training epoch: 2346, train loss: 0.01605, val loss: 0.01649\n",
      "Training epoch: 2347, train loss: 0.01573, val loss: 0.01613\n",
      "Training epoch: 2348, train loss: 0.01553, val loss: 0.01593\n",
      "Training epoch: 2349, train loss: 0.01646, val loss: 0.01693\n",
      "Training epoch: 2350, train loss: 0.01598, val loss: 0.01641\n",
      "Training epoch: 2351, train loss: 0.01601, val loss: 0.01638\n",
      "Training epoch: 2352, train loss: 0.01567, val loss: 0.01608\n",
      "Training epoch: 2353, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2354, train loss: 0.01565, val loss: 0.01607\n",
      "Training epoch: 2355, train loss: 0.01577, val loss: 0.01622\n",
      "Training epoch: 2356, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 2357, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 2358, train loss: 0.01538, val loss: 0.01579\n",
      "Training epoch: 2359, train loss: 0.01556, val loss: 0.01599\n",
      "Training epoch: 2360, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 2361, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 2362, train loss: 0.01576, val loss: 0.01615\n",
      "Training epoch: 2363, train loss: 0.01575, val loss: 0.01613\n",
      "Training epoch: 2364, train loss: 0.01577, val loss: 0.01616\n",
      "Training epoch: 2365, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 2366, train loss: 0.01553, val loss: 0.01594\n",
      "Training epoch: 2367, train loss: 0.01575, val loss: 0.01614\n",
      "Training epoch: 2368, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 2369, train loss: 0.01550, val loss: 0.01589\n",
      "Training epoch: 2370, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 2371, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 2372, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 2373, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 2374, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 2375, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2376, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 2377, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 2378, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 2379, train loss: 0.01566, val loss: 0.01606\n",
      "Training epoch: 2380, train loss: 0.01570, val loss: 0.01609\n",
      "Training epoch: 2381, train loss: 0.01640, val loss: 0.01685\n",
      "Training epoch: 2382, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 2383, train loss: 0.01625, val loss: 0.01669\n",
      "Training epoch: 2384, train loss: 0.01593, val loss: 0.01634\n",
      "Training epoch: 2385, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 2386, train loss: 0.01628, val loss: 0.01666\n",
      "Training epoch: 2387, train loss: 0.01552, val loss: 0.01595\n",
      "Training epoch: 2388, train loss: 0.01573, val loss: 0.01610\n",
      "Training epoch: 2389, train loss: 0.01635, val loss: 0.01678\n",
      "Training epoch: 2390, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 2391, train loss: 0.01575, val loss: 0.01617\n",
      "Training epoch: 2392, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2393, train loss: 0.01546, val loss: 0.01589\n",
      "Training epoch: 2394, train loss: 0.01558, val loss: 0.01594\n",
      "Training epoch: 2395, train loss: 0.01560, val loss: 0.01598\n",
      "Training epoch: 2396, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 2397, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 2398, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 2399, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 2400, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2401, train loss: 0.01546, val loss: 0.01590\n",
      "Training epoch: 2402, train loss: 0.01630, val loss: 0.01672\n",
      "Training epoch: 2403, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2404, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2405, train loss: 0.01557, val loss: 0.01598\n",
      "Training epoch: 2406, train loss: 0.01590, val loss: 0.01633\n",
      "Training epoch: 2407, train loss: 0.01584, val loss: 0.01625\n",
      "Training epoch: 2408, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2409, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 2410, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2411, train loss: 0.01591, val loss: 0.01630\n",
      "Training epoch: 2412, train loss: 0.01599, val loss: 0.01642\n",
      "Training epoch: 2413, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2414, train loss: 0.01562, val loss: 0.01600\n",
      "Training epoch: 2415, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2416, train loss: 0.01558, val loss: 0.01599\n",
      "Training epoch: 2417, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2418, train loss: 0.01557, val loss: 0.01601\n",
      "Training epoch: 2419, train loss: 0.01560, val loss: 0.01603\n",
      "Training epoch: 2420, train loss: 0.01553, val loss: 0.01593\n",
      "Training epoch: 2421, train loss: 0.01554, val loss: 0.01594\n",
      "Training epoch: 2422, train loss: 0.01571, val loss: 0.01615\n",
      "Training epoch: 2423, train loss: 0.01572, val loss: 0.01616\n",
      "Training epoch: 2424, train loss: 0.01575, val loss: 0.01615\n",
      "Training epoch: 2425, train loss: 0.01640, val loss: 0.01683\n",
      "Training epoch: 2426, train loss: 0.01570, val loss: 0.01608\n",
      "Training epoch: 2427, train loss: 0.01560, val loss: 0.01601\n",
      "Training epoch: 2428, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 2429, train loss: 0.01585, val loss: 0.01628\n",
      "Training epoch: 2430, train loss: 0.01556, val loss: 0.01597\n",
      "Training epoch: 2431, train loss: 0.01538, val loss: 0.01579\n",
      "Training epoch: 2432, train loss: 0.01584, val loss: 0.01626\n",
      "Training epoch: 2433, train loss: 0.01600, val loss: 0.01641\n",
      "Training epoch: 2434, train loss: 0.01567, val loss: 0.01607\n",
      "Training epoch: 2435, train loss: 0.01552, val loss: 0.01594\n",
      "Training epoch: 2436, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 2437, train loss: 0.01562, val loss: 0.01604\n",
      "Training epoch: 2438, train loss: 0.01561, val loss: 0.01605\n",
      "Training epoch: 2439, train loss: 0.01601, val loss: 0.01644\n",
      "Training epoch: 2440, train loss: 0.01581, val loss: 0.01623\n",
      "Training epoch: 2441, train loss: 0.01592, val loss: 0.01634\n",
      "Training epoch: 2442, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 2443, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 2444, train loss: 0.01587, val loss: 0.01623\n",
      "Training epoch: 2445, train loss: 0.01600, val loss: 0.01641\n",
      "Training epoch: 2446, train loss: 0.01568, val loss: 0.01608\n",
      "Training epoch: 2447, train loss: 0.01594, val loss: 0.01630\n",
      "Training epoch: 2448, train loss: 0.01549, val loss: 0.01587\n",
      "Training epoch: 2449, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 2450, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 2451, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 2452, train loss: 0.01590, val loss: 0.01631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2453, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 2454, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 2455, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 2456, train loss: 0.01566, val loss: 0.01608\n",
      "Training epoch: 2457, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 2458, train loss: 0.01568, val loss: 0.01608\n",
      "Training epoch: 2459, train loss: 0.01567, val loss: 0.01607\n",
      "Training epoch: 2460, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 2461, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2462, train loss: 0.01574, val loss: 0.01616\n",
      "Training epoch: 2463, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 2464, train loss: 0.01561, val loss: 0.01600\n",
      "Training epoch: 2465, train loss: 0.01564, val loss: 0.01606\n",
      "Training epoch: 2466, train loss: 0.01556, val loss: 0.01597\n",
      "Training epoch: 2467, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2468, train loss: 0.01572, val loss: 0.01612\n",
      "Training epoch: 2469, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 2470, train loss: 0.01557, val loss: 0.01598\n",
      "Training epoch: 2471, train loss: 0.01598, val loss: 0.01640\n",
      "Training epoch: 2472, train loss: 0.01559, val loss: 0.01598\n",
      "Training epoch: 2473, train loss: 0.01590, val loss: 0.01630\n",
      "Training epoch: 2474, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2475, train loss: 0.01556, val loss: 0.01597\n",
      "Training epoch: 2476, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 2477, train loss: 0.01553, val loss: 0.01596\n",
      "Training epoch: 2478, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2479, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 2480, train loss: 0.01556, val loss: 0.01592\n",
      "Training epoch: 2481, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 2482, train loss: 0.01569, val loss: 0.01611\n",
      "Training epoch: 2483, train loss: 0.01573, val loss: 0.01613\n",
      "Training epoch: 2484, train loss: 0.01578, val loss: 0.01618\n",
      "Training epoch: 2485, train loss: 0.01560, val loss: 0.01601\n",
      "Training epoch: 2486, train loss: 0.01556, val loss: 0.01598\n",
      "Training epoch: 2487, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 2488, train loss: 0.01569, val loss: 0.01608\n",
      "Training epoch: 2489, train loss: 0.01607, val loss: 0.01651\n",
      "Training epoch: 2490, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 2491, train loss: 0.01592, val loss: 0.01633\n",
      "Training epoch: 2492, train loss: 0.01584, val loss: 0.01625\n",
      "Training epoch: 2493, train loss: 0.01590, val loss: 0.01633\n",
      "Training epoch: 2494, train loss: 0.01569, val loss: 0.01611\n",
      "Training epoch: 2495, train loss: 0.01580, val loss: 0.01619\n",
      "Training epoch: 2496, train loss: 0.01589, val loss: 0.01629\n",
      "Training epoch: 2497, train loss: 0.01591, val loss: 0.01632\n",
      "Training epoch: 2498, train loss: 0.01616, val loss: 0.01659\n",
      "Training epoch: 2499, train loss: 0.01588, val loss: 0.01631\n",
      "Training epoch: 2500, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 2501, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2502, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 2503, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 2504, train loss: 0.01579, val loss: 0.01620\n",
      "Training epoch: 2505, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 2506, train loss: 0.01680, val loss: 0.01724\n",
      "Training epoch: 2507, train loss: 0.01574, val loss: 0.01616\n",
      "Training epoch: 2508, train loss: 0.01564, val loss: 0.01605\n",
      "Training epoch: 2509, train loss: 0.01547, val loss: 0.01584\n",
      "Training epoch: 2510, train loss: 0.01560, val loss: 0.01598\n",
      "Training epoch: 2511, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 2512, train loss: 0.01570, val loss: 0.01608\n",
      "Training epoch: 2513, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2514, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 2515, train loss: 0.01560, val loss: 0.01603\n",
      "Training epoch: 2516, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 2517, train loss: 0.01553, val loss: 0.01591\n",
      "Training epoch: 2518, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 2519, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 2520, train loss: 0.01599, val loss: 0.01638\n",
      "Training epoch: 2521, train loss: 0.01607, val loss: 0.01650\n",
      "Training epoch: 2522, train loss: 0.01677, val loss: 0.01720\n",
      "Training epoch: 2523, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 2524, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 2525, train loss: 0.01584, val loss: 0.01627\n",
      "Training epoch: 2526, train loss: 0.01542, val loss: 0.01583\n",
      "Training epoch: 2527, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 2528, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 2529, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2530, train loss: 0.01584, val loss: 0.01621\n",
      "Training epoch: 2531, train loss: 0.01569, val loss: 0.01614\n",
      "Training epoch: 2532, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 2533, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 2534, train loss: 0.01575, val loss: 0.01613\n",
      "Training epoch: 2535, train loss: 0.01595, val loss: 0.01633\n",
      "Training epoch: 2536, train loss: 0.01617, val loss: 0.01655\n",
      "Training epoch: 2537, train loss: 0.01569, val loss: 0.01608\n",
      "Training epoch: 2538, train loss: 0.01594, val loss: 0.01632\n",
      "Training epoch: 2539, train loss: 0.01607, val loss: 0.01646\n",
      "Training epoch: 2540, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 2541, train loss: 0.01566, val loss: 0.01604\n",
      "Training epoch: 2542, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 2543, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 2544, train loss: 0.01550, val loss: 0.01591\n",
      "Training epoch: 2545, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 2546, train loss: 0.01575, val loss: 0.01617\n",
      "Training epoch: 2547, train loss: 0.01578, val loss: 0.01621\n",
      "Training epoch: 2548, train loss: 0.01568, val loss: 0.01610\n",
      "Training epoch: 2549, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 2550, train loss: 0.01591, val loss: 0.01631\n",
      "Training epoch: 2551, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 2552, train loss: 0.01595, val loss: 0.01632\n",
      "Training epoch: 2553, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 2554, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 2555, train loss: 0.01572, val loss: 0.01609\n",
      "Training epoch: 2556, train loss: 0.01576, val loss: 0.01617\n",
      "Training epoch: 2557, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2558, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 2559, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 2560, train loss: 0.01555, val loss: 0.01590\n",
      "Training epoch: 2561, train loss: 0.01575, val loss: 0.01618\n",
      "Training epoch: 2562, train loss: 0.01571, val loss: 0.01610\n",
      "Training epoch: 2563, train loss: 0.01597, val loss: 0.01639\n",
      "Training epoch: 2564, train loss: 0.01584, val loss: 0.01625\n",
      "Training epoch: 2565, train loss: 0.01576, val loss: 0.01619\n",
      "Training epoch: 2566, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2567, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 2568, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 2569, train loss: 0.01569, val loss: 0.01611\n",
      "Training epoch: 2570, train loss: 0.01575, val loss: 0.01617\n",
      "Training epoch: 2571, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 2572, train loss: 0.01564, val loss: 0.01602\n",
      "Training epoch: 2573, train loss: 0.01579, val loss: 0.01615\n",
      "Training epoch: 2574, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 2575, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 2576, train loss: 0.01565, val loss: 0.01610\n",
      "Training epoch: 2577, train loss: 0.01615, val loss: 0.01656\n",
      "Training epoch: 2578, train loss: 0.01550, val loss: 0.01587\n",
      "Training epoch: 2579, train loss: 0.01559, val loss: 0.01602\n",
      "Training epoch: 2580, train loss: 0.01556, val loss: 0.01595\n",
      "Training epoch: 2581, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 2582, train loss: 0.01542, val loss: 0.01583\n",
      "Training epoch: 2583, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 2584, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 2585, train loss: 0.01555, val loss: 0.01597\n",
      "Training epoch: 2586, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 2587, train loss: 0.01542, val loss: 0.01584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2588, train loss: 0.01561, val loss: 0.01600\n",
      "Training epoch: 2589, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2590, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 2591, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 2592, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 2593, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 2594, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 2595, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 2596, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 2597, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 2598, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 2599, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 2600, train loss: 0.01569, val loss: 0.01611\n",
      "Training epoch: 2601, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2602, train loss: 0.01659, val loss: 0.01697\n",
      "Training epoch: 2603, train loss: 0.01545, val loss: 0.01588\n",
      "Training epoch: 2604, train loss: 0.01571, val loss: 0.01613\n",
      "Training epoch: 2605, train loss: 0.01618, val loss: 0.01660\n",
      "Training epoch: 2606, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2607, train loss: 0.01561, val loss: 0.01601\n",
      "Training epoch: 2608, train loss: 0.01567, val loss: 0.01603\n",
      "Training epoch: 2609, train loss: 0.01564, val loss: 0.01605\n",
      "Training epoch: 2610, train loss: 0.01575, val loss: 0.01616\n",
      "Training epoch: 2611, train loss: 0.01563, val loss: 0.01603\n",
      "Training epoch: 2612, train loss: 0.01574, val loss: 0.01617\n",
      "Training epoch: 2613, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 2614, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2615, train loss: 0.01566, val loss: 0.01605\n",
      "Training epoch: 2616, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2617, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 2618, train loss: 0.01584, val loss: 0.01623\n",
      "Training epoch: 2619, train loss: 0.01604, val loss: 0.01641\n",
      "Training epoch: 2620, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 2621, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 2622, train loss: 0.01569, val loss: 0.01609\n",
      "Training epoch: 2623, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 2624, train loss: 0.01564, val loss: 0.01605\n",
      "Training epoch: 2625, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2626, train loss: 0.01587, val loss: 0.01625\n",
      "Training epoch: 2627, train loss: 0.01598, val loss: 0.01636\n",
      "Training epoch: 2628, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 2629, train loss: 0.01586, val loss: 0.01624\n",
      "Training epoch: 2630, train loss: 0.01559, val loss: 0.01599\n",
      "Training epoch: 2631, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2632, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 2633, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 2634, train loss: 0.01553, val loss: 0.01594\n",
      "Training epoch: 2635, train loss: 0.01561, val loss: 0.01597\n",
      "Training epoch: 2636, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 2637, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2638, train loss: 0.01654, val loss: 0.01692\n",
      "Training epoch: 2639, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 2640, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 2641, train loss: 0.01565, val loss: 0.01598\n",
      "Training epoch: 2642, train loss: 0.01561, val loss: 0.01604\n",
      "Training epoch: 2643, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 2644, train loss: 0.01581, val loss: 0.01623\n",
      "Training epoch: 2645, train loss: 0.01575, val loss: 0.01616\n",
      "Training epoch: 2646, train loss: 0.01569, val loss: 0.01608\n",
      "Training epoch: 2647, train loss: 0.01577, val loss: 0.01620\n",
      "Training epoch: 2648, train loss: 0.01557, val loss: 0.01599\n",
      "Training epoch: 2649, train loss: 0.01613, val loss: 0.01662\n",
      "Training epoch: 2650, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 2651, train loss: 0.01560, val loss: 0.01601\n",
      "Training epoch: 2652, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 2653, train loss: 0.01576, val loss: 0.01615\n",
      "Training epoch: 2654, train loss: 0.01577, val loss: 0.01616\n",
      "Training epoch: 2655, train loss: 0.01567, val loss: 0.01608\n",
      "Training epoch: 2656, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 2657, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2658, train loss: 0.01579, val loss: 0.01622\n",
      "Training epoch: 2659, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 2660, train loss: 0.01564, val loss: 0.01605\n",
      "Training epoch: 2661, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 2662, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 2663, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 2664, train loss: 0.01574, val loss: 0.01618\n",
      "Training epoch: 2665, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 2666, train loss: 0.01583, val loss: 0.01622\n",
      "Training epoch: 2667, train loss: 0.01566, val loss: 0.01607\n",
      "Training epoch: 2668, train loss: 0.01572, val loss: 0.01611\n",
      "Training epoch: 2669, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 2670, train loss: 0.01570, val loss: 0.01612\n",
      "Training epoch: 2671, train loss: 0.01585, val loss: 0.01623\n",
      "Training epoch: 2672, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 2673, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 2674, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 2675, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 2676, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2677, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 2678, train loss: 0.01555, val loss: 0.01597\n",
      "Training epoch: 2679, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 2680, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 2681, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 2682, train loss: 0.01558, val loss: 0.01599\n",
      "Training epoch: 2683, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2684, train loss: 0.01563, val loss: 0.01604\n",
      "Training epoch: 2685, train loss: 0.01566, val loss: 0.01604\n",
      "Training epoch: 2686, train loss: 0.01579, val loss: 0.01614\n",
      "Training epoch: 2687, train loss: 0.01648, val loss: 0.01694\n",
      "Training epoch: 2688, train loss: 0.01601, val loss: 0.01641\n",
      "Training epoch: 2689, train loss: 0.01589, val loss: 0.01631\n",
      "Training epoch: 2690, train loss: 0.01575, val loss: 0.01615\n",
      "Training epoch: 2691, train loss: 0.01560, val loss: 0.01601\n",
      "Training epoch: 2692, train loss: 0.01559, val loss: 0.01602\n",
      "Training epoch: 2693, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 2694, train loss: 0.01575, val loss: 0.01617\n",
      "Training epoch: 2695, train loss: 0.01559, val loss: 0.01598\n",
      "Training epoch: 2696, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 2697, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 2698, train loss: 0.01602, val loss: 0.01645\n",
      "Training epoch: 2699, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 2700, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 2701, train loss: 0.01562, val loss: 0.01602\n",
      "Training epoch: 2702, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 2703, train loss: 0.01564, val loss: 0.01606\n",
      "Training epoch: 2704, train loss: 0.01601, val loss: 0.01640\n",
      "Training epoch: 2705, train loss: 0.01559, val loss: 0.01597\n",
      "Training epoch: 2706, train loss: 0.01568, val loss: 0.01607\n",
      "Training epoch: 2707, train loss: 0.01572, val loss: 0.01610\n",
      "Training epoch: 2708, train loss: 0.01567, val loss: 0.01604\n",
      "Training epoch: 2709, train loss: 0.01596, val loss: 0.01635\n",
      "Training epoch: 2710, train loss: 0.01609, val loss: 0.01648\n",
      "Training epoch: 2711, train loss: 0.01605, val loss: 0.01646\n",
      "Training epoch: 2712, train loss: 0.01596, val loss: 0.01638\n",
      "Training epoch: 2713, train loss: 0.01567, val loss: 0.01609\n",
      "Training epoch: 2714, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 2715, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 2716, train loss: 0.01602, val loss: 0.01645\n",
      "Training epoch: 2717, train loss: 0.01538, val loss: 0.01581\n",
      "Training epoch: 2718, train loss: 0.01570, val loss: 0.01609\n",
      "Training epoch: 2719, train loss: 0.01564, val loss: 0.01607\n",
      "Training epoch: 2720, train loss: 0.01552, val loss: 0.01594\n",
      "Training epoch: 2721, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 2722, train loss: 0.01552, val loss: 0.01588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2723, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 2724, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 2725, train loss: 0.01550, val loss: 0.01594\n",
      "Training epoch: 2726, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 2727, train loss: 0.01567, val loss: 0.01606\n",
      "Training epoch: 2728, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 2729, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 2730, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2731, train loss: 0.01545, val loss: 0.01580\n",
      "Training epoch: 2732, train loss: 0.01572, val loss: 0.01612\n",
      "Training epoch: 2733, train loss: 0.01575, val loss: 0.01614\n",
      "Training epoch: 2734, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 2735, train loss: 0.01572, val loss: 0.01611\n",
      "Training epoch: 2736, train loss: 0.01556, val loss: 0.01592\n",
      "Training epoch: 2737, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2738, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2739, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 2740, train loss: 0.01565, val loss: 0.01603\n",
      "Training epoch: 2741, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2742, train loss: 0.01585, val loss: 0.01628\n",
      "Training epoch: 2743, train loss: 0.01567, val loss: 0.01612\n",
      "Training epoch: 2744, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 2745, train loss: 0.01618, val loss: 0.01658\n",
      "Training epoch: 2746, train loss: 0.01585, val loss: 0.01629\n",
      "Training epoch: 2747, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 2748, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 2749, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 2750, train loss: 0.01551, val loss: 0.01593\n",
      "Training epoch: 2751, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 2752, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 2753, train loss: 0.01562, val loss: 0.01603\n",
      "Training epoch: 2754, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 2755, train loss: 0.01602, val loss: 0.01641\n",
      "Training epoch: 2756, train loss: 0.01544, val loss: 0.01587\n",
      "Training epoch: 2757, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 2758, train loss: 0.01578, val loss: 0.01620\n",
      "Training epoch: 2759, train loss: 0.01535, val loss: 0.01571\n",
      "Training epoch: 2760, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2761, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 2762, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 2763, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2764, train loss: 0.01554, val loss: 0.01590\n",
      "Training epoch: 2765, train loss: 0.01555, val loss: 0.01592\n",
      "Training epoch: 2766, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 2767, train loss: 0.01602, val loss: 0.01641\n",
      "Training epoch: 2768, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 2769, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 2770, train loss: 0.01581, val loss: 0.01620\n",
      "Training epoch: 2771, train loss: 0.01586, val loss: 0.01628\n",
      "Training epoch: 2772, train loss: 0.01557, val loss: 0.01598\n",
      "Training epoch: 2773, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 2774, train loss: 0.01555, val loss: 0.01596\n",
      "Training epoch: 2775, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 2776, train loss: 0.01547, val loss: 0.01584\n",
      "Training epoch: 2777, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 2778, train loss: 0.01568, val loss: 0.01606\n",
      "Training epoch: 2779, train loss: 0.01547, val loss: 0.01585\n",
      "Training epoch: 2780, train loss: 0.01568, val loss: 0.01609\n",
      "Training epoch: 2781, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 2782, train loss: 0.01589, val loss: 0.01630\n",
      "Training epoch: 2783, train loss: 0.01559, val loss: 0.01599\n",
      "Training epoch: 2784, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 2785, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 2786, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 2787, train loss: 0.01543, val loss: 0.01585\n",
      "Training epoch: 2788, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 2789, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 2790, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 2791, train loss: 0.01588, val loss: 0.01633\n",
      "Training epoch: 2792, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 2793, train loss: 0.01566, val loss: 0.01604\n",
      "Training epoch: 2794, train loss: 0.01592, val loss: 0.01633\n",
      "Training epoch: 2795, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 2796, train loss: 0.01562, val loss: 0.01603\n",
      "Training epoch: 2797, train loss: 0.01569, val loss: 0.01611\n",
      "Training epoch: 2798, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 2799, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 2800, train loss: 0.01644, val loss: 0.01686\n",
      "Training epoch: 2801, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 2802, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 2803, train loss: 0.01597, val loss: 0.01637\n",
      "Training epoch: 2804, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 2805, train loss: 0.01559, val loss: 0.01595\n",
      "Training epoch: 2806, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 2807, train loss: 0.01558, val loss: 0.01601\n",
      "Training epoch: 2808, train loss: 0.01544, val loss: 0.01588\n",
      "Training epoch: 2809, train loss: 0.01562, val loss: 0.01599\n",
      "Training epoch: 2810, train loss: 0.01571, val loss: 0.01613\n",
      "Training epoch: 2811, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 2812, train loss: 0.01616, val loss: 0.01655\n",
      "Training epoch: 2813, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 2814, train loss: 0.01548, val loss: 0.01585\n",
      "Training epoch: 2815, train loss: 0.01566, val loss: 0.01607\n",
      "Training epoch: 2816, train loss: 0.01557, val loss: 0.01599\n",
      "Training epoch: 2817, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2818, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2819, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 2820, train loss: 0.01573, val loss: 0.01613\n",
      "Training epoch: 2821, train loss: 0.01577, val loss: 0.01617\n",
      "Training epoch: 2822, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 2823, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 2824, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 2825, train loss: 0.01588, val loss: 0.01631\n",
      "Training epoch: 2826, train loss: 0.01566, val loss: 0.01608\n",
      "Training epoch: 2827, train loss: 0.01600, val loss: 0.01635\n",
      "Training epoch: 2828, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2829, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 2830, train loss: 0.01562, val loss: 0.01602\n",
      "Training epoch: 2831, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 2832, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 2833, train loss: 0.01568, val loss: 0.01612\n",
      "Training epoch: 2834, train loss: 0.01602, val loss: 0.01644\n",
      "Training epoch: 2835, train loss: 0.01615, val loss: 0.01656\n",
      "Training epoch: 2836, train loss: 0.01569, val loss: 0.01609\n",
      "Training epoch: 2837, train loss: 0.01583, val loss: 0.01622\n",
      "Training epoch: 2838, train loss: 0.01551, val loss: 0.01593\n",
      "Training epoch: 2839, train loss: 0.01551, val loss: 0.01593\n",
      "Training epoch: 2840, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 2841, train loss: 0.01569, val loss: 0.01612\n",
      "Training epoch: 2842, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 2843, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 2844, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 2845, train loss: 0.01564, val loss: 0.01602\n",
      "Training epoch: 2846, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 2847, train loss: 0.01593, val loss: 0.01634\n",
      "Training epoch: 2848, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 2849, train loss: 0.01568, val loss: 0.01607\n",
      "Training epoch: 2850, train loss: 0.01565, val loss: 0.01603\n",
      "Training epoch: 2851, train loss: 0.01584, val loss: 0.01624\n",
      "Training epoch: 2852, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 2853, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 2854, train loss: 0.01571, val loss: 0.01614\n",
      "Training epoch: 2855, train loss: 0.01567, val loss: 0.01606\n",
      "Training epoch: 2856, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2857, train loss: 0.01577, val loss: 0.01619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2858, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 2859, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 2860, train loss: 0.01598, val loss: 0.01639\n",
      "Training epoch: 2861, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 2862, train loss: 0.01569, val loss: 0.01607\n",
      "Training epoch: 2863, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 2864, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 2865, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 2866, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 2867, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 2868, train loss: 0.01558, val loss: 0.01593\n",
      "Training epoch: 2869, train loss: 0.01553, val loss: 0.01594\n",
      "Training epoch: 2870, train loss: 0.01573, val loss: 0.01613\n",
      "Training epoch: 2871, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 2872, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 2873, train loss: 0.01621, val loss: 0.01662\n",
      "Training epoch: 2874, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 2875, train loss: 0.01561, val loss: 0.01601\n",
      "Training epoch: 2876, train loss: 0.01553, val loss: 0.01591\n",
      "Training epoch: 2877, train loss: 0.01560, val loss: 0.01599\n",
      "Training epoch: 2878, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2879, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 2880, train loss: 0.01546, val loss: 0.01589\n",
      "Training epoch: 2881, train loss: 0.01562, val loss: 0.01600\n",
      "Training epoch: 2882, train loss: 0.01582, val loss: 0.01621\n",
      "Training epoch: 2883, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 2884, train loss: 0.01554, val loss: 0.01592\n",
      "Training epoch: 2885, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 2886, train loss: 0.01546, val loss: 0.01583\n",
      "Training epoch: 2887, train loss: 0.01559, val loss: 0.01599\n",
      "Training epoch: 2888, train loss: 0.01552, val loss: 0.01594\n",
      "Training epoch: 2889, train loss: 0.01590, val loss: 0.01632\n",
      "Training epoch: 2890, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2891, train loss: 0.01542, val loss: 0.01583\n",
      "Training epoch: 2892, train loss: 0.01559, val loss: 0.01602\n",
      "Training epoch: 2893, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 2894, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 2895, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 2896, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 2897, train loss: 0.01570, val loss: 0.01613\n",
      "Training epoch: 2898, train loss: 0.01571, val loss: 0.01607\n",
      "Training epoch: 2899, train loss: 0.01565, val loss: 0.01605\n",
      "Training epoch: 2900, train loss: 0.01559, val loss: 0.01599\n",
      "Training epoch: 2901, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 2902, train loss: 0.01554, val loss: 0.01596\n",
      "Training epoch: 2903, train loss: 0.01557, val loss: 0.01598\n",
      "Training epoch: 2904, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 2905, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 2906, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2907, train loss: 0.01553, val loss: 0.01589\n",
      "Training epoch: 2908, train loss: 0.01572, val loss: 0.01611\n",
      "Training epoch: 2909, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 2910, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 2911, train loss: 0.01558, val loss: 0.01596\n",
      "Training epoch: 2912, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 2913, train loss: 0.01555, val loss: 0.01598\n",
      "Training epoch: 2914, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 2915, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 2916, train loss: 0.01568, val loss: 0.01606\n",
      "Training epoch: 2917, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 2918, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 2919, train loss: 0.01565, val loss: 0.01603\n",
      "Training epoch: 2920, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 2921, train loss: 0.01568, val loss: 0.01609\n",
      "Training epoch: 2922, train loss: 0.01552, val loss: 0.01594\n",
      "Training epoch: 2923, train loss: 0.01564, val loss: 0.01606\n",
      "Training epoch: 2924, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 2925, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 2926, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 2927, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 2928, train loss: 0.01547, val loss: 0.01584\n",
      "Training epoch: 2929, train loss: 0.01576, val loss: 0.01618\n",
      "Training epoch: 2930, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 2931, train loss: 0.01600, val loss: 0.01645\n",
      "Training epoch: 2932, train loss: 0.01562, val loss: 0.01603\n",
      "Training epoch: 2933, train loss: 0.01581, val loss: 0.01620\n",
      "Training epoch: 2934, train loss: 0.01566, val loss: 0.01608\n",
      "Training epoch: 2935, train loss: 0.01563, val loss: 0.01606\n",
      "Training epoch: 2936, train loss: 0.01610, val loss: 0.01650\n",
      "Training epoch: 2937, train loss: 0.01554, val loss: 0.01594\n",
      "Training epoch: 2938, train loss: 0.01566, val loss: 0.01605\n",
      "Training epoch: 2939, train loss: 0.01572, val loss: 0.01612\n",
      "Training epoch: 2940, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 2941, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2942, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 2943, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 2944, train loss: 0.01577, val loss: 0.01617\n",
      "Training epoch: 2945, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 2946, train loss: 0.01560, val loss: 0.01603\n",
      "Training epoch: 2947, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 2948, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 2949, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 2950, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2951, train loss: 0.01567, val loss: 0.01608\n",
      "Training epoch: 2952, train loss: 0.01600, val loss: 0.01642\n",
      "Training epoch: 2953, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 2954, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 2955, train loss: 0.01573, val loss: 0.01611\n",
      "Training epoch: 2956, train loss: 0.01621, val loss: 0.01665\n",
      "Training epoch: 2957, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 2958, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 2959, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 2960, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 2961, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 2962, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 2963, train loss: 0.01559, val loss: 0.01601\n",
      "Training epoch: 2964, train loss: 0.01556, val loss: 0.01593\n",
      "Training epoch: 2965, train loss: 0.01570, val loss: 0.01607\n",
      "Training epoch: 2966, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 2967, train loss: 0.01592, val loss: 0.01634\n",
      "Training epoch: 2968, train loss: 0.01560, val loss: 0.01597\n",
      "Training epoch: 2969, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 2970, train loss: 0.01559, val loss: 0.01593\n",
      "Training epoch: 2971, train loss: 0.01569, val loss: 0.01612\n",
      "Training epoch: 2972, train loss: 0.01572, val loss: 0.01614\n",
      "Training epoch: 2973, train loss: 0.01572, val loss: 0.01612\n",
      "Training epoch: 2974, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 2975, train loss: 0.01597, val loss: 0.01639\n",
      "Training epoch: 2976, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 2977, train loss: 0.01565, val loss: 0.01603\n",
      "Training epoch: 2978, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 2979, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 2980, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 2981, train loss: 0.01596, val loss: 0.01632\n",
      "Training epoch: 2982, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 2983, train loss: 0.01563, val loss: 0.01603\n",
      "Training epoch: 2984, train loss: 0.01580, val loss: 0.01621\n",
      "Training epoch: 2985, train loss: 0.01569, val loss: 0.01609\n",
      "Training epoch: 2986, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 2987, train loss: 0.01541, val loss: 0.01577\n",
      "Training epoch: 2988, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 2989, train loss: 0.01551, val loss: 0.01593\n",
      "Training epoch: 2990, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 2991, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 2992, train loss: 0.01554, val loss: 0.01597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2993, train loss: 0.01581, val loss: 0.01619\n",
      "Training epoch: 2994, train loss: 0.01600, val loss: 0.01641\n",
      "Training epoch: 2995, train loss: 0.01569, val loss: 0.01609\n",
      "Training epoch: 2996, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 2997, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 2998, train loss: 0.01565, val loss: 0.01607\n",
      "Training epoch: 2999, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 3000, train loss: 0.01553, val loss: 0.01593\n",
      "Training epoch: 3001, train loss: 0.01562, val loss: 0.01602\n",
      "Training epoch: 3002, train loss: 0.01572, val loss: 0.01607\n",
      "Training epoch: 3003, train loss: 0.01540, val loss: 0.01582\n",
      "Training epoch: 3004, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 3005, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 3006, train loss: 0.01568, val loss: 0.01605\n",
      "Training epoch: 3007, train loss: 0.01554, val loss: 0.01591\n",
      "Training epoch: 3008, train loss: 0.01571, val loss: 0.01613\n",
      "Training epoch: 3009, train loss: 0.01552, val loss: 0.01589\n",
      "Training epoch: 3010, train loss: 0.01576, val loss: 0.01616\n",
      "Training epoch: 3011, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 3012, train loss: 0.01555, val loss: 0.01591\n",
      "Training epoch: 3013, train loss: 0.01550, val loss: 0.01591\n",
      "Training epoch: 3014, train loss: 0.01562, val loss: 0.01600\n",
      "Training epoch: 3015, train loss: 0.01589, val loss: 0.01626\n",
      "Training epoch: 3016, train loss: 0.01552, val loss: 0.01587\n",
      "Training epoch: 3017, train loss: 0.01564, val loss: 0.01606\n",
      "Training epoch: 3018, train loss: 0.01564, val loss: 0.01605\n",
      "Training epoch: 3019, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3020, train loss: 0.01605, val loss: 0.01645\n",
      "Training epoch: 3021, train loss: 0.01597, val loss: 0.01639\n",
      "Training epoch: 3022, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 3023, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 3024, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 3025, train loss: 0.01570, val loss: 0.01612\n",
      "Training epoch: 3026, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3027, train loss: 0.01595, val loss: 0.01637\n",
      "Training epoch: 3028, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 3029, train loss: 0.01575, val loss: 0.01617\n",
      "Training epoch: 3030, train loss: 0.01575, val loss: 0.01613\n",
      "Training epoch: 3031, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 3032, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 3033, train loss: 0.01615, val loss: 0.01656\n",
      "Training epoch: 3034, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 3035, train loss: 0.01558, val loss: 0.01595\n",
      "Training epoch: 3036, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3037, train loss: 0.01565, val loss: 0.01606\n",
      "Training epoch: 3038, train loss: 0.01575, val loss: 0.01616\n",
      "Training epoch: 3039, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 3040, train loss: 0.01561, val loss: 0.01604\n",
      "Training epoch: 3041, train loss: 0.01584, val loss: 0.01626\n",
      "Training epoch: 3042, train loss: 0.01573, val loss: 0.01615\n",
      "Training epoch: 3043, train loss: 0.01580, val loss: 0.01622\n",
      "Training epoch: 3044, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 3045, train loss: 0.01554, val loss: 0.01594\n",
      "Training epoch: 3046, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 3047, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3048, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 3049, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 3050, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3051, train loss: 0.01550, val loss: 0.01591\n",
      "Training epoch: 3052, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3053, train loss: 0.01553, val loss: 0.01593\n",
      "Training epoch: 3054, train loss: 0.01585, val loss: 0.01626\n",
      "Training epoch: 3055, train loss: 0.01572, val loss: 0.01612\n",
      "Training epoch: 3056, train loss: 0.01587, val loss: 0.01627\n",
      "Training epoch: 3057, train loss: 0.01557, val loss: 0.01599\n",
      "Training epoch: 3058, train loss: 0.01590, val loss: 0.01628\n",
      "Training epoch: 3059, train loss: 0.01651, val loss: 0.01692\n",
      "Training epoch: 3060, train loss: 0.01592, val loss: 0.01633\n",
      "Training epoch: 3061, train loss: 0.01563, val loss: 0.01597\n",
      "Training epoch: 3062, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 3063, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 3064, train loss: 0.01599, val loss: 0.01643\n",
      "Training epoch: 3065, train loss: 0.01601, val loss: 0.01643\n",
      "Training epoch: 3066, train loss: 0.01576, val loss: 0.01617\n",
      "Training epoch: 3067, train loss: 0.01542, val loss: 0.01580\n",
      "Training epoch: 3068, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3069, train loss: 0.01567, val loss: 0.01607\n",
      "Training epoch: 3070, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3071, train loss: 0.01549, val loss: 0.01587\n",
      "Training epoch: 3072, train loss: 0.01619, val loss: 0.01660\n",
      "Training epoch: 3073, train loss: 0.01554, val loss: 0.01596\n",
      "Training epoch: 3074, train loss: 0.01600, val loss: 0.01641\n",
      "Training epoch: 3075, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 3076, train loss: 0.01571, val loss: 0.01610\n",
      "Training epoch: 3077, train loss: 0.01555, val loss: 0.01597\n",
      "Training epoch: 3078, train loss: 0.01575, val loss: 0.01612\n",
      "Training epoch: 3079, train loss: 0.01554, val loss: 0.01589\n",
      "Training epoch: 3080, train loss: 0.01564, val loss: 0.01605\n",
      "Training epoch: 3081, train loss: 0.01566, val loss: 0.01604\n",
      "Training epoch: 3082, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3083, train loss: 0.01588, val loss: 0.01628\n",
      "Training epoch: 3084, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 3085, train loss: 0.01557, val loss: 0.01594\n",
      "Training epoch: 3086, train loss: 0.01558, val loss: 0.01596\n",
      "Training epoch: 3087, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 3088, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 3089, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 3090, train loss: 0.01594, val loss: 0.01635\n",
      "Training epoch: 3091, train loss: 0.01557, val loss: 0.01594\n",
      "Training epoch: 3092, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 3093, train loss: 0.01562, val loss: 0.01602\n",
      "Training epoch: 3094, train loss: 0.01563, val loss: 0.01600\n",
      "Training epoch: 3095, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3096, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 3097, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 3098, train loss: 0.01558, val loss: 0.01596\n",
      "Training epoch: 3099, train loss: 0.01556, val loss: 0.01595\n",
      "Training epoch: 3100, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3101, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 3102, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3103, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 3104, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 3105, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 3106, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 3107, train loss: 0.01550, val loss: 0.01581\n",
      "Training epoch: 3108, train loss: 0.01574, val loss: 0.01614\n",
      "Training epoch: 3109, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 3110, train loss: 0.01558, val loss: 0.01599\n",
      "Training epoch: 3111, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 3112, train loss: 0.01551, val loss: 0.01593\n",
      "Training epoch: 3113, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 3114, train loss: 0.01585, val loss: 0.01625\n",
      "Training epoch: 3115, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 3116, train loss: 0.01566, val loss: 0.01611\n",
      "Training epoch: 3117, train loss: 0.01565, val loss: 0.01607\n",
      "Training epoch: 3118, train loss: 0.01563, val loss: 0.01604\n",
      "Training epoch: 3119, train loss: 0.01568, val loss: 0.01608\n",
      "Training epoch: 3120, train loss: 0.01615, val loss: 0.01656\n",
      "Training epoch: 3121, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3122, train loss: 0.01551, val loss: 0.01588\n",
      "Training epoch: 3123, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 3124, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 3125, train loss: 0.01565, val loss: 0.01604\n",
      "Training epoch: 3126, train loss: 0.01565, val loss: 0.01606\n",
      "Training epoch: 3127, train loss: 0.01557, val loss: 0.01599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3128, train loss: 0.01576, val loss: 0.01614\n",
      "Training epoch: 3129, train loss: 0.01568, val loss: 0.01612\n",
      "Training epoch: 3130, train loss: 0.01578, val loss: 0.01619\n",
      "Training epoch: 3131, train loss: 0.01569, val loss: 0.01610\n",
      "Training epoch: 3132, train loss: 0.01570, val loss: 0.01607\n",
      "Training epoch: 3133, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3134, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3135, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 3136, train loss: 0.01536, val loss: 0.01579\n",
      "Training epoch: 3137, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 3138, train loss: 0.01567, val loss: 0.01608\n",
      "Training epoch: 3139, train loss: 0.01565, val loss: 0.01607\n",
      "Training epoch: 3140, train loss: 0.01609, val loss: 0.01649\n",
      "Training epoch: 3141, train loss: 0.01556, val loss: 0.01597\n",
      "Training epoch: 3142, train loss: 0.01538, val loss: 0.01579\n",
      "Training epoch: 3143, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 3144, train loss: 0.01568, val loss: 0.01607\n",
      "Training epoch: 3145, train loss: 0.01540, val loss: 0.01576\n",
      "Training epoch: 3146, train loss: 0.01601, val loss: 0.01645\n",
      "Training epoch: 3147, train loss: 0.01614, val loss: 0.01657\n",
      "Training epoch: 3148, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 3149, train loss: 0.01557, val loss: 0.01595\n",
      "Training epoch: 3150, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3151, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 3152, train loss: 0.01577, val loss: 0.01618\n",
      "Training epoch: 3153, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 3154, train loss: 0.01582, val loss: 0.01622\n",
      "Training epoch: 3155, train loss: 0.01562, val loss: 0.01600\n",
      "Training epoch: 3156, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 3157, train loss: 0.01564, val loss: 0.01604\n",
      "Training epoch: 3158, train loss: 0.01570, val loss: 0.01612\n",
      "Training epoch: 3159, train loss: 0.01571, val loss: 0.01611\n",
      "Training epoch: 3160, train loss: 0.01580, val loss: 0.01624\n",
      "Training epoch: 3161, train loss: 0.01559, val loss: 0.01599\n",
      "Training epoch: 3162, train loss: 0.01569, val loss: 0.01610\n",
      "Training epoch: 3163, train loss: 0.01568, val loss: 0.01610\n",
      "Training epoch: 3164, train loss: 0.01549, val loss: 0.01592\n",
      "Training epoch: 3165, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 3166, train loss: 0.01561, val loss: 0.01598\n",
      "Training epoch: 3167, train loss: 0.01572, val loss: 0.01612\n",
      "Training epoch: 3168, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 3169, train loss: 0.01554, val loss: 0.01592\n",
      "Training epoch: 3170, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 3171, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3172, train loss: 0.01589, val loss: 0.01628\n",
      "Training epoch: 3173, train loss: 0.01596, val loss: 0.01634\n",
      "Training epoch: 3174, train loss: 0.01597, val loss: 0.01637\n",
      "Training epoch: 3175, train loss: 0.01577, val loss: 0.01615\n",
      "Training epoch: 3176, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 3177, train loss: 0.01546, val loss: 0.01583\n",
      "Training epoch: 3178, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 3179, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3180, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 3181, train loss: 0.01556, val loss: 0.01599\n",
      "Training epoch: 3182, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 3183, train loss: 0.01573, val loss: 0.01612\n",
      "Training epoch: 3184, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 3185, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 3186, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 3187, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 3188, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 3189, train loss: 0.01560, val loss: 0.01599\n",
      "Training epoch: 3190, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3191, train loss: 0.01543, val loss: 0.01586\n",
      "Training epoch: 3192, train loss: 0.01578, val loss: 0.01615\n",
      "Training epoch: 3193, train loss: 0.01573, val loss: 0.01613\n",
      "Training epoch: 3194, train loss: 0.01568, val loss: 0.01608\n",
      "Training epoch: 3195, train loss: 0.01552, val loss: 0.01595\n",
      "Training epoch: 3196, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3197, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 3198, train loss: 0.01616, val loss: 0.01657\n",
      "Training epoch: 3199, train loss: 0.01578, val loss: 0.01618\n",
      "Training epoch: 3200, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3201, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3202, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3203, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3204, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 3205, train loss: 0.01654, val loss: 0.01697\n",
      "Training epoch: 3206, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 3207, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 3208, train loss: 0.01541, val loss: 0.01576\n",
      "Training epoch: 3209, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3210, train loss: 0.01607, val loss: 0.01641\n",
      "Training epoch: 3211, train loss: 0.01576, val loss: 0.01616\n",
      "Training epoch: 3212, train loss: 0.01577, val loss: 0.01618\n",
      "Training epoch: 3213, train loss: 0.01578, val loss: 0.01617\n",
      "Training epoch: 3214, train loss: 0.01567, val loss: 0.01608\n",
      "Training epoch: 3215, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3216, train loss: 0.01563, val loss: 0.01600\n",
      "Training epoch: 3217, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 3218, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 3219, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 3220, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 3221, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3222, train loss: 0.01565, val loss: 0.01604\n",
      "Training epoch: 3223, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 3224, train loss: 0.01588, val loss: 0.01629\n",
      "Training epoch: 3225, train loss: 0.01622, val loss: 0.01663\n",
      "Training epoch: 3226, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 3227, train loss: 0.01566, val loss: 0.01603\n",
      "Training epoch: 3228, train loss: 0.01547, val loss: 0.01585\n",
      "Training epoch: 3229, train loss: 0.01575, val loss: 0.01620\n",
      "Training epoch: 3230, train loss: 0.01561, val loss: 0.01600\n",
      "Training epoch: 3231, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 3232, train loss: 0.01583, val loss: 0.01626\n",
      "Training epoch: 3233, train loss: 0.01586, val loss: 0.01624\n",
      "Training epoch: 3234, train loss: 0.01554, val loss: 0.01597\n",
      "Training epoch: 3235, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 3236, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 3237, train loss: 0.01607, val loss: 0.01643\n",
      "Training epoch: 3238, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 3239, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 3240, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3241, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 3242, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 3243, train loss: 0.01542, val loss: 0.01578\n",
      "Training epoch: 3244, train loss: 0.01606, val loss: 0.01646\n",
      "Training epoch: 3245, train loss: 0.01599, val loss: 0.01639\n",
      "Training epoch: 3246, train loss: 0.01587, val loss: 0.01628\n",
      "Training epoch: 3247, train loss: 0.01556, val loss: 0.01593\n",
      "Training epoch: 3248, train loss: 0.01571, val loss: 0.01613\n",
      "Training epoch: 3249, train loss: 0.01578, val loss: 0.01621\n",
      "Training epoch: 3250, train loss: 0.01595, val loss: 0.01635\n",
      "Training epoch: 3251, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 3252, train loss: 0.01561, val loss: 0.01601\n",
      "Training epoch: 3253, train loss: 0.01612, val loss: 0.01654\n",
      "Training epoch: 3254, train loss: 0.01584, val loss: 0.01621\n",
      "Training epoch: 3255, train loss: 0.01625, val loss: 0.01666\n",
      "Training epoch: 3256, train loss: 0.01580, val loss: 0.01617\n",
      "Training epoch: 3257, train loss: 0.01589, val loss: 0.01627\n",
      "Training epoch: 3258, train loss: 0.01565, val loss: 0.01607\n",
      "Training epoch: 3259, train loss: 0.01567, val loss: 0.01610\n",
      "Training epoch: 3260, train loss: 0.01566, val loss: 0.01603\n",
      "Training epoch: 3261, train loss: 0.01572, val loss: 0.01609\n",
      "Training epoch: 3262, train loss: 0.01562, val loss: 0.01599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3263, train loss: 0.01570, val loss: 0.01610\n",
      "Training epoch: 3264, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3265, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 3266, train loss: 0.01562, val loss: 0.01600\n",
      "Training epoch: 3267, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3268, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 3269, train loss: 0.01561, val loss: 0.01600\n",
      "Training epoch: 3270, train loss: 0.01547, val loss: 0.01585\n",
      "Training epoch: 3271, train loss: 0.01609, val loss: 0.01646\n",
      "Training epoch: 3272, train loss: 0.01565, val loss: 0.01603\n",
      "Training epoch: 3273, train loss: 0.01558, val loss: 0.01594\n",
      "Training epoch: 3274, train loss: 0.01638, val loss: 0.01682\n",
      "Training epoch: 3275, train loss: 0.01558, val loss: 0.01599\n",
      "Training epoch: 3276, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 3277, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3278, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 3279, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 3280, train loss: 0.01571, val loss: 0.01613\n",
      "Training epoch: 3281, train loss: 0.01596, val loss: 0.01637\n",
      "Training epoch: 3282, train loss: 0.01581, val loss: 0.01621\n",
      "Training epoch: 3283, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 3284, train loss: 0.01547, val loss: 0.01582\n",
      "Training epoch: 3285, train loss: 0.01565, val loss: 0.01602\n",
      "Training epoch: 3286, train loss: 0.01583, val loss: 0.01625\n",
      "Training epoch: 3287, train loss: 0.01571, val loss: 0.01609\n",
      "Training epoch: 3288, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 3289, train loss: 0.01573, val loss: 0.01613\n",
      "Training epoch: 3290, train loss: 0.01576, val loss: 0.01616\n",
      "Training epoch: 3291, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3292, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 3293, train loss: 0.01569, val loss: 0.01614\n",
      "Training epoch: 3294, train loss: 0.01623, val loss: 0.01664\n",
      "Training epoch: 3295, train loss: 0.01624, val loss: 0.01666\n",
      "Training epoch: 3296, train loss: 0.01577, val loss: 0.01620\n",
      "Training epoch: 3297, train loss: 0.01564, val loss: 0.01606\n",
      "Training epoch: 3298, train loss: 0.01563, val loss: 0.01603\n",
      "Training epoch: 3299, train loss: 0.01606, val loss: 0.01649\n",
      "Training epoch: 3300, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 3301, train loss: 0.01540, val loss: 0.01574\n",
      "Training epoch: 3302, train loss: 0.01570, val loss: 0.01612\n",
      "Training epoch: 3303, train loss: 0.01583, val loss: 0.01623\n",
      "Training epoch: 3304, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3305, train loss: 0.01566, val loss: 0.01604\n",
      "Training epoch: 3306, train loss: 0.01609, val loss: 0.01645\n",
      "Training epoch: 3307, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 3308, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 3309, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3310, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 3311, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 3312, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3313, train loss: 0.01571, val loss: 0.01603\n",
      "Training epoch: 3314, train loss: 0.01547, val loss: 0.01584\n",
      "Training epoch: 3315, train loss: 0.01582, val loss: 0.01621\n",
      "Training epoch: 3316, train loss: 0.01564, val loss: 0.01605\n",
      "Training epoch: 3317, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 3318, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 3319, train loss: 0.01543, val loss: 0.01580\n",
      "Training epoch: 3320, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 3321, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 3322, train loss: 0.01571, val loss: 0.01611\n",
      "Training epoch: 3323, train loss: 0.01570, val loss: 0.01613\n",
      "Training epoch: 3324, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 3325, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 3326, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3327, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3328, train loss: 0.01571, val loss: 0.01609\n",
      "Training epoch: 3329, train loss: 0.01559, val loss: 0.01598\n",
      "Training epoch: 3330, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 3331, train loss: 0.01561, val loss: 0.01604\n",
      "Training epoch: 3332, train loss: 0.01570, val loss: 0.01611\n",
      "Training epoch: 3333, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 3334, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3335, train loss: 0.01547, val loss: 0.01585\n",
      "Training epoch: 3336, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3337, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 3338, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 3339, train loss: 0.01559, val loss: 0.01593\n",
      "Training epoch: 3340, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 3341, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 3342, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 3343, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 3344, train loss: 0.01565, val loss: 0.01605\n",
      "Training epoch: 3345, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 3346, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 3347, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 3348, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3349, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 3350, train loss: 0.01568, val loss: 0.01609\n",
      "Training epoch: 3351, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 3352, train loss: 0.01536, val loss: 0.01579\n",
      "Training epoch: 3353, train loss: 0.01576, val loss: 0.01618\n",
      "Training epoch: 3354, train loss: 0.01597, val loss: 0.01639\n",
      "Training epoch: 3355, train loss: 0.01550, val loss: 0.01587\n",
      "Training epoch: 3356, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3357, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3358, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 3359, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 3360, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 3361, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 3362, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 3363, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3364, train loss: 0.01573, val loss: 0.01611\n",
      "Training epoch: 3365, train loss: 0.01545, val loss: 0.01582\n",
      "Training epoch: 3366, train loss: 0.01602, val loss: 0.01639\n",
      "Training epoch: 3367, train loss: 0.01555, val loss: 0.01591\n",
      "Training epoch: 3368, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 3369, train loss: 0.01632, val loss: 0.01676\n",
      "Training epoch: 3370, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 3371, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3372, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 3373, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 3374, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3375, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 3376, train loss: 0.01576, val loss: 0.01617\n",
      "Training epoch: 3377, train loss: 0.01594, val loss: 0.01637\n",
      "Training epoch: 3378, train loss: 0.01608, val loss: 0.01651\n",
      "Training epoch: 3379, train loss: 0.01565, val loss: 0.01604\n",
      "Training epoch: 3380, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 3381, train loss: 0.01624, val loss: 0.01665\n",
      "Training epoch: 3382, train loss: 0.01575, val loss: 0.01616\n",
      "Training epoch: 3383, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3384, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3385, train loss: 0.01584, val loss: 0.01616\n",
      "Training epoch: 3386, train loss: 0.01558, val loss: 0.01599\n",
      "Training epoch: 3387, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 3388, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 3389, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 3390, train loss: 0.01554, val loss: 0.01591\n",
      "Training epoch: 3391, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 3392, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 3393, train loss: 0.01564, val loss: 0.01603\n",
      "Training epoch: 3394, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 3395, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 3396, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3397, train loss: 0.01532, val loss: 0.01571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3398, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 3399, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 3400, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 3401, train loss: 0.01579, val loss: 0.01616\n",
      "Training epoch: 3402, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3403, train loss: 0.01550, val loss: 0.01587\n",
      "Training epoch: 3404, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 3405, train loss: 0.01554, val loss: 0.01592\n",
      "Training epoch: 3406, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3407, train loss: 0.01565, val loss: 0.01605\n",
      "Training epoch: 3408, train loss: 0.01547, val loss: 0.01590\n",
      "Training epoch: 3409, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 3410, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 3411, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 3412, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 3413, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 3414, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 3415, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3416, train loss: 0.01570, val loss: 0.01611\n",
      "Training epoch: 3417, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3418, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 3419, train loss: 0.01560, val loss: 0.01599\n",
      "Training epoch: 3420, train loss: 0.01559, val loss: 0.01596\n",
      "Training epoch: 3421, train loss: 0.01554, val loss: 0.01592\n",
      "Training epoch: 3422, train loss: 0.01579, val loss: 0.01619\n",
      "Training epoch: 3423, train loss: 0.01553, val loss: 0.01596\n",
      "Training epoch: 3424, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 3425, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3426, train loss: 0.01552, val loss: 0.01594\n",
      "Training epoch: 3427, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 3428, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3429, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 3430, train loss: 0.01563, val loss: 0.01603\n",
      "Training epoch: 3431, train loss: 0.01555, val loss: 0.01597\n",
      "Training epoch: 3432, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 3433, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 3434, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 3435, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 3436, train loss: 0.01570, val loss: 0.01609\n",
      "Training epoch: 3437, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 3438, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 3439, train loss: 0.01549, val loss: 0.01587\n",
      "Training epoch: 3440, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3441, train loss: 0.01561, val loss: 0.01601\n",
      "Training epoch: 3442, train loss: 0.01593, val loss: 0.01635\n",
      "Training epoch: 3443, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 3444, train loss: 0.01586, val loss: 0.01630\n",
      "Training epoch: 3445, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 3446, train loss: 0.01586, val loss: 0.01626\n",
      "Training epoch: 3447, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 3448, train loss: 0.01568, val loss: 0.01609\n",
      "Training epoch: 3449, train loss: 0.01561, val loss: 0.01596\n",
      "Training epoch: 3450, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 3451, train loss: 0.01577, val loss: 0.01618\n",
      "Training epoch: 3452, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3453, train loss: 0.01580, val loss: 0.01620\n",
      "Training epoch: 3454, train loss: 0.01566, val loss: 0.01607\n",
      "Training epoch: 3455, train loss: 0.01579, val loss: 0.01620\n",
      "Training epoch: 3456, train loss: 0.01557, val loss: 0.01599\n",
      "Training epoch: 3457, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 3458, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 3459, train loss: 0.01563, val loss: 0.01599\n",
      "Training epoch: 3460, train loss: 0.01563, val loss: 0.01603\n",
      "Training epoch: 3461, train loss: 0.01572, val loss: 0.01614\n",
      "Training epoch: 3462, train loss: 0.01534, val loss: 0.01575\n",
      "Training epoch: 3463, train loss: 0.01552, val loss: 0.01590\n",
      "Training epoch: 3464, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 3465, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 3466, train loss: 0.01622, val loss: 0.01658\n",
      "Training epoch: 3467, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 3468, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3469, train loss: 0.01573, val loss: 0.01611\n",
      "Training epoch: 3470, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3471, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 3472, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 3473, train loss: 0.01572, val loss: 0.01609\n",
      "Training epoch: 3474, train loss: 0.01585, val loss: 0.01627\n",
      "Training epoch: 3475, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 3476, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 3477, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 3478, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3479, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 3480, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 3481, train loss: 0.01573, val loss: 0.01616\n",
      "Training epoch: 3482, train loss: 0.01581, val loss: 0.01614\n",
      "Training epoch: 3483, train loss: 0.01573, val loss: 0.01611\n",
      "Training epoch: 3484, train loss: 0.01562, val loss: 0.01603\n",
      "Training epoch: 3485, train loss: 0.01543, val loss: 0.01579\n",
      "Training epoch: 3486, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 3487, train loss: 0.01625, val loss: 0.01668\n",
      "Training epoch: 3488, train loss: 0.01565, val loss: 0.01605\n",
      "Training epoch: 3489, train loss: 0.01598, val loss: 0.01640\n",
      "Training epoch: 3490, train loss: 0.01557, val loss: 0.01598\n",
      "Training epoch: 3491, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 3492, train loss: 0.01556, val loss: 0.01595\n",
      "Training epoch: 3493, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 3494, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 3495, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 3496, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 3497, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 3498, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 3499, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 3500, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 3501, train loss: 0.01573, val loss: 0.01613\n",
      "Training epoch: 3502, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 3503, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3504, train loss: 0.01560, val loss: 0.01598\n",
      "Training epoch: 3505, train loss: 0.01583, val loss: 0.01622\n",
      "Training epoch: 3506, train loss: 0.01541, val loss: 0.01583\n",
      "Training epoch: 3507, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 3508, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3509, train loss: 0.01582, val loss: 0.01622\n",
      "Training epoch: 3510, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 3511, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 3512, train loss: 0.01572, val loss: 0.01612\n",
      "Training epoch: 3513, train loss: 0.01563, val loss: 0.01599\n",
      "Training epoch: 3514, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3515, train loss: 0.01563, val loss: 0.01598\n",
      "Training epoch: 3516, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 3517, train loss: 0.01563, val loss: 0.01599\n",
      "Training epoch: 3518, train loss: 0.01548, val loss: 0.01584\n",
      "Training epoch: 3519, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 3520, train loss: 0.01563, val loss: 0.01599\n",
      "Training epoch: 3521, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 3522, train loss: 0.01543, val loss: 0.01584\n",
      "Training epoch: 3523, train loss: 0.01584, val loss: 0.01625\n",
      "Training epoch: 3524, train loss: 0.01563, val loss: 0.01601\n",
      "Training epoch: 3525, train loss: 0.01559, val loss: 0.01601\n",
      "Training epoch: 3526, train loss: 0.01575, val loss: 0.01612\n",
      "Training epoch: 3527, train loss: 0.01635, val loss: 0.01678\n",
      "Training epoch: 3528, train loss: 0.01556, val loss: 0.01594\n",
      "Training epoch: 3529, train loss: 0.01565, val loss: 0.01605\n",
      "Training epoch: 3530, train loss: 0.01583, val loss: 0.01622\n",
      "Training epoch: 3531, train loss: 0.01560, val loss: 0.01599\n",
      "Training epoch: 3532, train loss: 0.01559, val loss: 0.01599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3533, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3534, train loss: 0.01597, val loss: 0.01641\n",
      "Training epoch: 3535, train loss: 0.01580, val loss: 0.01622\n",
      "Training epoch: 3536, train loss: 0.01545, val loss: 0.01581\n",
      "Training epoch: 3537, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3538, train loss: 0.01561, val loss: 0.01601\n",
      "Training epoch: 3539, train loss: 0.01552, val loss: 0.01588\n",
      "Training epoch: 3540, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 3541, train loss: 0.01561, val loss: 0.01604\n",
      "Training epoch: 3542, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 3543, train loss: 0.01589, val loss: 0.01632\n",
      "Training epoch: 3544, train loss: 0.01566, val loss: 0.01605\n",
      "Training epoch: 3545, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3546, train loss: 0.01588, val loss: 0.01632\n",
      "Training epoch: 3547, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 3548, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3549, train loss: 0.01556, val loss: 0.01597\n",
      "Training epoch: 3550, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3551, train loss: 0.01576, val loss: 0.01615\n",
      "Training epoch: 3552, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3553, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 3554, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 3555, train loss: 0.01562, val loss: 0.01603\n",
      "Training epoch: 3556, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3557, train loss: 0.01581, val loss: 0.01621\n",
      "Training epoch: 3558, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 3559, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 3560, train loss: 0.01595, val loss: 0.01635\n",
      "Training epoch: 3561, train loss: 0.01556, val loss: 0.01592\n",
      "Training epoch: 3562, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 3563, train loss: 0.01574, val loss: 0.01613\n",
      "Training epoch: 3564, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 3565, train loss: 0.01582, val loss: 0.01617\n",
      "Training epoch: 3566, train loss: 0.01615, val loss: 0.01657\n",
      "Training epoch: 3567, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3568, train loss: 0.01559, val loss: 0.01600\n",
      "Training epoch: 3569, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 3570, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 3571, train loss: 0.01562, val loss: 0.01602\n",
      "Training epoch: 3572, train loss: 0.01561, val loss: 0.01595\n",
      "Training epoch: 3573, train loss: 0.01568, val loss: 0.01606\n",
      "Training epoch: 3574, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 3575, train loss: 0.01586, val loss: 0.01625\n",
      "Training epoch: 3576, train loss: 0.01581, val loss: 0.01617\n",
      "Training epoch: 3577, train loss: 0.01549, val loss: 0.01588\n",
      "Training epoch: 3578, train loss: 0.01550, val loss: 0.01591\n",
      "Training epoch: 3579, train loss: 0.01560, val loss: 0.01597\n",
      "Training epoch: 3580, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3581, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 3582, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 3583, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 3584, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3585, train loss: 0.01548, val loss: 0.01589\n",
      "Training epoch: 3586, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3587, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 3588, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 3589, train loss: 0.01629, val loss: 0.01674\n",
      "Training epoch: 3590, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 3591, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3592, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 3593, train loss: 0.01562, val loss: 0.01598\n",
      "Training epoch: 3594, train loss: 0.01575, val loss: 0.01613\n",
      "Training epoch: 3595, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 3596, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3597, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3598, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 3599, train loss: 0.01549, val loss: 0.01585\n",
      "Training epoch: 3600, train loss: 0.01557, val loss: 0.01595\n",
      "Training epoch: 3601, train loss: 0.01563, val loss: 0.01597\n",
      "Training epoch: 3602, train loss: 0.01547, val loss: 0.01588\n",
      "Training epoch: 3603, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3604, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 3605, train loss: 0.01579, val loss: 0.01620\n",
      "Training epoch: 3606, train loss: 0.01552, val loss: 0.01591\n",
      "Training epoch: 3607, train loss: 0.01550, val loss: 0.01591\n",
      "Training epoch: 3608, train loss: 0.01542, val loss: 0.01577\n",
      "Training epoch: 3609, train loss: 0.01572, val loss: 0.01617\n",
      "Training epoch: 3610, train loss: 0.01546, val loss: 0.01585\n",
      "Training epoch: 3611, train loss: 0.01585, val loss: 0.01625\n",
      "Training epoch: 3612, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 3613, train loss: 0.01561, val loss: 0.01602\n",
      "Training epoch: 3614, train loss: 0.01563, val loss: 0.01602\n",
      "Training epoch: 3615, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 3616, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 3617, train loss: 0.01560, val loss: 0.01599\n",
      "Training epoch: 3618, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 3619, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 3620, train loss: 0.01548, val loss: 0.01584\n",
      "Training epoch: 3621, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3622, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 3623, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 3624, train loss: 0.01544, val loss: 0.01580\n",
      "Training epoch: 3625, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3626, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3627, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3628, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3629, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 3630, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 3631, train loss: 0.01577, val loss: 0.01618\n",
      "Training epoch: 3632, train loss: 0.01562, val loss: 0.01601\n",
      "Training epoch: 3633, train loss: 0.01575, val loss: 0.01615\n",
      "Training epoch: 3634, train loss: 0.01587, val loss: 0.01628\n",
      "Training epoch: 3635, train loss: 0.01543, val loss: 0.01579\n",
      "Training epoch: 3636, train loss: 0.01544, val loss: 0.01587\n",
      "Training epoch: 3637, train loss: 0.01550, val loss: 0.01584\n",
      "Training epoch: 3638, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3639, train loss: 0.01556, val loss: 0.01598\n",
      "Training epoch: 3640, train loss: 0.01563, val loss: 0.01599\n",
      "Training epoch: 3641, train loss: 0.01587, val loss: 0.01626\n",
      "Training epoch: 3642, train loss: 0.01569, val loss: 0.01605\n",
      "Training epoch: 3643, train loss: 0.01559, val loss: 0.01593\n",
      "Training epoch: 3644, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 3645, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3646, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3647, train loss: 0.01550, val loss: 0.01587\n",
      "Training epoch: 3648, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 3649, train loss: 0.01568, val loss: 0.01608\n",
      "Training epoch: 3650, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 3651, train loss: 0.01587, val loss: 0.01627\n",
      "Training epoch: 3652, train loss: 0.01559, val loss: 0.01595\n",
      "Training epoch: 3653, train loss: 0.01559, val loss: 0.01596\n",
      "Training epoch: 3654, train loss: 0.01560, val loss: 0.01601\n",
      "Training epoch: 3655, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 3656, train loss: 0.01556, val loss: 0.01596\n",
      "Training epoch: 3657, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3658, train loss: 0.01553, val loss: 0.01592\n",
      "Training epoch: 3659, train loss: 0.01569, val loss: 0.01606\n",
      "Training epoch: 3660, train loss: 0.01583, val loss: 0.01620\n",
      "Training epoch: 3661, train loss: 0.01558, val loss: 0.01596\n",
      "Training epoch: 3662, train loss: 0.01589, val loss: 0.01628\n",
      "Training epoch: 3663, train loss: 0.01553, val loss: 0.01590\n",
      "Training epoch: 3664, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 3665, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 3666, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 3667, train loss: 0.01611, val loss: 0.01650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3668, train loss: 0.01560, val loss: 0.01602\n",
      "Training epoch: 3669, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 3670, train loss: 0.01547, val loss: 0.01587\n",
      "Training epoch: 3671, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3672, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 3673, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3674, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3675, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3676, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3677, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 3678, train loss: 0.01545, val loss: 0.01580\n",
      "Training epoch: 3679, train loss: 0.01569, val loss: 0.01606\n",
      "Training epoch: 3680, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 3681, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 3682, train loss: 0.01563, val loss: 0.01601\n",
      "Training epoch: 3683, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 3684, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 3685, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3686, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 3687, train loss: 0.01596, val loss: 0.01635\n",
      "Training epoch: 3688, train loss: 0.01556, val loss: 0.01595\n",
      "Training epoch: 3689, train loss: 0.01677, val loss: 0.01717\n",
      "Training epoch: 3690, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 3691, train loss: 0.01547, val loss: 0.01584\n",
      "Training epoch: 3692, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3693, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3694, train loss: 0.01562, val loss: 0.01604\n",
      "Training epoch: 3695, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 3696, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3697, train loss: 0.01551, val loss: 0.01590\n",
      "Training epoch: 3698, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 3699, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 3700, train loss: 0.01573, val loss: 0.01614\n",
      "Training epoch: 3701, train loss: 0.01568, val loss: 0.01610\n",
      "Training epoch: 3702, train loss: 0.01574, val loss: 0.01614\n",
      "Training epoch: 3703, train loss: 0.01553, val loss: 0.01593\n",
      "Training epoch: 3704, train loss: 0.01601, val loss: 0.01639\n",
      "Training epoch: 3705, train loss: 0.01549, val loss: 0.01586\n",
      "Training epoch: 3706, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3707, train loss: 0.01569, val loss: 0.01610\n",
      "Training epoch: 3708, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 3709, train loss: 0.01549, val loss: 0.01585\n",
      "Training epoch: 3710, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3711, train loss: 0.01548, val loss: 0.01586\n",
      "Training epoch: 3712, train loss: 0.01578, val loss: 0.01614\n",
      "Training epoch: 3713, train loss: 0.01600, val loss: 0.01640\n",
      "Training epoch: 3714, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 3715, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 3716, train loss: 0.01592, val loss: 0.01631\n",
      "Training epoch: 3717, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3718, train loss: 0.01588, val loss: 0.01624\n",
      "Training epoch: 3719, train loss: 0.01563, val loss: 0.01603\n",
      "Training epoch: 3720, train loss: 0.01545, val loss: 0.01583\n",
      "Training epoch: 3721, train loss: 0.01595, val loss: 0.01633\n",
      "Training epoch: 3722, train loss: 0.01593, val loss: 0.01631\n",
      "Training epoch: 3723, train loss: 0.01574, val loss: 0.01613\n",
      "Training epoch: 3724, train loss: 0.01579, val loss: 0.01619\n",
      "Training epoch: 3725, train loss: 0.01555, val loss: 0.01594\n",
      "Training epoch: 3726, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 3727, train loss: 0.01562, val loss: 0.01599\n",
      "Training epoch: 3728, train loss: 0.01557, val loss: 0.01594\n",
      "Training epoch: 3729, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3730, train loss: 0.01557, val loss: 0.01597\n",
      "Training epoch: 3731, train loss: 0.01551, val loss: 0.01590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0304 14:02:31.733825 140488656930624 ultratb.py:149] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-5f1864c29d38>\", line 19, in <module>\n",
      "    model.fit(train_x, train_y)\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/exnn/base.py\", line 208, in fit\n",
      "    self.train_step_init(tf.cast(batch_xx, tf.float32), batch_yy)\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 599, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2363, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1611, in _filtered_call\n",
      "    self.captured_inputs)\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 545, in call\n",
      "    ctx=ctx)\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\n",
      "    num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "model = ExNN(meta_info=meta_info,\n",
    "               subnet_num=10,\n",
    "               subnet_arch=[10, 6],\n",
    "               task_type=task_type,\n",
    "               activation_func=tf.tanh,\n",
    "               batch_size=min(1000, int(train_x.shape[0] * 0.2)),\n",
    "               training_epochs=10000,\n",
    "               lr_bp=0.001,\n",
    "               lr_cl=0.1,\n",
    "               beta_threshold=0.05,\n",
    "               tuning_epochs=100,\n",
    "               l1_proj=best_l1_prob,\n",
    "               l1_subnet=best_l1_subnet,\n",
    "               smooth_lambda=10**(-6),\n",
    "               verbose=False,\n",
    "               val_ratio=0.2,\n",
    "               early_stop_thres=500)\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"exnn_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred = model.predict(train_x[model.tr_idx]) \n",
    "val_pred = model.predict(train_x[model.val_idx]) \n",
    "pred_test = model.predict(test_x)\n",
    "\n",
    "mse_stat = np.hstack([np.round(get_metric(train_y[model.tr_idx], tr_pred), 5),\\\n",
    "                      np.round(get_metric(train_y[model.val_idx], val_pred),5),\\\n",
    "                      np.round(get_metric(test_y, pred_test),5)])\n",
    "print(mse_stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
