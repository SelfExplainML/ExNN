{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "from exnn import ExNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(label, pred, scaler):\n",
    "    pred = scaler.inverse_transform(pred.reshape([-1, 1]))\n",
    "    label = scaler.inverse_transform(label.reshape([-1, 1]))\n",
    "    return np.mean((pred - label)**2)\n",
    "\n",
    "def simu_loader(generator, datanum, testnum, noise_sigma):\n",
    "    def wrapper(rand_seed=0):\n",
    "        return generator(datanum, testnum=testnum, noise_sigma=noise_sigma, rand_seed=rand_seed)\n",
    "    return wrapper\n",
    "\n",
    "def data_generator1(datanum, testnum=10000, noise_sigma=1, rand_seed=0):\n",
    "    \n",
    "    corr = 0.5\n",
    "    np.random.seed(rand_seed)\n",
    "    proj_matrix = np.zeros((10, 4))\n",
    "    proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "    proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "    proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "    proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "    u = np.random.uniform(-1, 1, [datanum + testnum, 1])\n",
    "    t = np.sqrt(corr / (1 - corr))\n",
    "    x = np.zeros((datanum + testnum, 10))\n",
    "    for i in range(10):\n",
    "        x[:, i:i + 1] = (np.random.uniform(-1, 1, [datanum + testnum, 1]) + t * u) / (1 + t)\n",
    "\n",
    "    y = np.reshape(2 * np.dot(x, proj_matrix[:, 0]) + 0.2 * np.exp(-4 * np.dot(x, proj_matrix[:, 1])) + \\\n",
    "                   3 * (np.dot(x, proj_matrix[:, 2]))**2 + 2.5 * np.sin(np.pi * np.dot(x, proj_matrix[:, 3])), [-1, 1]) + \\\n",
    "              noise_sigma * np.random.normal(0, 1, [datanum + testnum, 1])\n",
    "    \n",
    "    task_type = \"Regression\"\n",
    "    meta_info = {\"X1\":{\"type\":\"continuous\"},\n",
    "             \"X2\":{\"type\":\"continuous\"},\n",
    "             \"X3\":{\"type\":\"continuous\"},\n",
    "             \"X4\":{\"type\":\"continuous\"},\n",
    "             \"X5\":{\"type\":\"continuous\"},\n",
    "             \"X6\":{\"type\":\"continuous\"},\n",
    "             \"X7\":{\"type\":\"continuous\"},\n",
    "             \"X8\":{\"type\":\"continuous\"},\n",
    "             \"X9\":{\"type\":\"continuous\"},\n",
    "             \"X10\":{\"type\":\"continuous\"},\n",
    "             \"Y\":{\"type\":\"target\"}}\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == \"target\":\n",
    "            sy = MinMaxScaler((-1, 1))\n",
    "            y = sy.fit_transform(y)\n",
    "            meta_info[key][\"scaler\"] = sy\n",
    "        elif item['type'] == \"categorical\":\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(x[:,[i]])\n",
    "            ordinal_feature = enc.transform(x[:,[i]])\n",
    "            x[:,[i]] = ordinal_feature\n",
    "            meta_info[key][\"values\"] = enc.categories_[0].tolist()\n",
    "        else:\n",
    "            sx = MinMaxScaler((-1, 1))\n",
    "            x[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "            meta_info[key][\"scaler\"] = sx\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=testnum, random_state=rand_seed)\n",
    "    return train_x, test_x, train_y, test_y, task_type, meta_info\n",
    "\n",
    "train_x, test_x, train_y, test_y, task_type, meta_info = data_generator1(datanum=10000, testnum=10000, noise_sigma=1, rand_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exnn_repeat(folder, name, data_generator,\n",
    "                  subnet_num=10,\n",
    "                  subnet_arch=[10, 6],\n",
    "                  task=\"Regression\",\n",
    "                  activation_func=tf.tanh,\n",
    "                  lr_bp=0.001,\n",
    "                  lr_cl=0.1,\n",
    "                  l1_proj=0.001,\n",
    "                  l1_subnet=0.001,\n",
    "                  smooth_lambda=0.00001,\n",
    "                  batch_size=1000,\n",
    "                  training_epochs=5000,\n",
    "                  tuning_epochs=500,\n",
    "                  beta_threshold=0.05,\n",
    "                  verbose=False,\n",
    "                  val_ratio=0.2,\n",
    "                  early_stop_thres=1000,\n",
    "                  rand_seed=0):\n",
    "\n",
    "    train_x, test_x, train_y, test_y, task_type, meta_info = data_generator(rand_seed)\n",
    "\n",
    "    input_num = train_x.shape[1]\n",
    "    model = ExNN(meta_info=meta_info,\n",
    "                   subnet_num=10,\n",
    "                   subnet_arch=subnet_arch,\n",
    "                   task_type=task_type,\n",
    "                   activation_func=tf.tanh,\n",
    "                   batch_size=min(batch_size, int(train_x.shape[0] * 0.2)),\n",
    "                   training_epochs=training_epochs,\n",
    "                   lr_bp=lr_bp,\n",
    "                   lr_cl=lr_cl,\n",
    "                   beta_threshold=beta_threshold,\n",
    "                   tuning_epochs=tuning_epochs,\n",
    "                   l1_proj=l1_proj,\n",
    "                   l1_subnet=l1_subnet,\n",
    "                   smooth_lambda=smooth_lambda,\n",
    "                   verbose=verbose,\n",
    "                   val_ratio=val_ratio,\n",
    "                   early_stop_thres=early_stop_thres)\n",
    "    model.fit(train_x, train_y)\n",
    "    model.visualize(folder=folder,\n",
    "                    name=name,\n",
    "                    save_eps=True)\n",
    "\n",
    "    tr_pred = model.predict(model.tr_x)\n",
    "    val_pred = model.predict(model.val_x)\n",
    "    pred_test = model.predict(test_x)\n",
    "\n",
    "    if task_type == \"Regression\":\n",
    "        stat = np.hstack([np.round(mse(model.tr_y, tr_pred, meta_info[\"Y\"][\"scaler\"]), 5),\\\n",
    "                              np.round(mse(model.val_y, val_pred, meta_info[\"Y\"][\"scaler\"]), 5),\\\n",
    "                              np.round(mse(test_y, pred_test, meta_info[\"Y\"][\"scaler\"]), 5)])\n",
    "    elif task_type == \"Classification\":\n",
    "        stat = np.hstack([np.round(auc(model.tr_y, tr_pred), 5),\\\n",
    "                          np.round(auc(model.val_y, val_pred), 5),\\\n",
    "                          np.round(auc(test_y, pred_test), 5)])\n",
    "\n",
    "    res_stat = pd.DataFrame(np.vstack([stat[0], stat[1], stat[2]]).T, columns=['train_metric', \"val_metric\", \"test_metric\"])\n",
    "    res_stat[\"Subnet_Number\"] = min(input_num, 10)\n",
    "    res_stat[\"lr_BP\"] = lr_bp\n",
    "    res_stat[\"lr_CL\"] = lr_cl\n",
    "    res_stat[\"L1_Penalty_Proj\"] = l1_proj\n",
    "    res_stat[\"L1_Penalty_Subnet\"] = l1_subnet\n",
    "    res_stat[\"Smooth_labmda\"] = smooth_lambda\n",
    "    res_stat[\"Training_Epochs\"] = training_epochs\n",
    "    return res_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_metric</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>Subnet_Number</th>\n",
       "      <th>lr_BP</th>\n",
       "      <th>lr_CL</th>\n",
       "      <th>L1_Penalty_Proj</th>\n",
       "      <th>L1_Penalty_Subnet</th>\n",
       "      <th>Smooth_labmda</th>\n",
       "      <th>Training_Epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00320</td>\n",
       "      <td>1.02624</td>\n",
       "      <td>0.98589</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00355</td>\n",
       "      <td>1.02668</td>\n",
       "      <td>0.98648</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00915</td>\n",
       "      <td>1.03278</td>\n",
       "      <td>0.99326</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00964</td>\n",
       "      <td>1.03482</td>\n",
       "      <td>0.99087</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.01787</td>\n",
       "      <td>1.05362</td>\n",
       "      <td>1.00659</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.03090</td>\n",
       "      <td>1.05991</td>\n",
       "      <td>1.01453</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.03150</td>\n",
       "      <td>1.06278</td>\n",
       "      <td>1.01392</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.02520</td>\n",
       "      <td>1.06287</td>\n",
       "      <td>1.01590</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.02707</td>\n",
       "      <td>1.06513</td>\n",
       "      <td>1.01832</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.04411</td>\n",
       "      <td>1.06749</td>\n",
       "      <td>1.02144</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.05281</td>\n",
       "      <td>1.07616</td>\n",
       "      <td>1.02750</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.05571</td>\n",
       "      <td>1.08536</td>\n",
       "      <td>1.04128</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.07632</td>\n",
       "      <td>1.10822</td>\n",
       "      <td>1.05517</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08109</td>\n",
       "      <td>1.11505</td>\n",
       "      <td>1.05877</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08136</td>\n",
       "      <td>1.11539</td>\n",
       "      <td>1.05913</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13296</td>\n",
       "      <td>1.17036</td>\n",
       "      <td>1.10715</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13246</td>\n",
       "      <td>1.17040</td>\n",
       "      <td>1.10856</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13254</td>\n",
       "      <td>1.17174</td>\n",
       "      <td>1.10848</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13393</td>\n",
       "      <td>1.17226</td>\n",
       "      <td>1.11058</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13234</td>\n",
       "      <td>1.17263</td>\n",
       "      <td>1.11034</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13639</td>\n",
       "      <td>1.17792</td>\n",
       "      <td>1.11322</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13677</td>\n",
       "      <td>1.17822</td>\n",
       "      <td>1.11350</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13647</td>\n",
       "      <td>1.17884</td>\n",
       "      <td>1.11373</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.14733</td>\n",
       "      <td>1.18745</td>\n",
       "      <td>1.12014</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.14861</td>\n",
       "      <td>1.18809</td>\n",
       "      <td>1.12168</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_metric  val_metric  test_metric  Subnet_Number  lr_BP  lr_CL  \\\n",
       "0       1.00320     1.02624      0.98589             10  0.001    0.1   \n",
       "0       1.00355     1.02668      0.98648             10  0.001    0.1   \n",
       "0       1.00915     1.03278      0.99326             10  0.001    0.1   \n",
       "0       1.00964     1.03482      0.99087             10  0.001    0.1   \n",
       "0       1.01787     1.05362      1.00659             10  0.001    0.1   \n",
       "0       1.03090     1.05991      1.01453             10  0.001    0.1   \n",
       "0       1.03150     1.06278      1.01392             10  0.001    0.1   \n",
       "0       1.02520     1.06287      1.01590             10  0.001    0.1   \n",
       "0       1.02707     1.06513      1.01832             10  0.001    0.1   \n",
       "0       1.04411     1.06749      1.02144             10  0.001    0.1   \n",
       "0       1.05281     1.07616      1.02750             10  0.001    0.1   \n",
       "0       1.05571     1.08536      1.04128             10  0.001    0.1   \n",
       "0       1.07632     1.10822      1.05517             10  0.001    0.1   \n",
       "0       1.08109     1.11505      1.05877             10  0.001    0.1   \n",
       "0       1.08136     1.11539      1.05913             10  0.001    0.1   \n",
       "0       1.13296     1.17036      1.10715             10  0.001    0.1   \n",
       "0       1.13246     1.17040      1.10856             10  0.001    0.1   \n",
       "0       1.13254     1.17174      1.10848             10  0.001    0.1   \n",
       "0       1.13393     1.17226      1.11058             10  0.001    0.1   \n",
       "0       1.13234     1.17263      1.11034             10  0.001    0.1   \n",
       "0       1.13639     1.17792      1.11322             10  0.001    0.1   \n",
       "0       1.13677     1.17822      1.11350             10  0.001    0.1   \n",
       "0       1.13647     1.17884      1.11373             10  0.001    0.1   \n",
       "0       1.14733     1.18745      1.12014             10  0.001    0.1   \n",
       "0       1.14861     1.18809      1.12168             10  0.001    0.1   \n",
       "\n",
       "   L1_Penalty_Proj  L1_Penalty_Subnet  Smooth_labmda  Training_Epochs  \n",
       "0         0.000316           0.003162       0.000001            10000  \n",
       "0         0.000316           0.010000       0.000001            10000  \n",
       "0         0.000100           0.003162       0.000001            10000  \n",
       "0         0.000100           0.010000       0.000001            10000  \n",
       "0         0.000100           0.001000       0.000001            10000  \n",
       "0         0.000316           0.000316       0.000001            10000  \n",
       "0         0.000316           0.000100       0.000001            10000  \n",
       "0         0.000100           0.000316       0.000001            10000  \n",
       "0         0.000100           0.000100       0.000001            10000  \n",
       "0         0.001000           0.010000       0.000001            10000  \n",
       "0         0.001000           0.003162       0.000001            10000  \n",
       "0         0.000316           0.001000       0.000001            10000  \n",
       "0         0.001000           0.001000       0.000001            10000  \n",
       "0         0.001000           0.000316       0.000001            10000  \n",
       "0         0.001000           0.000100       0.000001            10000  \n",
       "0         0.010000           0.001000       0.000001            10000  \n",
       "0         0.010000           0.010000       0.000001            10000  \n",
       "0         0.010000           0.000100       0.000001            10000  \n",
       "0         0.003162           0.010000       0.000001            10000  \n",
       "0         0.003162           0.003162       0.000001            10000  \n",
       "0         0.003162           0.001000       0.000001            10000  \n",
       "0         0.003162           0.000100       0.000001            10000  \n",
       "0         0.003162           0.000316       0.000001            10000  \n",
       "0         0.010000           0.003162       0.000001            10000  \n",
       "0         0.010000           0.000316       0.000001            10000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = Parallel(n_jobs=10)(delayed(exnn_repeat)(folder=\"./results/S1_exnn/\",\n",
    "                      name=str(i + 1).zfill(2) + \"_\" + str(j + 1).zfill(2),\n",
    "                      data_generator=simu_loader(data_generator1, 10000, 10000, 1),\n",
    "                      task=task_type,\n",
    "                      subnet_arch=[10, 6],\n",
    "                      beta_threshold=0.05,\n",
    "                      l1_proj=10**(-2 - i*0.5),\n",
    "                      l1_subnet=10**(-2 - j*0.5),\n",
    "                      smooth_lambda=10**(-5 - k),\n",
    "                      training_epochs=10000,\n",
    "                      lr_bp=0.001,\n",
    "                      lr_cl=0.1,\n",
    "                      batch_size=1000,\n",
    "                      early_stop_thres=500,\n",
    "                      tuning_epochs=100,\n",
    "                      rand_seed=0) for i in range(5) for j in range(5) for k in [1])\n",
    "exnn_stat_all = pd.concat(cv_results)\n",
    "exnn_stat_all.sort_values(\"val_metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_l1_prob = exnn_stat_all.sort_values(\"val_metric\").loc[:,\"L1_Penalty_Proj\"].iloc[0]\n",
    "best_l1_subnet = exnn_stat_all.sort_values(\"val_metric\").loc[:,\"L1_Penalty_Subnet\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0825 00:49:49.283602 140343974143808 deprecation.py:323] From /home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1, train loss: 0.16716, val loss: 0.16760\n",
      "Training epoch: 2, train loss: 0.13424, val loss: 0.13548\n",
      "Training epoch: 3, train loss: 0.11456, val loss: 0.11571\n",
      "Training epoch: 4, train loss: 0.09981, val loss: 0.10082\n",
      "Training epoch: 5, train loss: 0.08709, val loss: 0.08770\n",
      "Training epoch: 6, train loss: 0.07618, val loss: 0.07620\n",
      "Training epoch: 7, train loss: 0.06944, val loss: 0.06937\n",
      "Training epoch: 8, train loss: 0.06408, val loss: 0.06399\n",
      "Training epoch: 9, train loss: 0.05923, val loss: 0.05918\n",
      "Training epoch: 10, train loss: 0.05566, val loss: 0.05602\n",
      "Training epoch: 11, train loss: 0.05226, val loss: 0.05260\n",
      "Training epoch: 12, train loss: 0.04936, val loss: 0.04981\n",
      "Training epoch: 13, train loss: 0.04689, val loss: 0.04731\n",
      "Training epoch: 14, train loss: 0.04469, val loss: 0.04491\n",
      "Training epoch: 15, train loss: 0.04270, val loss: 0.04296\n",
      "Training epoch: 16, train loss: 0.04093, val loss: 0.04126\n",
      "Training epoch: 17, train loss: 0.03939, val loss: 0.03971\n",
      "Training epoch: 18, train loss: 0.03799, val loss: 0.03813\n",
      "Training epoch: 19, train loss: 0.03673, val loss: 0.03686\n",
      "Training epoch: 20, train loss: 0.03559, val loss: 0.03576\n",
      "Training epoch: 21, train loss: 0.03454, val loss: 0.03476\n",
      "Training epoch: 22, train loss: 0.03358, val loss: 0.03374\n",
      "Training epoch: 23, train loss: 0.03270, val loss: 0.03288\n",
      "Training epoch: 24, train loss: 0.03189, val loss: 0.03203\n",
      "Training epoch: 25, train loss: 0.03116, val loss: 0.03125\n",
      "Training epoch: 26, train loss: 0.03045, val loss: 0.03058\n",
      "Training epoch: 27, train loss: 0.02983, val loss: 0.03000\n",
      "Training epoch: 28, train loss: 0.02925, val loss: 0.02937\n",
      "Training epoch: 29, train loss: 0.02868, val loss: 0.02886\n",
      "Training epoch: 30, train loss: 0.02815, val loss: 0.02833\n",
      "Training epoch: 31, train loss: 0.02773, val loss: 0.02790\n",
      "Training epoch: 32, train loss: 0.02723, val loss: 0.02747\n",
      "Training epoch: 33, train loss: 0.02687, val loss: 0.02708\n",
      "Training epoch: 34, train loss: 0.02646, val loss: 0.02669\n",
      "Training epoch: 35, train loss: 0.02611, val loss: 0.02631\n",
      "Training epoch: 36, train loss: 0.02581, val loss: 0.02611\n",
      "Training epoch: 37, train loss: 0.02548, val loss: 0.02567\n",
      "Training epoch: 38, train loss: 0.02520, val loss: 0.02548\n",
      "Training epoch: 39, train loss: 0.02486, val loss: 0.02512\n",
      "Training epoch: 40, train loss: 0.02459, val loss: 0.02488\n",
      "Training epoch: 41, train loss: 0.02438, val loss: 0.02466\n",
      "Training epoch: 42, train loss: 0.02424, val loss: 0.02450\n",
      "Training epoch: 43, train loss: 0.02392, val loss: 0.02424\n",
      "Training epoch: 44, train loss: 0.02377, val loss: 0.02407\n",
      "Training epoch: 45, train loss: 0.02359, val loss: 0.02389\n",
      "Training epoch: 46, train loss: 0.02345, val loss: 0.02377\n",
      "Training epoch: 47, train loss: 0.02321, val loss: 0.02354\n",
      "Training epoch: 48, train loss: 0.02308, val loss: 0.02338\n",
      "Training epoch: 49, train loss: 0.02292, val loss: 0.02328\n",
      "Training epoch: 50, train loss: 0.02279, val loss: 0.02315\n",
      "Training epoch: 51, train loss: 0.02269, val loss: 0.02300\n",
      "Training epoch: 52, train loss: 0.02257, val loss: 0.02296\n",
      "Training epoch: 53, train loss: 0.02262, val loss: 0.02296\n",
      "Training epoch: 54, train loss: 0.02248, val loss: 0.02285\n",
      "Training epoch: 55, train loss: 0.02229, val loss: 0.02265\n",
      "Training epoch: 56, train loss: 0.02231, val loss: 0.02264\n",
      "Training epoch: 57, train loss: 0.02207, val loss: 0.02243\n",
      "Training epoch: 58, train loss: 0.02197, val loss: 0.02236\n",
      "Training epoch: 59, train loss: 0.02192, val loss: 0.02232\n",
      "Training epoch: 60, train loss: 0.02182, val loss: 0.02218\n",
      "Training epoch: 61, train loss: 0.02180, val loss: 0.02218\n",
      "Training epoch: 62, train loss: 0.02167, val loss: 0.02206\n",
      "Training epoch: 63, train loss: 0.02158, val loss: 0.02198\n",
      "Training epoch: 64, train loss: 0.02152, val loss: 0.02192\n",
      "Training epoch: 65, train loss: 0.02152, val loss: 0.02190\n",
      "Training epoch: 66, train loss: 0.02148, val loss: 0.02186\n",
      "Training epoch: 67, train loss: 0.02138, val loss: 0.02178\n",
      "Training epoch: 68, train loss: 0.02143, val loss: 0.02182\n",
      "Training epoch: 69, train loss: 0.02144, val loss: 0.02186\n",
      "Training epoch: 70, train loss: 0.02140, val loss: 0.02176\n",
      "Training epoch: 71, train loss: 0.02131, val loss: 0.02175\n",
      "Training epoch: 72, train loss: 0.02124, val loss: 0.02162\n",
      "Training epoch: 73, train loss: 0.02123, val loss: 0.02163\n",
      "Training epoch: 74, train loss: 0.02116, val loss: 0.02157\n",
      "Training epoch: 75, train loss: 0.02110, val loss: 0.02151\n",
      "Training epoch: 76, train loss: 0.02102, val loss: 0.02145\n",
      "Training epoch: 77, train loss: 0.02106, val loss: 0.02146\n",
      "Training epoch: 78, train loss: 0.02102, val loss: 0.02144\n",
      "Training epoch: 79, train loss: 0.02095, val loss: 0.02135\n",
      "Training epoch: 80, train loss: 0.02092, val loss: 0.02132\n",
      "Training epoch: 81, train loss: 0.02089, val loss: 0.02135\n",
      "Training epoch: 82, train loss: 0.02090, val loss: 0.02134\n",
      "Training epoch: 83, train loss: 0.02083, val loss: 0.02128\n",
      "Training epoch: 84, train loss: 0.02077, val loss: 0.02122\n",
      "Training epoch: 85, train loss: 0.02075, val loss: 0.02118\n",
      "Training epoch: 86, train loss: 0.02078, val loss: 0.02123\n",
      "Training epoch: 87, train loss: 0.02081, val loss: 0.02124\n",
      "Training epoch: 88, train loss: 0.02073, val loss: 0.02117\n",
      "Training epoch: 89, train loss: 0.02078, val loss: 0.02123\n",
      "Training epoch: 90, train loss: 0.02065, val loss: 0.02111\n",
      "Training epoch: 91, train loss: 0.02070, val loss: 0.02113\n",
      "Training epoch: 92, train loss: 0.02062, val loss: 0.02110\n",
      "Training epoch: 93, train loss: 0.02064, val loss: 0.02109\n",
      "Training epoch: 94, train loss: 0.02071, val loss: 0.02117\n",
      "Training epoch: 95, train loss: 0.02053, val loss: 0.02097\n",
      "Training epoch: 96, train loss: 0.02053, val loss: 0.02101\n",
      "Training epoch: 97, train loss: 0.02050, val loss: 0.02095\n",
      "Training epoch: 98, train loss: 0.02049, val loss: 0.02098\n",
      "Training epoch: 99, train loss: 0.02045, val loss: 0.02093\n",
      "Training epoch: 100, train loss: 0.02042, val loss: 0.02089\n",
      "Training epoch: 101, train loss: 0.02038, val loss: 0.02087\n",
      "Training epoch: 102, train loss: 0.02063, val loss: 0.02113\n",
      "Training epoch: 103, train loss: 0.02035, val loss: 0.02079\n",
      "Training epoch: 104, train loss: 0.02034, val loss: 0.02084\n",
      "Training epoch: 105, train loss: 0.02036, val loss: 0.02087\n",
      "Training epoch: 106, train loss: 0.02033, val loss: 0.02079\n",
      "Training epoch: 107, train loss: 0.02033, val loss: 0.02078\n",
      "Training epoch: 108, train loss: 0.02025, val loss: 0.02077\n",
      "Training epoch: 109, train loss: 0.02028, val loss: 0.02070\n",
      "Training epoch: 110, train loss: 0.02026, val loss: 0.02080\n",
      "Training epoch: 111, train loss: 0.02023, val loss: 0.02074\n",
      "Training epoch: 112, train loss: 0.02018, val loss: 0.02067\n",
      "Training epoch: 113, train loss: 0.02016, val loss: 0.02064\n",
      "Training epoch: 114, train loss: 0.02016, val loss: 0.02067\n",
      "Training epoch: 115, train loss: 0.02013, val loss: 0.02063\n",
      "Training epoch: 116, train loss: 0.02014, val loss: 0.02060\n",
      "Training epoch: 117, train loss: 0.02012, val loss: 0.02063\n",
      "Training epoch: 118, train loss: 0.02023, val loss: 0.02073\n",
      "Training epoch: 119, train loss: 0.02005, val loss: 0.02055\n",
      "Training epoch: 120, train loss: 0.02003, val loss: 0.02051\n",
      "Training epoch: 121, train loss: 0.02027, val loss: 0.02074\n",
      "Training epoch: 122, train loss: 0.02000, val loss: 0.02053\n",
      "Training epoch: 123, train loss: 0.01999, val loss: 0.02050\n",
      "Training epoch: 124, train loss: 0.02008, val loss: 0.02061\n",
      "Training epoch: 125, train loss: 0.02029, val loss: 0.02081\n",
      "Training epoch: 126, train loss: 0.01997, val loss: 0.02048\n",
      "Training epoch: 127, train loss: 0.02015, val loss: 0.02065\n",
      "Training epoch: 128, train loss: 0.01992, val loss: 0.02043\n",
      "Training epoch: 129, train loss: 0.02015, val loss: 0.02067\n",
      "Training epoch: 130, train loss: 0.01987, val loss: 0.02037\n",
      "Training epoch: 131, train loss: 0.01985, val loss: 0.02039\n",
      "Training epoch: 132, train loss: 0.02008, val loss: 0.02056\n",
      "Training epoch: 133, train loss: 0.01984, val loss: 0.02037\n",
      "Training epoch: 134, train loss: 0.01980, val loss: 0.02033\n",
      "Training epoch: 135, train loss: 0.01980, val loss: 0.02032\n",
      "Training epoch: 136, train loss: 0.01984, val loss: 0.02036\n",
      "Training epoch: 137, train loss: 0.01984, val loss: 0.02038\n",
      "Training epoch: 138, train loss: 0.01979, val loss: 0.02031\n",
      "Training epoch: 139, train loss: 0.01975, val loss: 0.02028\n",
      "Training epoch: 140, train loss: 0.01971, val loss: 0.02024\n",
      "Training epoch: 141, train loss: 0.01978, val loss: 0.02029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 142, train loss: 0.01980, val loss: 0.02036\n",
      "Training epoch: 143, train loss: 0.01968, val loss: 0.02021\n",
      "Training epoch: 144, train loss: 0.01974, val loss: 0.02025\n",
      "Training epoch: 145, train loss: 0.01969, val loss: 0.02026\n",
      "Training epoch: 146, train loss: 0.01972, val loss: 0.02025\n",
      "Training epoch: 147, train loss: 0.01968, val loss: 0.02023\n",
      "Training epoch: 148, train loss: 0.01970, val loss: 0.02022\n",
      "Training epoch: 149, train loss: 0.01956, val loss: 0.02009\n",
      "Training epoch: 150, train loss: 0.01967, val loss: 0.02019\n",
      "Training epoch: 151, train loss: 0.01972, val loss: 0.02028\n",
      "Training epoch: 152, train loss: 0.01956, val loss: 0.02009\n",
      "Training epoch: 153, train loss: 0.01953, val loss: 0.02006\n",
      "Training epoch: 154, train loss: 0.01950, val loss: 0.02005\n",
      "Training epoch: 155, train loss: 0.01946, val loss: 0.02000\n",
      "Training epoch: 156, train loss: 0.01956, val loss: 0.02010\n",
      "Training epoch: 157, train loss: 0.01945, val loss: 0.02000\n",
      "Training epoch: 158, train loss: 0.01944, val loss: 0.01998\n",
      "Training epoch: 159, train loss: 0.01945, val loss: 0.01997\n",
      "Training epoch: 160, train loss: 0.01939, val loss: 0.01997\n",
      "Training epoch: 161, train loss: 0.01935, val loss: 0.01991\n",
      "Training epoch: 162, train loss: 0.01937, val loss: 0.01988\n",
      "Training epoch: 163, train loss: 0.01933, val loss: 0.01990\n",
      "Training epoch: 164, train loss: 0.01946, val loss: 0.02002\n",
      "Training epoch: 165, train loss: 0.01940, val loss: 0.01994\n",
      "Training epoch: 166, train loss: 0.01928, val loss: 0.01985\n",
      "Training epoch: 167, train loss: 0.01928, val loss: 0.01983\n",
      "Training epoch: 168, train loss: 0.01927, val loss: 0.01981\n",
      "Training epoch: 169, train loss: 0.01924, val loss: 0.01982\n",
      "Training epoch: 170, train loss: 0.01921, val loss: 0.01976\n",
      "Training epoch: 171, train loss: 0.01920, val loss: 0.01976\n",
      "Training epoch: 172, train loss: 0.01921, val loss: 0.01977\n",
      "Training epoch: 173, train loss: 0.01916, val loss: 0.01974\n",
      "Training epoch: 174, train loss: 0.01920, val loss: 0.01975\n",
      "Training epoch: 175, train loss: 0.01912, val loss: 0.01970\n",
      "Training epoch: 176, train loss: 0.01913, val loss: 0.01970\n",
      "Training epoch: 177, train loss: 0.01910, val loss: 0.01969\n",
      "Training epoch: 178, train loss: 0.01909, val loss: 0.01965\n",
      "Training epoch: 179, train loss: 0.01906, val loss: 0.01964\n",
      "Training epoch: 180, train loss: 0.01903, val loss: 0.01958\n",
      "Training epoch: 181, train loss: 0.01903, val loss: 0.01962\n",
      "Training epoch: 182, train loss: 0.01908, val loss: 0.01965\n",
      "Training epoch: 183, train loss: 0.01922, val loss: 0.01981\n",
      "Training epoch: 184, train loss: 0.01898, val loss: 0.01955\n",
      "Training epoch: 185, train loss: 0.01896, val loss: 0.01957\n",
      "Training epoch: 186, train loss: 0.01894, val loss: 0.01954\n",
      "Training epoch: 187, train loss: 0.01910, val loss: 0.01969\n",
      "Training epoch: 188, train loss: 0.01908, val loss: 0.01970\n",
      "Training epoch: 189, train loss: 0.01916, val loss: 0.01979\n",
      "Training epoch: 190, train loss: 0.01889, val loss: 0.01947\n",
      "Training epoch: 191, train loss: 0.01893, val loss: 0.01954\n",
      "Training epoch: 192, train loss: 0.01891, val loss: 0.01949\n",
      "Training epoch: 193, train loss: 0.01893, val loss: 0.01953\n",
      "Training epoch: 194, train loss: 0.01889, val loss: 0.01947\n",
      "Training epoch: 195, train loss: 0.01880, val loss: 0.01942\n",
      "Training epoch: 196, train loss: 0.01876, val loss: 0.01935\n",
      "Training epoch: 197, train loss: 0.01878, val loss: 0.01936\n",
      "Training epoch: 198, train loss: 0.01877, val loss: 0.01938\n",
      "Training epoch: 199, train loss: 0.01872, val loss: 0.01931\n",
      "Training epoch: 200, train loss: 0.01873, val loss: 0.01936\n",
      "Training epoch: 201, train loss: 0.01867, val loss: 0.01929\n",
      "Training epoch: 202, train loss: 0.01876, val loss: 0.01939\n",
      "Training epoch: 203, train loss: 0.01865, val loss: 0.01928\n",
      "Training epoch: 204, train loss: 0.01862, val loss: 0.01922\n",
      "Training epoch: 205, train loss: 0.01865, val loss: 0.01926\n",
      "Training epoch: 206, train loss: 0.01860, val loss: 0.01923\n",
      "Training epoch: 207, train loss: 0.01856, val loss: 0.01918\n",
      "Training epoch: 208, train loss: 0.01854, val loss: 0.01916\n",
      "Training epoch: 209, train loss: 0.01855, val loss: 0.01918\n",
      "Training epoch: 210, train loss: 0.01865, val loss: 0.01927\n",
      "Training epoch: 211, train loss: 0.01848, val loss: 0.01910\n",
      "Training epoch: 212, train loss: 0.01846, val loss: 0.01908\n",
      "Training epoch: 213, train loss: 0.01846, val loss: 0.01912\n",
      "Training epoch: 214, train loss: 0.01847, val loss: 0.01911\n",
      "Training epoch: 215, train loss: 0.01843, val loss: 0.01907\n",
      "Training epoch: 216, train loss: 0.01840, val loss: 0.01907\n",
      "Training epoch: 217, train loss: 0.01844, val loss: 0.01906\n",
      "Training epoch: 218, train loss: 0.01837, val loss: 0.01901\n",
      "Training epoch: 219, train loss: 0.01834, val loss: 0.01896\n",
      "Training epoch: 220, train loss: 0.01832, val loss: 0.01899\n",
      "Training epoch: 221, train loss: 0.01847, val loss: 0.01916\n",
      "Training epoch: 222, train loss: 0.01835, val loss: 0.01902\n",
      "Training epoch: 223, train loss: 0.01826, val loss: 0.01892\n",
      "Training epoch: 224, train loss: 0.01830, val loss: 0.01899\n",
      "Training epoch: 225, train loss: 0.01824, val loss: 0.01891\n",
      "Training epoch: 226, train loss: 0.01820, val loss: 0.01886\n",
      "Training epoch: 227, train loss: 0.01818, val loss: 0.01885\n",
      "Training epoch: 228, train loss: 0.01826, val loss: 0.01889\n",
      "Training epoch: 229, train loss: 0.01825, val loss: 0.01895\n",
      "Training epoch: 230, train loss: 0.01813, val loss: 0.01880\n",
      "Training epoch: 231, train loss: 0.01810, val loss: 0.01877\n",
      "Training epoch: 232, train loss: 0.01807, val loss: 0.01873\n",
      "Training epoch: 233, train loss: 0.01807, val loss: 0.01876\n",
      "Training epoch: 234, train loss: 0.01803, val loss: 0.01869\n",
      "Training epoch: 235, train loss: 0.01843, val loss: 0.01905\n",
      "Training epoch: 236, train loss: 0.01847, val loss: 0.01911\n",
      "Training epoch: 237, train loss: 0.01830, val loss: 0.01897\n",
      "Training epoch: 238, train loss: 0.01797, val loss: 0.01864\n",
      "Training epoch: 239, train loss: 0.01802, val loss: 0.01872\n",
      "Training epoch: 240, train loss: 0.01793, val loss: 0.01863\n",
      "Training epoch: 241, train loss: 0.01798, val loss: 0.01862\n",
      "Training epoch: 242, train loss: 0.01789, val loss: 0.01860\n",
      "Training epoch: 243, train loss: 0.01786, val loss: 0.01855\n",
      "Training epoch: 244, train loss: 0.01783, val loss: 0.01852\n",
      "Training epoch: 245, train loss: 0.01829, val loss: 0.01907\n",
      "Training epoch: 246, train loss: 0.01784, val loss: 0.01855\n",
      "Training epoch: 247, train loss: 0.01782, val loss: 0.01849\n",
      "Training epoch: 248, train loss: 0.01775, val loss: 0.01846\n",
      "Training epoch: 249, train loss: 0.01776, val loss: 0.01848\n",
      "Training epoch: 250, train loss: 0.01834, val loss: 0.01898\n",
      "Training epoch: 251, train loss: 0.01819, val loss: 0.01888\n",
      "Training epoch: 252, train loss: 0.01770, val loss: 0.01842\n",
      "Training epoch: 253, train loss: 0.01768, val loss: 0.01837\n",
      "Training epoch: 254, train loss: 0.01767, val loss: 0.01841\n",
      "Training epoch: 255, train loss: 0.01763, val loss: 0.01836\n",
      "Training epoch: 256, train loss: 0.01762, val loss: 0.01835\n",
      "Training epoch: 257, train loss: 0.01761, val loss: 0.01834\n",
      "Training epoch: 258, train loss: 0.01757, val loss: 0.01831\n",
      "Training epoch: 259, train loss: 0.01755, val loss: 0.01826\n",
      "Training epoch: 260, train loss: 0.01757, val loss: 0.01827\n",
      "Training epoch: 261, train loss: 0.01755, val loss: 0.01828\n",
      "Training epoch: 262, train loss: 0.01794, val loss: 0.01860\n",
      "Training epoch: 263, train loss: 0.01773, val loss: 0.01850\n",
      "Training epoch: 264, train loss: 0.01750, val loss: 0.01822\n",
      "Training epoch: 265, train loss: 0.01753, val loss: 0.01825\n",
      "Training epoch: 266, train loss: 0.01744, val loss: 0.01817\n",
      "Training epoch: 267, train loss: 0.01743, val loss: 0.01816\n",
      "Training epoch: 268, train loss: 0.01743, val loss: 0.01814\n",
      "Training epoch: 269, train loss: 0.01759, val loss: 0.01836\n",
      "Training epoch: 270, train loss: 0.01766, val loss: 0.01837\n",
      "Training epoch: 271, train loss: 0.01735, val loss: 0.01808\n",
      "Training epoch: 272, train loss: 0.01743, val loss: 0.01815\n",
      "Training epoch: 273, train loss: 0.01732, val loss: 0.01805\n",
      "Training epoch: 274, train loss: 0.01734, val loss: 0.01808\n",
      "Training epoch: 275, train loss: 0.01750, val loss: 0.01826\n",
      "Training epoch: 276, train loss: 0.01755, val loss: 0.01827\n",
      "Training epoch: 277, train loss: 0.01757, val loss: 0.01826\n",
      "Training epoch: 278, train loss: 0.01735, val loss: 0.01811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 279, train loss: 0.01731, val loss: 0.01808\n",
      "Training epoch: 280, train loss: 0.01726, val loss: 0.01800\n",
      "Training epoch: 281, train loss: 0.01760, val loss: 0.01831\n",
      "Training epoch: 282, train loss: 0.01722, val loss: 0.01796\n",
      "Training epoch: 283, train loss: 0.01720, val loss: 0.01795\n",
      "Training epoch: 284, train loss: 0.01719, val loss: 0.01792\n",
      "Training epoch: 285, train loss: 0.01718, val loss: 0.01792\n",
      "Training epoch: 286, train loss: 0.01729, val loss: 0.01807\n",
      "Training epoch: 287, train loss: 0.01743, val loss: 0.01814\n",
      "Training epoch: 288, train loss: 0.01748, val loss: 0.01820\n",
      "Training epoch: 289, train loss: 0.01712, val loss: 0.01788\n",
      "Training epoch: 290, train loss: 0.01712, val loss: 0.01786\n",
      "Training epoch: 291, train loss: 0.01731, val loss: 0.01811\n",
      "Training epoch: 292, train loss: 0.01709, val loss: 0.01785\n",
      "Training epoch: 293, train loss: 0.01711, val loss: 0.01784\n",
      "Training epoch: 294, train loss: 0.01709, val loss: 0.01787\n",
      "Training epoch: 295, train loss: 0.01728, val loss: 0.01809\n",
      "Training epoch: 296, train loss: 0.01705, val loss: 0.01778\n",
      "Training epoch: 297, train loss: 0.01701, val loss: 0.01777\n",
      "Training epoch: 298, train loss: 0.01712, val loss: 0.01783\n",
      "Training epoch: 299, train loss: 0.01715, val loss: 0.01790\n",
      "Training epoch: 300, train loss: 0.01700, val loss: 0.01776\n",
      "Training epoch: 301, train loss: 0.01721, val loss: 0.01791\n",
      "Training epoch: 302, train loss: 0.01705, val loss: 0.01780\n",
      "Training epoch: 303, train loss: 0.01698, val loss: 0.01772\n",
      "Training epoch: 304, train loss: 0.01696, val loss: 0.01773\n",
      "Training epoch: 305, train loss: 0.01695, val loss: 0.01768\n",
      "Training epoch: 306, train loss: 0.01714, val loss: 0.01794\n",
      "Training epoch: 307, train loss: 0.01692, val loss: 0.01767\n",
      "Training epoch: 308, train loss: 0.01707, val loss: 0.01779\n",
      "Training epoch: 309, train loss: 0.01690, val loss: 0.01764\n",
      "Training epoch: 310, train loss: 0.01688, val loss: 0.01764\n",
      "Training epoch: 311, train loss: 0.01696, val loss: 0.01768\n",
      "Training epoch: 312, train loss: 0.01706, val loss: 0.01787\n",
      "Training epoch: 313, train loss: 0.01688, val loss: 0.01759\n",
      "Training epoch: 314, train loss: 0.01684, val loss: 0.01760\n",
      "Training epoch: 315, train loss: 0.01684, val loss: 0.01760\n",
      "Training epoch: 316, train loss: 0.01699, val loss: 0.01769\n",
      "Training epoch: 317, train loss: 0.01689, val loss: 0.01767\n",
      "Training epoch: 318, train loss: 0.01685, val loss: 0.01759\n",
      "Training epoch: 319, train loss: 0.01710, val loss: 0.01782\n",
      "Training epoch: 320, train loss: 0.01680, val loss: 0.01757\n",
      "Training epoch: 321, train loss: 0.01688, val loss: 0.01763\n",
      "Training epoch: 322, train loss: 0.01690, val loss: 0.01766\n",
      "Training epoch: 323, train loss: 0.01706, val loss: 0.01780\n",
      "Training epoch: 324, train loss: 0.01686, val loss: 0.01760\n",
      "Training epoch: 325, train loss: 0.01683, val loss: 0.01762\n",
      "Training epoch: 326, train loss: 0.01676, val loss: 0.01753\n",
      "Training epoch: 327, train loss: 0.01676, val loss: 0.01751\n",
      "Training epoch: 328, train loss: 0.01674, val loss: 0.01750\n",
      "Training epoch: 329, train loss: 0.01672, val loss: 0.01748\n",
      "Training epoch: 330, train loss: 0.01675, val loss: 0.01751\n",
      "Training epoch: 331, train loss: 0.01674, val loss: 0.01746\n",
      "Training epoch: 332, train loss: 0.01674, val loss: 0.01755\n",
      "Training epoch: 333, train loss: 0.01784, val loss: 0.01865\n",
      "Training epoch: 334, train loss: 0.01682, val loss: 0.01759\n",
      "Training epoch: 335, train loss: 0.01672, val loss: 0.01749\n",
      "Training epoch: 336, train loss: 0.01673, val loss: 0.01745\n",
      "Training epoch: 337, train loss: 0.01668, val loss: 0.01744\n",
      "Training epoch: 338, train loss: 0.01671, val loss: 0.01752\n",
      "Training epoch: 339, train loss: 0.01666, val loss: 0.01741\n",
      "Training epoch: 340, train loss: 0.01698, val loss: 0.01772\n",
      "Training epoch: 341, train loss: 0.01666, val loss: 0.01742\n",
      "Training epoch: 342, train loss: 0.01678, val loss: 0.01754\n",
      "Training epoch: 343, train loss: 0.01693, val loss: 0.01770\n",
      "Training epoch: 344, train loss: 0.01684, val loss: 0.01757\n",
      "Training epoch: 345, train loss: 0.01669, val loss: 0.01745\n",
      "Training epoch: 346, train loss: 0.01668, val loss: 0.01746\n",
      "Training epoch: 347, train loss: 0.01669, val loss: 0.01745\n",
      "Training epoch: 348, train loss: 0.01665, val loss: 0.01743\n",
      "Training epoch: 349, train loss: 0.01666, val loss: 0.01745\n",
      "Training epoch: 350, train loss: 0.01662, val loss: 0.01737\n",
      "Training epoch: 351, train loss: 0.01672, val loss: 0.01754\n",
      "Training epoch: 352, train loss: 0.01677, val loss: 0.01755\n",
      "Training epoch: 353, train loss: 0.01686, val loss: 0.01766\n",
      "Training epoch: 354, train loss: 0.01659, val loss: 0.01737\n",
      "Training epoch: 355, train loss: 0.01661, val loss: 0.01736\n",
      "Training epoch: 356, train loss: 0.01680, val loss: 0.01753\n",
      "Training epoch: 357, train loss: 0.01673, val loss: 0.01743\n",
      "Training epoch: 358, train loss: 0.01675, val loss: 0.01752\n",
      "Training epoch: 359, train loss: 0.01655, val loss: 0.01728\n",
      "Training epoch: 360, train loss: 0.01654, val loss: 0.01727\n",
      "Training epoch: 361, train loss: 0.01654, val loss: 0.01729\n",
      "Training epoch: 362, train loss: 0.01663, val loss: 0.01741\n",
      "Training epoch: 363, train loss: 0.01665, val loss: 0.01740\n",
      "Training epoch: 364, train loss: 0.01654, val loss: 0.01729\n",
      "Training epoch: 365, train loss: 0.01658, val loss: 0.01731\n",
      "Training epoch: 366, train loss: 0.01649, val loss: 0.01723\n",
      "Training epoch: 367, train loss: 0.01661, val loss: 0.01739\n",
      "Training epoch: 368, train loss: 0.01651, val loss: 0.01725\n",
      "Training epoch: 369, train loss: 0.01661, val loss: 0.01734\n",
      "Training epoch: 370, train loss: 0.01657, val loss: 0.01730\n",
      "Training epoch: 371, train loss: 0.01658, val loss: 0.01728\n",
      "Training epoch: 372, train loss: 0.01649, val loss: 0.01725\n",
      "Training epoch: 373, train loss: 0.01647, val loss: 0.01718\n",
      "Training epoch: 374, train loss: 0.01655, val loss: 0.01729\n",
      "Training epoch: 375, train loss: 0.01647, val loss: 0.01717\n",
      "Training epoch: 376, train loss: 0.01644, val loss: 0.01717\n",
      "Training epoch: 377, train loss: 0.01642, val loss: 0.01714\n",
      "Training epoch: 378, train loss: 0.01675, val loss: 0.01747\n",
      "Training epoch: 379, train loss: 0.01659, val loss: 0.01735\n",
      "Training epoch: 380, train loss: 0.01642, val loss: 0.01715\n",
      "Training epoch: 381, train loss: 0.01652, val loss: 0.01723\n",
      "Training epoch: 382, train loss: 0.01639, val loss: 0.01710\n",
      "Training epoch: 383, train loss: 0.01688, val loss: 0.01765\n",
      "Training epoch: 384, train loss: 0.01669, val loss: 0.01741\n",
      "Training epoch: 385, train loss: 0.01663, val loss: 0.01733\n",
      "Training epoch: 386, train loss: 0.01716, val loss: 0.01787\n",
      "Training epoch: 387, train loss: 0.01641, val loss: 0.01715\n",
      "Training epoch: 388, train loss: 0.01657, val loss: 0.01732\n",
      "Training epoch: 389, train loss: 0.01643, val loss: 0.01717\n",
      "Training epoch: 390, train loss: 0.01637, val loss: 0.01708\n",
      "Training epoch: 391, train loss: 0.01675, val loss: 0.01746\n",
      "Training epoch: 392, train loss: 0.01635, val loss: 0.01710\n",
      "Training epoch: 393, train loss: 0.01636, val loss: 0.01708\n",
      "Training epoch: 394, train loss: 0.01646, val loss: 0.01720\n",
      "Training epoch: 395, train loss: 0.01636, val loss: 0.01710\n",
      "Training epoch: 396, train loss: 0.01632, val loss: 0.01704\n",
      "Training epoch: 397, train loss: 0.01668, val loss: 0.01739\n",
      "Training epoch: 398, train loss: 0.01631, val loss: 0.01703\n",
      "Training epoch: 399, train loss: 0.01630, val loss: 0.01703\n",
      "Training epoch: 400, train loss: 0.01630, val loss: 0.01702\n",
      "Training epoch: 401, train loss: 0.01637, val loss: 0.01706\n",
      "Training epoch: 402, train loss: 0.01629, val loss: 0.01702\n",
      "Training epoch: 403, train loss: 0.01635, val loss: 0.01708\n",
      "Training epoch: 404, train loss: 0.01628, val loss: 0.01698\n",
      "Training epoch: 405, train loss: 0.01629, val loss: 0.01699\n",
      "Training epoch: 406, train loss: 0.01630, val loss: 0.01699\n",
      "Training epoch: 407, train loss: 0.01626, val loss: 0.01698\n",
      "Training epoch: 408, train loss: 0.01629, val loss: 0.01698\n",
      "Training epoch: 409, train loss: 0.01638, val loss: 0.01711\n",
      "Training epoch: 410, train loss: 0.01634, val loss: 0.01701\n",
      "Training epoch: 411, train loss: 0.01677, val loss: 0.01748\n",
      "Training epoch: 412, train loss: 0.01626, val loss: 0.01698\n",
      "Training epoch: 413, train loss: 0.01665, val loss: 0.01738\n",
      "Training epoch: 414, train loss: 0.01624, val loss: 0.01695\n",
      "Training epoch: 415, train loss: 0.01625, val loss: 0.01698\n",
      "Training epoch: 416, train loss: 0.01623, val loss: 0.01693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 417, train loss: 0.01658, val loss: 0.01735\n",
      "Training epoch: 418, train loss: 0.01626, val loss: 0.01695\n",
      "Training epoch: 419, train loss: 0.01622, val loss: 0.01694\n",
      "Training epoch: 420, train loss: 0.01644, val loss: 0.01712\n",
      "Training epoch: 421, train loss: 0.01625, val loss: 0.01695\n",
      "Training epoch: 422, train loss: 0.01619, val loss: 0.01690\n",
      "Training epoch: 423, train loss: 0.01683, val loss: 0.01757\n",
      "Training epoch: 424, train loss: 0.01619, val loss: 0.01692\n",
      "Training epoch: 425, train loss: 0.01635, val loss: 0.01705\n",
      "Training epoch: 426, train loss: 0.01630, val loss: 0.01699\n",
      "Training epoch: 427, train loss: 0.01618, val loss: 0.01691\n",
      "Training epoch: 428, train loss: 0.01618, val loss: 0.01691\n",
      "Training epoch: 429, train loss: 0.01623, val loss: 0.01694\n",
      "Training epoch: 430, train loss: 0.01654, val loss: 0.01728\n",
      "Training epoch: 431, train loss: 0.01615, val loss: 0.01689\n",
      "Training epoch: 432, train loss: 0.01623, val loss: 0.01693\n",
      "Training epoch: 433, train loss: 0.01615, val loss: 0.01683\n",
      "Training epoch: 434, train loss: 0.01613, val loss: 0.01683\n",
      "Training epoch: 435, train loss: 0.01614, val loss: 0.01686\n",
      "Training epoch: 436, train loss: 0.01618, val loss: 0.01688\n",
      "Training epoch: 437, train loss: 0.01611, val loss: 0.01682\n",
      "Training epoch: 438, train loss: 0.01611, val loss: 0.01679\n",
      "Training epoch: 439, train loss: 0.01629, val loss: 0.01699\n",
      "Training epoch: 440, train loss: 0.01623, val loss: 0.01694\n",
      "Training epoch: 441, train loss: 0.01611, val loss: 0.01681\n",
      "Training epoch: 442, train loss: 0.01609, val loss: 0.01681\n",
      "Training epoch: 443, train loss: 0.01611, val loss: 0.01678\n",
      "Training epoch: 444, train loss: 0.01615, val loss: 0.01683\n",
      "Training epoch: 445, train loss: 0.01614, val loss: 0.01686\n",
      "Training epoch: 446, train loss: 0.01642, val loss: 0.01710\n",
      "Training epoch: 447, train loss: 0.01606, val loss: 0.01679\n",
      "Training epoch: 448, train loss: 0.01616, val loss: 0.01684\n",
      "Training epoch: 449, train loss: 0.01612, val loss: 0.01682\n",
      "Training epoch: 450, train loss: 0.01605, val loss: 0.01674\n",
      "Training epoch: 451, train loss: 0.01612, val loss: 0.01682\n",
      "Training epoch: 452, train loss: 0.01624, val loss: 0.01695\n",
      "Training epoch: 453, train loss: 0.01675, val loss: 0.01744\n",
      "Training epoch: 454, train loss: 0.01645, val loss: 0.01714\n",
      "Training epoch: 455, train loss: 0.01605, val loss: 0.01676\n",
      "Training epoch: 456, train loss: 0.01665, val loss: 0.01737\n",
      "Training epoch: 457, train loss: 0.01606, val loss: 0.01678\n",
      "Training epoch: 458, train loss: 0.01609, val loss: 0.01682\n",
      "Training epoch: 459, train loss: 0.01613, val loss: 0.01683\n",
      "Training epoch: 460, train loss: 0.01635, val loss: 0.01706\n",
      "Training epoch: 461, train loss: 0.01638, val loss: 0.01710\n",
      "Training epoch: 462, train loss: 0.01628, val loss: 0.01700\n",
      "Training epoch: 463, train loss: 0.01602, val loss: 0.01672\n",
      "Training epoch: 464, train loss: 0.01602, val loss: 0.01675\n",
      "Training epoch: 465, train loss: 0.01601, val loss: 0.01672\n",
      "Training epoch: 466, train loss: 0.01602, val loss: 0.01672\n",
      "Training epoch: 467, train loss: 0.01608, val loss: 0.01681\n",
      "Training epoch: 468, train loss: 0.01598, val loss: 0.01668\n",
      "Training epoch: 469, train loss: 0.01633, val loss: 0.01706\n",
      "Training epoch: 470, train loss: 0.01618, val loss: 0.01685\n",
      "Training epoch: 471, train loss: 0.01598, val loss: 0.01671\n",
      "Training epoch: 472, train loss: 0.01625, val loss: 0.01696\n",
      "Training epoch: 473, train loss: 0.01596, val loss: 0.01666\n",
      "Training epoch: 474, train loss: 0.01628, val loss: 0.01701\n",
      "Training epoch: 475, train loss: 0.01607, val loss: 0.01677\n",
      "Training epoch: 476, train loss: 0.01595, val loss: 0.01666\n",
      "Training epoch: 477, train loss: 0.01595, val loss: 0.01663\n",
      "Training epoch: 478, train loss: 0.01610, val loss: 0.01683\n",
      "Training epoch: 479, train loss: 0.01595, val loss: 0.01666\n",
      "Training epoch: 480, train loss: 0.01594, val loss: 0.01664\n",
      "Training epoch: 481, train loss: 0.01593, val loss: 0.01664\n",
      "Training epoch: 482, train loss: 0.01595, val loss: 0.01665\n",
      "Training epoch: 483, train loss: 0.01600, val loss: 0.01674\n",
      "Training epoch: 484, train loss: 0.01603, val loss: 0.01670\n",
      "Training epoch: 485, train loss: 0.01660, val loss: 0.01730\n",
      "Training epoch: 486, train loss: 0.01604, val loss: 0.01673\n",
      "Training epoch: 487, train loss: 0.01594, val loss: 0.01666\n",
      "Training epoch: 488, train loss: 0.01593, val loss: 0.01663\n",
      "Training epoch: 489, train loss: 0.01590, val loss: 0.01662\n",
      "Training epoch: 490, train loss: 0.01590, val loss: 0.01662\n",
      "Training epoch: 491, train loss: 0.01591, val loss: 0.01658\n",
      "Training epoch: 492, train loss: 0.01598, val loss: 0.01668\n",
      "Training epoch: 493, train loss: 0.01592, val loss: 0.01662\n",
      "Training epoch: 494, train loss: 0.01587, val loss: 0.01658\n",
      "Training epoch: 495, train loss: 0.01588, val loss: 0.01656\n",
      "Training epoch: 496, train loss: 0.01602, val loss: 0.01673\n",
      "Training epoch: 497, train loss: 0.01597, val loss: 0.01668\n",
      "Training epoch: 498, train loss: 0.01586, val loss: 0.01656\n",
      "Training epoch: 499, train loss: 0.01585, val loss: 0.01655\n",
      "Training epoch: 500, train loss: 0.01598, val loss: 0.01665\n",
      "Training epoch: 501, train loss: 0.01597, val loss: 0.01666\n",
      "Training epoch: 502, train loss: 0.01588, val loss: 0.01659\n",
      "Training epoch: 503, train loss: 0.01631, val loss: 0.01705\n",
      "Training epoch: 504, train loss: 0.01591, val loss: 0.01659\n",
      "Training epoch: 505, train loss: 0.01640, val loss: 0.01711\n",
      "Training epoch: 506, train loss: 0.01589, val loss: 0.01660\n",
      "Training epoch: 507, train loss: 0.01588, val loss: 0.01656\n",
      "Training epoch: 508, train loss: 0.01598, val loss: 0.01669\n",
      "Training epoch: 509, train loss: 0.01611, val loss: 0.01681\n",
      "Training epoch: 510, train loss: 0.01585, val loss: 0.01654\n",
      "Training epoch: 511, train loss: 0.01601, val loss: 0.01675\n",
      "Training epoch: 512, train loss: 0.01597, val loss: 0.01669\n",
      "Training epoch: 513, train loss: 0.01583, val loss: 0.01654\n",
      "Training epoch: 514, train loss: 0.01588, val loss: 0.01656\n",
      "Training epoch: 515, train loss: 0.01582, val loss: 0.01654\n",
      "Training epoch: 516, train loss: 0.01581, val loss: 0.01652\n",
      "Training epoch: 517, train loss: 0.01589, val loss: 0.01658\n",
      "Training epoch: 518, train loss: 0.01586, val loss: 0.01655\n",
      "Training epoch: 519, train loss: 0.01604, val loss: 0.01675\n",
      "Training epoch: 520, train loss: 0.01582, val loss: 0.01652\n",
      "Training epoch: 521, train loss: 0.01593, val loss: 0.01665\n",
      "Training epoch: 522, train loss: 0.01582, val loss: 0.01654\n",
      "Training epoch: 523, train loss: 0.01579, val loss: 0.01649\n",
      "Training epoch: 524, train loss: 0.01582, val loss: 0.01650\n",
      "Training epoch: 525, train loss: 0.01578, val loss: 0.01650\n",
      "Training epoch: 526, train loss: 0.01577, val loss: 0.01647\n",
      "Training epoch: 527, train loss: 0.01582, val loss: 0.01651\n",
      "Training epoch: 528, train loss: 0.01580, val loss: 0.01650\n",
      "Training epoch: 529, train loss: 0.01579, val loss: 0.01648\n",
      "Training epoch: 530, train loss: 0.01621, val loss: 0.01693\n",
      "Training epoch: 531, train loss: 0.01594, val loss: 0.01663\n",
      "Training epoch: 532, train loss: 0.01577, val loss: 0.01648\n",
      "Training epoch: 533, train loss: 0.01630, val loss: 0.01705\n",
      "Training epoch: 534, train loss: 0.01580, val loss: 0.01650\n",
      "Training epoch: 535, train loss: 0.01580, val loss: 0.01648\n",
      "Training epoch: 536, train loss: 0.01598, val loss: 0.01668\n",
      "Training epoch: 537, train loss: 0.01606, val loss: 0.01678\n",
      "Training epoch: 538, train loss: 0.01596, val loss: 0.01665\n",
      "Training epoch: 539, train loss: 0.01579, val loss: 0.01650\n",
      "Training epoch: 540, train loss: 0.01585, val loss: 0.01656\n",
      "Training epoch: 541, train loss: 0.01625, val loss: 0.01698\n",
      "Training epoch: 542, train loss: 0.01642, val loss: 0.01714\n",
      "Training epoch: 543, train loss: 0.01610, val loss: 0.01683\n",
      "Training epoch: 544, train loss: 0.01578, val loss: 0.01650\n",
      "Training epoch: 545, train loss: 0.01579, val loss: 0.01650\n",
      "Training epoch: 546, train loss: 0.01599, val loss: 0.01670\n",
      "Training epoch: 547, train loss: 0.01588, val loss: 0.01661\n",
      "Training epoch: 548, train loss: 0.01590, val loss: 0.01662\n",
      "Training epoch: 549, train loss: 0.01582, val loss: 0.01652\n",
      "Training epoch: 550, train loss: 0.01579, val loss: 0.01650\n",
      "Training epoch: 551, train loss: 0.01574, val loss: 0.01645\n",
      "Training epoch: 552, train loss: 0.01581, val loss: 0.01653\n",
      "Training epoch: 553, train loss: 0.01572, val loss: 0.01642\n",
      "Training epoch: 554, train loss: 0.01573, val loss: 0.01645\n",
      "Training epoch: 555, train loss: 0.01572, val loss: 0.01643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 556, train loss: 0.01580, val loss: 0.01650\n",
      "Training epoch: 557, train loss: 0.01579, val loss: 0.01650\n",
      "Training epoch: 558, train loss: 0.01576, val loss: 0.01644\n",
      "Training epoch: 559, train loss: 0.01575, val loss: 0.01646\n",
      "Training epoch: 560, train loss: 0.01581, val loss: 0.01652\n",
      "Training epoch: 561, train loss: 0.01572, val loss: 0.01642\n",
      "Training epoch: 562, train loss: 0.01574, val loss: 0.01644\n",
      "Training epoch: 563, train loss: 0.01597, val loss: 0.01669\n",
      "Training epoch: 564, train loss: 0.01571, val loss: 0.01642\n",
      "Training epoch: 565, train loss: 0.01580, val loss: 0.01652\n",
      "Training epoch: 566, train loss: 0.01599, val loss: 0.01671\n",
      "Training epoch: 567, train loss: 0.01581, val loss: 0.01652\n",
      "Training epoch: 568, train loss: 0.01575, val loss: 0.01643\n",
      "Training epoch: 569, train loss: 0.01603, val loss: 0.01675\n",
      "Training epoch: 570, train loss: 0.01574, val loss: 0.01648\n",
      "Training epoch: 571, train loss: 0.01576, val loss: 0.01646\n",
      "Training epoch: 572, train loss: 0.01572, val loss: 0.01641\n",
      "Training epoch: 573, train loss: 0.01574, val loss: 0.01646\n",
      "Training epoch: 574, train loss: 0.01571, val loss: 0.01642\n",
      "Training epoch: 575, train loss: 0.01600, val loss: 0.01673\n",
      "Training epoch: 576, train loss: 0.01571, val loss: 0.01640\n",
      "Training epoch: 577, train loss: 0.01574, val loss: 0.01645\n",
      "Training epoch: 578, train loss: 0.01581, val loss: 0.01653\n",
      "Training epoch: 579, train loss: 0.01570, val loss: 0.01639\n",
      "Training epoch: 580, train loss: 0.01598, val loss: 0.01669\n",
      "Training epoch: 581, train loss: 0.01592, val loss: 0.01661\n",
      "Training epoch: 582, train loss: 0.01572, val loss: 0.01644\n",
      "Training epoch: 583, train loss: 0.01573, val loss: 0.01643\n",
      "Training epoch: 584, train loss: 0.01575, val loss: 0.01646\n",
      "Training epoch: 585, train loss: 0.01572, val loss: 0.01642\n",
      "Training epoch: 586, train loss: 0.01601, val loss: 0.01675\n",
      "Training epoch: 587, train loss: 0.01569, val loss: 0.01642\n",
      "Training epoch: 588, train loss: 0.01566, val loss: 0.01637\n",
      "Training epoch: 589, train loss: 0.01609, val loss: 0.01678\n",
      "Training epoch: 590, train loss: 0.01569, val loss: 0.01638\n",
      "Training epoch: 591, train loss: 0.01579, val loss: 0.01650\n",
      "Training epoch: 592, train loss: 0.01569, val loss: 0.01640\n",
      "Training epoch: 593, train loss: 0.01580, val loss: 0.01649\n",
      "Training epoch: 594, train loss: 0.01570, val loss: 0.01643\n",
      "Training epoch: 595, train loss: 0.01566, val loss: 0.01636\n",
      "Training epoch: 596, train loss: 0.01582, val loss: 0.01654\n",
      "Training epoch: 597, train loss: 0.01573, val loss: 0.01645\n",
      "Training epoch: 598, train loss: 0.01566, val loss: 0.01635\n",
      "Training epoch: 599, train loss: 0.01595, val loss: 0.01665\n",
      "Training epoch: 600, train loss: 0.01568, val loss: 0.01641\n",
      "Training epoch: 601, train loss: 0.01568, val loss: 0.01638\n",
      "Training epoch: 602, train loss: 0.01569, val loss: 0.01638\n",
      "Training epoch: 603, train loss: 0.01566, val loss: 0.01638\n",
      "Training epoch: 604, train loss: 0.01565, val loss: 0.01636\n",
      "Training epoch: 605, train loss: 0.01566, val loss: 0.01636\n",
      "Training epoch: 606, train loss: 0.01569, val loss: 0.01639\n",
      "Training epoch: 607, train loss: 0.01574, val loss: 0.01643\n",
      "Training epoch: 608, train loss: 0.01573, val loss: 0.01641\n",
      "Training epoch: 609, train loss: 0.01575, val loss: 0.01645\n",
      "Training epoch: 610, train loss: 0.01591, val loss: 0.01663\n",
      "Training epoch: 611, train loss: 0.01613, val loss: 0.01685\n",
      "Training epoch: 612, train loss: 0.01566, val loss: 0.01637\n",
      "Training epoch: 613, train loss: 0.01570, val loss: 0.01637\n",
      "Training epoch: 614, train loss: 0.01568, val loss: 0.01640\n",
      "Training epoch: 615, train loss: 0.01574, val loss: 0.01644\n",
      "Training epoch: 616, train loss: 0.01579, val loss: 0.01648\n",
      "Training epoch: 617, train loss: 0.01565, val loss: 0.01634\n",
      "Training epoch: 618, train loss: 0.01592, val loss: 0.01664\n",
      "Training epoch: 619, train loss: 0.01578, val loss: 0.01649\n",
      "Training epoch: 620, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 621, train loss: 0.01565, val loss: 0.01635\n",
      "Training epoch: 622, train loss: 0.01566, val loss: 0.01637\n",
      "Training epoch: 623, train loss: 0.01574, val loss: 0.01645\n",
      "Training epoch: 624, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 625, train loss: 0.01584, val loss: 0.01654\n",
      "Training epoch: 626, train loss: 0.01576, val loss: 0.01646\n",
      "Training epoch: 627, train loss: 0.01564, val loss: 0.01634\n",
      "Training epoch: 628, train loss: 0.01564, val loss: 0.01634\n",
      "Training epoch: 629, train loss: 0.01562, val loss: 0.01632\n",
      "Training epoch: 630, train loss: 0.01562, val loss: 0.01633\n",
      "Training epoch: 631, train loss: 0.01580, val loss: 0.01650\n",
      "Training epoch: 632, train loss: 0.01565, val loss: 0.01635\n",
      "Training epoch: 633, train loss: 0.01563, val loss: 0.01630\n",
      "Training epoch: 634, train loss: 0.01564, val loss: 0.01632\n",
      "Training epoch: 635, train loss: 0.01568, val loss: 0.01638\n",
      "Training epoch: 636, train loss: 0.01571, val loss: 0.01640\n",
      "Training epoch: 637, train loss: 0.01561, val loss: 0.01633\n",
      "Training epoch: 638, train loss: 0.01573, val loss: 0.01643\n",
      "Training epoch: 639, train loss: 0.01563, val loss: 0.01633\n",
      "Training epoch: 640, train loss: 0.01561, val loss: 0.01632\n",
      "Training epoch: 641, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 642, train loss: 0.01565, val loss: 0.01635\n",
      "Training epoch: 643, train loss: 0.01561, val loss: 0.01631\n",
      "Training epoch: 644, train loss: 0.01562, val loss: 0.01631\n",
      "Training epoch: 645, train loss: 0.01561, val loss: 0.01631\n",
      "Training epoch: 646, train loss: 0.01571, val loss: 0.01640\n",
      "Training epoch: 647, train loss: 0.01568, val loss: 0.01638\n",
      "Training epoch: 648, train loss: 0.01563, val loss: 0.01631\n",
      "Training epoch: 649, train loss: 0.01581, val loss: 0.01651\n",
      "Training epoch: 650, train loss: 0.01562, val loss: 0.01633\n",
      "Training epoch: 651, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 652, train loss: 0.01560, val loss: 0.01630\n",
      "Training epoch: 653, train loss: 0.01572, val loss: 0.01642\n",
      "Training epoch: 654, train loss: 0.01564, val loss: 0.01633\n",
      "Training epoch: 655, train loss: 0.01559, val loss: 0.01629\n",
      "Training epoch: 656, train loss: 0.01567, val loss: 0.01633\n",
      "Training epoch: 657, train loss: 0.01572, val loss: 0.01641\n",
      "Training epoch: 658, train loss: 0.01560, val loss: 0.01631\n",
      "Training epoch: 659, train loss: 0.01565, val loss: 0.01634\n",
      "Training epoch: 660, train loss: 0.01560, val loss: 0.01627\n",
      "Training epoch: 661, train loss: 0.01565, val loss: 0.01630\n",
      "Training epoch: 662, train loss: 0.01577, val loss: 0.01648\n",
      "Training epoch: 663, train loss: 0.01559, val loss: 0.01630\n",
      "Training epoch: 664, train loss: 0.01563, val loss: 0.01631\n",
      "Training epoch: 665, train loss: 0.01581, val loss: 0.01646\n",
      "Training epoch: 666, train loss: 0.01562, val loss: 0.01631\n",
      "Training epoch: 667, train loss: 0.01562, val loss: 0.01631\n",
      "Training epoch: 668, train loss: 0.01600, val loss: 0.01669\n",
      "Training epoch: 669, train loss: 0.01572, val loss: 0.01642\n",
      "Training epoch: 670, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 671, train loss: 0.01568, val loss: 0.01637\n",
      "Training epoch: 672, train loss: 0.01568, val loss: 0.01635\n",
      "Training epoch: 673, train loss: 0.01568, val loss: 0.01638\n",
      "Training epoch: 674, train loss: 0.01562, val loss: 0.01632\n",
      "Training epoch: 675, train loss: 0.01559, val loss: 0.01629\n",
      "Training epoch: 676, train loss: 0.01560, val loss: 0.01629\n",
      "Training epoch: 677, train loss: 0.01581, val loss: 0.01649\n",
      "Training epoch: 678, train loss: 0.01559, val loss: 0.01628\n",
      "Training epoch: 679, train loss: 0.01560, val loss: 0.01626\n",
      "Training epoch: 680, train loss: 0.01569, val loss: 0.01638\n",
      "Training epoch: 681, train loss: 0.01573, val loss: 0.01643\n",
      "Training epoch: 682, train loss: 0.01558, val loss: 0.01627\n",
      "Training epoch: 683, train loss: 0.01558, val loss: 0.01625\n",
      "Training epoch: 684, train loss: 0.01572, val loss: 0.01640\n",
      "Training epoch: 685, train loss: 0.01566, val loss: 0.01633\n",
      "Training epoch: 686, train loss: 0.01557, val loss: 0.01625\n",
      "Training epoch: 687, train loss: 0.01589, val loss: 0.01657\n",
      "Training epoch: 688, train loss: 0.01618, val loss: 0.01687\n",
      "Training epoch: 689, train loss: 0.01585, val loss: 0.01654\n",
      "Training epoch: 690, train loss: 0.01558, val loss: 0.01626\n",
      "Training epoch: 691, train loss: 0.01559, val loss: 0.01626\n",
      "Training epoch: 692, train loss: 0.01567, val loss: 0.01635\n",
      "Training epoch: 693, train loss: 0.01579, val loss: 0.01648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 694, train loss: 0.01559, val loss: 0.01627\n",
      "Training epoch: 695, train loss: 0.01581, val loss: 0.01648\n",
      "Training epoch: 696, train loss: 0.01564, val loss: 0.01633\n",
      "Training epoch: 697, train loss: 0.01568, val loss: 0.01636\n",
      "Training epoch: 698, train loss: 0.01560, val loss: 0.01628\n",
      "Training epoch: 699, train loss: 0.01563, val loss: 0.01631\n",
      "Training epoch: 700, train loss: 0.01558, val loss: 0.01629\n",
      "Training epoch: 701, train loss: 0.01556, val loss: 0.01622\n",
      "Training epoch: 702, train loss: 0.01569, val loss: 0.01635\n",
      "Training epoch: 703, train loss: 0.01570, val loss: 0.01638\n",
      "Training epoch: 704, train loss: 0.01571, val loss: 0.01639\n",
      "Training epoch: 705, train loss: 0.01557, val loss: 0.01624\n",
      "Training epoch: 706, train loss: 0.01556, val loss: 0.01624\n",
      "Training epoch: 707, train loss: 0.01603, val loss: 0.01672\n",
      "Training epoch: 708, train loss: 0.01613, val loss: 0.01682\n",
      "Training epoch: 709, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 710, train loss: 0.01559, val loss: 0.01626\n",
      "Training epoch: 711, train loss: 0.01567, val loss: 0.01635\n",
      "Training epoch: 712, train loss: 0.01614, val loss: 0.01683\n",
      "Training epoch: 713, train loss: 0.01560, val loss: 0.01628\n",
      "Training epoch: 714, train loss: 0.01557, val loss: 0.01626\n",
      "Training epoch: 715, train loss: 0.01573, val loss: 0.01640\n",
      "Training epoch: 716, train loss: 0.01566, val loss: 0.01634\n",
      "Training epoch: 717, train loss: 0.01563, val loss: 0.01630\n",
      "Training epoch: 718, train loss: 0.01571, val loss: 0.01638\n",
      "Training epoch: 719, train loss: 0.01603, val loss: 0.01671\n",
      "Training epoch: 720, train loss: 0.01577, val loss: 0.01644\n",
      "Training epoch: 721, train loss: 0.01559, val loss: 0.01626\n",
      "Training epoch: 722, train loss: 0.01564, val loss: 0.01631\n",
      "Training epoch: 723, train loss: 0.01558, val loss: 0.01624\n",
      "Training epoch: 724, train loss: 0.01576, val loss: 0.01642\n",
      "Training epoch: 725, train loss: 0.01578, val loss: 0.01646\n",
      "Training epoch: 726, train loss: 0.01556, val loss: 0.01624\n",
      "Training epoch: 727, train loss: 0.01557, val loss: 0.01625\n",
      "Training epoch: 728, train loss: 0.01569, val loss: 0.01633\n",
      "Training epoch: 729, train loss: 0.01556, val loss: 0.01622\n",
      "Training epoch: 730, train loss: 0.01557, val loss: 0.01625\n",
      "Training epoch: 731, train loss: 0.01565, val loss: 0.01632\n",
      "Training epoch: 732, train loss: 0.01557, val loss: 0.01623\n",
      "Training epoch: 733, train loss: 0.01555, val loss: 0.01618\n",
      "Training epoch: 734, train loss: 0.01554, val loss: 0.01620\n",
      "Training epoch: 735, train loss: 0.01554, val loss: 0.01621\n",
      "Training epoch: 736, train loss: 0.01555, val loss: 0.01621\n",
      "Training epoch: 737, train loss: 0.01562, val loss: 0.01625\n",
      "Training epoch: 738, train loss: 0.01558, val loss: 0.01623\n",
      "Training epoch: 739, train loss: 0.01569, val loss: 0.01636\n",
      "Training epoch: 740, train loss: 0.01561, val loss: 0.01626\n",
      "Training epoch: 741, train loss: 0.01555, val loss: 0.01621\n",
      "Training epoch: 742, train loss: 0.01563, val loss: 0.01628\n",
      "Training epoch: 743, train loss: 0.01561, val loss: 0.01625\n",
      "Training epoch: 744, train loss: 0.01565, val loss: 0.01629\n",
      "Training epoch: 745, train loss: 0.01555, val loss: 0.01619\n",
      "Training epoch: 746, train loss: 0.01589, val loss: 0.01655\n",
      "Training epoch: 747, train loss: 0.01555, val loss: 0.01619\n",
      "Training epoch: 748, train loss: 0.01555, val loss: 0.01620\n",
      "Training epoch: 749, train loss: 0.01561, val loss: 0.01626\n",
      "Training epoch: 750, train loss: 0.01553, val loss: 0.01618\n",
      "Training epoch: 751, train loss: 0.01560, val loss: 0.01625\n",
      "Training epoch: 752, train loss: 0.01562, val loss: 0.01625\n",
      "Training epoch: 753, train loss: 0.01555, val loss: 0.01618\n",
      "Training epoch: 754, train loss: 0.01562, val loss: 0.01625\n",
      "Training epoch: 755, train loss: 0.01554, val loss: 0.01618\n",
      "Training epoch: 756, train loss: 0.01555, val loss: 0.01619\n",
      "Training epoch: 757, train loss: 0.01564, val loss: 0.01629\n",
      "Training epoch: 758, train loss: 0.01561, val loss: 0.01623\n",
      "Training epoch: 759, train loss: 0.01579, val loss: 0.01641\n",
      "Training epoch: 760, train loss: 0.01556, val loss: 0.01621\n",
      "Training epoch: 761, train loss: 0.01557, val loss: 0.01621\n",
      "Training epoch: 762, train loss: 0.01582, val loss: 0.01646\n",
      "Training epoch: 763, train loss: 0.01556, val loss: 0.01620\n",
      "Training epoch: 764, train loss: 0.01570, val loss: 0.01632\n",
      "Training epoch: 765, train loss: 0.01570, val loss: 0.01633\n",
      "Training epoch: 766, train loss: 0.01553, val loss: 0.01618\n",
      "Training epoch: 767, train loss: 0.01591, val loss: 0.01653\n",
      "Training epoch: 768, train loss: 0.01568, val loss: 0.01630\n",
      "Training epoch: 769, train loss: 0.01557, val loss: 0.01618\n",
      "Training epoch: 770, train loss: 0.01581, val loss: 0.01647\n",
      "Training epoch: 771, train loss: 0.01591, val loss: 0.01654\n",
      "Training epoch: 772, train loss: 0.01558, val loss: 0.01619\n",
      "Training epoch: 773, train loss: 0.01563, val loss: 0.01627\n",
      "Training epoch: 774, train loss: 0.01565, val loss: 0.01628\n",
      "Training epoch: 775, train loss: 0.01555, val loss: 0.01621\n",
      "Training epoch: 776, train loss: 0.01569, val loss: 0.01630\n",
      "Training epoch: 777, train loss: 0.01560, val loss: 0.01620\n",
      "Training epoch: 778, train loss: 0.01554, val loss: 0.01617\n",
      "Training epoch: 779, train loss: 0.01554, val loss: 0.01617\n",
      "Training epoch: 780, train loss: 0.01555, val loss: 0.01619\n",
      "Training epoch: 781, train loss: 0.01563, val loss: 0.01624\n",
      "Training epoch: 782, train loss: 0.01554, val loss: 0.01614\n",
      "Training epoch: 783, train loss: 0.01552, val loss: 0.01612\n",
      "Training epoch: 784, train loss: 0.01568, val loss: 0.01629\n",
      "Training epoch: 785, train loss: 0.01552, val loss: 0.01617\n",
      "Training epoch: 786, train loss: 0.01561, val loss: 0.01624\n",
      "Training epoch: 787, train loss: 0.01554, val loss: 0.01615\n",
      "Training epoch: 788, train loss: 0.01561, val loss: 0.01621\n",
      "Training epoch: 789, train loss: 0.01563, val loss: 0.01624\n",
      "Training epoch: 790, train loss: 0.01556, val loss: 0.01617\n",
      "Training epoch: 791, train loss: 0.01552, val loss: 0.01612\n",
      "Training epoch: 792, train loss: 0.01570, val loss: 0.01632\n",
      "Training epoch: 793, train loss: 0.01551, val loss: 0.01611\n",
      "Training epoch: 794, train loss: 0.01551, val loss: 0.01611\n",
      "Training epoch: 795, train loss: 0.01552, val loss: 0.01613\n",
      "Training epoch: 796, train loss: 0.01554, val loss: 0.01614\n",
      "Training epoch: 797, train loss: 0.01564, val loss: 0.01625\n",
      "Training epoch: 798, train loss: 0.01555, val loss: 0.01613\n",
      "Training epoch: 799, train loss: 0.01555, val loss: 0.01615\n",
      "Training epoch: 800, train loss: 0.01572, val loss: 0.01634\n",
      "Training epoch: 801, train loss: 0.01556, val loss: 0.01616\n",
      "Training epoch: 802, train loss: 0.01552, val loss: 0.01611\n",
      "Training epoch: 803, train loss: 0.01559, val loss: 0.01618\n",
      "Training epoch: 804, train loss: 0.01552, val loss: 0.01612\n",
      "Training epoch: 805, train loss: 0.01562, val loss: 0.01625\n",
      "Training epoch: 806, train loss: 0.01582, val loss: 0.01641\n",
      "Training epoch: 807, train loss: 0.01560, val loss: 0.01620\n",
      "Training epoch: 808, train loss: 0.01561, val loss: 0.01620\n",
      "Training epoch: 809, train loss: 0.01561, val loss: 0.01619\n",
      "Training epoch: 810, train loss: 0.01563, val loss: 0.01624\n",
      "Training epoch: 811, train loss: 0.01552, val loss: 0.01612\n",
      "Training epoch: 812, train loss: 0.01551, val loss: 0.01610\n",
      "Training epoch: 813, train loss: 0.01551, val loss: 0.01610\n",
      "Training epoch: 814, train loss: 0.01552, val loss: 0.01610\n",
      "Training epoch: 815, train loss: 0.01574, val loss: 0.01633\n",
      "Training epoch: 816, train loss: 0.01554, val loss: 0.01614\n",
      "Training epoch: 817, train loss: 0.01551, val loss: 0.01611\n",
      "Training epoch: 818, train loss: 0.01554, val loss: 0.01612\n",
      "Training epoch: 819, train loss: 0.01551, val loss: 0.01608\n",
      "Training epoch: 820, train loss: 0.01569, val loss: 0.01627\n",
      "Training epoch: 821, train loss: 0.01572, val loss: 0.01630\n",
      "Training epoch: 822, train loss: 0.01558, val loss: 0.01618\n",
      "Training epoch: 823, train loss: 0.01554, val loss: 0.01612\n",
      "Training epoch: 824, train loss: 0.01551, val loss: 0.01610\n",
      "Training epoch: 825, train loss: 0.01555, val loss: 0.01614\n",
      "Training epoch: 826, train loss: 0.01556, val loss: 0.01613\n",
      "Training epoch: 827, train loss: 0.01613, val loss: 0.01671\n",
      "Training epoch: 828, train loss: 0.01555, val loss: 0.01613\n",
      "Training epoch: 829, train loss: 0.01558, val loss: 0.01615\n",
      "Training epoch: 830, train loss: 0.01568, val loss: 0.01625\n",
      "Training epoch: 831, train loss: 0.01554, val loss: 0.01609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 832, train loss: 0.01553, val loss: 0.01610\n",
      "Training epoch: 833, train loss: 0.01553, val loss: 0.01613\n",
      "Training epoch: 834, train loss: 0.01554, val loss: 0.01612\n",
      "Training epoch: 835, train loss: 0.01554, val loss: 0.01612\n",
      "Training epoch: 836, train loss: 0.01551, val loss: 0.01610\n",
      "Training epoch: 837, train loss: 0.01550, val loss: 0.01607\n",
      "Training epoch: 838, train loss: 0.01551, val loss: 0.01608\n",
      "Training epoch: 839, train loss: 0.01552, val loss: 0.01608\n",
      "Training epoch: 840, train loss: 0.01559, val loss: 0.01615\n",
      "Training epoch: 841, train loss: 0.01551, val loss: 0.01608\n",
      "Training epoch: 842, train loss: 0.01553, val loss: 0.01609\n",
      "Training epoch: 843, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 844, train loss: 0.01553, val loss: 0.01610\n",
      "Training epoch: 845, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 846, train loss: 0.01550, val loss: 0.01607\n",
      "Training epoch: 847, train loss: 0.01559, val loss: 0.01615\n",
      "Training epoch: 848, train loss: 0.01553, val loss: 0.01610\n",
      "Training epoch: 849, train loss: 0.01571, val loss: 0.01630\n",
      "Training epoch: 850, train loss: 0.01553, val loss: 0.01611\n",
      "Training epoch: 851, train loss: 0.01603, val loss: 0.01661\n",
      "Training epoch: 852, train loss: 0.01576, val loss: 0.01633\n",
      "Training epoch: 853, train loss: 0.01574, val loss: 0.01632\n",
      "Training epoch: 854, train loss: 0.01553, val loss: 0.01610\n",
      "Training epoch: 855, train loss: 0.01554, val loss: 0.01612\n",
      "Training epoch: 856, train loss: 0.01552, val loss: 0.01608\n",
      "Training epoch: 857, train loss: 0.01568, val loss: 0.01623\n",
      "Training epoch: 858, train loss: 0.01557, val loss: 0.01613\n",
      "Training epoch: 859, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 860, train loss: 0.01560, val loss: 0.01615\n",
      "Training epoch: 861, train loss: 0.01552, val loss: 0.01608\n",
      "Training epoch: 862, train loss: 0.01553, val loss: 0.01610\n",
      "Training epoch: 863, train loss: 0.01562, val loss: 0.01618\n",
      "Training epoch: 864, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 865, train loss: 0.01556, val loss: 0.01612\n",
      "Training epoch: 866, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 867, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 868, train loss: 0.01551, val loss: 0.01609\n",
      "Training epoch: 869, train loss: 0.01553, val loss: 0.01608\n",
      "Training epoch: 870, train loss: 0.01561, val loss: 0.01615\n",
      "Training epoch: 871, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 872, train loss: 0.01550, val loss: 0.01608\n",
      "Training epoch: 873, train loss: 0.01561, val loss: 0.01617\n",
      "Training epoch: 874, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 875, train loss: 0.01570, val loss: 0.01626\n",
      "Training epoch: 876, train loss: 0.01559, val loss: 0.01613\n",
      "Training epoch: 877, train loss: 0.01550, val loss: 0.01605\n",
      "Training epoch: 878, train loss: 0.01565, val loss: 0.01621\n",
      "Training epoch: 879, train loss: 0.01571, val loss: 0.01628\n",
      "Training epoch: 880, train loss: 0.01552, val loss: 0.01608\n",
      "Training epoch: 881, train loss: 0.01550, val loss: 0.01606\n",
      "Training epoch: 882, train loss: 0.01566, val loss: 0.01621\n",
      "Training epoch: 883, train loss: 0.01557, val loss: 0.01612\n",
      "Training epoch: 884, train loss: 0.01564, val loss: 0.01619\n",
      "Training epoch: 885, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 886, train loss: 0.01550, val loss: 0.01604\n",
      "Training epoch: 887, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 888, train loss: 0.01550, val loss: 0.01606\n",
      "Training epoch: 889, train loss: 0.01553, val loss: 0.01611\n",
      "Training epoch: 890, train loss: 0.01553, val loss: 0.01608\n",
      "Training epoch: 891, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 892, train loss: 0.01552, val loss: 0.01606\n",
      "Training epoch: 893, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 894, train loss: 0.01554, val loss: 0.01609\n",
      "Training epoch: 895, train loss: 0.01565, val loss: 0.01621\n",
      "Training epoch: 896, train loss: 0.01555, val loss: 0.01610\n",
      "Training epoch: 897, train loss: 0.01561, val loss: 0.01617\n",
      "Training epoch: 898, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 899, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 900, train loss: 0.01551, val loss: 0.01608\n",
      "Training epoch: 901, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 902, train loss: 0.01552, val loss: 0.01604\n",
      "Training epoch: 903, train loss: 0.01573, val loss: 0.01629\n",
      "Training epoch: 904, train loss: 0.01579, val loss: 0.01636\n",
      "Training epoch: 905, train loss: 0.01572, val loss: 0.01626\n",
      "Training epoch: 906, train loss: 0.01555, val loss: 0.01608\n",
      "Training epoch: 907, train loss: 0.01555, val loss: 0.01611\n",
      "Training epoch: 908, train loss: 0.01562, val loss: 0.01616\n",
      "Training epoch: 909, train loss: 0.01558, val loss: 0.01611\n",
      "Training epoch: 910, train loss: 0.01563, val loss: 0.01616\n",
      "Training epoch: 911, train loss: 0.01588, val loss: 0.01645\n",
      "Training epoch: 912, train loss: 0.01578, val loss: 0.01631\n",
      "Training epoch: 913, train loss: 0.01561, val loss: 0.01615\n",
      "Training epoch: 914, train loss: 0.01553, val loss: 0.01609\n",
      "Training epoch: 915, train loss: 0.01553, val loss: 0.01607\n",
      "Training epoch: 916, train loss: 0.01558, val loss: 0.01613\n",
      "Training epoch: 917, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 918, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 919, train loss: 0.01550, val loss: 0.01606\n",
      "Training epoch: 920, train loss: 0.01550, val loss: 0.01605\n",
      "Training epoch: 921, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 922, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 923, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 924, train loss: 0.01558, val loss: 0.01612\n",
      "Training epoch: 925, train loss: 0.01595, val loss: 0.01650\n",
      "Training epoch: 926, train loss: 0.01582, val loss: 0.01638\n",
      "Training epoch: 927, train loss: 0.01569, val loss: 0.01622\n",
      "Training epoch: 928, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 929, train loss: 0.01556, val loss: 0.01610\n",
      "Training epoch: 930, train loss: 0.01553, val loss: 0.01607\n",
      "Training epoch: 931, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 932, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 933, train loss: 0.01562, val loss: 0.01617\n",
      "Training epoch: 934, train loss: 0.01552, val loss: 0.01606\n",
      "Training epoch: 935, train loss: 0.01558, val loss: 0.01609\n",
      "Training epoch: 936, train loss: 0.01558, val loss: 0.01613\n",
      "Training epoch: 937, train loss: 0.01559, val loss: 0.01615\n",
      "Training epoch: 938, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 939, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 940, train loss: 0.01579, val loss: 0.01634\n",
      "Training epoch: 941, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 942, train loss: 0.01569, val loss: 0.01622\n",
      "Training epoch: 943, train loss: 0.01557, val loss: 0.01610\n",
      "Training epoch: 944, train loss: 0.01556, val loss: 0.01609\n",
      "Training epoch: 945, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 946, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 947, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 948, train loss: 0.01554, val loss: 0.01609\n",
      "Training epoch: 949, train loss: 0.01550, val loss: 0.01604\n",
      "Training epoch: 950, train loss: 0.01556, val loss: 0.01609\n",
      "Training epoch: 951, train loss: 0.01557, val loss: 0.01608\n",
      "Training epoch: 952, train loss: 0.01554, val loss: 0.01607\n",
      "Training epoch: 953, train loss: 0.01575, val loss: 0.01629\n",
      "Training epoch: 954, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 955, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 956, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 957, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 958, train loss: 0.01557, val loss: 0.01612\n",
      "Training epoch: 959, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 960, train loss: 0.01560, val loss: 0.01613\n",
      "Training epoch: 961, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 962, train loss: 0.01554, val loss: 0.01609\n",
      "Training epoch: 963, train loss: 0.01551, val loss: 0.01604\n",
      "Training epoch: 964, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 965, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 966, train loss: 0.01556, val loss: 0.01609\n",
      "Training epoch: 967, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 968, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 969, train loss: 0.01577, val loss: 0.01631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 970, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 971, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 972, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 973, train loss: 0.01555, val loss: 0.01609\n",
      "Training epoch: 974, train loss: 0.01563, val loss: 0.01617\n",
      "Training epoch: 975, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 976, train loss: 0.01552, val loss: 0.01606\n",
      "Training epoch: 977, train loss: 0.01570, val loss: 0.01623\n",
      "Training epoch: 978, train loss: 0.01561, val loss: 0.01614\n",
      "Training epoch: 979, train loss: 0.01548, val loss: 0.01603\n",
      "Training epoch: 980, train loss: 0.01558, val loss: 0.01611\n",
      "Training epoch: 981, train loss: 0.01593, val loss: 0.01645\n",
      "Training epoch: 982, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 983, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 984, train loss: 0.01551, val loss: 0.01603\n",
      "Training epoch: 985, train loss: 0.01552, val loss: 0.01606\n",
      "Training epoch: 986, train loss: 0.01564, val loss: 0.01618\n",
      "Training epoch: 987, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 988, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 989, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 990, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 991, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 992, train loss: 0.01568, val loss: 0.01620\n",
      "Training epoch: 993, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 994, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 995, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 996, train loss: 0.01550, val loss: 0.01604\n",
      "Training epoch: 997, train loss: 0.01569, val loss: 0.01623\n",
      "Training epoch: 998, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 999, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 1000, train loss: 0.01592, val loss: 0.01648\n",
      "Training epoch: 1001, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1002, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1003, train loss: 0.01565, val loss: 0.01617\n",
      "Training epoch: 1004, train loss: 0.01574, val loss: 0.01629\n",
      "Training epoch: 1005, train loss: 0.01552, val loss: 0.01606\n",
      "Training epoch: 1006, train loss: 0.01572, val loss: 0.01625\n",
      "Training epoch: 1007, train loss: 0.01565, val loss: 0.01618\n",
      "Training epoch: 1008, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 1009, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 1010, train loss: 0.01565, val loss: 0.01617\n",
      "Training epoch: 1011, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1012, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 1013, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 1014, train loss: 0.01598, val loss: 0.01654\n",
      "Training epoch: 1015, train loss: 0.01578, val loss: 0.01631\n",
      "Training epoch: 1016, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 1017, train loss: 0.01594, val loss: 0.01647\n",
      "Training epoch: 1018, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1019, train loss: 0.01554, val loss: 0.01605\n",
      "Training epoch: 1020, train loss: 0.01549, val loss: 0.01604\n",
      "Training epoch: 1021, train loss: 0.01568, val loss: 0.01621\n",
      "Training epoch: 1022, train loss: 0.01561, val loss: 0.01613\n",
      "Training epoch: 1023, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1024, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1025, train loss: 0.01562, val loss: 0.01615\n",
      "Training epoch: 1026, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1027, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1028, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 1029, train loss: 0.01585, val loss: 0.01640\n",
      "Training epoch: 1030, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1031, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1032, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1033, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1034, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1035, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1036, train loss: 0.01562, val loss: 0.01616\n",
      "Training epoch: 1037, train loss: 0.01569, val loss: 0.01622\n",
      "Training epoch: 1038, train loss: 0.01557, val loss: 0.01608\n",
      "Training epoch: 1039, train loss: 0.01551, val loss: 0.01604\n",
      "Training epoch: 1040, train loss: 0.01555, val loss: 0.01608\n",
      "Training epoch: 1041, train loss: 0.01574, val loss: 0.01627\n",
      "Training epoch: 1042, train loss: 0.01561, val loss: 0.01612\n",
      "Training epoch: 1043, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 1044, train loss: 0.01559, val loss: 0.01611\n",
      "Training epoch: 1045, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 1046, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 1047, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1048, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1049, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1050, train loss: 0.01552, val loss: 0.01607\n",
      "Training epoch: 1051, train loss: 0.01557, val loss: 0.01611\n",
      "Training epoch: 1052, train loss: 0.01552, val loss: 0.01604\n",
      "Training epoch: 1053, train loss: 0.01568, val loss: 0.01622\n",
      "Training epoch: 1054, train loss: 0.01577, val loss: 0.01630\n",
      "Training epoch: 1055, train loss: 0.01570, val loss: 0.01623\n",
      "Training epoch: 1056, train loss: 0.01571, val loss: 0.01623\n",
      "Training epoch: 1057, train loss: 0.01562, val loss: 0.01617\n",
      "Training epoch: 1058, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1059, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1060, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1061, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1062, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1063, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 1064, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1065, train loss: 0.01561, val loss: 0.01615\n",
      "Training epoch: 1066, train loss: 0.01575, val loss: 0.01630\n",
      "Training epoch: 1067, train loss: 0.01570, val loss: 0.01620\n",
      "Training epoch: 1068, train loss: 0.01574, val loss: 0.01628\n",
      "Training epoch: 1069, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1070, train loss: 0.01558, val loss: 0.01610\n",
      "Training epoch: 1071, train loss: 0.01569, val loss: 0.01621\n",
      "Training epoch: 1072, train loss: 0.01554, val loss: 0.01605\n",
      "Training epoch: 1073, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1074, train loss: 0.01577, val loss: 0.01630\n",
      "Training epoch: 1075, train loss: 0.01606, val loss: 0.01658\n",
      "Training epoch: 1076, train loss: 0.01607, val loss: 0.01661\n",
      "Training epoch: 1077, train loss: 0.01564, val loss: 0.01615\n",
      "Training epoch: 1078, train loss: 0.01551, val loss: 0.01603\n",
      "Training epoch: 1079, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1080, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1081, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1082, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1083, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1084, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1085, train loss: 0.01551, val loss: 0.01604\n",
      "Training epoch: 1086, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 1087, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1088, train loss: 0.01566, val loss: 0.01619\n",
      "Training epoch: 1089, train loss: 0.01557, val loss: 0.01608\n",
      "Training epoch: 1090, train loss: 0.01553, val loss: 0.01608\n",
      "Training epoch: 1091, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 1092, train loss: 0.01561, val loss: 0.01614\n",
      "Training epoch: 1093, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1094, train loss: 0.01553, val loss: 0.01604\n",
      "Training epoch: 1095, train loss: 0.01553, val loss: 0.01604\n",
      "Training epoch: 1096, train loss: 0.01558, val loss: 0.01610\n",
      "Training epoch: 1097, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1098, train loss: 0.01562, val loss: 0.01613\n",
      "Training epoch: 1099, train loss: 0.01562, val loss: 0.01615\n",
      "Training epoch: 1100, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1101, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1102, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1103, train loss: 0.01564, val loss: 0.01617\n",
      "Training epoch: 1104, train loss: 0.01592, val loss: 0.01646\n",
      "Training epoch: 1105, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 1106, train loss: 0.01551, val loss: 0.01601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1107, train loss: 0.01578, val loss: 0.01632\n",
      "Training epoch: 1108, train loss: 0.01575, val loss: 0.01629\n",
      "Training epoch: 1109, train loss: 0.01556, val loss: 0.01607\n",
      "Training epoch: 1110, train loss: 0.01559, val loss: 0.01610\n",
      "Training epoch: 1111, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 1112, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1113, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1114, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1115, train loss: 0.01557, val loss: 0.01609\n",
      "Training epoch: 1116, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1117, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1118, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1119, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1120, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1121, train loss: 0.01551, val loss: 0.01604\n",
      "Training epoch: 1122, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1123, train loss: 0.01546, val loss: 0.01597\n",
      "Training epoch: 1124, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1125, train loss: 0.01579, val loss: 0.01631\n",
      "Training epoch: 1126, train loss: 0.01564, val loss: 0.01615\n",
      "Training epoch: 1127, train loss: 0.01568, val loss: 0.01620\n",
      "Training epoch: 1128, train loss: 0.01565, val loss: 0.01617\n",
      "Training epoch: 1129, train loss: 0.01565, val loss: 0.01618\n",
      "Training epoch: 1130, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1131, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1132, train loss: 0.01559, val loss: 0.01611\n",
      "Training epoch: 1133, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1134, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1135, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1136, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1137, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 1138, train loss: 0.01564, val loss: 0.01616\n",
      "Training epoch: 1139, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1140, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1141, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1142, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1143, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1144, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1145, train loss: 0.01569, val loss: 0.01621\n",
      "Training epoch: 1146, train loss: 0.01559, val loss: 0.01611\n",
      "Training epoch: 1147, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1148, train loss: 0.01569, val loss: 0.01618\n",
      "Training epoch: 1149, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1150, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1151, train loss: 0.01553, val loss: 0.01604\n",
      "Training epoch: 1152, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1153, train loss: 0.01547, val loss: 0.01598\n",
      "Training epoch: 1154, train loss: 0.01553, val loss: 0.01604\n",
      "Training epoch: 1155, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1156, train loss: 0.01552, val loss: 0.01603\n",
      "Training epoch: 1157, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1158, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1159, train loss: 0.01607, val loss: 0.01660\n",
      "Training epoch: 1160, train loss: 0.01575, val loss: 0.01628\n",
      "Training epoch: 1161, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1162, train loss: 0.01561, val loss: 0.01613\n",
      "Training epoch: 1163, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1164, train loss: 0.01565, val loss: 0.01616\n",
      "Training epoch: 1165, train loss: 0.01598, val loss: 0.01648\n",
      "Training epoch: 1166, train loss: 0.01570, val loss: 0.01622\n",
      "Training epoch: 1167, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1168, train loss: 0.01564, val loss: 0.01617\n",
      "Training epoch: 1169, train loss: 0.01589, val loss: 0.01640\n",
      "Training epoch: 1170, train loss: 0.01583, val loss: 0.01638\n",
      "Training epoch: 1171, train loss: 0.01574, val loss: 0.01627\n",
      "Training epoch: 1172, train loss: 0.01571, val loss: 0.01622\n",
      "Training epoch: 1173, train loss: 0.01574, val loss: 0.01627\n",
      "Training epoch: 1174, train loss: 0.01557, val loss: 0.01609\n",
      "Training epoch: 1175, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1176, train loss: 0.01563, val loss: 0.01615\n",
      "Training epoch: 1177, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 1178, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1179, train loss: 0.01584, val loss: 0.01634\n",
      "Training epoch: 1180, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1181, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 1182, train loss: 0.01560, val loss: 0.01611\n",
      "Training epoch: 1183, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1184, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1185, train loss: 0.01556, val loss: 0.01608\n",
      "Training epoch: 1186, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1187, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1188, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1189, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1190, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1191, train loss: 0.01555, val loss: 0.01605\n",
      "Training epoch: 1192, train loss: 0.01555, val loss: 0.01605\n",
      "Training epoch: 1193, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1194, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 1195, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1196, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1197, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1198, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1199, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 1200, train loss: 0.01551, val loss: 0.01603\n",
      "Training epoch: 1201, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1202, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1203, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1204, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1205, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1206, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1207, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1208, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1209, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1210, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1211, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1212, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1213, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1214, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1215, train loss: 0.01547, val loss: 0.01600\n",
      "Training epoch: 1216, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1217, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 1218, train loss: 0.01579, val loss: 0.01629\n",
      "Training epoch: 1219, train loss: 0.01571, val loss: 0.01622\n",
      "Training epoch: 1220, train loss: 0.01569, val loss: 0.01617\n",
      "Training epoch: 1221, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 1222, train loss: 0.01555, val loss: 0.01608\n",
      "Training epoch: 1223, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1224, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1225, train loss: 0.01556, val loss: 0.01609\n",
      "Training epoch: 1226, train loss: 0.01571, val loss: 0.01622\n",
      "Training epoch: 1227, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1228, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1229, train loss: 0.01557, val loss: 0.01606\n",
      "Training epoch: 1230, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1231, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1232, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1233, train loss: 0.01570, val loss: 0.01621\n",
      "Training epoch: 1234, train loss: 0.01578, val loss: 0.01631\n",
      "Training epoch: 1235, train loss: 0.01559, val loss: 0.01609\n",
      "Training epoch: 1236, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 1237, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1238, train loss: 0.01556, val loss: 0.01605\n",
      "Training epoch: 1239, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 1240, train loss: 0.01582, val loss: 0.01633\n",
      "Training epoch: 1241, train loss: 0.01549, val loss: 0.01599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1242, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1243, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1244, train loss: 0.01558, val loss: 0.01608\n",
      "Training epoch: 1245, train loss: 0.01547, val loss: 0.01598\n",
      "Training epoch: 1246, train loss: 0.01564, val loss: 0.01613\n",
      "Training epoch: 1247, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1248, train loss: 0.01568, val loss: 0.01619\n",
      "Training epoch: 1249, train loss: 0.01564, val loss: 0.01617\n",
      "Training epoch: 1250, train loss: 0.01559, val loss: 0.01607\n",
      "Training epoch: 1251, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1252, train loss: 0.01559, val loss: 0.01610\n",
      "Training epoch: 1253, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1254, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1255, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1256, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1257, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1258, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1259, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1260, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1261, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1262, train loss: 0.01552, val loss: 0.01604\n",
      "Training epoch: 1263, train loss: 0.01570, val loss: 0.01622\n",
      "Training epoch: 1264, train loss: 0.01588, val loss: 0.01639\n",
      "Training epoch: 1265, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1266, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1267, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1268, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1269, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1270, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1271, train loss: 0.01567, val loss: 0.01616\n",
      "Training epoch: 1272, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1273, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1274, train loss: 0.01558, val loss: 0.01609\n",
      "Training epoch: 1275, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1276, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1277, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1278, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1279, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1280, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1281, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1282, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1283, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1284, train loss: 0.01558, val loss: 0.01606\n",
      "Training epoch: 1285, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1286, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1287, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1288, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1289, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1290, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1291, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1292, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1293, train loss: 0.01547, val loss: 0.01598\n",
      "Training epoch: 1294, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1295, train loss: 0.01560, val loss: 0.01611\n",
      "Training epoch: 1296, train loss: 0.01555, val loss: 0.01606\n",
      "Training epoch: 1297, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1298, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1299, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1300, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1301, train loss: 0.01566, val loss: 0.01616\n",
      "Training epoch: 1302, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1303, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1304, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1305, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1306, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1307, train loss: 0.01559, val loss: 0.01609\n",
      "Training epoch: 1308, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1309, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1310, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 1311, train loss: 0.01572, val loss: 0.01624\n",
      "Training epoch: 1312, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 1313, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1314, train loss: 0.01551, val loss: 0.01603\n",
      "Training epoch: 1315, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1316, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1317, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1318, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1319, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1320, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 1321, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1322, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1323, train loss: 0.01558, val loss: 0.01608\n",
      "Training epoch: 1324, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1325, train loss: 0.01564, val loss: 0.01615\n",
      "Training epoch: 1326, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1327, train loss: 0.01567, val loss: 0.01614\n",
      "Training epoch: 1328, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1329, train loss: 0.01579, val loss: 0.01629\n",
      "Training epoch: 1330, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1331, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1332, train loss: 0.01562, val loss: 0.01611\n",
      "Training epoch: 1333, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 1334, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1335, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1336, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1337, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1338, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1339, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1340, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1341, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1342, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1343, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1344, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1345, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1346, train loss: 0.01557, val loss: 0.01608\n",
      "Training epoch: 1347, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1348, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1349, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1350, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1351, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1352, train loss: 0.01552, val loss: 0.01603\n",
      "Training epoch: 1353, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 1354, train loss: 0.01569, val loss: 0.01619\n",
      "Training epoch: 1355, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 1356, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1357, train loss: 0.01588, val loss: 0.01637\n",
      "Training epoch: 1358, train loss: 0.01557, val loss: 0.01606\n",
      "Training epoch: 1359, train loss: 0.01577, val loss: 0.01626\n",
      "Training epoch: 1360, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1361, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1362, train loss: 0.01557, val loss: 0.01609\n",
      "Training epoch: 1363, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1364, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1365, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1366, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1367, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1368, train loss: 0.01572, val loss: 0.01621\n",
      "Training epoch: 1369, train loss: 0.01557, val loss: 0.01606\n",
      "Training epoch: 1370, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1371, train loss: 0.01565, val loss: 0.01616\n",
      "Training epoch: 1372, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1373, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1374, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1375, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1376, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1377, train loss: 0.01549, val loss: 0.01597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1378, train loss: 0.01567, val loss: 0.01617\n",
      "Training epoch: 1379, train loss: 0.01563, val loss: 0.01613\n",
      "Training epoch: 1380, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1381, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1382, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1383, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1384, train loss: 0.01569, val loss: 0.01618\n",
      "Training epoch: 1385, train loss: 0.01557, val loss: 0.01607\n",
      "Training epoch: 1386, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1387, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1388, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1389, train loss: 0.01556, val loss: 0.01605\n",
      "Training epoch: 1390, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1391, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1392, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1393, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1394, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1395, train loss: 0.01552, val loss: 0.01603\n",
      "Training epoch: 1396, train loss: 0.01555, val loss: 0.01605\n",
      "Training epoch: 1397, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1398, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1399, train loss: 0.01565, val loss: 0.01615\n",
      "Training epoch: 1400, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1401, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1402, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1403, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1404, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1405, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1406, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1407, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1408, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1409, train loss: 0.01552, val loss: 0.01603\n",
      "Training epoch: 1410, train loss: 0.01565, val loss: 0.01614\n",
      "Training epoch: 1411, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1412, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1413, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1414, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1415, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1416, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1417, train loss: 0.01570, val loss: 0.01619\n",
      "Training epoch: 1418, train loss: 0.01598, val loss: 0.01649\n",
      "Training epoch: 1419, train loss: 0.01576, val loss: 0.01627\n",
      "Training epoch: 1420, train loss: 0.01559, val loss: 0.01610\n",
      "Training epoch: 1421, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1422, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1423, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1424, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1425, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1426, train loss: 0.01592, val loss: 0.01641\n",
      "Training epoch: 1427, train loss: 0.01590, val loss: 0.01638\n",
      "Training epoch: 1428, train loss: 0.01562, val loss: 0.01614\n",
      "Training epoch: 1429, train loss: 0.01563, val loss: 0.01613\n",
      "Training epoch: 1430, train loss: 0.01587, val loss: 0.01637\n",
      "Training epoch: 1431, train loss: 0.01574, val loss: 0.01624\n",
      "Training epoch: 1432, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1433, train loss: 0.01558, val loss: 0.01608\n",
      "Training epoch: 1434, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1435, train loss: 0.01564, val loss: 0.01610\n",
      "Training epoch: 1436, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1437, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1438, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1439, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1440, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1441, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1442, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1443, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1444, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1445, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 1446, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1447, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 1448, train loss: 0.01560, val loss: 0.01610\n",
      "Training epoch: 1449, train loss: 0.01555, val loss: 0.01604\n",
      "Training epoch: 1450, train loss: 0.01595, val loss: 0.01645\n",
      "Training epoch: 1451, train loss: 0.01590, val loss: 0.01642\n",
      "Training epoch: 1452, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1453, train loss: 0.01562, val loss: 0.01610\n",
      "Training epoch: 1454, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 1455, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1456, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1457, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1458, train loss: 0.01563, val loss: 0.01613\n",
      "Training epoch: 1459, train loss: 0.01601, val loss: 0.01653\n",
      "Training epoch: 1460, train loss: 0.01592, val loss: 0.01644\n",
      "Training epoch: 1461, train loss: 0.01570, val loss: 0.01618\n",
      "Training epoch: 1462, train loss: 0.01577, val loss: 0.01627\n",
      "Training epoch: 1463, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1464, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1465, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1466, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1467, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1468, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1469, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1470, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1471, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1472, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1473, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1474, train loss: 0.01592, val loss: 0.01641\n",
      "Training epoch: 1475, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1476, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1477, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1478, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1479, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1480, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1481, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1482, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1483, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1484, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1485, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1486, train loss: 0.01584, val loss: 0.01635\n",
      "Training epoch: 1487, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 1488, train loss: 0.01570, val loss: 0.01618\n",
      "Training epoch: 1489, train loss: 0.01558, val loss: 0.01606\n",
      "Training epoch: 1490, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1491, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1492, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1493, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1494, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1495, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1496, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1497, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1498, train loss: 0.01575, val loss: 0.01624\n",
      "Training epoch: 1499, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 1500, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1501, train loss: 0.01555, val loss: 0.01606\n",
      "Training epoch: 1502, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1503, train loss: 0.01559, val loss: 0.01608\n",
      "Training epoch: 1504, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1505, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1506, train loss: 0.01562, val loss: 0.01611\n",
      "Training epoch: 1507, train loss: 0.01555, val loss: 0.01604\n",
      "Training epoch: 1508, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1509, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1510, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1511, train loss: 0.01557, val loss: 0.01607\n",
      "Training epoch: 1512, train loss: 0.01565, val loss: 0.01615\n",
      "Training epoch: 1513, train loss: 0.01551, val loss: 0.01598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1514, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1515, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 1516, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1517, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1518, train loss: 0.01589, val loss: 0.01639\n",
      "Training epoch: 1519, train loss: 0.01559, val loss: 0.01608\n",
      "Training epoch: 1520, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1521, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1522, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1523, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1524, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1525, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1526, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1527, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1528, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1529, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1530, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1531, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1532, train loss: 0.01578, val loss: 0.01630\n",
      "Training epoch: 1533, train loss: 0.01587, val loss: 0.01638\n",
      "Training epoch: 1534, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1535, train loss: 0.01566, val loss: 0.01613\n",
      "Training epoch: 1536, train loss: 0.01559, val loss: 0.01608\n",
      "Training epoch: 1537, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1538, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1539, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1540, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1541, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1542, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1543, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1544, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1545, train loss: 0.01573, val loss: 0.01624\n",
      "Training epoch: 1546, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1547, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1548, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1549, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1550, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1551, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1552, train loss: 0.01579, val loss: 0.01626\n",
      "Training epoch: 1553, train loss: 0.01558, val loss: 0.01607\n",
      "Training epoch: 1554, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 1555, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1556, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1557, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1558, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 1559, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1560, train loss: 0.01564, val loss: 0.01613\n",
      "Training epoch: 1561, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1562, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 1563, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1564, train loss: 0.01571, val loss: 0.01618\n",
      "Training epoch: 1565, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1566, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1567, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1568, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1569, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1570, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1571, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1572, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1573, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1574, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1575, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1576, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1577, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1578, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1579, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1580, train loss: 0.01562, val loss: 0.01610\n",
      "Training epoch: 1581, train loss: 0.01593, val loss: 0.01643\n",
      "Training epoch: 1582, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1583, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1584, train loss: 0.01580, val loss: 0.01629\n",
      "Training epoch: 1585, train loss: 0.01577, val loss: 0.01627\n",
      "Training epoch: 1586, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 1587, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1588, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1589, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1590, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1591, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1592, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1593, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1594, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1595, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 1596, train loss: 0.01560, val loss: 0.01611\n",
      "Training epoch: 1597, train loss: 0.01575, val loss: 0.01623\n",
      "Training epoch: 1598, train loss: 0.01558, val loss: 0.01607\n",
      "Training epoch: 1599, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1600, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1601, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1602, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1603, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1604, train loss: 0.01555, val loss: 0.01605\n",
      "Training epoch: 1605, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1606, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1607, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 1608, train loss: 0.01577, val loss: 0.01626\n",
      "Training epoch: 1609, train loss: 0.01578, val loss: 0.01628\n",
      "Training epoch: 1610, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1611, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1612, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1613, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 1614, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1615, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1616, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1617, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1618, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1619, train loss: 0.01582, val loss: 0.01631\n",
      "Training epoch: 1620, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1621, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1622, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1623, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1624, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1625, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1626, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1627, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1628, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1629, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1630, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1631, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1632, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1633, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1634, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 1635, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1636, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 1637, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1638, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1639, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1640, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1641, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1642, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1643, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1644, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1645, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1646, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1647, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1648, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1649, train loss: 0.01550, val loss: 0.01597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1650, train loss: 0.01601, val loss: 0.01650\n",
      "Training epoch: 1651, train loss: 0.01568, val loss: 0.01616\n",
      "Training epoch: 1652, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1653, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1654, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1655, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1656, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1657, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1658, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1659, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1660, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1661, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1662, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1663, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1664, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1665, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1666, train loss: 0.01569, val loss: 0.01615\n",
      "Training epoch: 1667, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1668, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1669, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1670, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1671, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1672, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1673, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1674, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1675, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 1676, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1677, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1678, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 1679, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1680, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1681, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1682, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1683, train loss: 0.01562, val loss: 0.01608\n",
      "Training epoch: 1684, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1685, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1686, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1687, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1688, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1689, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1690, train loss: 0.01561, val loss: 0.01606\n",
      "Training epoch: 1691, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1692, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1693, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1694, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1695, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1696, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 1697, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1698, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1699, train loss: 0.01573, val loss: 0.01620\n",
      "Training epoch: 1700, train loss: 0.01564, val loss: 0.01609\n",
      "Training epoch: 1701, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 1702, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1703, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1704, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1705, train loss: 0.01581, val loss: 0.01627\n",
      "Training epoch: 1706, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 1707, train loss: 0.01577, val loss: 0.01625\n",
      "Training epoch: 1708, train loss: 0.01602, val loss: 0.01651\n",
      "Training epoch: 1709, train loss: 0.01576, val loss: 0.01624\n",
      "Training epoch: 1710, train loss: 0.01612, val loss: 0.01660\n",
      "Training epoch: 1711, train loss: 0.01570, val loss: 0.01618\n",
      "Training epoch: 1712, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 1713, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1714, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 1715, train loss: 0.01579, val loss: 0.01626\n",
      "Training epoch: 1716, train loss: 0.01567, val loss: 0.01613\n",
      "Training epoch: 1717, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1718, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1719, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1720, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1721, train loss: 0.01554, val loss: 0.01599\n",
      "Training epoch: 1722, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1723, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 1724, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1725, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1726, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1727, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1728, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1729, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1730, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1731, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1732, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1733, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1734, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1735, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1736, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1737, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1738, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 1739, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1740, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1741, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1742, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1743, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1744, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1745, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1746, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1747, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1748, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1749, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 1750, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1751, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1752, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1753, train loss: 0.01559, val loss: 0.01607\n",
      "Training epoch: 1754, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1755, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 1756, train loss: 0.01568, val loss: 0.01615\n",
      "Training epoch: 1757, train loss: 0.01558, val loss: 0.01606\n",
      "Training epoch: 1758, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1759, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1760, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1761, train loss: 0.01575, val loss: 0.01622\n",
      "Training epoch: 1762, train loss: 0.01586, val loss: 0.01635\n",
      "Training epoch: 1763, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1764, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 1765, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1766, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1767, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1768, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 1769, train loss: 0.01561, val loss: 0.01608\n",
      "Training epoch: 1770, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1771, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1772, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1773, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1774, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1775, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1776, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1777, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1778, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1779, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1780, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1781, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1782, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1783, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 1784, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1785, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1786, train loss: 0.01563, val loss: 0.01610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1787, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1788, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1789, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1790, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1791, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1792, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1793, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1794, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1795, train loss: 0.01557, val loss: 0.01603\n",
      "Training epoch: 1796, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1797, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1798, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 1799, train loss: 0.01571, val loss: 0.01618\n",
      "Training epoch: 1800, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1801, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1802, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1803, train loss: 0.01559, val loss: 0.01604\n",
      "Training epoch: 1804, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1805, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1806, train loss: 0.01558, val loss: 0.01606\n",
      "Training epoch: 1807, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1808, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1809, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1810, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1811, train loss: 0.01562, val loss: 0.01610\n",
      "Training epoch: 1812, train loss: 0.01564, val loss: 0.01612\n",
      "Training epoch: 1813, train loss: 0.01577, val loss: 0.01624\n",
      "Training epoch: 1814, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1815, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 1816, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1817, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 1818, train loss: 0.01572, val loss: 0.01619\n",
      "Training epoch: 1819, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1820, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1821, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1822, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 1823, train loss: 0.01564, val loss: 0.01610\n",
      "Training epoch: 1824, train loss: 0.01563, val loss: 0.01610\n",
      "Training epoch: 1825, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 1826, train loss: 0.01572, val loss: 0.01620\n",
      "Training epoch: 1827, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1828, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1829, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1830, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1831, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1832, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1833, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1834, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1835, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1836, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1837, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1838, train loss: 0.01566, val loss: 0.01611\n",
      "Training epoch: 1839, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1840, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1841, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1842, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1843, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1844, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1845, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1846, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1847, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1848, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1849, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1850, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1851, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 1852, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1853, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 1854, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 1855, train loss: 0.01586, val loss: 0.01634\n",
      "Training epoch: 1856, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1857, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1858, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1859, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1860, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1861, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 1862, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1863, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1864, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1865, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1866, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1867, train loss: 0.01569, val loss: 0.01617\n",
      "Training epoch: 1868, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1869, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 1870, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1871, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 1872, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1873, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1874, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1875, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 1876, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1877, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1878, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1879, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1880, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1881, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1882, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1883, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1884, train loss: 0.01594, val loss: 0.01644\n",
      "Training epoch: 1885, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 1886, train loss: 0.01567, val loss: 0.01613\n",
      "Training epoch: 1887, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1888, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 1889, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 1890, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1891, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1892, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1893, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1894, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1895, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 1896, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1897, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1898, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1899, train loss: 0.01557, val loss: 0.01603\n",
      "Training epoch: 1900, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1901, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 1902, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1903, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1904, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1905, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1906, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1907, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1908, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1909, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1910, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1911, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1912, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1913, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1914, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 1915, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 1916, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1917, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1918, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1919, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1920, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1921, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1922, train loss: 0.01554, val loss: 0.01600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1923, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1924, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1925, train loss: 0.01547, val loss: 0.01596\n",
      "Training epoch: 1926, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 1927, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1928, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1929, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1930, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1931, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 1932, train loss: 0.01644, val loss: 0.01696\n",
      "Training epoch: 1933, train loss: 0.01581, val loss: 0.01629\n",
      "Training epoch: 1934, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 1935, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1936, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1937, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 1938, train loss: 0.01567, val loss: 0.01615\n",
      "Training epoch: 1939, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 1940, train loss: 0.01578, val loss: 0.01625\n",
      "Training epoch: 1941, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1942, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 1943, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1944, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 1945, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1946, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1947, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1948, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 1949, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 1950, train loss: 0.01582, val loss: 0.01627\n",
      "Training epoch: 1951, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1952, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1953, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1954, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1955, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1956, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1957, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 1958, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1959, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1960, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 1961, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1962, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1963, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1964, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1965, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1966, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1967, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1968, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1969, train loss: 0.01585, val loss: 0.01634\n",
      "Training epoch: 1970, train loss: 0.01562, val loss: 0.01610\n",
      "Training epoch: 1971, train loss: 0.01568, val loss: 0.01616\n",
      "Training epoch: 1972, train loss: 0.01559, val loss: 0.01607\n",
      "Training epoch: 1973, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1974, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1975, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1976, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1977, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1978, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1979, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1980, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1981, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 1982, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1983, train loss: 0.01562, val loss: 0.01607\n",
      "Training epoch: 1984, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 1985, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1986, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 1987, train loss: 0.01561, val loss: 0.01608\n",
      "Training epoch: 1988, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1989, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1990, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1991, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1992, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1993, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1994, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1995, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1996, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1997, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1998, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 1999, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 2000, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 2001, train loss: 0.01577, val loss: 0.01623\n",
      "Training epoch: 2002, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 2003, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 2004, train loss: 0.01546, val loss: 0.01596\n",
      "Training epoch: 2005, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 2006, train loss: 0.01573, val loss: 0.01620\n",
      "Training epoch: 2007, train loss: 0.01575, val loss: 0.01623\n",
      "Training epoch: 2008, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2009, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 2010, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2011, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2012, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 2013, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 2014, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 2015, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2016, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 2017, train loss: 0.01567, val loss: 0.01615\n",
      "Training epoch: 2018, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 2019, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2020, train loss: 0.01563, val loss: 0.01609\n",
      "Training epoch: 2021, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 2022, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2023, train loss: 0.01575, val loss: 0.01622\n",
      "Training epoch: 2024, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 2025, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 2026, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2027, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2028, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2029, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2030, train loss: 0.01577, val loss: 0.01623\n",
      "Training epoch: 2031, train loss: 0.01559, val loss: 0.01604\n",
      "Training epoch: 2032, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 2033, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2034, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 2035, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 2036, train loss: 0.01566, val loss: 0.01613\n",
      "Training epoch: 2037, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 2038, train loss: 0.01567, val loss: 0.01615\n",
      "Training epoch: 2039, train loss: 0.01569, val loss: 0.01616\n",
      "Training epoch: 2040, train loss: 0.01587, val loss: 0.01634\n",
      "Training epoch: 2041, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2042, train loss: 0.01561, val loss: 0.01611\n",
      "Training epoch: 2043, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 2044, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 2045, train loss: 0.01562, val loss: 0.01608\n",
      "Training epoch: 2046, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2047, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2048, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2049, train loss: 0.01547, val loss: 0.01596\n",
      "Training epoch: 2050, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 2051, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2052, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2053, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2054, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2055, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2056, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2057, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2058, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2059, train loss: 0.01546, val loss: 0.01591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2060, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2061, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2062, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 2063, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2064, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2065, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2066, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2067, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2068, train loss: 0.01580, val loss: 0.01628\n",
      "Training epoch: 2069, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2070, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2071, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2072, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 2073, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 2074, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 2075, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 2076, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2077, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 2078, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 2079, train loss: 0.01576, val loss: 0.01626\n",
      "Training epoch: 2080, train loss: 0.01562, val loss: 0.01611\n",
      "Training epoch: 2081, train loss: 0.01588, val loss: 0.01636\n",
      "Training epoch: 2082, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2083, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2084, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2085, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 2086, train loss: 0.01571, val loss: 0.01618\n",
      "Training epoch: 2087, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 2088, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 2089, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 2090, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2091, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2092, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2093, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 2094, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2095, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2096, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2097, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 2098, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 2099, train loss: 0.01560, val loss: 0.01605\n",
      "Training epoch: 2100, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2101, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 2102, train loss: 0.01573, val loss: 0.01619\n",
      "Training epoch: 2103, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2104, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2105, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2106, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 2107, train loss: 0.01569, val loss: 0.01617\n",
      "Training epoch: 2108, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 2109, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 2110, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 2111, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 2112, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2113, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 2114, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 2115, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2116, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2117, train loss: 0.01561, val loss: 0.01608\n",
      "Training epoch: 2118, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 2119, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 2120, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 2121, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2122, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2123, train loss: 0.01545, val loss: 0.01594\n",
      "Training epoch: 2124, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2125, train loss: 0.01558, val loss: 0.01603\n",
      "Training epoch: 2126, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2127, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 2128, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2129, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2130, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2131, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2132, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2133, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 2134, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 2135, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2136, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 2137, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2138, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2139, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 2140, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2141, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2142, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2143, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2144, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 2145, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2146, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 2147, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2148, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2149, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2150, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2151, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2152, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2153, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 2154, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2155, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 2156, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2157, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 2158, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 2159, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 2160, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2161, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2162, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 2163, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2164, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2165, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 2166, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2167, train loss: 0.01572, val loss: 0.01619\n",
      "Training epoch: 2168, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 2169, train loss: 0.01556, val loss: 0.01605\n",
      "Training epoch: 2170, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2171, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2172, train loss: 0.01567, val loss: 0.01613\n",
      "Training epoch: 2173, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 2174, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2175, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2176, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2177, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2178, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2179, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 2180, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2181, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2182, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2183, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 2184, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2185, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2186, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2187, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2188, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 2189, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2190, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 2191, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2192, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2193, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2194, train loss: 0.01544, val loss: 0.01590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2195, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2196, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2197, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2198, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2199, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2200, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2201, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 2202, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2203, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2204, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2205, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2206, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2207, train loss: 0.01551, val loss: 0.01595\n",
      "Training epoch: 2208, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2209, train loss: 0.01543, val loss: 0.01591\n",
      "Training epoch: 2210, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2211, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2212, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2213, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2214, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2215, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2216, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2217, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2218, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 2219, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2220, train loss: 0.01569, val loss: 0.01616\n",
      "Training epoch: 2221, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 2222, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2223, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2224, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2225, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2226, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2227, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2228, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2229, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2230, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2231, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2232, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2233, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2234, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2235, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2236, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2237, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2238, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2239, train loss: 0.01542, val loss: 0.01590\n",
      "Training epoch: 2240, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2241, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2242, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2243, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2244, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2245, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2246, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2247, train loss: 0.01543, val loss: 0.01591\n",
      "Training epoch: 2248, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 2249, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2250, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 2251, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 2252, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2253, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2254, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2255, train loss: 0.01568, val loss: 0.01616\n",
      "Training epoch: 2256, train loss: 0.01572, val loss: 0.01618\n",
      "Training epoch: 2257, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2258, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2259, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 2260, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2261, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2262, train loss: 0.01543, val loss: 0.01592\n",
      "Training epoch: 2263, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 2264, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 2265, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2266, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 2267, train loss: 0.01543, val loss: 0.01591\n",
      "Training epoch: 2268, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 2269, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 2270, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 2271, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 2272, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2273, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2274, train loss: 0.01571, val loss: 0.01618\n",
      "Training epoch: 2275, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 2276, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2277, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2278, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 2279, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 2280, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 2281, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2282, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2283, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2284, train loss: 0.01542, val loss: 0.01590\n",
      "Training epoch: 2285, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2286, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2287, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2288, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 2289, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2290, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2291, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2292, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2293, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2294, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2295, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2296, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2297, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2298, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2299, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2300, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2301, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2302, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2303, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2304, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2305, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2306, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2307, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2308, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 2309, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 2310, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2311, train loss: 0.01542, val loss: 0.01590\n",
      "Training epoch: 2312, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2313, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2314, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2315, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 2316, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2317, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2318, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2319, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2320, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 2321, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2322, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2323, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2324, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2325, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 2326, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 2327, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2328, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2329, train loss: 0.01541, val loss: 0.01586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2330, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2331, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2332, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2333, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2334, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 2335, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 2336, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 2337, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 2338, train loss: 0.01563, val loss: 0.01609\n",
      "Training epoch: 2339, train loss: 0.01548, val loss: 0.01592\n",
      "Training epoch: 2340, train loss: 0.01559, val loss: 0.01604\n",
      "Training epoch: 2341, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 2342, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2343, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2344, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2345, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 2346, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2347, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2348, train loss: 0.01556, val loss: 0.01601\n",
      "Training epoch: 2349, train loss: 0.01587, val loss: 0.01634\n",
      "Training epoch: 2350, train loss: 0.01570, val loss: 0.01616\n",
      "Training epoch: 2351, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2352, train loss: 0.01563, val loss: 0.01609\n",
      "Training epoch: 2353, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2354, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2355, train loss: 0.01571, val loss: 0.01620\n",
      "Training epoch: 2356, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 2357, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 2358, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2359, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2360, train loss: 0.01541, val loss: 0.01589\n",
      "Training epoch: 2361, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 2362, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 2363, train loss: 0.01566, val loss: 0.01610\n",
      "Training epoch: 2364, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 2365, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2366, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2367, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2368, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2369, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 2370, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2371, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2372, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2373, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2374, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2375, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2376, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2377, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2378, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2379, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2380, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2381, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2382, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2383, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2384, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2385, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2386, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2387, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2388, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 2389, train loss: 0.01562, val loss: 0.01608\n",
      "Training epoch: 2390, train loss: 0.01542, val loss: 0.01586\n",
      "Training epoch: 2391, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2392, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2393, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 2394, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2395, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2396, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2397, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2398, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2399, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2400, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2401, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 2402, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2403, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2404, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2405, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2406, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 2407, train loss: 0.01547, val loss: 0.01596\n",
      "Training epoch: 2408, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2409, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 2410, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2411, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2412, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2413, train loss: 0.01538, val loss: 0.01585\n",
      "Training epoch: 2414, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2415, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2416, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 2417, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2418, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2419, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2420, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 2421, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2422, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2423, train loss: 0.01562, val loss: 0.01610\n",
      "Training epoch: 2424, train loss: 0.01565, val loss: 0.01611\n",
      "Training epoch: 2425, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 2426, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2427, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2428, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2429, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2430, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2431, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2432, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 2433, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2434, train loss: 0.01539, val loss: 0.01583\n",
      "Training epoch: 2435, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2436, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2437, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2438, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 2439, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2440, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2441, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2442, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2443, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2444, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2445, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 2446, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2447, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2448, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2449, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2450, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2451, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2452, train loss: 0.01545, val loss: 0.01588\n",
      "Training epoch: 2453, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2454, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2455, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2456, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2457, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2458, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2459, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2460, train loss: 0.01541, val loss: 0.01585\n",
      "Training epoch: 2461, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2462, train loss: 0.01561, val loss: 0.01606\n",
      "Training epoch: 2463, train loss: 0.01549, val loss: 0.01593\n",
      "Training epoch: 2464, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 2465, train loss: 0.01545, val loss: 0.01590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2466, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 2467, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 2468, train loss: 0.01540, val loss: 0.01588\n",
      "Training epoch: 2469, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2470, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2471, train loss: 0.01569, val loss: 0.01614\n",
      "Training epoch: 2472, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2473, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 2474, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 2475, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 2476, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2477, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2478, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2479, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2480, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2481, train loss: 0.01539, val loss: 0.01583\n",
      "Training epoch: 2482, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2483, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2484, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2485, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2486, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2487, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2488, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2489, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2490, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2491, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2492, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2493, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2494, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2495, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2496, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2497, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2498, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2499, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2500, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2501, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2502, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2503, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2504, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2505, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2506, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2507, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2508, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 2509, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2510, train loss: 0.01541, val loss: 0.01585\n",
      "Training epoch: 2511, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 2512, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2513, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2514, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 2515, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2516, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 2517, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2518, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2519, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2520, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2521, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 2522, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2523, train loss: 0.01582, val loss: 0.01630\n",
      "Training epoch: 2524, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2525, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2526, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2527, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2528, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2529, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2530, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2531, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2532, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2533, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 2534, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 2535, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2536, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 2537, train loss: 0.01565, val loss: 0.01610\n",
      "Training epoch: 2538, train loss: 0.01576, val loss: 0.01621\n",
      "Training epoch: 2539, train loss: 0.01582, val loss: 0.01625\n",
      "Training epoch: 2540, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2541, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2542, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 2543, train loss: 0.01538, val loss: 0.01587\n",
      "Training epoch: 2544, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2545, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 2546, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 2547, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 2548, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 2549, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2550, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2551, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2552, train loss: 0.01536, val loss: 0.01579\n",
      "Training epoch: 2553, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2554, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2555, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2556, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2557, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2558, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2559, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2560, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2561, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2562, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2563, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2564, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2565, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2566, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2567, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2568, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2569, train loss: 0.01534, val loss: 0.01581\n",
      "Training epoch: 2570, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 2571, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2572, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2573, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2574, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2575, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2576, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2577, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2578, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2579, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 2580, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2581, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2582, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2583, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 2584, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2585, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2586, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2587, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2588, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 2589, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2590, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2591, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2592, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2593, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2594, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2595, train loss: 0.01536, val loss: 0.01579\n",
      "Training epoch: 2596, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2597, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2598, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2599, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2600, train loss: 0.01544, val loss: 0.01588\n",
      "Training epoch: 2601, train loss: 0.01534, val loss: 0.01579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2602, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2603, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 2604, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2605, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2606, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2607, train loss: 0.01546, val loss: 0.01590\n",
      "Training epoch: 2608, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2609, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2610, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2611, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 2612, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2613, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2614, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2615, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2616, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 2617, train loss: 0.01563, val loss: 0.01607\n",
      "Training epoch: 2618, train loss: 0.01564, val loss: 0.01608\n",
      "Training epoch: 2619, train loss: 0.01553, val loss: 0.01597\n",
      "Training epoch: 2620, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 2621, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 2622, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 2623, train loss: 0.01539, val loss: 0.01582\n",
      "Training epoch: 2624, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 2625, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 2626, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 2627, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2628, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2629, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2630, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2631, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2632, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 2633, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2634, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2635, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 2636, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2637, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2638, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 2639, train loss: 0.01550, val loss: 0.01594\n",
      "Training epoch: 2640, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2641, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 2642, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2643, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2644, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2645, train loss: 0.01538, val loss: 0.01581\n",
      "Training epoch: 2646, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2647, train loss: 0.01564, val loss: 0.01610\n",
      "Training epoch: 2648, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 2649, train loss: 0.01566, val loss: 0.01613\n",
      "Training epoch: 2650, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2651, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 2652, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2653, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2654, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2655, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2656, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 2657, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2658, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2659, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2660, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2661, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2662, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 2663, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2664, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2665, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2666, train loss: 0.01553, val loss: 0.01597\n",
      "Training epoch: 2667, train loss: 0.01549, val loss: 0.01591\n",
      "Training epoch: 2668, train loss: 0.01564, val loss: 0.01607\n",
      "Training epoch: 2669, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2670, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 2671, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 2672, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2673, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 2674, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2675, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2676, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2677, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2678, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2679, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2680, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2681, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2682, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2683, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2684, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2685, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2686, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 2687, train loss: 0.01556, val loss: 0.01598\n",
      "Training epoch: 2688, train loss: 0.01547, val loss: 0.01590\n",
      "Training epoch: 2689, train loss: 0.01548, val loss: 0.01591\n",
      "Training epoch: 2690, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2691, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2692, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2693, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2694, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2695, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2696, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2697, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2698, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2699, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 2700, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2701, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2702, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 2703, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2704, train loss: 0.01539, val loss: 0.01582\n",
      "Training epoch: 2705, train loss: 0.01545, val loss: 0.01587\n",
      "Training epoch: 2706, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 2707, train loss: 0.01564, val loss: 0.01606\n",
      "Training epoch: 2708, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 2709, train loss: 0.01553, val loss: 0.01596\n",
      "Training epoch: 2710, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 2711, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2712, train loss: 0.01539, val loss: 0.01582\n",
      "Training epoch: 2713, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 2714, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2715, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2716, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2717, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 2718, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 2719, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 2720, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2721, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2722, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 2723, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2724, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2725, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 2726, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2727, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2728, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2729, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2730, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2731, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2732, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2733, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2734, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2735, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2736, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2737, train loss: 0.01536, val loss: 0.01580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2738, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2739, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2740, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2741, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2742, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2743, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2744, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2745, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2746, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2747, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 2748, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2749, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2750, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 2751, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2752, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 2753, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2754, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 2755, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2756, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2757, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2758, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2759, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2760, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2761, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 2762, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2763, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 2764, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2765, train loss: 0.01537, val loss: 0.01580\n",
      "Training epoch: 2766, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2767, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2768, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2769, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2770, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2771, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 2772, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2773, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 2774, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2775, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 2776, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 2777, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2778, train loss: 0.01537, val loss: 0.01580\n",
      "Training epoch: 2779, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2780, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 2781, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 2782, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2783, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2784, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 2785, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2786, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 2787, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2788, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2789, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2790, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2791, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2792, train loss: 0.01528, val loss: 0.01572\n",
      "Training epoch: 2793, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 2794, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2795, train loss: 0.01538, val loss: 0.01581\n",
      "Training epoch: 2796, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2797, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2798, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 2799, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2800, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2801, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2802, train loss: 0.01540, val loss: 0.01582\n",
      "Training epoch: 2803, train loss: 0.01565, val loss: 0.01606\n",
      "Training epoch: 2804, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 2805, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2806, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 2807, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 2808, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2809, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2810, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 2811, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2812, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2813, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2814, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2815, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2816, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2817, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2818, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2819, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2820, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2821, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 2822, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2823, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2824, train loss: 0.01539, val loss: 0.01583\n",
      "Training epoch: 2825, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 2826, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2827, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2828, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 2829, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2830, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 2831, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 2832, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2833, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 2834, train loss: 0.01562, val loss: 0.01606\n",
      "Training epoch: 2835, train loss: 0.01551, val loss: 0.01595\n",
      "Training epoch: 2836, train loss: 0.01554, val loss: 0.01598\n",
      "Training epoch: 2837, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2838, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2839, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2840, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2841, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 2842, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2843, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 2844, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2845, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2846, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2847, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2848, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2849, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 2850, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 2851, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2852, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2853, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2854, train loss: 0.01555, val loss: 0.01598\n",
      "Training epoch: 2855, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2856, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2857, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 2858, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2859, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2860, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2861, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2862, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2863, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2864, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2865, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 2866, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2867, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2868, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2869, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2870, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2871, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2872, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2873, train loss: 0.01546, val loss: 0.01586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2874, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2875, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2876, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2877, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2878, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2879, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 2880, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2881, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 2882, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 2883, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2884, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 2885, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 2886, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2887, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2888, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2889, train loss: 0.01542, val loss: 0.01585\n",
      "Training epoch: 2890, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2891, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2892, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2893, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2894, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 2895, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 2896, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 2897, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 2898, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 2899, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 2900, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2901, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2902, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 2903, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 2904, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2905, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2906, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2907, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2908, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 2909, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2910, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2911, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2912, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2913, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2914, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2915, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2916, train loss: 0.01547, val loss: 0.01591\n",
      "Training epoch: 2917, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2918, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2919, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 2920, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2921, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2922, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2923, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2924, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2925, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2926, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2927, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2928, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 2929, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2930, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 2931, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 2932, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2933, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 2934, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 2935, train loss: 0.01543, val loss: 0.01586\n",
      "Training epoch: 2936, train loss: 0.01548, val loss: 0.01591\n",
      "Training epoch: 2937, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2938, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 2939, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2940, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 2941, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2942, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2943, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2944, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2945, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 2946, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 2947, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2948, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2949, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2950, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2951, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2952, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2953, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2954, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 2955, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 2956, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2957, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2958, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2959, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2960, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2961, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2962, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2963, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2964, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2965, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2966, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2967, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2968, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 2969, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 2970, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2971, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2972, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2973, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2974, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2975, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 2976, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2977, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2978, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 2979, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2980, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2981, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2982, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2983, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2984, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2985, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 2986, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2987, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2988, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2989, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2990, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2991, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 2992, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 2993, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2994, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2995, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 2996, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2997, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 2998, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 2999, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3000, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 3001, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3002, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3003, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3004, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 3005, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3006, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 3007, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 3008, train loss: 0.01525, val loss: 0.01566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3009, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3010, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 3011, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 3012, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3013, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3014, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 3015, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3016, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3017, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 3018, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3019, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 3020, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 3021, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3022, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3023, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3024, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3025, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3026, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3027, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3028, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3029, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3030, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 3031, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3032, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3033, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3034, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3035, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3036, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3037, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3038, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3039, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3040, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 3041, train loss: 0.01557, val loss: 0.01600\n",
      "Training epoch: 3042, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 3043, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 3044, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 3045, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 3046, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 3047, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3048, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3049, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3050, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3051, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3052, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3053, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3054, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 3055, train loss: 0.01551, val loss: 0.01592\n",
      "Training epoch: 3056, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3057, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 3058, train loss: 0.01560, val loss: 0.01597\n",
      "Training epoch: 3059, train loss: 0.01562, val loss: 0.01599\n",
      "Training epoch: 3060, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3061, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3062, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3063, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 3064, train loss: 0.01576, val loss: 0.01620\n",
      "Training epoch: 3065, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 3066, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3067, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3068, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3069, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3070, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3071, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 3072, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3073, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3074, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3075, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3076, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3077, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3078, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3079, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3080, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3081, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3082, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3083, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3084, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3085, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3086, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3087, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3088, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 3089, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3090, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3091, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3092, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3093, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3094, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3095, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3096, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3097, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3098, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3099, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3100, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3101, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3102, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3103, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3104, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3105, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3106, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3107, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3108, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3109, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3110, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 3111, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3112, train loss: 0.01539, val loss: 0.01582\n",
      "Training epoch: 3113, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3114, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3115, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3116, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 3117, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3118, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3119, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3120, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 3121, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 3122, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3123, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3124, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3125, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3126, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3127, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 3128, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 3129, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3130, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3131, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3132, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3133, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3134, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3135, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3136, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3137, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3138, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 3139, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 3140, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3141, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3142, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3143, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3144, train loss: 0.01532, val loss: 0.01570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3145, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3146, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 3147, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3148, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3149, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3150, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3151, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3152, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 3153, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3154, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3155, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3156, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 3157, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3158, train loss: 0.01549, val loss: 0.01591\n",
      "Training epoch: 3159, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 3160, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 3161, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3162, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3163, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 3164, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 3165, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3166, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3167, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3168, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3169, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3170, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3171, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3172, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3173, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 3174, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3175, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3176, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 3177, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 3178, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3179, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3180, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3181, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3182, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3183, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3184, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3185, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3186, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3187, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 3188, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3189, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3190, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 3191, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3192, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3193, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3194, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3195, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3196, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3197, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3198, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3199, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 3200, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3201, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 3202, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3203, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3204, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3205, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3206, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3207, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3208, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3209, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3210, train loss: 0.01554, val loss: 0.01589\n",
      "Training epoch: 3211, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 3212, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3213, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 3214, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3215, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3216, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3217, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3218, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3219, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3220, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3221, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3222, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3223, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3224, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3225, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3226, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3227, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3228, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3229, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3230, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3231, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 3232, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3233, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3234, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3235, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 3236, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3237, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3238, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3239, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3240, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3241, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3242, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3243, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3244, train loss: 0.01545, val loss: 0.01580\n",
      "Training epoch: 3245, train loss: 0.01556, val loss: 0.01591\n",
      "Training epoch: 3246, train loss: 0.01539, val loss: 0.01575\n",
      "Training epoch: 3247, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3248, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3249, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3250, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3251, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3252, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3253, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3254, train loss: 0.01567, val loss: 0.01602\n",
      "Training epoch: 3255, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 3256, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3257, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3258, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3259, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3260, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3261, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3262, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3263, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3264, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3265, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3266, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3267, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3268, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3269, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3270, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3271, train loss: 0.01582, val loss: 0.01617\n",
      "Training epoch: 3272, train loss: 0.01545, val loss: 0.01580\n",
      "Training epoch: 3273, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3274, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3275, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3276, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3277, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3278, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3279, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3280, train loss: 0.01549, val loss: 0.01590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3281, train loss: 0.01548, val loss: 0.01591\n",
      "Training epoch: 3282, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 3283, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3284, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3285, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3286, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3287, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3288, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3289, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3290, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 3291, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3292, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3293, train loss: 0.01551, val loss: 0.01587\n",
      "Training epoch: 3294, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 3295, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3296, train loss: 0.01545, val loss: 0.01587\n",
      "Training epoch: 3297, train loss: 0.01574, val loss: 0.01618\n",
      "Training epoch: 3298, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3299, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3300, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3301, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3302, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3303, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3304, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3305, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3306, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3307, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3308, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 3309, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3310, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3311, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3312, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3313, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 3314, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3315, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3316, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3317, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3318, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3319, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3320, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3321, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3322, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3323, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 3324, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3325, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3326, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3327, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3328, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3329, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3330, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3331, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3332, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3333, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3334, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3335, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3336, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3337, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3338, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3339, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3340, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3341, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3342, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3343, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3344, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3345, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3346, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3347, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3348, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3349, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 3350, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3351, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3352, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3353, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 3354, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3355, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3356, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3357, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3358, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3359, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3360, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 3361, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3362, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3363, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3364, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3365, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3366, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 3367, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3368, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3369, train loss: 0.01533, val loss: 0.01569\n",
      "Training epoch: 3370, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3371, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3372, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3373, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3374, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3375, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3376, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 3377, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 3378, train loss: 0.01545, val loss: 0.01584\n",
      "Training epoch: 3379, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3380, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 3381, train loss: 0.01565, val loss: 0.01600\n",
      "Training epoch: 3382, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3383, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3384, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3385, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3386, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3387, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3388, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3389, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3390, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3391, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3392, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3393, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3394, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3395, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3396, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3397, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 3398, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3399, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3400, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3401, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3402, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3403, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3404, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3405, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 3406, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3407, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3408, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3409, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3410, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 3411, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3412, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3413, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3414, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3415, train loss: 0.01524, val loss: 0.01561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3416, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3417, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3418, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3419, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3420, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 3421, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3422, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3423, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3424, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3425, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3426, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3427, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3428, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3429, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3430, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3431, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 3432, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3433, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3434, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3435, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3436, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 3437, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3438, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3439, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 3440, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3441, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3442, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 3443, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3444, train loss: 0.01557, val loss: 0.01598\n",
      "Training epoch: 3445, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3446, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3447, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 3448, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 3449, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3450, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3451, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3452, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3453, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3454, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3455, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3456, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 3457, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3458, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3459, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3460, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3461, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3462, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3463, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3464, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3465, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3466, train loss: 0.01546, val loss: 0.01581\n",
      "Training epoch: 3467, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3468, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3469, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3470, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3471, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3472, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3473, train loss: 0.01550, val loss: 0.01586\n",
      "Training epoch: 3474, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3475, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3476, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3477, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3478, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3479, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3480, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3481, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 3482, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3483, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3484, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3485, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 3486, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 3487, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 3488, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 3489, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3490, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3491, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3492, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3493, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3494, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3495, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3496, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3497, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3498, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3499, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3500, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3501, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3502, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3503, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3504, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3505, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3506, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3507, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3508, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3509, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3510, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3511, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3512, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 3513, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3514, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3515, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3516, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3517, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3518, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3519, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3520, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3521, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3522, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3523, train loss: 0.01558, val loss: 0.01597\n",
      "Training epoch: 3524, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 3525, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 3526, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 3527, train loss: 0.01533, val loss: 0.01567\n",
      "Training epoch: 3528, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3529, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3530, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3531, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3532, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3533, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 3534, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3535, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3536, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3537, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3538, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3539, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3540, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3541, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3542, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3543, train loss: 0.01566, val loss: 0.01608\n",
      "Training epoch: 3544, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3545, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3546, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3547, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3548, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3549, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3550, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3551, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3552, train loss: 0.01523, val loss: 0.01559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3553, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3554, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3555, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3556, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3557, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3558, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3559, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3560, train loss: 0.01549, val loss: 0.01583\n",
      "Training epoch: 3561, train loss: 0.01534, val loss: 0.01568\n",
      "Training epoch: 3562, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3563, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3564, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3565, train loss: 0.01536, val loss: 0.01570\n",
      "Training epoch: 3566, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 3567, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3568, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3569, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3570, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3571, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3572, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3573, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3574, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3575, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3576, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3577, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3578, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 3579, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3580, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3581, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3582, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3583, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3584, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 3585, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3586, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3587, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3588, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3589, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3590, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3591, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3592, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3593, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3594, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3595, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3596, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3597, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3598, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3599, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3600, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3601, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3602, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3603, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3604, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3605, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3606, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3607, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3608, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3609, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 3610, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3611, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3612, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 3613, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3614, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3615, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3616, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3617, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3618, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3619, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3620, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3621, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3622, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3623, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3624, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3625, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3626, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3627, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3628, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3629, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3630, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3631, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3632, train loss: 0.01554, val loss: 0.01595\n",
      "Training epoch: 3633, train loss: 0.01561, val loss: 0.01600\n",
      "Training epoch: 3634, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3635, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3636, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3637, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3638, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3639, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3640, train loss: 0.01548, val loss: 0.01583\n",
      "Training epoch: 3641, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 3642, train loss: 0.01533, val loss: 0.01567\n",
      "Training epoch: 3643, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 3644, train loss: 0.01533, val loss: 0.01569\n",
      "Training epoch: 3645, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3646, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3647, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3648, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3649, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3650, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3651, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3652, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 3653, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3654, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3655, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3656, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3657, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 3658, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 3659, train loss: 0.01539, val loss: 0.01572\n",
      "Training epoch: 3660, train loss: 0.01544, val loss: 0.01578\n",
      "Training epoch: 3661, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3662, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3663, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 3664, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3665, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3666, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3667, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3668, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3669, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3670, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3671, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3672, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3673, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3674, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3675, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3676, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3677, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3678, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3679, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3680, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 3681, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3682, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 3683, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3684, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3685, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3686, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3687, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3688, train loss: 0.01523, val loss: 0.01561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3689, train loss: 0.01539, val loss: 0.01579\n",
      "Training epoch: 3690, train loss: 0.01533, val loss: 0.01569\n",
      "Training epoch: 3691, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 3692, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3693, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3694, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3695, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3696, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3697, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3698, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3699, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3700, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 3701, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3702, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3703, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3704, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3705, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3706, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3707, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 3708, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3709, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3710, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3711, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3712, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3713, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3714, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3715, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3716, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3717, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3718, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 3719, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 3720, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3721, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3722, train loss: 0.01537, val loss: 0.01574\n",
      "Training epoch: 3723, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3724, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3725, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 3726, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3727, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3728, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3729, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3730, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3731, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3732, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3733, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3734, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 3735, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3736, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3737, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3738, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3739, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3740, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3741, train loss: 0.01551, val loss: 0.01586\n",
      "Training epoch: 3742, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3743, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3744, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3745, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3746, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3747, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3748, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3749, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 3750, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3751, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3752, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3753, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3754, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3755, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3756, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3757, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3758, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3759, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3760, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3761, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3762, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3763, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3764, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3765, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3766, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3767, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 3768, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3769, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3770, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3771, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3772, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3773, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 3774, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3775, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3776, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3777, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3778, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3779, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3780, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3781, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3782, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3783, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 3784, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 3785, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 3786, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 3787, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3788, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3789, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3790, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3791, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3792, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3793, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3794, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3795, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3796, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3797, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3798, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3799, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3800, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3801, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 3802, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3803, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3804, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 3805, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3806, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 3807, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3808, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3809, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3810, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3811, train loss: 0.01569, val loss: 0.01611\n",
      "Training epoch: 3812, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3813, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3814, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3815, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3816, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3817, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3818, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3819, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3820, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3821, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3822, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3823, train loss: 0.01526, val loss: 0.01563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3824, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3825, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3826, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3827, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3828, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3829, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3830, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3831, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3832, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 3833, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3834, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3835, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3836, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 3837, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3838, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3839, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 3840, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3841, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3842, train loss: 0.01535, val loss: 0.01569\n",
      "Training epoch: 3843, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3844, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 3845, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3846, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3847, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3848, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3849, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3850, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3851, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3852, train loss: 0.01547, val loss: 0.01580\n",
      "Training epoch: 3853, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 3854, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3855, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 3856, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3857, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3858, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3859, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3860, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3861, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3862, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3863, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 3864, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3865, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3866, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3867, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3868, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3869, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3870, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3871, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3872, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3873, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3874, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3875, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3876, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3877, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3878, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3879, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3880, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3881, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3882, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3883, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3884, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3885, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3886, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3887, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3888, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3889, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3890, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3891, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3892, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3893, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3894, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3895, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3896, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3897, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 3898, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3899, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3900, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3901, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3902, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3903, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3904, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3905, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3906, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3907, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3908, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3909, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3910, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 3911, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3912, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3913, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 3914, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3915, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3916, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3917, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3918, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3919, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3920, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 3921, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3922, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 3923, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3924, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3925, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3926, train loss: 0.01539, val loss: 0.01573\n",
      "Training epoch: 3927, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 3928, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3929, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3930, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3931, train loss: 0.01534, val loss: 0.01568\n",
      "Training epoch: 3932, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3933, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3934, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3935, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 3936, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3937, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3938, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3939, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3940, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 3941, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3942, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3943, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3944, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3945, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3946, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3947, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3948, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3949, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3950, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3951, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3952, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3953, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3954, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3955, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3956, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3957, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 3958, train loss: 0.01529, val loss: 0.01567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3959, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3960, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3961, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3962, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3963, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3964, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3965, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3966, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3967, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3968, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3969, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3970, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3971, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3972, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3973, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 3974, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3975, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3976, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3977, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3978, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3979, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 3980, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 3981, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3982, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3983, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 3984, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3985, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 3986, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3987, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3988, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3989, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3990, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3991, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3992, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3993, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3994, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3995, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3996, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3997, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3998, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3999, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 4000, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 4001, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 4002, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4003, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 4004, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4005, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4006, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4007, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4008, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4009, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4010, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 4011, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4012, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4013, train loss: 0.01555, val loss: 0.01588\n",
      "Training epoch: 4014, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4015, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4016, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 4017, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4018, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4019, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 4020, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4021, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4022, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 4023, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 4024, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4025, train loss: 0.01542, val loss: 0.01575\n",
      "Training epoch: 4026, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4027, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4028, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4029, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4030, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4031, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4032, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4033, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4034, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4035, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4036, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4037, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4038, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 4039, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4040, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 4041, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 4042, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4043, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4044, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 4045, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4046, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4047, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4048, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4049, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4050, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 4051, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4052, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4053, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4054, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4055, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4056, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4057, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4058, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4059, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4060, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 4061, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4062, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4063, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4064, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4065, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4066, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4067, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4068, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4069, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4070, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4071, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4072, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4073, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4074, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4075, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4076, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4077, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4078, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4079, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4080, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 4081, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4082, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4083, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4084, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 4085, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4086, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4087, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4088, train loss: 0.01555, val loss: 0.01589\n",
      "Training epoch: 4089, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 4090, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 4091, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4092, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 4093, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4094, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 4095, train loss: 0.01523, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4096, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 4097, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4098, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4099, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4100, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4101, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4102, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 4103, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4104, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 4105, train loss: 0.01572, val loss: 0.01613\n",
      "Training epoch: 4106, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 4107, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4108, train loss: 0.01553, val loss: 0.01587\n",
      "Training epoch: 4109, train loss: 0.01539, val loss: 0.01573\n",
      "Training epoch: 4110, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4111, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4112, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4113, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4114, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4115, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4116, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4117, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4118, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4119, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4120, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4121, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4122, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4123, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 4124, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4125, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4126, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4127, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4128, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4129, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4130, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4131, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4132, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4133, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4134, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4135, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4136, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4137, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4138, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4139, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4140, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4141, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4142, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4143, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4144, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4145, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4146, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4147, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4148, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4149, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4150, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4151, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4152, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4153, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 4154, train loss: 0.01546, val loss: 0.01580\n",
      "Training epoch: 4155, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4156, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 4157, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4158, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 4159, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4160, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 4161, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4162, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4163, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 4164, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 4165, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4166, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4167, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4168, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4169, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4170, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 4171, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4172, train loss: 0.01530, val loss: 0.01564\n",
      "Training epoch: 4173, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4174, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4175, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4176, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4177, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4178, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4179, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4180, train loss: 0.01543, val loss: 0.01576\n",
      "Training epoch: 4181, train loss: 0.01542, val loss: 0.01576\n",
      "Training epoch: 4182, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4183, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4184, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4185, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4186, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4187, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4188, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4189, train loss: 0.01541, val loss: 0.01576\n",
      "Training epoch: 4190, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4191, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4192, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4193, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4194, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4195, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4196, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4197, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4198, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4199, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4200, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4201, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4202, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4203, train loss: 0.01550, val loss: 0.01591\n",
      "Training epoch: 4204, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4205, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4206, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4207, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4208, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4209, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4210, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4211, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4212, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 4213, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 4214, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4215, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4216, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4217, train loss: 0.01538, val loss: 0.01572\n",
      "Training epoch: 4218, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4219, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4220, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4221, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4222, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4223, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4224, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4225, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4226, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4227, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4228, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4229, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4230, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4231, train loss: 0.01523, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4232, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4233, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4234, train loss: 0.01523, val loss: 0.01556\n",
      "Training epoch: 4235, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4236, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4237, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4238, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4239, train loss: 0.01527, val loss: 0.01560\n",
      "Training epoch: 4240, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4241, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4242, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4243, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4244, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4245, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4246, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4247, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4248, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 4249, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4250, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4251, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4252, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4253, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4254, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4255, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4256, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4257, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4258, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4259, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4260, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4261, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4262, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4263, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4264, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4265, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4266, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4267, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4268, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 4269, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4270, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4271, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 4272, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 4273, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4274, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4275, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4276, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4277, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4278, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4279, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4280, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4281, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 4282, train loss: 0.01550, val loss: 0.01591\n",
      "Training epoch: 4283, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4284, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4285, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4286, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4287, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4288, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4289, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4290, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4291, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4292, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4293, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4294, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4295, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4296, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4297, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4298, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4299, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4300, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4301, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4302, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4303, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4304, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 4305, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4306, train loss: 0.01539, val loss: 0.01573\n",
      "Training epoch: 4307, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4308, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4309, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 4310, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4311, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4312, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4313, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 4314, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4315, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4316, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4317, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4318, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4319, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4320, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4321, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4322, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 4323, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 4324, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4325, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4326, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4327, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4328, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 4329, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4330, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4331, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4332, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 4333, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4334, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4335, train loss: 0.01541, val loss: 0.01576\n",
      "Training epoch: 4336, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4337, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4338, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4339, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4340, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4341, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4342, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 4343, train loss: 0.01548, val loss: 0.01581\n",
      "Training epoch: 4344, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4345, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4346, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 4347, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 4348, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4349, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4350, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4351, train loss: 0.01538, val loss: 0.01572\n",
      "Training epoch: 4352, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4353, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4354, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 4355, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 4356, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4357, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4358, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4359, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4360, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4361, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4362, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 4363, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4364, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 4365, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4366, train loss: 0.01522, val loss: 0.01558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4367, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4368, train loss: 0.01540, val loss: 0.01574\n",
      "Training epoch: 4369, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4370, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4371, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4372, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4373, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4374, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4375, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4376, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4377, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4378, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4379, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4380, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4381, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 4382, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 4383, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4384, train loss: 0.01559, val loss: 0.01592\n",
      "Training epoch: 4385, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4386, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4387, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4388, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 4389, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4390, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4391, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 4392, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4393, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4394, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4395, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4396, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4397, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4398, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4399, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4400, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4401, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4402, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4403, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4404, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 4405, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 4406, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4407, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4408, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4409, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4410, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4411, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4412, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4413, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4414, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4415, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4416, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4417, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4418, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4419, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4420, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 4421, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4422, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4423, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4424, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 4425, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 4426, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4427, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4428, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4429, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4430, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4431, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4432, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4433, train loss: 0.01552, val loss: 0.01592\n",
      "Training epoch: 4434, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4435, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4436, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4437, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 4438, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4439, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4440, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4441, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4442, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 4443, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4444, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4445, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4446, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4447, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4448, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4449, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 4450, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4451, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4452, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4453, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4454, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4455, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4456, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4457, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4458, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4459, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4460, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4461, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4462, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4463, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 4464, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 4465, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 4466, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 4467, train loss: 0.01548, val loss: 0.01582\n",
      "Training epoch: 4468, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4469, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 4470, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4471, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4472, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4473, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4474, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4475, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4476, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4477, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4478, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4479, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4480, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4481, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4482, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4483, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4484, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4485, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4486, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4487, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4488, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4489, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4490, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4491, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4492, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4493, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4494, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4495, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4496, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 4497, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4498, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4499, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4500, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4501, train loss: 0.01522, val loss: 0.01558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4502, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4503, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4504, train loss: 0.01541, val loss: 0.01574\n",
      "Training epoch: 4505, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4506, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4507, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4508, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4509, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4510, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4511, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4512, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4513, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4514, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4515, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4516, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4517, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4518, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4519, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 4520, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 4521, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4522, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4523, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 4524, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4525, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4526, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4527, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4528, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 4529, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4530, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4531, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4532, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 4533, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4534, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4535, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4536, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 4537, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4538, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4539, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4540, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4541, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4542, train loss: 0.01535, val loss: 0.01571\n",
      "Training epoch: 4543, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4544, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4545, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 4546, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 4547, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4548, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4549, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4550, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4551, train loss: 0.01545, val loss: 0.01579\n",
      "Training epoch: 4552, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4553, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4554, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4555, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 4556, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4557, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4558, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4559, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4560, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4561, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4562, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 4563, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4564, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4565, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4566, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4567, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4568, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 4569, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4570, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4571, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4572, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4573, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 4574, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4575, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4576, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4577, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 4578, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4579, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4580, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4581, train loss: 0.01533, val loss: 0.01567\n",
      "Training epoch: 4582, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4583, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 4584, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 4585, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 4586, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 4587, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4588, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4589, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4590, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4591, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4592, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4593, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4594, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4595, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4596, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4597, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4598, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4599, train loss: 0.01537, val loss: 0.01572\n",
      "Training epoch: 4600, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4601, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4602, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4603, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4604, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4605, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4606, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4607, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4608, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4609, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4610, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4611, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4612, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4613, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 4614, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4615, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4616, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4617, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4618, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4619, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4620, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4621, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4622, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4623, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4624, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4625, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4626, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4627, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4628, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 4629, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4630, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4631, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4632, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4633, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4634, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4635, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4636, train loss: 0.01544, val loss: 0.01579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4637, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4638, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4639, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4640, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4641, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4642, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4643, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 4644, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4645, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4646, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4647, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 4648, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4649, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4650, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 4651, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4652, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4653, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4654, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4655, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4656, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 4657, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4658, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 4659, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4660, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4661, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4662, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4663, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4664, train loss: 0.01538, val loss: 0.01571\n",
      "Training epoch: 4665, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4666, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4667, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4668, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 4669, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4670, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4671, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4672, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4673, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4674, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4675, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4676, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4677, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4678, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4679, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4680, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4681, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4682, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4683, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4684, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 4685, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4686, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4687, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4688, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4689, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4690, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4691, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4692, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4693, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4694, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4695, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4696, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4697, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4698, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4699, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4700, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4701, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4702, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4703, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4704, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 4705, train loss: 0.01530, val loss: 0.01564\n",
      "Training epoch: 4706, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4707, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4708, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4709, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4710, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4711, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4712, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4713, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4714, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4715, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4716, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4717, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4718, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4719, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4720, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4721, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4722, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4723, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4724, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4725, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 4726, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4727, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4728, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4729, train loss: 0.01529, val loss: 0.01562\n",
      "Training epoch: 4730, train loss: 0.01535, val loss: 0.01569\n",
      "Training epoch: 4731, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4732, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 4733, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4734, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4735, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4736, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4737, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4738, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4739, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 4740, train loss: 0.01555, val loss: 0.01595\n",
      "Training epoch: 4741, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4742, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4743, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4744, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4745, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4746, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4747, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4748, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4749, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4750, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4751, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4752, train loss: 0.01549, val loss: 0.01591\n",
      "Training epoch: 4753, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4754, train loss: 0.01542, val loss: 0.01576\n",
      "Training epoch: 4755, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4756, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4757, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4758, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4759, train loss: 0.01537, val loss: 0.01572\n",
      "Training epoch: 4760, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4761, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4762, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4763, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4764, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4765, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4766, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4767, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4768, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4769, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4770, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4771, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 4772, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 4773, train loss: 0.01521, val loss: 0.01557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4774, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 4775, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4776, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 4777, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4778, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4779, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4780, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4781, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4782, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4783, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4784, train loss: 0.01546, val loss: 0.01584\n",
      "Training epoch: 4785, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4786, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4787, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 4788, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4789, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4790, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4791, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4792, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4793, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 4794, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4795, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 4796, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4797, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4798, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4799, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4800, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4801, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4802, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4803, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4804, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4805, train loss: 0.01548, val loss: 0.01587\n",
      "Training epoch: 4806, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 4807, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 4808, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4809, train loss: 0.01534, val loss: 0.01568\n",
      "Training epoch: 4810, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4811, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4812, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4813, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4814, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4815, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4816, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4817, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4818, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4819, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4820, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4821, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4822, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4823, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4824, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4825, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4826, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4827, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 4828, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4829, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4830, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4831, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4832, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4833, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4834, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 4835, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 4836, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 4837, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4838, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4839, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4840, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4841, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4842, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4843, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4844, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4845, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4846, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4847, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4848, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4849, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4850, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4851, train loss: 0.01522, val loss: 0.01556\n",
      "Training epoch: 4852, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4853, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4854, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4855, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 4856, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4857, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4858, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4859, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4860, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4861, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4862, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4863, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4864, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4865, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4866, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4867, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4868, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4869, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4870, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 4871, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4872, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4873, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 4874, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4875, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 4876, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4877, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4878, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4879, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4880, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4881, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4882, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 4883, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4884, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4885, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4886, train loss: 0.01528, val loss: 0.01561\n",
      "Training epoch: 4887, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 4888, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4889, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4890, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4891, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4892, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 4893, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4894, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 4895, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4896, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4897, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4898, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4899, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4900, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 4901, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4902, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4903, train loss: 0.01524, val loss: 0.01557\n",
      "Training epoch: 4904, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4905, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4906, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4907, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4908, train loss: 0.01528, val loss: 0.01564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4909, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4910, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 4911, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 4912, train loss: 0.01539, val loss: 0.01576\n",
      "Training epoch: 4913, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4914, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4915, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 4916, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 4917, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4918, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4919, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4920, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4921, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4922, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4923, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4924, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4925, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4926, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 4927, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4928, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4929, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4930, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4931, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4932, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 4933, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4934, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 4935, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4936, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4937, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4938, train loss: 0.01520, val loss: 0.01559\n",
      "Training epoch: 4939, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 4940, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4941, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4942, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4943, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4944, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4945, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4946, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 4947, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 4948, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 4949, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4950, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4951, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4952, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 4953, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 4954, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4955, train loss: 0.01520, val loss: 0.01559\n",
      "Training epoch: 4956, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4957, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4958, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 4959, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 4960, train loss: 0.01532, val loss: 0.01565\n",
      "Training epoch: 4961, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4962, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4963, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4964, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 4965, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4966, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4967, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4968, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4969, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4970, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4971, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4972, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4973, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 4974, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 4975, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4976, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 4977, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4978, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 4979, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4980, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4981, train loss: 0.01522, val loss: 0.01556\n",
      "Training epoch: 4982, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4983, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4984, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4985, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4986, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 4987, train loss: 0.01526, val loss: 0.01559\n",
      "Training epoch: 4988, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 4989, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4990, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4991, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 4992, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 4993, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 4994, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4995, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4996, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 4997, train loss: 0.01534, val loss: 0.01568\n",
      "Training epoch: 4998, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4999, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5000, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 5001, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5002, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 5003, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5004, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5005, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5006, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5007, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5008, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 5009, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 5010, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5011, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 5012, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 5013, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 5014, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5015, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 5016, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5017, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5018, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 5019, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 5020, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5021, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5022, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5023, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5024, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5025, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 5026, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5027, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5028, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 5029, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5030, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 5031, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5032, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5033, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 5034, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 5035, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 5036, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5037, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 5038, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 5039, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5040, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5041, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 5042, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 5043, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5044, train loss: 0.01532, val loss: 0.01570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5045, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 5046, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5047, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 5048, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5049, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5050, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5051, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5052, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5053, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5054, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5055, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5056, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 5057, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5058, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5059, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 5060, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 5061, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 5062, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5063, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 5064, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5065, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5066, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5067, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5068, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5069, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 5070, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 5071, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5072, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5073, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5074, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 5075, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 5076, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5077, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 5078, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 5079, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 5080, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5081, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5082, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5083, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5084, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 5085, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5086, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5087, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5088, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 5089, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5090, train loss: 0.01520, val loss: 0.01559\n",
      "Training epoch: 5091, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 5092, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 5093, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5094, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5095, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5096, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5097, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 5098, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5099, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5100, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 5101, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5102, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 5103, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5104, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 5105, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5106, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 5107, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 5108, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5109, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5110, train loss: 0.01530, val loss: 0.01563\n",
      "Training epoch: 5111, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5112, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 5113, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5114, train loss: 0.01537, val loss: 0.01571\n",
      "Training epoch: 5115, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5116, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 5117, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 5118, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5119, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5120, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 5121, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 5122, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5123, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5124, train loss: 0.01522, val loss: 0.01556\n",
      "Training epoch: 5125, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 5126, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5127, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 5128, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 5129, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 5130, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5131, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 5132, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5133, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 5134, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5135, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5136, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5137, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5138, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5139, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5140, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5141, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5142, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5143, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5144, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5145, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5146, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 5147, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5148, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5149, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5150, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 5151, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5152, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5153, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5154, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5155, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5156, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 5157, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5158, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5159, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5160, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5161, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5162, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 5163, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 5164, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 5165, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5166, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5167, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5168, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5169, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5170, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5171, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 5172, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5173, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5174, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5175, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5176, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 5177, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5178, train loss: 0.01542, val loss: 0.01576\n",
      "Training epoch: 5179, train loss: 0.01520, val loss: 0.01556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5180, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5181, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5182, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5183, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5184, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5185, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5186, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 5187, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5188, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5189, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5190, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5191, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 5192, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5193, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5194, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5195, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5196, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 5197, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5198, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5199, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5200, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5201, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5202, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5203, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 5204, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5205, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 5206, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 5207, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5208, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 5209, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5210, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 5211, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5212, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 5213, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5214, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5215, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5216, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5217, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5218, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 5219, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5220, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5221, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5222, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5223, train loss: 0.01543, val loss: 0.01577\n",
      "Training epoch: 5224, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5225, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5226, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5227, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5228, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5229, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5230, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 5231, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5232, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 5233, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5234, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5235, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5236, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5237, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5238, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5239, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 5240, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 5241, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5242, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5243, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5244, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5245, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 5246, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5247, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5248, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5249, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5250, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5251, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 5252, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 5253, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5254, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 5255, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5256, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 5257, train loss: 0.01522, val loss: 0.01556\n",
      "Training epoch: 5258, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5259, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5260, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5261, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5262, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 5263, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5264, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5265, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5266, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5267, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 5268, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5269, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 5270, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 5271, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5272, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5273, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 5274, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5275, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5276, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5277, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5278, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5279, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 5280, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5281, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5282, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5283, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5284, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5285, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5286, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5287, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5288, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 5289, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5290, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 5291, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5292, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5293, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5294, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5295, train loss: 0.01534, val loss: 0.01568\n",
      "Training epoch: 5296, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5297, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5298, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 5299, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5300, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5301, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5302, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5303, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5304, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5305, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 5306, train loss: 0.01520, val loss: 0.01554\n",
      "Training epoch: 5307, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5308, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 5309, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5310, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 5311, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5312, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5313, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5314, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 5315, train loss: 0.01527, val loss: 0.01562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5316, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5317, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5318, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5319, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5320, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5321, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 5322, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5323, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5324, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5325, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 5326, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5327, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5328, train loss: 0.01524, val loss: 0.01557\n",
      "Training epoch: 5329, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5330, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5331, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 5332, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 5333, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5334, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5335, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 5336, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5337, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5338, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 5339, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5340, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 5341, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5342, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 5343, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5344, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5345, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 5346, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5347, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 5348, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5349, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5350, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5351, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5352, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5353, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5354, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 5355, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5356, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5357, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 5358, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5359, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 5360, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5361, train loss: 0.01534, val loss: 0.01567\n",
      "Training epoch: 5362, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5363, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5364, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5365, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5366, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5367, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5368, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5369, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5370, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 5371, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5372, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5373, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5374, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5375, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 5376, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5377, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5378, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5379, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5380, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5381, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5382, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5383, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5384, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5385, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5386, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5387, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5388, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 5389, train loss: 0.01518, val loss: 0.01557\n",
      "Training epoch: 5390, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5391, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5392, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 5393, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 5394, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5395, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 5396, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5397, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5398, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5399, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5400, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5401, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5402, train loss: 0.01523, val loss: 0.01556\n",
      "Training epoch: 5403, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5404, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 5405, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 5406, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5407, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5408, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5409, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5410, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5411, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 5412, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5413, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 5414, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5415, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 5416, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 5417, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5418, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5419, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 5420, train loss: 0.01522, val loss: 0.01556\n",
      "Training epoch: 5421, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 5422, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5423, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5424, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5425, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5426, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5427, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5428, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5429, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5430, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 5431, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5432, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 5433, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 5434, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5435, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 5436, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5437, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5438, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5439, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5440, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 5441, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 5442, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5443, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 5444, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5445, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5446, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5447, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5448, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5449, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5450, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5451, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5452, train loss: 0.01528, val loss: 0.01565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5453, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5454, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5455, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5456, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5457, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5458, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 5459, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5460, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5461, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5462, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5463, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 5464, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5465, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 5466, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5467, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5468, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5469, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5470, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5471, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5472, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 5473, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5474, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 5475, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5476, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5477, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5478, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 5479, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5480, train loss: 0.01522, val loss: 0.01556\n",
      "Training epoch: 5481, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5482, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 5483, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5484, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5485, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5486, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 5487, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5488, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5489, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5490, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5491, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5492, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5493, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5494, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5495, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5496, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5497, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5498, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5499, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5500, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5501, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5502, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5503, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5504, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5505, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5506, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5507, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5508, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5509, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5510, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5511, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5512, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 5513, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5514, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 5515, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5516, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 5517, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 5518, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5519, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 5520, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5521, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5522, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 5523, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5524, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5525, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5526, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5527, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5528, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5529, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5530, train loss: 0.01523, val loss: 0.01556\n",
      "Training epoch: 5531, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5532, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5533, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 5534, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 5535, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5536, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5537, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5538, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 5539, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5540, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5541, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5542, train loss: 0.01520, val loss: 0.01554\n",
      "Training epoch: 5543, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5544, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5545, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5546, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 5547, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 5548, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5549, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5550, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 5551, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 5552, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5553, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 5554, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5555, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5556, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5557, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5558, train loss: 0.01520, val loss: 0.01553\n",
      "Training epoch: 5559, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5560, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5561, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5562, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5563, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5564, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5565, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5566, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5567, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5568, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 5569, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5570, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5571, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5572, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5573, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 5574, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5575, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5576, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 5577, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5578, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5579, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5580, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 5581, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5582, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5583, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 5584, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5585, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5586, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5587, train loss: 0.01524, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5588, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5589, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 5590, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5591, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5592, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5593, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5594, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5595, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 5596, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5597, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5598, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5599, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5600, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5601, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5602, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5603, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5604, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5605, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5606, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5607, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5608, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5609, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5610, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5611, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5612, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 5613, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5614, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5615, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 5616, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5617, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5618, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 5619, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 5620, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 5621, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5622, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5623, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 5624, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5625, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5626, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5627, train loss: 0.01535, val loss: 0.01569\n",
      "Training epoch: 5628, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5629, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 5630, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5631, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5632, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 5633, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5634, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5635, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5636, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 5637, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5638, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5639, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5640, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5641, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5642, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5643, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 5644, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5645, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5646, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5647, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5648, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5649, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5650, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 5651, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5652, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 5653, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5654, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5655, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5656, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5657, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 5658, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 5659, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5660, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5661, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 5662, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5663, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5664, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5665, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5666, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5667, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5668, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5669, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5670, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 5671, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 5672, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5673, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5674, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5675, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5676, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5677, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5678, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5679, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5680, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5681, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5682, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5683, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5684, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5685, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 5686, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5687, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5688, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 5689, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5690, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5691, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5692, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5693, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5694, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5695, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5696, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 5697, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5698, train loss: 0.01520, val loss: 0.01554\n",
      "Training epoch: 5699, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5700, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5701, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5702, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5703, train loss: 0.01520, val loss: 0.01554\n",
      "Training epoch: 5704, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5705, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5706, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 5707, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5708, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5709, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5710, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5711, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5712, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5713, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5714, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5715, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5716, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 5717, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5718, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5719, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5720, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5721, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5722, train loss: 0.01517, val loss: 0.01552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5723, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5724, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5725, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5726, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5727, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 5728, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 5729, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5730, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 5731, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5732, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5733, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5734, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5735, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5736, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5737, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5738, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5739, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5740, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5741, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5742, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 5743, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5744, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5745, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5746, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5747, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 5748, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5749, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 5750, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5751, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5752, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 5753, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 5754, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5755, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5756, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 5757, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 5758, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5759, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5760, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5761, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5762, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5763, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5764, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5765, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5766, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5767, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5768, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 5769, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5770, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5771, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 5772, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5773, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5774, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5775, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 5776, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 5777, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 5778, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5779, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5780, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5781, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 5782, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5783, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5784, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5785, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 5786, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5787, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 5788, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5789, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5790, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 5791, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5792, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5793, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5794, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5795, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5796, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5797, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5798, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5799, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5800, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5801, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5802, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5803, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5804, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5805, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5806, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5807, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 5808, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 5809, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5810, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5811, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 5812, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5813, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5814, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5815, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5816, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5817, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5818, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 5819, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5820, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5821, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5822, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5823, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5824, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5825, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5826, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5827, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5828, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 5829, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5830, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 5831, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5832, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5833, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5834, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5835, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5836, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5837, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5838, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5839, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5840, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5841, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5842, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 5843, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5844, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5845, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5846, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5847, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5848, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 5849, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 5850, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5851, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5852, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 5853, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5854, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5855, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5856, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5857, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5858, train loss: 0.01522, val loss: 0.01559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5859, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5860, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5861, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5862, train loss: 0.01522, val loss: 0.01556\n",
      "Training epoch: 5863, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5864, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5865, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5866, train loss: 0.01519, val loss: 0.01552\n",
      "Training epoch: 5867, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5868, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5869, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5870, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5871, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5872, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5873, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5874, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5875, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5876, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5877, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5878, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 5879, train loss: 0.01527, val loss: 0.01560\n",
      "Training epoch: 5880, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5881, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 5882, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5883, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 5884, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5885, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5886, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5887, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5888, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5889, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 5890, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 5891, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5892, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5893, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5894, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5895, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5896, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5897, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5898, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5899, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 5900, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5901, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5902, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5903, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5904, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5905, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5906, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5907, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 5908, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5909, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 5910, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5911, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 5912, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5913, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5914, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5915, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5916, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 5917, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 5918, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5919, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5920, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5921, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5922, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5923, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5924, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5925, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5926, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5927, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5928, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5929, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5930, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5931, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5932, train loss: 0.01520, val loss: 0.01554\n",
      "Training epoch: 5933, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 5934, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 5935, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5936, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5937, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 5938, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5939, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5940, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5941, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5942, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5943, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5944, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5945, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5946, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5947, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5948, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5949, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5950, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 5951, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 5952, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5953, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5954, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5955, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5956, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5957, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5958, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 5959, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5960, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5961, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5962, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 5963, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5964, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5965, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5966, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 5967, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5968, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 5969, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 5970, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 5971, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 5972, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5973, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5974, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5975, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 5976, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5977, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5978, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5979, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 5980, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 5981, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 5982, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5983, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5984, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5985, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5986, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 5987, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5988, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 5989, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5990, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 5991, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 5992, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 5993, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 5994, train loss: 0.01519, val loss: 0.01555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 5995, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 5996, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 5997, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 5998, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 5999, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6000, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6001, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6002, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6003, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6004, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 6005, train loss: 0.01518, val loss: 0.01557\n",
      "Training epoch: 6006, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6007, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6008, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6009, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6010, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6011, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6012, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6013, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 6014, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6015, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 6016, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6017, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6018, train loss: 0.01520, val loss: 0.01554\n",
      "Training epoch: 6019, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6020, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6021, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 6022, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6023, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6024, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6025, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6026, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6027, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6028, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6029, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 6030, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 6031, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6032, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6033, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6034, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6035, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6036, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6037, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6038, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6039, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6040, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6041, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6042, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6043, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6044, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 6045, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6046, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6047, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6048, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 6049, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6050, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6051, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6052, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6053, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6054, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6055, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6056, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6057, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6058, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6059, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6060, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 6061, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6062, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6063, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6064, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6065, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6066, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 6067, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6068, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6069, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 6070, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6071, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 6072, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6073, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6074, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6075, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6076, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6077, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 6078, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 6079, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 6080, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6081, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6082, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 6083, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 6084, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6085, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6086, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6087, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 6088, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6089, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6090, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6091, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6092, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6093, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6094, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6095, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 6096, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6097, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6098, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6099, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6100, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6101, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6102, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6103, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6104, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6105, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6106, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6107, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6108, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6109, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 6110, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6111, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 6112, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6113, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6114, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6115, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6116, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 6117, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6118, train loss: 0.01520, val loss: 0.01554\n",
      "Training epoch: 6119, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6120, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6121, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6122, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 6123, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6124, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6125, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 6126, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6127, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 6128, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6129, train loss: 0.01516, val loss: 0.01552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 6130, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6131, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 6132, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 6133, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 6134, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6135, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6136, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6137, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6138, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6139, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 6140, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6141, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6142, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6143, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6144, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6145, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6146, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6147, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 6148, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6149, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 6150, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 6151, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6152, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6153, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6154, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6155, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6156, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6157, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6158, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6159, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6160, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6161, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6162, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6163, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6164, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6165, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 6166, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6167, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6168, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6169, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6170, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6171, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6172, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6173, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6174, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6175, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6176, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6177, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6178, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 6179, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 6180, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6181, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6182, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6183, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6184, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6185, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6186, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6187, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6188, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6189, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6190, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6191, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6192, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6193, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6194, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6195, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6196, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6197, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6198, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6199, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6200, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 6201, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 6202, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 6203, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 6204, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6205, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6206, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6207, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6208, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6209, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6210, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6211, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6212, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6213, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6214, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6215, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6216, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6217, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6218, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6219, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6220, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6221, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6222, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6223, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6224, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6225, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6226, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6227, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6228, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6229, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6230, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6231, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6232, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6233, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 6234, train loss: 0.01521, val loss: 0.01555\n",
      "Training epoch: 6235, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 6236, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6237, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6238, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6239, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6240, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6241, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6242, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6243, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6244, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6245, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6246, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6247, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6248, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6249, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6250, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6251, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6252, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6253, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6254, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 6255, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 6256, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6257, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6258, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6259, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6260, train loss: 0.01517, val loss: 0.01550\n",
      "Training epoch: 6261, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6262, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6263, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6264, train loss: 0.01516, val loss: 0.01551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 6265, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6266, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6267, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6268, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6269, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6270, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6271, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6272, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6273, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6274, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6275, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6276, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6277, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6278, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6279, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6280, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6281, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6282, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6283, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6284, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6285, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6286, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6287, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6288, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6289, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6290, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 6291, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6292, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 6293, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6294, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6295, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6296, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6297, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 6298, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 6299, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6300, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6301, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6302, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6303, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 6304, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 6305, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 6306, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6307, train loss: 0.01516, val loss: 0.01555\n",
      "Training epoch: 6308, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6309, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6310, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6311, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6312, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6313, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6314, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6315, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 6316, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6317, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6318, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6319, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6320, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6321, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6322, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6323, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6324, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6325, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6326, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6327, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6328, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6329, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6330, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6331, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6332, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6333, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6334, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6335, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6336, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6337, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6338, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6339, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6340, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6341, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6342, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6343, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6344, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6345, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6346, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 6347, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6348, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6349, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6350, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6351, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6352, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6353, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6354, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6355, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6356, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6357, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6358, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6359, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6360, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6361, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6362, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6363, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6364, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6365, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6366, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6367, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6368, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6369, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6370, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6371, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6372, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6373, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6374, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6375, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6376, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6377, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6378, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6379, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 6380, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 6381, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 6382, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6383, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6384, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6385, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6386, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6387, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6388, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6389, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6390, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6391, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6392, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6393, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6394, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6395, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6396, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 6397, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 6398, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6399, train loss: 0.01517, val loss: 0.01553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 6400, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6401, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6402, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6403, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 6404, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6405, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 6406, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6407, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6408, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6409, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6410, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6411, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6412, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6413, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6414, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6415, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6416, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6417, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6418, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6419, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6420, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6421, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6422, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6423, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6424, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6425, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6426, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6427, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6428, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6429, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6430, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6431, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6432, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6433, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6434, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 6435, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6436, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6437, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6438, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6439, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6440, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6441, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6442, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6443, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6444, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6445, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6446, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6447, train loss: 0.01519, val loss: 0.01553\n",
      "Training epoch: 6448, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6449, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6450, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6451, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6452, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6453, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6454, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6455, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6456, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6457, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 6458, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6459, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6460, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6461, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6462, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6463, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6464, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6465, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6466, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6467, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6468, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6469, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6470, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6471, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6472, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6473, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6474, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6475, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6476, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6477, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6478, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6479, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 6480, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 6481, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 6482, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6483, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6484, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6485, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6486, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6487, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6488, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6489, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6490, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6491, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6492, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6493, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6494, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6495, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6496, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6497, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6498, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6499, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6500, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6501, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6502, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 6503, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6504, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6505, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6506, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6507, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 6508, train loss: 0.01515, val loss: 0.01554\n",
      "Training epoch: 6509, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6510, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6511, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6512, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6513, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6514, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6515, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6516, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6517, train loss: 0.01518, val loss: 0.01552\n",
      "Training epoch: 6518, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6519, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6520, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6521, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6522, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6523, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6524, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 6525, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6526, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6527, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6528, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6529, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6530, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6531, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6532, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6533, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 6534, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6535, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6536, train loss: 0.01517, val loss: 0.01554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 6537, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6538, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6539, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6540, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6541, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6542, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6543, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6544, train loss: 0.01517, val loss: 0.01550\n",
      "Training epoch: 6545, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6546, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6547, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6548, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6549, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6550, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6551, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6552, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6553, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6554, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6555, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6556, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 6557, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6558, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6559, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6560, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6561, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6562, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6563, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6564, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6565, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6566, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6567, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6568, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6569, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6570, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6571, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6572, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6573, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6574, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6575, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6576, train loss: 0.01519, val loss: 0.01558\n",
      "Training epoch: 6577, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6578, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 6579, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6580, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6581, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6582, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6583, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 6584, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6585, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6586, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6587, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6588, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6589, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6590, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6591, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6592, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6593, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6594, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6595, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6596, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6597, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6598, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 6599, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6600, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 6601, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6602, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6603, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6604, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6605, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 6606, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6607, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6608, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6609, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6610, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6611, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6612, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6613, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6614, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6615, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6616, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6617, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6618, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 6619, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6620, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6621, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6622, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6623, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6624, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6625, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6626, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6627, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6628, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6629, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6630, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6631, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 6632, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6633, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6634, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6635, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6636, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6637, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6638, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6639, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6640, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6641, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6642, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6643, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6644, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6645, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6646, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6647, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6648, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6649, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6650, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6651, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6652, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6653, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6654, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6655, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6656, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6657, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6658, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6659, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6660, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6661, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6662, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6663, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6664, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 6665, train loss: 0.01520, val loss: 0.01555\n",
      "Training epoch: 6666, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 6667, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6668, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 6669, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6670, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6671, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6672, train loss: 0.01515, val loss: 0.01553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 6673, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6674, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6675, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6676, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6677, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6678, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6679, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6680, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6681, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6682, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6683, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6684, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6685, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6686, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6687, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6688, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6689, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6690, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6691, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6692, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6693, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6694, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6695, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6696, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6697, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6698, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6699, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6700, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6701, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6702, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6703, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6704, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6705, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6706, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6707, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6708, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6709, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6710, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6711, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6712, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6713, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6714, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6715, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6716, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6717, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6718, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6719, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6720, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6721, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6722, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6723, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 6724, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6725, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 6726, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6727, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6728, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6729, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6730, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6731, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6732, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6733, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6734, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6735, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6736, train loss: 0.01519, val loss: 0.01554\n",
      "Training epoch: 6737, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6738, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6739, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6740, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6741, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6742, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6743, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6744, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6745, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6746, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6747, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6748, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6749, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 6750, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6751, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6752, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 6753, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 6754, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6755, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 6756, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6757, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6758, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6759, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6760, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6761, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6762, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6763, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6764, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6765, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6766, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6767, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6768, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6769, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6770, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6771, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6772, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6773, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 6774, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6775, train loss: 0.01516, val loss: 0.01555\n",
      "Training epoch: 6776, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6777, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6778, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6779, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6780, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6781, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6782, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6783, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6784, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6785, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6786, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 6787, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6788, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6789, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6790, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6791, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6792, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 6793, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6794, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6795, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6796, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6797, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6798, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6799, train loss: 0.01520, val loss: 0.01556\n",
      "Training epoch: 6800, train loss: 0.01518, val loss: 0.01557\n",
      "Training epoch: 6801, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6802, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6803, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6804, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6805, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6806, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6807, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6808, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6809, train loss: 0.01516, val loss: 0.01553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 6810, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6811, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6812, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6813, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6814, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6815, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6816, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6817, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6818, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6819, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 6820, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6821, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6822, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6823, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6824, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6825, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6826, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6827, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 6828, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 6829, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6830, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 6831, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6832, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6833, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6834, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6835, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6836, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 6837, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6838, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6839, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6840, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6841, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6842, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6843, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6844, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6845, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6846, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 6847, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6848, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6849, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6850, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6851, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6852, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6853, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6854, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6855, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6856, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6857, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6858, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6859, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6860, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6861, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6862, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6863, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6864, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6865, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6866, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6867, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6868, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6869, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6870, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6871, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6872, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6873, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 6874, train loss: 0.01520, val loss: 0.01554\n",
      "Training epoch: 6875, train loss: 0.01519, val loss: 0.01558\n",
      "Training epoch: 6876, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6877, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6878, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6879, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6880, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6881, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 6882, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6883, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6884, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6885, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6886, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6887, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 6888, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 6889, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6890, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6891, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6892, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6893, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6894, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6895, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 6896, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6897, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6898, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6899, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6900, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6901, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6902, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6903, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6904, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 6905, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6906, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 6907, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6908, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6909, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6910, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6911, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6912, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6913, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6914, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6915, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6916, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6917, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6918, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6919, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6920, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6921, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6922, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6923, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6924, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6925, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6926, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 6927, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6928, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6929, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6930, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6931, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6932, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 6933, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6934, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 6935, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6936, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6937, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6938, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6939, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6940, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6941, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6942, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6943, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6944, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 6945, train loss: 0.01515, val loss: 0.01551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 6946, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6947, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6948, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6949, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 6950, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 6951, train loss: 0.01521, val loss: 0.01560\n",
      "Training epoch: 6952, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 6953, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 6954, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 6955, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 6956, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6957, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6958, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6959, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6960, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 6961, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6962, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6963, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6964, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6965, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6966, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6967, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6968, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6969, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6970, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6971, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6972, train loss: 0.01515, val loss: 0.01549\n",
      "Training epoch: 6973, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6974, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 6975, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6976, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6977, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 6978, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6979, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 6980, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6981, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6982, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6983, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6984, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 6985, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6986, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6987, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6988, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 6989, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6990, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 6991, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6992, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 6993, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6994, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 6995, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 6996, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 6997, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 6998, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 6999, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7000, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7001, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7002, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7003, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7004, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7005, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7006, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7007, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 7008, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 7009, train loss: 0.01520, val loss: 0.01557\n",
      "Training epoch: 7010, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 7011, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 7012, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7013, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7014, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7015, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7016, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7017, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7018, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 7019, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 7020, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7021, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7022, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7023, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7024, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 7025, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 7026, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 7027, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7028, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 7029, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7030, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 7031, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7032, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7033, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7034, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7035, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7036, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7037, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7038, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7039, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7040, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7041, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7042, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 7043, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7044, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7045, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7046, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7047, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7048, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7049, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7050, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7051, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7052, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7053, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7054, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7055, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7056, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7057, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 7058, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 7059, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7060, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7061, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 7062, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7063, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 7064, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7065, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 7066, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7067, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 7068, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7069, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7070, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 7071, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7072, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7073, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7074, train loss: 0.01515, val loss: 0.01549\n",
      "Training epoch: 7075, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7076, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7077, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7078, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7079, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7080, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7081, train loss: 0.01515, val loss: 0.01552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 7082, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7083, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7084, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 7085, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7086, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7087, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7088, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 7089, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7090, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7091, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7092, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7093, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7094, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7095, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7096, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7097, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 7098, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7099, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7100, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7101, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7102, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7103, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7104, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7105, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 7106, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7107, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7108, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7109, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 7110, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 7111, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7112, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7113, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7114, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7115, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7116, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7117, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7118, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7119, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7120, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7121, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7122, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7123, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7124, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7125, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7126, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7127, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7128, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7129, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7130, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7131, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7132, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7133, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7134, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 7135, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7136, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7137, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7138, train loss: 0.01519, val loss: 0.01556\n",
      "Training epoch: 7139, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7140, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7141, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7142, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7143, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7144, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7145, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 7146, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7147, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7148, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7149, train loss: 0.01517, val loss: 0.01551\n",
      "Training epoch: 7150, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7151, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7152, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7153, train loss: 0.01515, val loss: 0.01549\n",
      "Training epoch: 7154, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7155, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7156, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 7157, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7158, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7159, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7160, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7161, train loss: 0.01514, val loss: 0.01550\n",
      "Training epoch: 7162, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7163, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7164, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7165, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7166, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7167, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7168, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7169, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7170, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7171, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7172, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7173, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7174, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 7175, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7176, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7177, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7178, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7179, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7180, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7181, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7182, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7183, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7184, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7185, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7186, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7187, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7188, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 7189, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 7190, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7191, train loss: 0.01514, val loss: 0.01550\n",
      "Training epoch: 7192, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7193, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7194, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7195, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7196, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7197, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7198, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7199, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7200, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7201, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7202, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7203, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7204, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7205, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7206, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7207, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7208, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7209, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7210, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7211, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7212, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7213, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 7214, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7215, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7216, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7217, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7218, train loss: 0.01515, val loss: 0.01551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 7219, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7220, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7221, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7222, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7223, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7224, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7225, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7226, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7227, train loss: 0.01514, val loss: 0.01553\n",
      "Training epoch: 7228, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7229, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7230, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 7231, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7232, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7233, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7234, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7235, train loss: 0.01514, val loss: 0.01550\n",
      "Training epoch: 7236, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7237, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 7238, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7239, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7240, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7241, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7242, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7243, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 7244, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 7245, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 7246, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7247, train loss: 0.01514, val loss: 0.01550\n",
      "Training epoch: 7248, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7249, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7250, train loss: 0.01516, val loss: 0.01555\n",
      "Training epoch: 7251, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 7252, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7253, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7254, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7255, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7256, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7257, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7258, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7259, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 7260, train loss: 0.01520, val loss: 0.01554\n",
      "Training epoch: 7261, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7262, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7263, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 7264, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7265, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7266, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7267, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7268, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7269, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7270, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7271, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7272, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7273, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7274, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7275, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7276, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7277, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7278, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7279, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7280, train loss: 0.01516, val loss: 0.01555\n",
      "Training epoch: 7281, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7282, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7283, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7284, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 7285, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7286, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7287, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7288, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7289, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7290, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7291, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7292, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7293, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 7294, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7295, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7296, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7297, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7298, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7299, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 7300, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7301, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 7302, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7303, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7304, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7305, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7306, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7307, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7308, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7309, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7310, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7311, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7312, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7313, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7314, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7315, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7316, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7317, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7318, train loss: 0.01519, val loss: 0.01557\n",
      "Training epoch: 7319, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7320, train loss: 0.01519, val loss: 0.01555\n",
      "Training epoch: 7321, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7322, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7323, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7324, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7325, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7326, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7327, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7328, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7329, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7330, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7331, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 7332, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7333, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7334, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7335, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7336, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7337, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7338, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7339, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7340, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7341, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7342, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7343, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7344, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7345, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7346, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7347, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 7348, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 7349, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7350, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7351, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7352, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7353, train loss: 0.01515, val loss: 0.01550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 7354, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7355, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7356, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7357, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7358, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7359, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7360, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7361, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7362, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 7363, train loss: 0.01518, val loss: 0.01553\n",
      "Training epoch: 7364, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 7365, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7366, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7367, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7368, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7369, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7370, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7371, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7372, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7373, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7374, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7375, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7376, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7377, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7378, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7379, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 7380, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7381, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7382, train loss: 0.01514, val loss: 0.01550\n",
      "Training epoch: 7383, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7384, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 7385, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7386, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7387, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7388, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7389, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7390, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7391, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7392, train loss: 0.01514, val loss: 0.01550\n",
      "Training epoch: 7393, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7394, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7395, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7396, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7397, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 7398, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7399, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7400, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 7401, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7402, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7403, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 7404, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7405, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7406, train loss: 0.01517, val loss: 0.01555\n",
      "Training epoch: 7407, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7408, train loss: 0.01517, val loss: 0.01553\n",
      "Training epoch: 7409, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 7410, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7411, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7412, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7413, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7414, train loss: 0.01518, val loss: 0.01556\n",
      "Training epoch: 7415, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7416, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7417, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7418, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7419, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7420, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7421, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7422, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7423, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 7424, train loss: 0.01518, val loss: 0.01555\n",
      "Training epoch: 7425, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7426, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7427, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7428, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7429, train loss: 0.01514, val loss: 0.01550\n",
      "Training epoch: 7430, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7431, train loss: 0.01514, val loss: 0.01551\n",
      "Training epoch: 7432, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7433, train loss: 0.01516, val loss: 0.01551\n",
      "Training epoch: 7434, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7435, train loss: 0.01517, val loss: 0.01554\n",
      "Training epoch: 7436, train loss: 0.01518, val loss: 0.01554\n",
      "Training epoch: 7437, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7438, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7439, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7440, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7441, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7442, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 7443, train loss: 0.01516, val loss: 0.01550\n",
      "Training epoch: 7444, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7445, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7446, train loss: 0.01515, val loss: 0.01554\n",
      "Training epoch: 7447, train loss: 0.01517, val loss: 0.01556\n",
      "Training epoch: 7448, train loss: 0.01517, val loss: 0.01552\n",
      "Training epoch: 7449, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7450, train loss: 0.01515, val loss: 0.01553\n",
      "Training epoch: 7451, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7452, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7453, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7454, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7455, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7456, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7457, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7458, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7459, train loss: 0.01514, val loss: 0.01552\n",
      "Training epoch: 7460, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7461, train loss: 0.01516, val loss: 0.01553\n",
      "Training epoch: 7462, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7463, train loss: 0.01515, val loss: 0.01550\n",
      "Training epoch: 7464, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7465, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7466, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7467, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7468, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7469, train loss: 0.01516, val loss: 0.01552\n",
      "Training epoch: 7470, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7471, train loss: 0.01515, val loss: 0.01552\n",
      "Training epoch: 7472, train loss: 0.01515, val loss: 0.01551\n",
      "Training epoch: 7473, train loss: 0.01514, val loss: 0.01552\n",
      "Early stop at epoch 7473, With Testing Error: 0.01552\n",
      "Subnetwork pruning.\n",
      "Fine tuning.\n",
      "Tuning epoch: 1, train loss: 0.01515, val loss: 0.01550\n",
      "Tuning epoch: 2, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 3, train loss: 0.01514, val loss: 0.01550\n",
      "Tuning epoch: 4, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 5, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 6, train loss: 0.01514, val loss: 0.01547\n",
      "Tuning epoch: 7, train loss: 0.01515, val loss: 0.01551\n",
      "Tuning epoch: 8, train loss: 0.01516, val loss: 0.01550\n",
      "Tuning epoch: 9, train loss: 0.01519, val loss: 0.01555\n",
      "Tuning epoch: 10, train loss: 0.01515, val loss: 0.01548\n",
      "Tuning epoch: 11, train loss: 0.01517, val loss: 0.01551\n",
      "Tuning epoch: 12, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 13, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 14, train loss: 0.01513, val loss: 0.01548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning epoch: 15, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 16, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 17, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 18, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 19, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 20, train loss: 0.01513, val loss: 0.01549\n",
      "Tuning epoch: 21, train loss: 0.01515, val loss: 0.01548\n",
      "Tuning epoch: 22, train loss: 0.01515, val loss: 0.01551\n",
      "Tuning epoch: 23, train loss: 0.01516, val loss: 0.01549\n",
      "Tuning epoch: 24, train loss: 0.01515, val loss: 0.01549\n",
      "Tuning epoch: 25, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 26, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 27, train loss: 0.01514, val loss: 0.01550\n",
      "Tuning epoch: 28, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 29, train loss: 0.01515, val loss: 0.01548\n",
      "Tuning epoch: 30, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 31, train loss: 0.01513, val loss: 0.01549\n",
      "Tuning epoch: 32, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 33, train loss: 0.01513, val loss: 0.01547\n",
      "Tuning epoch: 34, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 35, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 36, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 37, train loss: 0.01514, val loss: 0.01550\n",
      "Tuning epoch: 38, train loss: 0.01514, val loss: 0.01547\n",
      "Tuning epoch: 39, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 40, train loss: 0.01514, val loss: 0.01551\n",
      "Tuning epoch: 41, train loss: 0.01513, val loss: 0.01549\n",
      "Tuning epoch: 42, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 43, train loss: 0.01513, val loss: 0.01547\n",
      "Tuning epoch: 44, train loss: 0.01513, val loss: 0.01549\n",
      "Tuning epoch: 45, train loss: 0.01513, val loss: 0.01547\n",
      "Tuning epoch: 46, train loss: 0.01513, val loss: 0.01547\n",
      "Tuning epoch: 47, train loss: 0.01514, val loss: 0.01550\n",
      "Tuning epoch: 48, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 49, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 50, train loss: 0.01514, val loss: 0.01547\n",
      "Tuning epoch: 51, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 52, train loss: 0.01515, val loss: 0.01550\n",
      "Tuning epoch: 53, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 54, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 55, train loss: 0.01514, val loss: 0.01550\n",
      "Tuning epoch: 56, train loss: 0.01513, val loss: 0.01549\n",
      "Tuning epoch: 57, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 58, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 59, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 60, train loss: 0.01513, val loss: 0.01549\n",
      "Tuning epoch: 61, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 62, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 63, train loss: 0.01515, val loss: 0.01548\n",
      "Tuning epoch: 64, train loss: 0.01513, val loss: 0.01549\n",
      "Tuning epoch: 65, train loss: 0.01515, val loss: 0.01550\n",
      "Tuning epoch: 66, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 67, train loss: 0.01513, val loss: 0.01549\n",
      "Tuning epoch: 68, train loss: 0.01513, val loss: 0.01547\n",
      "Tuning epoch: 69, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 70, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 71, train loss: 0.01514, val loss: 0.01547\n",
      "Tuning epoch: 72, train loss: 0.01515, val loss: 0.01550\n",
      "Tuning epoch: 73, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 74, train loss: 0.01517, val loss: 0.01551\n",
      "Tuning epoch: 75, train loss: 0.01515, val loss: 0.01549\n",
      "Tuning epoch: 76, train loss: 0.01515, val loss: 0.01550\n",
      "Tuning epoch: 77, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 78, train loss: 0.01513, val loss: 0.01547\n",
      "Tuning epoch: 79, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 80, train loss: 0.01513, val loss: 0.01547\n",
      "Tuning epoch: 81, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 82, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 83, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 84, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 85, train loss: 0.01514, val loss: 0.01550\n",
      "Tuning epoch: 86, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 87, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 88, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 89, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 90, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 91, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 92, train loss: 0.01515, val loss: 0.01551\n",
      "Tuning epoch: 93, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 94, train loss: 0.01514, val loss: 0.01548\n",
      "Tuning epoch: 95, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 96, train loss: 0.01513, val loss: 0.01547\n",
      "Tuning epoch: 97, train loss: 0.01516, val loss: 0.01550\n",
      "Tuning epoch: 98, train loss: 0.01514, val loss: 0.01549\n",
      "Tuning epoch: 99, train loss: 0.01513, val loss: 0.01548\n",
      "Tuning epoch: 100, train loss: 0.01514, val loss: 0.01548\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1296 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ExNN(meta_info=meta_info,\n",
    "               subnet_num=10,\n",
    "               subnet_arch=[10, 6],\n",
    "               task_type=task_type,\n",
    "               activation_func=tf.tanh,\n",
    "               batch_size=min(1000, int(train_x.shape[0] * 0.2)),\n",
    "               training_epochs=10000,\n",
    "               lr_bp=0.001,\n",
    "               lr_cl=0.1,\n",
    "               beta_threshold=0.05,\n",
    "               tuning_epochs=100,\n",
    "               l1_proj=best_l1_prob,\n",
    "               l1_subnet=best_l1_subnet,\n",
    "               smooth_lambda=10**(-6),\n",
    "               verbose=True,\n",
    "               val_ratio=0.2,\n",
    "               early_stop_thres=500)\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"exnn_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0032  1.02624 0.98589]\n"
     ]
    }
   ],
   "source": [
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "\n",
    "mse_stat = np.hstack([np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(tr_pred) - meta_info[\"Y\"][\"scaler\"].inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(val_pred) - meta_info[\"Y\"][\"scaler\"].inverse_transform(model.val_y))**2),5),\\\n",
    "               np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(pred_test) - meta_info[\"Y\"][\"scaler\"].inverse_transform(test_y))**2),5)])\n",
    "print(mse_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
