{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.lines as mlines\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.stats import ortho_group\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from xnn.sosxnn import SOSxNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "corr = 0.5\n",
    "noise_sigma = 1\n",
    "dummy_num = 0\n",
    "feature_num = 10\n",
    "test_num = 10000\n",
    "data_num = 10000\n",
    "\n",
    "proj_matrix = np.zeros((feature_num,4))\n",
    "proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "\n",
    "def data_generator1(data_num, feature_num, corr, proj_matrix, noise_sigma):\n",
    "    u = np.random.uniform(-1,1, [data_num, 1])\n",
    "    t= np.sqrt(corr/(1-corr))\n",
    "    X = np.zeros((data_num, feature_num))\n",
    "    for i in range(feature_num):\n",
    "        X[:, i:i+1] = (np.random.uniform(-1,1,[data_num,1])+t*u)/(1+t)\n",
    "    Y = np.reshape(2*np.dot(X, proj_matrix[:,0])+0.2*np.exp(-4*np.dot(X, proj_matrix[:,1])) + \\\n",
    "              3*(np.dot(X, proj_matrix[:,2]))**2+2.5*np.sin(np.pi*np.dot(X, proj_matrix[:,3])), [-1,1]) + \\\n",
    "              noise_sigma*np.random.normal(0,1, [data_num,1])\n",
    "    return X, Y\n",
    "\n",
    "np.random.seed(0)\n",
    "X, Y = data_generator1(data_num+test_num, feature_num+dummy_num, corr, proj_matrix, noise_sigma)\n",
    "scaler_x = MinMaxScaler((-1, 1)); scaler_y = MinMaxScaler((-1, 1))\n",
    "sX = scaler_x.fit_transform(X); sY = scaler_y.fit_transform(Y)\n",
    "train_x, test_x, train_y, test_y = train_test_split(sX, sY, test_size = test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n",
      "Training epoch: 1, train loss: 0.06195, val loss: 0.06164\n",
      "Training epoch: 2, train loss: 0.05207, val loss: 0.05043\n",
      "Training epoch: 3, train loss: 0.04411, val loss: 0.04369\n",
      "Training epoch: 4, train loss: 0.03922, val loss: 0.03843\n",
      "Training epoch: 5, train loss: 0.03542, val loss: 0.03502\n",
      "Training epoch: 6, train loss: 0.03252, val loss: 0.03197\n",
      "Training epoch: 7, train loss: 0.02998, val loss: 0.02948\n",
      "Training epoch: 8, train loss: 0.02831, val loss: 0.02788\n",
      "Training epoch: 9, train loss: 0.02779, val loss: 0.02741\n",
      "Training epoch: 10, train loss: 0.02700, val loss: 0.02672\n",
      "Training epoch: 11, train loss: 0.03081, val loss: 0.03048\n",
      "Training epoch: 12, train loss: 0.02381, val loss: 0.02370\n",
      "Training epoch: 13, train loss: 0.02322, val loss: 0.02349\n",
      "Training epoch: 14, train loss: 0.02220, val loss: 0.02220\n",
      "Training epoch: 15, train loss: 0.02177, val loss: 0.02193\n",
      "Training epoch: 16, train loss: 0.02174, val loss: 0.02180\n",
      "Training epoch: 17, train loss: 0.02135, val loss: 0.02160\n",
      "Training epoch: 18, train loss: 0.02184, val loss: 0.02189\n",
      "Training epoch: 19, train loss: 0.02107, val loss: 0.02123\n",
      "Training epoch: 20, train loss: 0.02108, val loss: 0.02137\n",
      "Training epoch: 21, train loss: 0.02189, val loss: 0.02217\n",
      "Training epoch: 22, train loss: 0.02105, val loss: 0.02131\n",
      "Training epoch: 23, train loss: 0.02070, val loss: 0.02095\n",
      "Training epoch: 24, train loss: 0.02096, val loss: 0.02113\n",
      "Training epoch: 25, train loss: 0.02063, val loss: 0.02075\n",
      "Training epoch: 26, train loss: 0.02265, val loss: 0.02310\n",
      "Training epoch: 27, train loss: 0.02067, val loss: 0.02089\n",
      "Training epoch: 28, train loss: 0.02085, val loss: 0.02120\n",
      "Training epoch: 29, train loss: 0.02055, val loss: 0.02074\n",
      "Training epoch: 30, train loss: 0.02039, val loss: 0.02056\n",
      "Training epoch: 31, train loss: 0.02037, val loss: 0.02063\n",
      "Training epoch: 32, train loss: 0.01987, val loss: 0.02009\n",
      "Training epoch: 33, train loss: 0.02036, val loss: 0.02071\n",
      "Training epoch: 34, train loss: 0.01977, val loss: 0.01999\n",
      "Training epoch: 35, train loss: 0.01999, val loss: 0.02031\n",
      "Training epoch: 36, train loss: 0.01974, val loss: 0.01999\n",
      "Training epoch: 37, train loss: 0.02023, val loss: 0.02058\n",
      "Training epoch: 38, train loss: 0.02022, val loss: 0.02041\n",
      "Training epoch: 39, train loss: 0.01998, val loss: 0.02027\n",
      "Training epoch: 40, train loss: 0.01961, val loss: 0.01987\n",
      "Training epoch: 41, train loss: 0.01942, val loss: 0.01967\n",
      "Training epoch: 42, train loss: 0.01951, val loss: 0.01965\n",
      "Training epoch: 43, train loss: 0.02014, val loss: 0.02040\n",
      "Training epoch: 44, train loss: 0.02073, val loss: 0.02099\n",
      "Training epoch: 45, train loss: 0.02005, val loss: 0.02037\n",
      "Training epoch: 46, train loss: 0.01973, val loss: 0.01987\n",
      "Training epoch: 47, train loss: 0.01940, val loss: 0.01967\n",
      "Training epoch: 48, train loss: 0.02009, val loss: 0.02038\n",
      "Training epoch: 49, train loss: 0.01943, val loss: 0.01976\n",
      "Training epoch: 50, train loss: 0.01957, val loss: 0.01972\n",
      "Training epoch: 51, train loss: 0.01950, val loss: 0.01970\n",
      "Training epoch: 52, train loss: 0.01913, val loss: 0.01934\n",
      "Training epoch: 53, train loss: 0.01908, val loss: 0.01924\n",
      "Training epoch: 54, train loss: 0.01904, val loss: 0.01923\n",
      "Training epoch: 55, train loss: 0.01891, val loss: 0.01912\n",
      "Training epoch: 56, train loss: 0.01882, val loss: 0.01898\n",
      "Training epoch: 57, train loss: 0.01905, val loss: 0.01927\n",
      "Training epoch: 58, train loss: 0.01890, val loss: 0.01916\n",
      "Training epoch: 59, train loss: 0.01874, val loss: 0.01889\n",
      "Training epoch: 60, train loss: 0.01987, val loss: 0.02012\n",
      "Training epoch: 61, train loss: 0.01888, val loss: 0.01915\n",
      "Training epoch: 62, train loss: 0.02009, val loss: 0.02020\n",
      "Training epoch: 63, train loss: 0.01878, val loss: 0.01899\n",
      "Training epoch: 64, train loss: 0.01946, val loss: 0.01963\n",
      "Training epoch: 65, train loss: 0.01926, val loss: 0.01949\n",
      "Training epoch: 66, train loss: 0.01960, val loss: 0.01971\n",
      "Training epoch: 67, train loss: 0.01870, val loss: 0.01875\n",
      "Training epoch: 68, train loss: 0.01858, val loss: 0.01884\n",
      "Training epoch: 69, train loss: 0.01874, val loss: 0.01880\n",
      "Training epoch: 70, train loss: 0.01848, val loss: 0.01863\n",
      "Training epoch: 71, train loss: 0.01839, val loss: 0.01858\n",
      "Training epoch: 72, train loss: 0.01880, val loss: 0.01896\n",
      "Training epoch: 73, train loss: 0.01919, val loss: 0.01944\n",
      "Training epoch: 74, train loss: 0.01888, val loss: 0.01897\n",
      "Training epoch: 75, train loss: 0.01872, val loss: 0.01884\n",
      "Training epoch: 76, train loss: 0.01844, val loss: 0.01856\n",
      "Training epoch: 77, train loss: 0.01819, val loss: 0.01836\n",
      "Training epoch: 78, train loss: 0.01812, val loss: 0.01829\n",
      "Training epoch: 79, train loss: 0.01883, val loss: 0.01903\n",
      "Training epoch: 80, train loss: 0.01845, val loss: 0.01867\n",
      "Training epoch: 81, train loss: 0.01874, val loss: 0.01879\n",
      "Training epoch: 82, train loss: 0.01824, val loss: 0.01840\n",
      "Training epoch: 83, train loss: 0.01820, val loss: 0.01831\n",
      "Training epoch: 84, train loss: 0.01803, val loss: 0.01816\n",
      "Training epoch: 85, train loss: 0.01909, val loss: 0.01916\n",
      "Training epoch: 86, train loss: 0.01812, val loss: 0.01824\n",
      "Training epoch: 87, train loss: 0.01840, val loss: 0.01862\n",
      "Training epoch: 88, train loss: 0.01804, val loss: 0.01822\n",
      "Training epoch: 89, train loss: 0.01852, val loss: 0.01858\n",
      "Training epoch: 90, train loss: 0.01879, val loss: 0.01894\n",
      "Training epoch: 91, train loss: 0.01810, val loss: 0.01816\n",
      "Training epoch: 92, train loss: 0.01840, val loss: 0.01852\n",
      "Training epoch: 93, train loss: 0.01800, val loss: 0.01810\n",
      "Training epoch: 94, train loss: 0.01826, val loss: 0.01832\n",
      "Training epoch: 95, train loss: 0.01976, val loss: 0.01988\n",
      "Training epoch: 96, train loss: 0.01811, val loss: 0.01824\n",
      "Training epoch: 97, train loss: 0.01787, val loss: 0.01791\n",
      "Training epoch: 98, train loss: 0.01770, val loss: 0.01781\n",
      "Training epoch: 99, train loss: 0.01762, val loss: 0.01773\n",
      "Training epoch: 100, train loss: 0.01772, val loss: 0.01779\n",
      "Training epoch: 101, train loss: 0.01783, val loss: 0.01781\n",
      "Training epoch: 102, train loss: 0.01815, val loss: 0.01828\n",
      "Training epoch: 103, train loss: 0.01782, val loss: 0.01797\n",
      "Training epoch: 104, train loss: 0.01767, val loss: 0.01780\n",
      "Training epoch: 105, train loss: 0.01800, val loss: 0.01808\n",
      "Training epoch: 106, train loss: 0.01941, val loss: 0.01963\n",
      "Training epoch: 107, train loss: 0.01851, val loss: 0.01864\n",
      "Training epoch: 108, train loss: 0.01768, val loss: 0.01780\n",
      "Training epoch: 109, train loss: 0.01808, val loss: 0.01817\n",
      "Training epoch: 110, train loss: 0.01788, val loss: 0.01802\n",
      "Training epoch: 111, train loss: 0.01766, val loss: 0.01774\n",
      "Training epoch: 112, train loss: 0.01786, val loss: 0.01789\n",
      "Training epoch: 113, train loss: 0.01826, val loss: 0.01835\n",
      "Training epoch: 114, train loss: 0.01786, val loss: 0.01792\n",
      "Training epoch: 115, train loss: 0.01760, val loss: 0.01774\n",
      "Training epoch: 116, train loss: 0.01845, val loss: 0.01859\n",
      "Training epoch: 117, train loss: 0.01868, val loss: 0.01869\n",
      "Training epoch: 118, train loss: 0.01908, val loss: 0.01904\n",
      "Training epoch: 119, train loss: 0.01827, val loss: 0.01836\n",
      "Training epoch: 120, train loss: 0.01874, val loss: 0.01870\n",
      "Training epoch: 121, train loss: 0.01751, val loss: 0.01769\n",
      "Training epoch: 122, train loss: 0.01747, val loss: 0.01760\n",
      "Training epoch: 123, train loss: 0.01856, val loss: 0.01873\n",
      "Training epoch: 124, train loss: 0.01767, val loss: 0.01774\n",
      "Training epoch: 125, train loss: 0.01816, val loss: 0.01835\n",
      "Training epoch: 126, train loss: 0.01765, val loss: 0.01770\n",
      "Training epoch: 127, train loss: 0.01720, val loss: 0.01730\n",
      "Training epoch: 128, train loss: 0.01837, val loss: 0.01852\n",
      "Training epoch: 129, train loss: 0.01733, val loss: 0.01738\n",
      "Training epoch: 130, train loss: 0.01788, val loss: 0.01813\n",
      "Training epoch: 131, train loss: 0.01732, val loss: 0.01743\n",
      "Training epoch: 132, train loss: 0.01801, val loss: 0.01813\n",
      "Training epoch: 133, train loss: 0.01750, val loss: 0.01762\n",
      "Training epoch: 134, train loss: 0.01735, val loss: 0.01741\n",
      "Training epoch: 135, train loss: 0.01781, val loss: 0.01796\n",
      "Training epoch: 136, train loss: 0.01795, val loss: 0.01799\n",
      "Training epoch: 137, train loss: 0.01725, val loss: 0.01739\n",
      "Training epoch: 138, train loss: 0.01808, val loss: 0.01814\n",
      "Training epoch: 139, train loss: 0.01748, val loss: 0.01751\n",
      "Training epoch: 140, train loss: 0.02024, val loss: 0.02035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 141, train loss: 0.01713, val loss: 0.01723\n",
      "Training epoch: 142, train loss: 0.01720, val loss: 0.01722\n",
      "Training epoch: 143, train loss: 0.01705, val loss: 0.01721\n",
      "Training epoch: 144, train loss: 0.01747, val loss: 0.01759\n",
      "Training epoch: 145, train loss: 0.02019, val loss: 0.02049\n",
      "Training epoch: 146, train loss: 0.01721, val loss: 0.01731\n",
      "Training epoch: 147, train loss: 0.01753, val loss: 0.01758\n",
      "Training epoch: 148, train loss: 0.01771, val loss: 0.01781\n",
      "Training epoch: 149, train loss: 0.01710, val loss: 0.01709\n",
      "Training epoch: 150, train loss: 0.01842, val loss: 0.01859\n",
      "Training epoch: 151, train loss: 0.01752, val loss: 0.01743\n",
      "Training epoch: 152, train loss: 0.01727, val loss: 0.01723\n",
      "Training epoch: 153, train loss: 0.01703, val loss: 0.01700\n",
      "Training epoch: 154, train loss: 0.01810, val loss: 0.01824\n",
      "Training epoch: 155, train loss: 0.01816, val loss: 0.01812\n",
      "Training epoch: 156, train loss: 0.01823, val loss: 0.01838\n",
      "Training epoch: 157, train loss: 0.01730, val loss: 0.01729\n",
      "Training epoch: 158, train loss: 0.01707, val loss: 0.01713\n",
      "Training epoch: 159, train loss: 0.01725, val loss: 0.01726\n",
      "Training epoch: 160, train loss: 0.01899, val loss: 0.01902\n",
      "Training epoch: 161, train loss: 0.01851, val loss: 0.01870\n",
      "Training epoch: 162, train loss: 0.01729, val loss: 0.01729\n",
      "Training epoch: 163, train loss: 0.01726, val loss: 0.01724\n",
      "Training epoch: 164, train loss: 0.01835, val loss: 0.01854\n",
      "Training epoch: 165, train loss: 0.01690, val loss: 0.01696\n",
      "Training epoch: 166, train loss: 0.01716, val loss: 0.01721\n",
      "Training epoch: 167, train loss: 0.01762, val loss: 0.01770\n",
      "Training epoch: 168, train loss: 0.01776, val loss: 0.01775\n",
      "Training epoch: 169, train loss: 0.01836, val loss: 0.01866\n",
      "Training epoch: 170, train loss: 0.01711, val loss: 0.01723\n",
      "Training epoch: 171, train loss: 0.01810, val loss: 0.01807\n",
      "Training epoch: 172, train loss: 0.01823, val loss: 0.01839\n",
      "Training epoch: 173, train loss: 0.01809, val loss: 0.01810\n",
      "Training epoch: 174, train loss: 0.01755, val loss: 0.01755\n",
      "Training epoch: 175, train loss: 0.01822, val loss: 0.01837\n",
      "Training epoch: 176, train loss: 0.01685, val loss: 0.01673\n",
      "Training epoch: 177, train loss: 0.01692, val loss: 0.01687\n",
      "Training epoch: 178, train loss: 0.01685, val loss: 0.01685\n",
      "Training epoch: 179, train loss: 0.01702, val loss: 0.01703\n",
      "Training epoch: 180, train loss: 0.01939, val loss: 0.01952\n",
      "Training epoch: 181, train loss: 0.01735, val loss: 0.01733\n",
      "Training epoch: 182, train loss: 0.01703, val loss: 0.01699\n",
      "Training epoch: 183, train loss: 0.01686, val loss: 0.01687\n",
      "Training epoch: 184, train loss: 0.01691, val loss: 0.01687\n",
      "Training epoch: 185, train loss: 0.01695, val loss: 0.01704\n",
      "Training epoch: 186, train loss: 0.01758, val loss: 0.01765\n",
      "Training epoch: 187, train loss: 0.01745, val loss: 0.01763\n",
      "Training epoch: 188, train loss: 0.01688, val loss: 0.01695\n",
      "Training epoch: 189, train loss: 0.01829, val loss: 0.01822\n",
      "Training epoch: 190, train loss: 0.02031, val loss: 0.02034\n",
      "Training epoch: 191, train loss: 0.01662, val loss: 0.01671\n",
      "Training epoch: 192, train loss: 0.01863, val loss: 0.01867\n",
      "Training epoch: 193, train loss: 0.01730, val loss: 0.01733\n",
      "Training epoch: 194, train loss: 0.01722, val loss: 0.01719\n",
      "Training epoch: 195, train loss: 0.02020, val loss: 0.02034\n",
      "Training epoch: 196, train loss: 0.02080, val loss: 0.02059\n",
      "Training epoch: 197, train loss: 0.01717, val loss: 0.01727\n",
      "Training epoch: 198, train loss: 0.01737, val loss: 0.01726\n",
      "Training epoch: 199, train loss: 0.01695, val loss: 0.01694\n",
      "Training epoch: 200, train loss: 0.01766, val loss: 0.01756\n",
      "Training epoch: 201, train loss: 0.01680, val loss: 0.01688\n",
      "Training epoch: 202, train loss: 0.01728, val loss: 0.01726\n",
      "Training epoch: 203, train loss: 0.01663, val loss: 0.01668\n",
      "Training epoch: 204, train loss: 0.01692, val loss: 0.01691\n",
      "Training epoch: 205, train loss: 0.01662, val loss: 0.01667\n",
      "Training epoch: 206, train loss: 0.01696, val loss: 0.01695\n",
      "Training epoch: 207, train loss: 0.01665, val loss: 0.01667\n",
      "Training epoch: 208, train loss: 0.01684, val loss: 0.01679\n",
      "Training epoch: 209, train loss: 0.01668, val loss: 0.01662\n",
      "Training epoch: 210, train loss: 0.01682, val loss: 0.01686\n",
      "Training epoch: 211, train loss: 0.01682, val loss: 0.01677\n",
      "Training epoch: 212, train loss: 0.01697, val loss: 0.01690\n",
      "Training epoch: 213, train loss: 0.01663, val loss: 0.01665\n",
      "Training epoch: 214, train loss: 0.01717, val loss: 0.01722\n",
      "Training epoch: 215, train loss: 0.01772, val loss: 0.01784\n",
      "Training epoch: 216, train loss: 0.01660, val loss: 0.01659\n",
      "Training epoch: 217, train loss: 0.01737, val loss: 0.01740\n",
      "Training epoch: 218, train loss: 0.01700, val loss: 0.01701\n",
      "Training epoch: 219, train loss: 0.01783, val loss: 0.01814\n",
      "Training epoch: 220, train loss: 0.01688, val loss: 0.01682\n",
      "Training epoch: 221, train loss: 0.01957, val loss: 0.01952\n",
      "Training epoch: 222, train loss: 0.01694, val loss: 0.01711\n",
      "Training epoch: 223, train loss: 0.01806, val loss: 0.01801\n",
      "Training epoch: 224, train loss: 0.01707, val loss: 0.01704\n",
      "Training epoch: 225, train loss: 0.01716, val loss: 0.01713\n",
      "Training epoch: 226, train loss: 0.01767, val loss: 0.01779\n",
      "Training epoch: 227, train loss: 0.01796, val loss: 0.01809\n",
      "Training epoch: 228, train loss: 0.01663, val loss: 0.01665\n",
      "Training epoch: 229, train loss: 0.01719, val loss: 0.01722\n",
      "Training epoch: 230, train loss: 0.01703, val loss: 0.01703\n",
      "Training epoch: 231, train loss: 0.01669, val loss: 0.01673\n",
      "Training epoch: 232, train loss: 0.01653, val loss: 0.01651\n",
      "Training epoch: 233, train loss: 0.01707, val loss: 0.01715\n",
      "Training epoch: 234, train loss: 0.01698, val loss: 0.01689\n",
      "Training epoch: 235, train loss: 0.01852, val loss: 0.01882\n",
      "Training epoch: 236, train loss: 0.01740, val loss: 0.01733\n",
      "Training epoch: 237, train loss: 0.01724, val loss: 0.01738\n",
      "Training epoch: 238, train loss: 0.01832, val loss: 0.01824\n",
      "Training epoch: 239, train loss: 0.01674, val loss: 0.01665\n",
      "Training epoch: 240, train loss: 0.01703, val loss: 0.01704\n",
      "Training epoch: 241, train loss: 0.01671, val loss: 0.01672\n",
      "Training epoch: 242, train loss: 0.01673, val loss: 0.01668\n",
      "Training epoch: 243, train loss: 0.01648, val loss: 0.01653\n",
      "Training epoch: 244, train loss: 0.01723, val loss: 0.01711\n",
      "Training epoch: 245, train loss: 0.01642, val loss: 0.01654\n",
      "Training epoch: 246, train loss: 0.01947, val loss: 0.01952\n",
      "Training epoch: 247, train loss: 0.01640, val loss: 0.01650\n",
      "Training epoch: 248, train loss: 0.01688, val loss: 0.01678\n",
      "Training epoch: 249, train loss: 0.01662, val loss: 0.01662\n",
      "Training epoch: 250, train loss: 0.01682, val loss: 0.01683\n",
      "Training epoch: 251, train loss: 0.01718, val loss: 0.01718\n",
      "Training epoch: 252, train loss: 0.01653, val loss: 0.01643\n",
      "Training epoch: 253, train loss: 0.01784, val loss: 0.01798\n",
      "Training epoch: 254, train loss: 0.01679, val loss: 0.01666\n",
      "Training epoch: 255, train loss: 0.01691, val loss: 0.01701\n",
      "Training epoch: 256, train loss: 0.01651, val loss: 0.01639\n",
      "Training epoch: 257, train loss: 0.01638, val loss: 0.01646\n",
      "Training epoch: 258, train loss: 0.01758, val loss: 0.01757\n",
      "Training epoch: 259, train loss: 0.01741, val loss: 0.01756\n",
      "Training epoch: 260, train loss: 0.01744, val loss: 0.01771\n",
      "Training epoch: 261, train loss: 0.01676, val loss: 0.01682\n",
      "Training epoch: 262, train loss: 0.01740, val loss: 0.01751\n",
      "Training epoch: 263, train loss: 0.01774, val loss: 0.01775\n",
      "Training epoch: 264, train loss: 0.01662, val loss: 0.01668\n",
      "Training epoch: 265, train loss: 0.01698, val loss: 0.01690\n",
      "Training epoch: 266, train loss: 0.01688, val loss: 0.01689\n",
      "Training epoch: 267, train loss: 0.01750, val loss: 0.01739\n",
      "Training epoch: 268, train loss: 0.02087, val loss: 0.02118\n",
      "Training epoch: 269, train loss: 0.01694, val loss: 0.01680\n",
      "Training epoch: 270, train loss: 0.01685, val loss: 0.01691\n",
      "Training epoch: 271, train loss: 0.01662, val loss: 0.01665\n",
      "Training epoch: 272, train loss: 0.01640, val loss: 0.01639\n",
      "Training epoch: 273, train loss: 0.01656, val loss: 0.01647\n",
      "Training epoch: 274, train loss: 0.01734, val loss: 0.01735\n",
      "Training epoch: 275, train loss: 0.01744, val loss: 0.01730\n",
      "Training epoch: 276, train loss: 0.01662, val loss: 0.01654\n",
      "Training epoch: 277, train loss: 0.01789, val loss: 0.01794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 278, train loss: 0.01659, val loss: 0.01655\n",
      "Training epoch: 279, train loss: 0.01771, val loss: 0.01777\n",
      "Training epoch: 280, train loss: 0.01691, val loss: 0.01695\n",
      "Training epoch: 281, train loss: 0.01698, val loss: 0.01682\n",
      "Training epoch: 282, train loss: 0.01678, val loss: 0.01688\n",
      "Training epoch: 283, train loss: 0.01651, val loss: 0.01646\n",
      "Training epoch: 284, train loss: 0.01651, val loss: 0.01660\n",
      "Training epoch: 285, train loss: 0.01788, val loss: 0.01783\n",
      "Training epoch: 286, train loss: 0.01710, val loss: 0.01698\n",
      "Training epoch: 287, train loss: 0.01773, val loss: 0.01766\n",
      "Training epoch: 288, train loss: 0.01724, val loss: 0.01704\n",
      "Training epoch: 289, train loss: 0.01661, val loss: 0.01677\n",
      "Training epoch: 290, train loss: 0.01694, val loss: 0.01692\n",
      "Training epoch: 291, train loss: 0.01738, val loss: 0.01734\n",
      "Training epoch: 292, train loss: 0.01667, val loss: 0.01688\n",
      "Training epoch: 293, train loss: 0.01709, val loss: 0.01710\n",
      "Training epoch: 294, train loss: 0.01653, val loss: 0.01650\n",
      "Training epoch: 295, train loss: 0.01790, val loss: 0.01803\n",
      "Training epoch: 296, train loss: 0.01708, val loss: 0.01702\n",
      "Training epoch: 297, train loss: 0.01639, val loss: 0.01631\n",
      "Training epoch: 298, train loss: 0.01672, val loss: 0.01665\n",
      "Training epoch: 299, train loss: 0.01711, val loss: 0.01710\n",
      "Training epoch: 300, train loss: 0.01666, val loss: 0.01664\n",
      "Training epoch: 301, train loss: 0.01762, val loss: 0.01760\n",
      "Training epoch: 302, train loss: 0.01649, val loss: 0.01645\n",
      "Training epoch: 303, train loss: 0.01667, val loss: 0.01663\n",
      "Training epoch: 304, train loss: 0.01644, val loss: 0.01649\n",
      "Training epoch: 305, train loss: 0.01756, val loss: 0.01746\n",
      "Training epoch: 306, train loss: 0.01671, val loss: 0.01683\n",
      "Training epoch: 307, train loss: 0.01751, val loss: 0.01742\n",
      "Training epoch: 308, train loss: 0.01665, val loss: 0.01666\n",
      "Training epoch: 309, train loss: 0.01762, val loss: 0.01762\n",
      "Training epoch: 310, train loss: 0.01648, val loss: 0.01649\n",
      "Training epoch: 311, train loss: 0.01654, val loss: 0.01650\n",
      "Training epoch: 312, train loss: 0.01648, val loss: 0.01654\n",
      "Training epoch: 313, train loss: 0.01668, val loss: 0.01669\n",
      "Training epoch: 314, train loss: 0.01655, val loss: 0.01659\n",
      "Training epoch: 315, train loss: 0.01767, val loss: 0.01801\n",
      "Training epoch: 316, train loss: 0.02222, val loss: 0.02241\n",
      "Training epoch: 317, train loss: 0.01698, val loss: 0.01709\n",
      "Training epoch: 318, train loss: 0.01700, val loss: 0.01704\n",
      "Training epoch: 319, train loss: 0.01761, val loss: 0.01753\n",
      "Training epoch: 320, train loss: 0.01677, val loss: 0.01680\n",
      "Training epoch: 321, train loss: 0.01672, val loss: 0.01679\n",
      "Training epoch: 322, train loss: 0.01677, val loss: 0.01683\n",
      "Training epoch: 323, train loss: 0.01726, val loss: 0.01730\n",
      "Training epoch: 324, train loss: 0.01640, val loss: 0.01641\n",
      "Training epoch: 325, train loss: 0.01705, val loss: 0.01710\n",
      "Training epoch: 326, train loss: 0.01682, val loss: 0.01689\n",
      "Training epoch: 327, train loss: 0.01691, val loss: 0.01698\n",
      "Training epoch: 328, train loss: 0.01692, val loss: 0.01687\n",
      "Training epoch: 329, train loss: 0.01772, val loss: 0.01790\n",
      "Training epoch: 330, train loss: 0.01863, val loss: 0.01867\n",
      "Training epoch: 331, train loss: 0.01780, val loss: 0.01778\n",
      "Training epoch: 332, train loss: 0.01694, val loss: 0.01694\n",
      "Training epoch: 333, train loss: 0.01654, val loss: 0.01665\n",
      "Training epoch: 334, train loss: 0.01729, val loss: 0.01733\n",
      "Training epoch: 335, train loss: 0.01698, val loss: 0.01719\n",
      "Training epoch: 336, train loss: 0.01686, val loss: 0.01699\n",
      "Training epoch: 337, train loss: 0.01653, val loss: 0.01654\n",
      "Training epoch: 338, train loss: 0.01659, val loss: 0.01661\n",
      "Training epoch: 339, train loss: 0.01671, val loss: 0.01671\n",
      "Training epoch: 340, train loss: 0.01719, val loss: 0.01720\n",
      "Training epoch: 341, train loss: 0.01698, val loss: 0.01710\n",
      "Training epoch: 342, train loss: 0.01667, val loss: 0.01669\n",
      "Training epoch: 343, train loss: 0.01681, val loss: 0.01694\n",
      "Training epoch: 344, train loss: 0.01671, val loss: 0.01677\n",
      "Training epoch: 345, train loss: 0.01696, val loss: 0.01703\n",
      "Training epoch: 346, train loss: 0.01728, val loss: 0.01735\n",
      "Training epoch: 347, train loss: 0.01641, val loss: 0.01660\n",
      "Training epoch: 348, train loss: 0.01777, val loss: 0.01772\n",
      "Training epoch: 349, train loss: 0.01655, val loss: 0.01656\n",
      "Training epoch: 350, train loss: 0.01649, val loss: 0.01661\n",
      "Training epoch: 351, train loss: 0.01765, val loss: 0.01770\n",
      "Training epoch: 352, train loss: 0.01651, val loss: 0.01662\n",
      "Training epoch: 353, train loss: 0.01638, val loss: 0.01645\n",
      "Training epoch: 354, train loss: 0.01665, val loss: 0.01670\n",
      "Training epoch: 355, train loss: 0.01713, val loss: 0.01720\n",
      "Training epoch: 356, train loss: 0.01719, val loss: 0.01745\n",
      "Training epoch: 357, train loss: 0.01676, val loss: 0.01686\n",
      "Training epoch: 358, train loss: 0.01657, val loss: 0.01668\n",
      "Training epoch: 359, train loss: 0.01724, val loss: 0.01740\n",
      "Training epoch: 360, train loss: 0.01918, val loss: 0.01925\n",
      "Training epoch: 361, train loss: 0.02092, val loss: 0.02090\n",
      "Training epoch: 362, train loss: 0.01719, val loss: 0.01743\n",
      "Training epoch: 363, train loss: 0.01775, val loss: 0.01773\n",
      "Training epoch: 364, train loss: 0.01713, val loss: 0.01724\n",
      "Training epoch: 365, train loss: 0.01680, val loss: 0.01689\n",
      "Training epoch: 366, train loss: 0.01663, val loss: 0.01676\n",
      "Training epoch: 367, train loss: 0.01678, val loss: 0.01687\n",
      "Training epoch: 368, train loss: 0.01673, val loss: 0.01685\n",
      "Training epoch: 369, train loss: 0.01673, val loss: 0.01682\n",
      "Training epoch: 370, train loss: 0.01683, val loss: 0.01705\n",
      "Training epoch: 371, train loss: 0.01692, val loss: 0.01698\n",
      "Training epoch: 372, train loss: 0.01707, val loss: 0.01711\n",
      "Training epoch: 373, train loss: 0.01642, val loss: 0.01655\n",
      "Training epoch: 374, train loss: 0.01688, val loss: 0.01711\n",
      "Training epoch: 375, train loss: 0.01857, val loss: 0.01864\n",
      "Training epoch: 376, train loss: 0.01659, val loss: 0.01671\n",
      "Training epoch: 377, train loss: 0.01762, val loss: 0.01777\n",
      "Training epoch: 378, train loss: 0.01686, val loss: 0.01697\n",
      "Training epoch: 379, train loss: 0.01663, val loss: 0.01683\n",
      "Training epoch: 380, train loss: 0.01700, val loss: 0.01708\n",
      "Training epoch: 381, train loss: 0.01722, val loss: 0.01745\n",
      "Training epoch: 382, train loss: 0.01722, val loss: 0.01732\n",
      "Training epoch: 383, train loss: 0.01643, val loss: 0.01653\n",
      "Training epoch: 384, train loss: 0.01749, val loss: 0.01755\n",
      "Training epoch: 385, train loss: 0.01641, val loss: 0.01650\n",
      "Training epoch: 386, train loss: 0.01787, val loss: 0.01810\n",
      "Training epoch: 387, train loss: 0.01724, val loss: 0.01738\n",
      "Training epoch: 388, train loss: 0.01709, val loss: 0.01720\n",
      "Training epoch: 389, train loss: 0.01644, val loss: 0.01648\n",
      "Training epoch: 390, train loss: 0.01669, val loss: 0.01691\n",
      "Training epoch: 391, train loss: 0.01685, val loss: 0.01700\n",
      "Training epoch: 392, train loss: 0.01639, val loss: 0.01654\n",
      "Training epoch: 393, train loss: 0.01776, val loss: 0.01781\n",
      "Training epoch: 394, train loss: 0.01659, val loss: 0.01673\n",
      "Training epoch: 395, train loss: 0.01781, val loss: 0.01791\n",
      "Training epoch: 396, train loss: 0.01637, val loss: 0.01656\n",
      "Training epoch: 397, train loss: 0.01662, val loss: 0.01681\n",
      "Training epoch: 398, train loss: 0.01632, val loss: 0.01639\n",
      "Training epoch: 399, train loss: 0.01729, val loss: 0.01749\n",
      "Training epoch: 400, train loss: 0.01701, val loss: 0.01718\n",
      "Training epoch: 401, train loss: 0.01760, val loss: 0.01790\n",
      "Training epoch: 402, train loss: 0.01899, val loss: 0.01922\n",
      "Training epoch: 403, train loss: 0.01724, val loss: 0.01732\n",
      "Training epoch: 404, train loss: 0.01653, val loss: 0.01659\n",
      "Training epoch: 405, train loss: 0.01779, val loss: 0.01812\n",
      "Training epoch: 406, train loss: 0.01655, val loss: 0.01669\n",
      "Training epoch: 407, train loss: 0.01684, val loss: 0.01708\n",
      "Training epoch: 408, train loss: 0.01668, val loss: 0.01677\n",
      "Training epoch: 409, train loss: 0.01682, val loss: 0.01691\n",
      "Training epoch: 410, train loss: 0.01664, val loss: 0.01669\n",
      "Training epoch: 411, train loss: 0.01674, val loss: 0.01679\n",
      "Training epoch: 412, train loss: 0.01658, val loss: 0.01674\n",
      "Training epoch: 413, train loss: 0.01663, val loss: 0.01671\n",
      "Training epoch: 414, train loss: 0.01666, val loss: 0.01695\n",
      "Training epoch: 415, train loss: 0.01635, val loss: 0.01648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 416, train loss: 0.01673, val loss: 0.01694\n",
      "Training epoch: 417, train loss: 0.01682, val loss: 0.01711\n",
      "Training epoch: 418, train loss: 0.01654, val loss: 0.01670\n",
      "Training epoch: 419, train loss: 0.01658, val loss: 0.01662\n",
      "Training epoch: 420, train loss: 0.01632, val loss: 0.01654\n",
      "Training epoch: 421, train loss: 0.01680, val loss: 0.01694\n",
      "Training epoch: 422, train loss: 0.01651, val loss: 0.01664\n",
      "Training epoch: 423, train loss: 0.01670, val loss: 0.01697\n",
      "Training epoch: 424, train loss: 0.01865, val loss: 0.01874\n",
      "Training epoch: 425, train loss: 0.01721, val loss: 0.01743\n",
      "Training epoch: 426, train loss: 0.01694, val loss: 0.01736\n",
      "Training epoch: 427, train loss: 0.01704, val loss: 0.01716\n",
      "Training epoch: 428, train loss: 0.01662, val loss: 0.01678\n",
      "Training epoch: 429, train loss: 0.01660, val loss: 0.01675\n",
      "Training epoch: 430, train loss: 0.01654, val loss: 0.01685\n",
      "Training epoch: 431, train loss: 0.01661, val loss: 0.01676\n",
      "Training epoch: 432, train loss: 0.01680, val loss: 0.01700\n",
      "Training epoch: 433, train loss: 0.01633, val loss: 0.01650\n",
      "Training epoch: 434, train loss: 0.01645, val loss: 0.01667\n",
      "Training epoch: 435, train loss: 0.01669, val loss: 0.01693\n",
      "Training epoch: 436, train loss: 0.01769, val loss: 0.01789\n",
      "Training epoch: 437, train loss: 0.01710, val loss: 0.01739\n",
      "Training epoch: 438, train loss: 0.01717, val loss: 0.01733\n",
      "Training epoch: 439, train loss: 0.01825, val loss: 0.01829\n",
      "Training epoch: 440, train loss: 0.01687, val loss: 0.01712\n",
      "Training epoch: 441, train loss: 0.01749, val loss: 0.01791\n",
      "Training epoch: 442, train loss: 0.01674, val loss: 0.01693\n",
      "Training epoch: 443, train loss: 0.01706, val loss: 0.01719\n",
      "Training epoch: 444, train loss: 0.01665, val loss: 0.01681\n",
      "Training epoch: 445, train loss: 0.01650, val loss: 0.01669\n",
      "Training epoch: 446, train loss: 0.01638, val loss: 0.01659\n",
      "Training epoch: 447, train loss: 0.01642, val loss: 0.01661\n",
      "Training epoch: 448, train loss: 0.01686, val loss: 0.01694\n",
      "Training epoch: 449, train loss: 0.01701, val loss: 0.01711\n",
      "Training epoch: 450, train loss: 0.01711, val loss: 0.01742\n",
      "Training epoch: 451, train loss: 0.01670, val loss: 0.01692\n",
      "Training epoch: 452, train loss: 0.01714, val loss: 0.01738\n",
      "Training epoch: 453, train loss: 0.01658, val loss: 0.01674\n",
      "Training epoch: 454, train loss: 0.01686, val loss: 0.01706\n",
      "Training epoch: 455, train loss: 0.01725, val loss: 0.01739\n",
      "Training epoch: 456, train loss: 0.01664, val loss: 0.01680\n",
      "Training epoch: 457, train loss: 0.01652, val loss: 0.01674\n",
      "Training epoch: 458, train loss: 0.01644, val loss: 0.01665\n",
      "Training epoch: 459, train loss: 0.01639, val loss: 0.01649\n",
      "Training epoch: 460, train loss: 0.01686, val loss: 0.01695\n",
      "Training epoch: 461, train loss: 0.01702, val loss: 0.01714\n",
      "Training epoch: 462, train loss: 0.01666, val loss: 0.01684\n",
      "Training epoch: 463, train loss: 0.01647, val loss: 0.01666\n",
      "Training epoch: 464, train loss: 0.01660, val loss: 0.01668\n",
      "Training epoch: 465, train loss: 0.01642, val loss: 0.01665\n",
      "Training epoch: 466, train loss: 0.01725, val loss: 0.01733\n",
      "Training epoch: 467, train loss: 0.01767, val loss: 0.01799\n",
      "Training epoch: 468, train loss: 0.01636, val loss: 0.01652\n",
      "Training epoch: 469, train loss: 0.01720, val loss: 0.01764\n",
      "Training epoch: 470, train loss: 0.02249, val loss: 0.02258\n",
      "Training epoch: 471, train loss: 0.01817, val loss: 0.01833\n",
      "Training epoch: 472, train loss: 0.01690, val loss: 0.01734\n",
      "Training epoch: 473, train loss: 0.01658, val loss: 0.01668\n",
      "Training epoch: 474, train loss: 0.01658, val loss: 0.01677\n",
      "Training epoch: 475, train loss: 0.01639, val loss: 0.01656\n",
      "Training epoch: 476, train loss: 0.01664, val loss: 0.01678\n",
      "Training epoch: 477, train loss: 0.01648, val loss: 0.01663\n",
      "Training epoch: 478, train loss: 0.01644, val loss: 0.01677\n",
      "Training epoch: 479, train loss: 0.01686, val loss: 0.01697\n",
      "Training epoch: 480, train loss: 0.01655, val loss: 0.01677\n",
      "Training epoch: 481, train loss: 0.01678, val loss: 0.01698\n",
      "Training epoch: 482, train loss: 0.01676, val loss: 0.01688\n",
      "Training epoch: 483, train loss: 0.01665, val loss: 0.01678\n",
      "Training epoch: 484, train loss: 0.01693, val loss: 0.01720\n",
      "Training epoch: 485, train loss: 0.01746, val loss: 0.01757\n",
      "Training epoch: 486, train loss: 0.01682, val loss: 0.01710\n",
      "Training epoch: 487, train loss: 0.01653, val loss: 0.01677\n",
      "Training epoch: 488, train loss: 0.01655, val loss: 0.01671\n",
      "Training epoch: 489, train loss: 0.01633, val loss: 0.01650\n",
      "Training epoch: 490, train loss: 0.01667, val loss: 0.01683\n",
      "Training epoch: 491, train loss: 0.01866, val loss: 0.01910\n",
      "Training epoch: 492, train loss: 0.01671, val loss: 0.01699\n",
      "Training epoch: 493, train loss: 0.01659, val loss: 0.01678\n",
      "Training epoch: 494, train loss: 0.01633, val loss: 0.01638\n",
      "Training epoch: 495, train loss: 0.01697, val loss: 0.01724\n",
      "Training epoch: 496, train loss: 0.01734, val loss: 0.01746\n",
      "Training epoch: 497, train loss: 0.01713, val loss: 0.01722\n",
      "Training epoch: 498, train loss: 0.01711, val loss: 0.01731\n",
      "Training epoch: 499, train loss: 0.01664, val loss: 0.01674\n",
      "Training epoch: 500, train loss: 0.01765, val loss: 0.01794\n",
      "Training epoch: 501, train loss: 0.01703, val loss: 0.01727\n",
      "Training epoch: 502, train loss: 0.01691, val loss: 0.01715\n",
      "Training epoch: 503, train loss: 0.01691, val loss: 0.01712\n",
      "Training epoch: 504, train loss: 0.01651, val loss: 0.01666\n",
      "Training epoch: 505, train loss: 0.01659, val loss: 0.01669\n",
      "Training epoch: 506, train loss: 0.01633, val loss: 0.01655\n",
      "Training epoch: 507, train loss: 0.01645, val loss: 0.01653\n",
      "Training epoch: 508, train loss: 0.01632, val loss: 0.01651\n",
      "Training epoch: 509, train loss: 0.01689, val loss: 0.01721\n",
      "Training epoch: 510, train loss: 0.01675, val loss: 0.01682\n",
      "Training epoch: 511, train loss: 0.01651, val loss: 0.01662\n",
      "Training epoch: 512, train loss: 0.01637, val loss: 0.01658\n",
      "Training epoch: 513, train loss: 0.01706, val loss: 0.01719\n",
      "Training epoch: 514, train loss: 0.01732, val loss: 0.01774\n",
      "Training epoch: 515, train loss: 0.01751, val loss: 0.01771\n",
      "Training epoch: 516, train loss: 0.01747, val loss: 0.01760\n",
      "Training epoch: 517, train loss: 0.01712, val loss: 0.01719\n",
      "Training epoch: 518, train loss: 0.01674, val loss: 0.01701\n",
      "Training epoch: 519, train loss: 0.01732, val loss: 0.01754\n",
      "Training epoch: 520, train loss: 0.01665, val loss: 0.01680\n",
      "Training epoch: 521, train loss: 0.01927, val loss: 0.01929\n",
      "Training epoch: 522, train loss: 0.01761, val loss: 0.01782\n",
      "Training epoch: 523, train loss: 0.01678, val loss: 0.01689\n",
      "Training epoch: 524, train loss: 0.01647, val loss: 0.01671\n",
      "Training epoch: 525, train loss: 0.01766, val loss: 0.01779\n",
      "Training epoch: 526, train loss: 0.01639, val loss: 0.01665\n",
      "Training epoch: 527, train loss: 0.01638, val loss: 0.01659\n",
      "Training epoch: 528, train loss: 0.01661, val loss: 0.01675\n",
      "Training epoch: 529, train loss: 0.01699, val loss: 0.01723\n",
      "Training epoch: 530, train loss: 0.01742, val loss: 0.01758\n",
      "Training epoch: 531, train loss: 0.01765, val loss: 0.01788\n",
      "Training epoch: 532, train loss: 0.01644, val loss: 0.01651\n",
      "Training epoch: 533, train loss: 0.01636, val loss: 0.01646\n",
      "Training epoch: 534, train loss: 0.01719, val loss: 0.01750\n",
      "Training epoch: 535, train loss: 0.01684, val loss: 0.01696\n",
      "Training epoch: 536, train loss: 0.01619, val loss: 0.01639\n",
      "Training epoch: 537, train loss: 0.01639, val loss: 0.01655\n",
      "Training epoch: 538, train loss: 0.01623, val loss: 0.01643\n",
      "Training epoch: 539, train loss: 0.01653, val loss: 0.01680\n",
      "Training epoch: 540, train loss: 0.01680, val loss: 0.01698\n",
      "Training epoch: 541, train loss: 0.01638, val loss: 0.01657\n",
      "Training epoch: 542, train loss: 0.01682, val loss: 0.01709\n",
      "Training epoch: 543, train loss: 0.01622, val loss: 0.01651\n",
      "Training epoch: 544, train loss: 0.01684, val loss: 0.01704\n",
      "Training epoch: 545, train loss: 0.01663, val loss: 0.01679\n",
      "Training epoch: 546, train loss: 0.01641, val loss: 0.01653\n",
      "Training epoch: 547, train loss: 0.01630, val loss: 0.01642\n",
      "Training epoch: 548, train loss: 0.01663, val loss: 0.01679\n",
      "Training epoch: 549, train loss: 0.01700, val loss: 0.01714\n",
      "Training epoch: 550, train loss: 0.01669, val loss: 0.01679\n",
      "Training epoch: 551, train loss: 0.01722, val loss: 0.01745\n",
      "Training epoch: 552, train loss: 0.01675, val loss: 0.01692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 553, train loss: 0.01660, val loss: 0.01676\n",
      "Training epoch: 554, train loss: 0.01742, val loss: 0.01771\n",
      "Training epoch: 555, train loss: 0.01625, val loss: 0.01650\n",
      "Training epoch: 556, train loss: 0.01641, val loss: 0.01660\n",
      "Training epoch: 557, train loss: 0.01636, val loss: 0.01655\n",
      "Training epoch: 558, train loss: 0.01726, val loss: 0.01745\n",
      "Training epoch: 559, train loss: 0.01820, val loss: 0.01827\n",
      "Training epoch: 560, train loss: 0.01704, val loss: 0.01732\n",
      "Training epoch: 561, train loss: 0.01649, val loss: 0.01667\n",
      "Training epoch: 562, train loss: 0.01761, val loss: 0.01772\n",
      "Training epoch: 563, train loss: 0.01692, val loss: 0.01714\n",
      "Training epoch: 564, train loss: 0.01717, val loss: 0.01750\n",
      "Training epoch: 565, train loss: 0.01619, val loss: 0.01637\n",
      "Training epoch: 566, train loss: 0.01689, val loss: 0.01720\n",
      "Training epoch: 567, train loss: 0.01620, val loss: 0.01638\n",
      "Training epoch: 568, train loss: 0.01631, val loss: 0.01647\n",
      "Training epoch: 569, train loss: 0.01645, val loss: 0.01661\n",
      "Training epoch: 570, train loss: 0.01708, val loss: 0.01735\n",
      "Training epoch: 571, train loss: 0.01754, val loss: 0.01791\n",
      "Training epoch: 572, train loss: 0.01762, val loss: 0.01775\n",
      "Training epoch: 573, train loss: 0.01694, val loss: 0.01715\n",
      "Training epoch: 574, train loss: 0.01647, val loss: 0.01676\n",
      "Training epoch: 575, train loss: 0.01657, val loss: 0.01671\n",
      "Training epoch: 576, train loss: 0.01658, val loss: 0.01694\n",
      "Training epoch: 577, train loss: 0.01625, val loss: 0.01651\n",
      "Training epoch: 578, train loss: 0.01626, val loss: 0.01648\n",
      "Training epoch: 579, train loss: 0.01741, val loss: 0.01762\n",
      "Training epoch: 580, train loss: 0.01710, val loss: 0.01744\n",
      "Training epoch: 581, train loss: 0.01692, val loss: 0.01704\n",
      "Training epoch: 582, train loss: 0.01642, val loss: 0.01669\n",
      "Training epoch: 583, train loss: 0.01676, val loss: 0.01681\n",
      "Training epoch: 584, train loss: 0.01685, val loss: 0.01705\n",
      "Training epoch: 585, train loss: 0.01631, val loss: 0.01650\n",
      "Training epoch: 586, train loss: 0.01687, val loss: 0.01704\n",
      "Training epoch: 587, train loss: 0.01656, val loss: 0.01669\n",
      "Training epoch: 588, train loss: 0.01751, val loss: 0.01789\n",
      "Training epoch: 589, train loss: 0.01638, val loss: 0.01648\n",
      "Training epoch: 590, train loss: 0.01615, val loss: 0.01633\n",
      "Training epoch: 591, train loss: 0.01652, val loss: 0.01668\n",
      "Training epoch: 592, train loss: 0.01642, val loss: 0.01655\n",
      "Training epoch: 593, train loss: 0.01782, val loss: 0.01793\n",
      "Training epoch: 594, train loss: 0.01799, val loss: 0.01824\n",
      "Training epoch: 595, train loss: 0.01666, val loss: 0.01682\n",
      "Training epoch: 596, train loss: 0.01621, val loss: 0.01643\n",
      "Training epoch: 597, train loss: 0.01673, val loss: 0.01691\n",
      "Training epoch: 598, train loss: 0.01709, val loss: 0.01741\n",
      "Training epoch: 599, train loss: 0.01614, val loss: 0.01632\n",
      "Training epoch: 600, train loss: 0.01646, val loss: 0.01672\n",
      "Training epoch: 601, train loss: 0.01657, val loss: 0.01684\n",
      "Training epoch: 602, train loss: 0.01666, val loss: 0.01683\n",
      "Training epoch: 603, train loss: 0.01681, val loss: 0.01696\n",
      "Training epoch: 604, train loss: 0.01644, val loss: 0.01654\n",
      "Training epoch: 605, train loss: 0.01656, val loss: 0.01680\n",
      "Training epoch: 606, train loss: 0.01718, val loss: 0.01726\n",
      "Training epoch: 607, train loss: 0.01618, val loss: 0.01651\n",
      "Training epoch: 608, train loss: 0.01634, val loss: 0.01650\n",
      "Training epoch: 609, train loss: 0.01648, val loss: 0.01658\n",
      "Training epoch: 610, train loss: 0.01639, val loss: 0.01672\n",
      "Training epoch: 611, train loss: 0.01686, val loss: 0.01700\n",
      "Training epoch: 612, train loss: 0.01701, val loss: 0.01717\n",
      "Training epoch: 613, train loss: 0.01691, val loss: 0.01722\n",
      "Training epoch: 614, train loss: 0.01633, val loss: 0.01650\n",
      "Training epoch: 615, train loss: 0.01627, val loss: 0.01643\n",
      "Training epoch: 616, train loss: 0.01679, val loss: 0.01696\n",
      "Training epoch: 617, train loss: 0.01679, val loss: 0.01711\n",
      "Training epoch: 618, train loss: 0.01702, val loss: 0.01717\n",
      "Training epoch: 619, train loss: 0.01657, val loss: 0.01681\n",
      "Training epoch: 620, train loss: 0.01631, val loss: 0.01650\n",
      "Training epoch: 621, train loss: 0.01652, val loss: 0.01676\n",
      "Training epoch: 622, train loss: 0.01676, val loss: 0.01690\n",
      "Training epoch: 623, train loss: 0.01707, val loss: 0.01730\n",
      "Training epoch: 624, train loss: 0.01652, val loss: 0.01668\n",
      "Training epoch: 625, train loss: 0.01637, val loss: 0.01661\n",
      "Training epoch: 626, train loss: 0.01714, val loss: 0.01740\n",
      "Training epoch: 627, train loss: 0.01631, val loss: 0.01653\n",
      "Training epoch: 628, train loss: 0.01681, val loss: 0.01698\n",
      "Training epoch: 629, train loss: 0.01657, val loss: 0.01678\n",
      "Training epoch: 630, train loss: 0.01617, val loss: 0.01640\n",
      "Training epoch: 631, train loss: 0.01675, val loss: 0.01706\n",
      "Training epoch: 632, train loss: 0.01634, val loss: 0.01647\n",
      "Training epoch: 633, train loss: 0.01641, val loss: 0.01657\n",
      "Training epoch: 634, train loss: 0.01644, val loss: 0.01661\n",
      "Training epoch: 635, train loss: 0.01631, val loss: 0.01644\n",
      "Training epoch: 636, train loss: 0.01651, val loss: 0.01668\n",
      "Training epoch: 637, train loss: 0.01743, val loss: 0.01791\n",
      "Training epoch: 638, train loss: 0.01791, val loss: 0.01802\n",
      "Training epoch: 639, train loss: 0.01719, val loss: 0.01754\n",
      "Training epoch: 640, train loss: 0.01670, val loss: 0.01688\n",
      "Training epoch: 641, train loss: 0.01691, val loss: 0.01717\n",
      "Training epoch: 642, train loss: 0.01725, val loss: 0.01758\n",
      "Training epoch: 643, train loss: 0.01746, val loss: 0.01777\n",
      "Training epoch: 644, train loss: 0.01680, val loss: 0.01689\n",
      "Training epoch: 645, train loss: 0.01677, val loss: 0.01694\n",
      "Training epoch: 646, train loss: 0.01639, val loss: 0.01663\n",
      "Training epoch: 647, train loss: 0.01803, val loss: 0.01832\n",
      "Training epoch: 648, train loss: 0.01638, val loss: 0.01661\n",
      "Training epoch: 649, train loss: 0.01633, val loss: 0.01668\n",
      "Training epoch: 650, train loss: 0.01632, val loss: 0.01652\n",
      "Training epoch: 651, train loss: 0.01611, val loss: 0.01629\n",
      "Training epoch: 652, train loss: 0.01627, val loss: 0.01645\n",
      "Training epoch: 653, train loss: 0.01706, val loss: 0.01722\n",
      "Training epoch: 654, train loss: 0.01707, val loss: 0.01736\n",
      "Training epoch: 655, train loss: 0.01657, val loss: 0.01673\n",
      "Training epoch: 656, train loss: 0.01601, val loss: 0.01630\n",
      "Training epoch: 657, train loss: 0.01643, val loss: 0.01658\n",
      "Training epoch: 658, train loss: 0.01615, val loss: 0.01647\n",
      "Training epoch: 659, train loss: 0.01655, val loss: 0.01666\n",
      "Training epoch: 660, train loss: 0.01653, val loss: 0.01671\n",
      "Training epoch: 661, train loss: 0.01679, val loss: 0.01695\n",
      "Training epoch: 662, train loss: 0.01665, val loss: 0.01697\n",
      "Training epoch: 663, train loss: 0.01782, val loss: 0.01815\n",
      "Training epoch: 664, train loss: 0.01642, val loss: 0.01671\n",
      "Training epoch: 665, train loss: 0.01737, val loss: 0.01761\n",
      "Training epoch: 666, train loss: 0.01631, val loss: 0.01655\n",
      "Training epoch: 667, train loss: 0.01620, val loss: 0.01635\n",
      "Training epoch: 668, train loss: 0.01658, val loss: 0.01674\n",
      "Training epoch: 669, train loss: 0.01635, val loss: 0.01658\n",
      "Training epoch: 670, train loss: 0.01645, val loss: 0.01667\n",
      "Training epoch: 671, train loss: 0.01692, val loss: 0.01712\n",
      "Training epoch: 672, train loss: 0.01788, val loss: 0.01810\n",
      "Training epoch: 673, train loss: 0.01627, val loss: 0.01648\n",
      "Training epoch: 674, train loss: 0.01624, val loss: 0.01636\n",
      "Training epoch: 675, train loss: 0.01647, val loss: 0.01675\n",
      "Training epoch: 676, train loss: 0.01665, val loss: 0.01692\n",
      "Training epoch: 677, train loss: 0.01696, val loss: 0.01720\n",
      "Training epoch: 678, train loss: 0.01736, val loss: 0.01748\n",
      "Training epoch: 679, train loss: 0.01626, val loss: 0.01659\n",
      "Training epoch: 680, train loss: 0.01798, val loss: 0.01813\n",
      "Training epoch: 681, train loss: 0.01705, val loss: 0.01717\n",
      "Training epoch: 682, train loss: 0.01632, val loss: 0.01649\n",
      "Training epoch: 683, train loss: 0.01673, val loss: 0.01691\n",
      "Training epoch: 684, train loss: 0.01616, val loss: 0.01644\n",
      "Training epoch: 685, train loss: 0.01852, val loss: 0.01885\n",
      "Training epoch: 686, train loss: 0.01672, val loss: 0.01695\n",
      "Training epoch: 687, train loss: 0.01702, val loss: 0.01727\n",
      "Training epoch: 688, train loss: 0.01656, val loss: 0.01685\n",
      "Training epoch: 689, train loss: 0.01811, val loss: 0.01823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 690, train loss: 0.01633, val loss: 0.01656\n",
      "Training epoch: 691, train loss: 0.01731, val loss: 0.01762\n",
      "Training epoch: 692, train loss: 0.01638, val loss: 0.01658\n",
      "Training epoch: 693, train loss: 0.01620, val loss: 0.01648\n",
      "Training epoch: 694, train loss: 0.01644, val loss: 0.01664\n",
      "Training epoch: 695, train loss: 0.01607, val loss: 0.01628\n",
      "Training epoch: 696, train loss: 0.01618, val loss: 0.01635\n",
      "Training epoch: 697, train loss: 0.01613, val loss: 0.01643\n",
      "Training epoch: 698, train loss: 0.01660, val loss: 0.01685\n",
      "Training epoch: 699, train loss: 0.01670, val loss: 0.01703\n",
      "Training epoch: 700, train loss: 0.01618, val loss: 0.01635\n",
      "Training epoch: 701, train loss: 0.01674, val loss: 0.01689\n",
      "Training epoch: 702, train loss: 0.01625, val loss: 0.01660\n",
      "Training epoch: 703, train loss: 0.01622, val loss: 0.01637\n",
      "Training epoch: 704, train loss: 0.01609, val loss: 0.01621\n",
      "Training epoch: 705, train loss: 0.01643, val loss: 0.01664\n",
      "Training epoch: 706, train loss: 0.01629, val loss: 0.01648\n",
      "Training epoch: 707, train loss: 0.01627, val loss: 0.01658\n",
      "Training epoch: 708, train loss: 0.01638, val loss: 0.01665\n",
      "Training epoch: 709, train loss: 0.01625, val loss: 0.01634\n",
      "Training epoch: 710, train loss: 0.01636, val loss: 0.01652\n",
      "Training epoch: 711, train loss: 0.01698, val loss: 0.01725\n",
      "Training epoch: 712, train loss: 0.01613, val loss: 0.01631\n",
      "Training epoch: 713, train loss: 0.01666, val loss: 0.01686\n",
      "Training epoch: 714, train loss: 0.01667, val loss: 0.01685\n",
      "Training epoch: 715, train loss: 0.01617, val loss: 0.01643\n",
      "Training epoch: 716, train loss: 0.01624, val loss: 0.01635\n",
      "Training epoch: 717, train loss: 0.01697, val loss: 0.01712\n",
      "Training epoch: 718, train loss: 0.01634, val loss: 0.01663\n",
      "Training epoch: 719, train loss: 0.01759, val loss: 0.01785\n",
      "Training epoch: 720, train loss: 0.01636, val loss: 0.01647\n",
      "Training epoch: 721, train loss: 0.01636, val loss: 0.01661\n",
      "Training epoch: 722, train loss: 0.01689, val loss: 0.01710\n",
      "Training epoch: 723, train loss: 0.01652, val loss: 0.01660\n",
      "Training epoch: 724, train loss: 0.01692, val loss: 0.01730\n",
      "Training epoch: 725, train loss: 0.01886, val loss: 0.01907\n",
      "Training epoch: 726, train loss: 0.01699, val loss: 0.01702\n",
      "Training epoch: 727, train loss: 0.01680, val loss: 0.01702\n",
      "Training epoch: 728, train loss: 0.01622, val loss: 0.01642\n",
      "Training epoch: 729, train loss: 0.01631, val loss: 0.01657\n",
      "Training epoch: 730, train loss: 0.01635, val loss: 0.01653\n",
      "Training epoch: 731, train loss: 0.01621, val loss: 0.01656\n",
      "Training epoch: 732, train loss: 0.01622, val loss: 0.01632\n",
      "Training epoch: 733, train loss: 0.01612, val loss: 0.01628\n",
      "Training epoch: 734, train loss: 0.01606, val loss: 0.01630\n",
      "Training epoch: 735, train loss: 0.01726, val loss: 0.01743\n",
      "Training epoch: 736, train loss: 0.01641, val loss: 0.01667\n",
      "Training epoch: 737, train loss: 0.01607, val loss: 0.01633\n",
      "Training epoch: 738, train loss: 0.01675, val loss: 0.01693\n",
      "Training epoch: 739, train loss: 0.01609, val loss: 0.01634\n",
      "Training epoch: 740, train loss: 0.01659, val loss: 0.01685\n",
      "Training epoch: 741, train loss: 0.01612, val loss: 0.01634\n",
      "Training epoch: 742, train loss: 0.01677, val loss: 0.01695\n",
      "Training epoch: 743, train loss: 0.01678, val loss: 0.01706\n",
      "Training epoch: 744, train loss: 0.01675, val loss: 0.01705\n",
      "Training epoch: 745, train loss: 0.01706, val loss: 0.01723\n",
      "Training epoch: 746, train loss: 0.01596, val loss: 0.01624\n",
      "Training epoch: 747, train loss: 0.01605, val loss: 0.01622\n",
      "Training epoch: 748, train loss: 0.01686, val loss: 0.01702\n",
      "Training epoch: 749, train loss: 0.01605, val loss: 0.01622\n",
      "Training epoch: 750, train loss: 0.01621, val loss: 0.01638\n",
      "Training epoch: 751, train loss: 0.01619, val loss: 0.01637\n",
      "Training epoch: 752, train loss: 0.01635, val loss: 0.01663\n",
      "Training epoch: 753, train loss: 0.01699, val loss: 0.01720\n",
      "Training epoch: 754, train loss: 0.01723, val loss: 0.01738\n",
      "Training epoch: 755, train loss: 0.01609, val loss: 0.01633\n",
      "Training epoch: 756, train loss: 0.01617, val loss: 0.01629\n",
      "Training epoch: 757, train loss: 0.01628, val loss: 0.01639\n",
      "Training epoch: 758, train loss: 0.01611, val loss: 0.01631\n",
      "Training epoch: 759, train loss: 0.01625, val loss: 0.01643\n",
      "Training epoch: 760, train loss: 0.01688, val loss: 0.01712\n",
      "Training epoch: 761, train loss: 0.01647, val loss: 0.01669\n",
      "Training epoch: 762, train loss: 0.01606, val loss: 0.01619\n",
      "Training epoch: 763, train loss: 0.01628, val loss: 0.01649\n",
      "Training epoch: 764, train loss: 0.01644, val loss: 0.01665\n",
      "Training epoch: 765, train loss: 0.01661, val loss: 0.01691\n",
      "Training epoch: 766, train loss: 0.01741, val loss: 0.01758\n",
      "Training epoch: 767, train loss: 0.01633, val loss: 0.01672\n",
      "Training epoch: 768, train loss: 0.01605, val loss: 0.01626\n",
      "Training epoch: 769, train loss: 0.01628, val loss: 0.01676\n",
      "Training epoch: 770, train loss: 0.01660, val loss: 0.01688\n",
      "Training epoch: 771, train loss: 0.01697, val loss: 0.01724\n",
      "Training epoch: 772, train loss: 0.01753, val loss: 0.01799\n",
      "Training epoch: 773, train loss: 0.01658, val loss: 0.01680\n",
      "Training epoch: 774, train loss: 0.01621, val loss: 0.01647\n",
      "Training epoch: 775, train loss: 0.01621, val loss: 0.01632\n",
      "Training epoch: 776, train loss: 0.01706, val loss: 0.01729\n",
      "Training epoch: 777, train loss: 0.01657, val loss: 0.01680\n",
      "Training epoch: 778, train loss: 0.01632, val loss: 0.01663\n",
      "Training epoch: 779, train loss: 0.01728, val loss: 0.01770\n",
      "Training epoch: 780, train loss: 0.01659, val loss: 0.01666\n",
      "Training epoch: 781, train loss: 0.01659, val loss: 0.01673\n",
      "Training epoch: 782, train loss: 0.01605, val loss: 0.01624\n",
      "Training epoch: 783, train loss: 0.01658, val loss: 0.01661\n",
      "Training epoch: 784, train loss: 0.01664, val loss: 0.01695\n",
      "Training epoch: 785, train loss: 0.01638, val loss: 0.01664\n",
      "Training epoch: 786, train loss: 0.01624, val loss: 0.01654\n",
      "Training epoch: 787, train loss: 0.01592, val loss: 0.01618\n",
      "Training epoch: 788, train loss: 0.01624, val loss: 0.01655\n",
      "Training epoch: 789, train loss: 0.01650, val loss: 0.01666\n",
      "Training epoch: 790, train loss: 0.01609, val loss: 0.01633\n",
      "Training epoch: 791, train loss: 0.01612, val loss: 0.01628\n",
      "Training epoch: 792, train loss: 0.01624, val loss: 0.01658\n",
      "Training epoch: 793, train loss: 0.01610, val loss: 0.01630\n",
      "Training epoch: 794, train loss: 0.01670, val loss: 0.01694\n",
      "Training epoch: 795, train loss: 0.01679, val loss: 0.01695\n",
      "Training epoch: 796, train loss: 0.01627, val loss: 0.01669\n",
      "Training epoch: 797, train loss: 0.01625, val loss: 0.01641\n",
      "Training epoch: 798, train loss: 0.01655, val loss: 0.01681\n",
      "Training epoch: 799, train loss: 0.01626, val loss: 0.01642\n",
      "Training epoch: 800, train loss: 0.01616, val loss: 0.01634\n",
      "Training epoch: 801, train loss: 0.01666, val loss: 0.01691\n",
      "Training epoch: 802, train loss: 0.01598, val loss: 0.01614\n",
      "Training epoch: 803, train loss: 0.01619, val loss: 0.01635\n",
      "Training epoch: 804, train loss: 0.01607, val loss: 0.01643\n",
      "Training epoch: 805, train loss: 0.01627, val loss: 0.01655\n",
      "Training epoch: 806, train loss: 0.01640, val loss: 0.01661\n",
      "Training epoch: 807, train loss: 0.01631, val loss: 0.01655\n",
      "Training epoch: 808, train loss: 0.01648, val loss: 0.01664\n",
      "Training epoch: 809, train loss: 0.01701, val loss: 0.01734\n",
      "Training epoch: 810, train loss: 0.01630, val loss: 0.01647\n",
      "Training epoch: 811, train loss: 0.01687, val loss: 0.01698\n",
      "Training epoch: 812, train loss: 0.01606, val loss: 0.01631\n",
      "Training epoch: 813, train loss: 0.01592, val loss: 0.01624\n",
      "Training epoch: 814, train loss: 0.01599, val loss: 0.01618\n",
      "Training epoch: 815, train loss: 0.01591, val loss: 0.01615\n",
      "Training epoch: 816, train loss: 0.01650, val loss: 0.01689\n",
      "Training epoch: 817, train loss: 0.01762, val loss: 0.01783\n",
      "Training epoch: 818, train loss: 0.01742, val loss: 0.01793\n",
      "Training epoch: 819, train loss: 0.01658, val loss: 0.01679\n",
      "Training epoch: 820, train loss: 0.01784, val loss: 0.01790\n",
      "Training epoch: 821, train loss: 0.01669, val loss: 0.01690\n",
      "Training epoch: 822, train loss: 0.01676, val loss: 0.01713\n",
      "Training epoch: 823, train loss: 0.01633, val loss: 0.01662\n",
      "Training epoch: 824, train loss: 0.01880, val loss: 0.01888\n",
      "Training epoch: 825, train loss: 0.01649, val loss: 0.01681\n",
      "Training epoch: 826, train loss: 0.01621, val loss: 0.01637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 827, train loss: 0.01606, val loss: 0.01632\n",
      "Training epoch: 828, train loss: 0.01626, val loss: 0.01649\n",
      "Training epoch: 829, train loss: 0.01634, val loss: 0.01647\n",
      "Training epoch: 830, train loss: 0.01602, val loss: 0.01623\n",
      "Training epoch: 831, train loss: 0.01612, val loss: 0.01636\n",
      "Training epoch: 832, train loss: 0.01619, val loss: 0.01643\n",
      "Training epoch: 833, train loss: 0.01628, val loss: 0.01647\n",
      "Training epoch: 834, train loss: 0.01636, val loss: 0.01662\n",
      "Training epoch: 835, train loss: 0.01776, val loss: 0.01821\n",
      "Training epoch: 836, train loss: 0.01698, val loss: 0.01718\n",
      "Training epoch: 837, train loss: 0.01622, val loss: 0.01641\n",
      "Training epoch: 838, train loss: 0.01664, val loss: 0.01686\n",
      "Training epoch: 839, train loss: 0.01626, val loss: 0.01641\n",
      "Training epoch: 840, train loss: 0.01608, val loss: 0.01634\n",
      "Training epoch: 841, train loss: 0.01675, val loss: 0.01699\n",
      "Training epoch: 842, train loss: 0.01671, val loss: 0.01690\n",
      "Training epoch: 843, train loss: 0.01626, val loss: 0.01640\n",
      "Training epoch: 844, train loss: 0.01619, val loss: 0.01649\n",
      "Training epoch: 845, train loss: 0.01623, val loss: 0.01633\n",
      "Training epoch: 846, train loss: 0.01624, val loss: 0.01651\n",
      "Training epoch: 847, train loss: 0.01642, val loss: 0.01666\n",
      "Training epoch: 848, train loss: 0.01598, val loss: 0.01613\n",
      "Training epoch: 849, train loss: 0.01600, val loss: 0.01621\n",
      "Training epoch: 850, train loss: 0.01623, val loss: 0.01639\n",
      "Training epoch: 851, train loss: 0.01600, val loss: 0.01623\n",
      "Training epoch: 852, train loss: 0.01602, val loss: 0.01629\n",
      "Training epoch: 853, train loss: 0.01589, val loss: 0.01607\n",
      "Training epoch: 854, train loss: 0.01637, val loss: 0.01662\n",
      "Training epoch: 855, train loss: 0.01584, val loss: 0.01603\n",
      "Training epoch: 856, train loss: 0.01624, val loss: 0.01648\n",
      "Training epoch: 857, train loss: 0.01647, val loss: 0.01670\n",
      "Training epoch: 858, train loss: 0.01632, val loss: 0.01656\n",
      "Training epoch: 859, train loss: 0.01606, val loss: 0.01630\n",
      "Training epoch: 860, train loss: 0.01622, val loss: 0.01642\n",
      "Training epoch: 861, train loss: 0.01597, val loss: 0.01620\n",
      "Training epoch: 862, train loss: 0.01748, val loss: 0.01768\n",
      "Training epoch: 863, train loss: 0.01743, val loss: 0.01778\n",
      "Training epoch: 864, train loss: 0.01681, val loss: 0.01693\n",
      "Training epoch: 865, train loss: 0.01623, val loss: 0.01644\n",
      "Training epoch: 866, train loss: 0.01630, val loss: 0.01660\n",
      "Training epoch: 867, train loss: 0.01714, val loss: 0.01756\n",
      "Training epoch: 868, train loss: 0.01655, val loss: 0.01673\n",
      "Training epoch: 869, train loss: 0.01641, val loss: 0.01673\n",
      "Training epoch: 870, train loss: 0.01628, val loss: 0.01643\n",
      "Training epoch: 871, train loss: 0.01648, val loss: 0.01670\n",
      "Training epoch: 872, train loss: 0.01599, val loss: 0.01622\n",
      "Training epoch: 873, train loss: 0.01602, val loss: 0.01622\n",
      "Training epoch: 874, train loss: 0.01609, val loss: 0.01629\n",
      "Training epoch: 875, train loss: 0.01618, val loss: 0.01636\n",
      "Training epoch: 876, train loss: 0.01597, val loss: 0.01621\n",
      "Training epoch: 877, train loss: 0.01646, val loss: 0.01675\n",
      "Training epoch: 878, train loss: 0.01685, val loss: 0.01695\n",
      "Training epoch: 879, train loss: 0.01620, val loss: 0.01639\n",
      "Training epoch: 880, train loss: 0.01615, val loss: 0.01649\n",
      "Training epoch: 881, train loss: 0.01609, val loss: 0.01626\n",
      "Training epoch: 882, train loss: 0.01607, val loss: 0.01630\n",
      "Training epoch: 883, train loss: 0.01596, val loss: 0.01617\n",
      "Training epoch: 884, train loss: 0.01707, val loss: 0.01736\n",
      "Training epoch: 885, train loss: 0.01601, val loss: 0.01618\n",
      "Training epoch: 886, train loss: 0.01629, val loss: 0.01640\n",
      "Training epoch: 887, train loss: 0.01621, val loss: 0.01653\n",
      "Training epoch: 888, train loss: 0.01599, val loss: 0.01620\n",
      "Training epoch: 889, train loss: 0.01592, val loss: 0.01612\n",
      "Training epoch: 890, train loss: 0.01588, val loss: 0.01610\n",
      "Training epoch: 891, train loss: 0.01598, val loss: 0.01618\n",
      "Training epoch: 892, train loss: 0.01590, val loss: 0.01604\n",
      "Training epoch: 893, train loss: 0.01606, val loss: 0.01629\n",
      "Training epoch: 894, train loss: 0.01635, val loss: 0.01649\n",
      "Training epoch: 895, train loss: 0.01697, val loss: 0.01730\n",
      "Training epoch: 896, train loss: 0.01591, val loss: 0.01607\n",
      "Training epoch: 897, train loss: 0.01587, val loss: 0.01615\n",
      "Training epoch: 898, train loss: 0.01633, val loss: 0.01645\n",
      "Training epoch: 899, train loss: 0.01629, val loss: 0.01644\n",
      "Training epoch: 900, train loss: 0.01692, val loss: 0.01712\n",
      "Training epoch: 901, train loss: 0.01671, val loss: 0.01696\n",
      "Training epoch: 902, train loss: 0.01646, val loss: 0.01652\n",
      "Training epoch: 903, train loss: 0.01601, val loss: 0.01620\n",
      "Training epoch: 904, train loss: 0.01635, val loss: 0.01665\n",
      "Training epoch: 905, train loss: 0.01602, val loss: 0.01613\n",
      "Training epoch: 906, train loss: 0.01595, val loss: 0.01610\n",
      "Training epoch: 907, train loss: 0.01625, val loss: 0.01642\n",
      "Training epoch: 908, train loss: 0.01592, val loss: 0.01608\n",
      "Training epoch: 909, train loss: 0.01633, val loss: 0.01667\n",
      "Training epoch: 910, train loss: 0.01638, val loss: 0.01657\n",
      "Training epoch: 911, train loss: 0.01668, val loss: 0.01706\n",
      "Training epoch: 912, train loss: 0.01599, val loss: 0.01614\n",
      "Training epoch: 913, train loss: 0.01655, val loss: 0.01688\n",
      "Training epoch: 914, train loss: 0.01770, val loss: 0.01794\n",
      "Training epoch: 915, train loss: 0.01666, val loss: 0.01691\n",
      "Training epoch: 916, train loss: 0.01651, val loss: 0.01674\n",
      "Training epoch: 917, train loss: 0.01659, val loss: 0.01673\n",
      "Training epoch: 918, train loss: 0.01624, val loss: 0.01642\n",
      "Training epoch: 919, train loss: 0.01707, val loss: 0.01743\n",
      "Training epoch: 920, train loss: 0.01657, val loss: 0.01690\n",
      "Training epoch: 921, train loss: 0.01585, val loss: 0.01616\n",
      "Training epoch: 922, train loss: 0.01589, val loss: 0.01606\n",
      "Training epoch: 923, train loss: 0.01622, val loss: 0.01638\n",
      "Training epoch: 924, train loss: 0.01627, val loss: 0.01643\n",
      "Training epoch: 925, train loss: 0.01596, val loss: 0.01619\n",
      "Training epoch: 926, train loss: 0.01678, val loss: 0.01700\n",
      "Training epoch: 927, train loss: 0.01593, val loss: 0.01620\n",
      "Training epoch: 928, train loss: 0.01609, val loss: 0.01633\n",
      "Training epoch: 929, train loss: 0.01668, val loss: 0.01686\n",
      "Training epoch: 930, train loss: 0.01633, val loss: 0.01660\n",
      "Training epoch: 931, train loss: 0.01607, val loss: 0.01621\n",
      "Training epoch: 932, train loss: 0.01597, val loss: 0.01617\n",
      "Training epoch: 933, train loss: 0.01581, val loss: 0.01603\n",
      "Training epoch: 934, train loss: 0.01651, val loss: 0.01674\n",
      "Training epoch: 935, train loss: 0.01616, val loss: 0.01641\n",
      "Training epoch: 936, train loss: 0.01631, val loss: 0.01640\n",
      "Training epoch: 937, train loss: 0.01584, val loss: 0.01611\n",
      "Training epoch: 938, train loss: 0.01611, val loss: 0.01640\n",
      "Training epoch: 939, train loss: 0.01592, val loss: 0.01614\n",
      "Training epoch: 940, train loss: 0.01668, val loss: 0.01681\n",
      "Training epoch: 941, train loss: 0.01671, val loss: 0.01693\n",
      "Training epoch: 942, train loss: 0.01654, val loss: 0.01681\n",
      "Training epoch: 943, train loss: 0.01655, val loss: 0.01685\n",
      "Training epoch: 944, train loss: 0.01608, val loss: 0.01634\n",
      "Training epoch: 945, train loss: 0.01693, val loss: 0.01714\n",
      "Training epoch: 946, train loss: 0.01673, val loss: 0.01695\n",
      "Training epoch: 947, train loss: 0.01664, val loss: 0.01686\n",
      "Training epoch: 948, train loss: 0.01587, val loss: 0.01611\n",
      "Training epoch: 949, train loss: 0.01598, val loss: 0.01612\n",
      "Training epoch: 950, train loss: 0.01594, val loss: 0.01613\n",
      "Training epoch: 951, train loss: 0.01655, val loss: 0.01687\n",
      "Training epoch: 952, train loss: 0.01593, val loss: 0.01610\n",
      "Training epoch: 953, train loss: 0.01796, val loss: 0.01845\n",
      "Training epoch: 954, train loss: 0.01672, val loss: 0.01686\n",
      "Training epoch: 955, train loss: 0.01580, val loss: 0.01609\n",
      "Training epoch: 956, train loss: 0.01598, val loss: 0.01626\n",
      "Training epoch: 957, train loss: 0.01678, val loss: 0.01701\n",
      "Training epoch: 958, train loss: 0.01612, val loss: 0.01635\n",
      "Training epoch: 959, train loss: 0.01628, val loss: 0.01658\n",
      "Training epoch: 960, train loss: 0.01824, val loss: 0.01859\n",
      "Training epoch: 961, train loss: 0.01658, val loss: 0.01669\n",
      "Training epoch: 962, train loss: 0.01598, val loss: 0.01614\n",
      "Training epoch: 963, train loss: 0.01751, val loss: 0.01788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 964, train loss: 0.01678, val loss: 0.01702\n",
      "Training epoch: 965, train loss: 0.01615, val loss: 0.01646\n",
      "Training epoch: 966, train loss: 0.01618, val loss: 0.01634\n",
      "Training epoch: 967, train loss: 0.01709, val loss: 0.01751\n",
      "Training epoch: 968, train loss: 0.01617, val loss: 0.01641\n",
      "Training epoch: 969, train loss: 0.01633, val loss: 0.01658\n",
      "Training epoch: 970, train loss: 0.01638, val loss: 0.01658\n",
      "Training epoch: 971, train loss: 0.01615, val loss: 0.01641\n",
      "Training epoch: 972, train loss: 0.01656, val loss: 0.01693\n",
      "Training epoch: 973, train loss: 0.01609, val loss: 0.01628\n",
      "Training epoch: 974, train loss: 0.01646, val loss: 0.01672\n",
      "Training epoch: 975, train loss: 0.01657, val loss: 0.01665\n",
      "Training epoch: 976, train loss: 0.01695, val loss: 0.01734\n",
      "Training epoch: 977, train loss: 0.01761, val loss: 0.01786\n",
      "Training epoch: 978, train loss: 0.01653, val loss: 0.01680\n",
      "Training epoch: 979, train loss: 0.01638, val loss: 0.01651\n",
      "Training epoch: 980, train loss: 0.01684, val loss: 0.01703\n",
      "Training epoch: 981, train loss: 0.01698, val loss: 0.01723\n",
      "Training epoch: 982, train loss: 0.01581, val loss: 0.01597\n",
      "Training epoch: 983, train loss: 0.01671, val loss: 0.01699\n",
      "Training epoch: 984, train loss: 0.01657, val loss: 0.01668\n",
      "Training epoch: 985, train loss: 0.01595, val loss: 0.01618\n",
      "Training epoch: 986, train loss: 0.01594, val loss: 0.01601\n",
      "Training epoch: 987, train loss: 0.01622, val loss: 0.01652\n",
      "Training epoch: 988, train loss: 0.01583, val loss: 0.01606\n",
      "Training epoch: 989, train loss: 0.01710, val loss: 0.01726\n",
      "Training epoch: 990, train loss: 0.01665, val loss: 0.01688\n",
      "Training epoch: 991, train loss: 0.01590, val loss: 0.01618\n",
      "Training epoch: 992, train loss: 0.01623, val loss: 0.01640\n",
      "Training epoch: 993, train loss: 0.01597, val loss: 0.01629\n",
      "Training epoch: 994, train loss: 0.01617, val loss: 0.01636\n",
      "Training epoch: 995, train loss: 0.01576, val loss: 0.01597\n",
      "Training epoch: 996, train loss: 0.01668, val loss: 0.01685\n",
      "Training epoch: 997, train loss: 0.01602, val loss: 0.01617\n",
      "Training epoch: 998, train loss: 0.01578, val loss: 0.01601\n",
      "Training epoch: 999, train loss: 0.01641, val loss: 0.01667\n",
      "Training epoch: 1000, train loss: 0.01605, val loss: 0.01629\n",
      "Training epoch: 1001, train loss: 0.01616, val loss: 0.01630\n",
      "Training epoch: 1002, train loss: 0.01655, val loss: 0.01668\n",
      "Training epoch: 1003, train loss: 0.01577, val loss: 0.01605\n",
      "Training epoch: 1004, train loss: 0.01604, val loss: 0.01614\n",
      "Training epoch: 1005, train loss: 0.01590, val loss: 0.01631\n",
      "Training epoch: 1006, train loss: 0.01610, val loss: 0.01622\n",
      "Training epoch: 1007, train loss: 0.01856, val loss: 0.01901\n",
      "Training epoch: 1008, train loss: 0.01593, val loss: 0.01616\n",
      "Training epoch: 1009, train loss: 0.01576, val loss: 0.01593\n",
      "Training epoch: 1010, train loss: 0.01603, val loss: 0.01630\n",
      "Training epoch: 1011, train loss: 0.01596, val loss: 0.01615\n",
      "Training epoch: 1012, train loss: 0.01595, val loss: 0.01607\n",
      "Training epoch: 1013, train loss: 0.01583, val loss: 0.01595\n",
      "Training epoch: 1014, train loss: 0.01607, val loss: 0.01634\n",
      "Training epoch: 1015, train loss: 0.01596, val loss: 0.01604\n",
      "Training epoch: 1016, train loss: 0.01616, val loss: 0.01635\n",
      "Training epoch: 1017, train loss: 0.01634, val loss: 0.01657\n",
      "Training epoch: 1018, train loss: 0.01583, val loss: 0.01608\n",
      "Training epoch: 1019, train loss: 0.01608, val loss: 0.01631\n",
      "Training epoch: 1020, train loss: 0.01628, val loss: 0.01642\n",
      "Training epoch: 1021, train loss: 0.01592, val loss: 0.01600\n",
      "Training epoch: 1022, train loss: 0.01644, val loss: 0.01664\n",
      "Training epoch: 1023, train loss: 0.01573, val loss: 0.01595\n",
      "Training epoch: 1024, train loss: 0.01617, val loss: 0.01640\n",
      "Training epoch: 1025, train loss: 0.01598, val loss: 0.01620\n",
      "Training epoch: 1026, train loss: 0.01633, val loss: 0.01646\n",
      "Training epoch: 1027, train loss: 0.01601, val loss: 0.01620\n",
      "Training epoch: 1028, train loss: 0.01587, val loss: 0.01601\n",
      "Training epoch: 1029, train loss: 0.01601, val loss: 0.01613\n",
      "Training epoch: 1030, train loss: 0.01591, val loss: 0.01599\n",
      "Training epoch: 1031, train loss: 0.01594, val loss: 0.01603\n",
      "Training epoch: 1032, train loss: 0.01581, val loss: 0.01601\n",
      "Training epoch: 1033, train loss: 0.01600, val loss: 0.01616\n",
      "Training epoch: 1034, train loss: 0.01629, val loss: 0.01642\n",
      "Training epoch: 1035, train loss: 0.01588, val loss: 0.01606\n",
      "Training epoch: 1036, train loss: 0.01613, val loss: 0.01634\n",
      "Training epoch: 1037, train loss: 0.01584, val loss: 0.01591\n",
      "Training epoch: 1038, train loss: 0.01583, val loss: 0.01602\n",
      "Training epoch: 1039, train loss: 0.01684, val loss: 0.01702\n",
      "Training epoch: 1040, train loss: 0.01602, val loss: 0.01625\n",
      "Training epoch: 1041, train loss: 0.01616, val loss: 0.01626\n",
      "Training epoch: 1042, train loss: 0.01606, val loss: 0.01621\n",
      "Training epoch: 1043, train loss: 0.01628, val loss: 0.01649\n",
      "Training epoch: 1044, train loss: 0.01652, val loss: 0.01670\n",
      "Training epoch: 1045, train loss: 0.01640, val loss: 0.01655\n",
      "Training epoch: 1046, train loss: 0.01639, val loss: 0.01656\n",
      "Training epoch: 1047, train loss: 0.01567, val loss: 0.01588\n",
      "Training epoch: 1048, train loss: 0.01658, val loss: 0.01663\n",
      "Training epoch: 1049, train loss: 0.01574, val loss: 0.01593\n",
      "Training epoch: 1050, train loss: 0.01605, val loss: 0.01618\n",
      "Training epoch: 1051, train loss: 0.01573, val loss: 0.01583\n",
      "Training epoch: 1052, train loss: 0.01645, val loss: 0.01666\n",
      "Training epoch: 1053, train loss: 0.01594, val loss: 0.01608\n",
      "Training epoch: 1054, train loss: 0.01591, val loss: 0.01609\n",
      "Training epoch: 1055, train loss: 0.01570, val loss: 0.01580\n",
      "Training epoch: 1056, train loss: 0.01621, val loss: 0.01641\n",
      "Training epoch: 1057, train loss: 0.01595, val loss: 0.01611\n",
      "Training epoch: 1058, train loss: 0.01642, val loss: 0.01665\n",
      "Training epoch: 1059, train loss: 0.01664, val loss: 0.01681\n",
      "Training epoch: 1060, train loss: 0.01623, val loss: 0.01631\n",
      "Training epoch: 1061, train loss: 0.01582, val loss: 0.01597\n",
      "Training epoch: 1062, train loss: 0.01646, val loss: 0.01659\n",
      "Training epoch: 1063, train loss: 0.01608, val loss: 0.01633\n",
      "Training epoch: 1064, train loss: 0.01575, val loss: 0.01589\n",
      "Training epoch: 1065, train loss: 0.01731, val loss: 0.01737\n",
      "Training epoch: 1066, train loss: 0.01647, val loss: 0.01668\n",
      "Training epoch: 1067, train loss: 0.01572, val loss: 0.01592\n",
      "Training epoch: 1068, train loss: 0.01576, val loss: 0.01587\n",
      "Training epoch: 1069, train loss: 0.01626, val loss: 0.01649\n",
      "Training epoch: 1070, train loss: 0.01598, val loss: 0.01600\n",
      "Training epoch: 1071, train loss: 0.01580, val loss: 0.01600\n",
      "Training epoch: 1072, train loss: 0.01584, val loss: 0.01597\n",
      "Training epoch: 1073, train loss: 0.01616, val loss: 0.01636\n",
      "Training epoch: 1074, train loss: 0.01636, val loss: 0.01642\n",
      "Training epoch: 1075, train loss: 0.01590, val loss: 0.01600\n",
      "Training epoch: 1076, train loss: 0.01610, val loss: 0.01620\n",
      "Training epoch: 1077, train loss: 0.01585, val loss: 0.01608\n",
      "Training epoch: 1078, train loss: 0.01597, val loss: 0.01603\n",
      "Training epoch: 1079, train loss: 0.01601, val loss: 0.01618\n",
      "Training epoch: 1080, train loss: 0.01596, val loss: 0.01608\n",
      "Training epoch: 1081, train loss: 0.01619, val loss: 0.01620\n",
      "Training epoch: 1082, train loss: 0.01612, val loss: 0.01630\n",
      "Training epoch: 1083, train loss: 0.01573, val loss: 0.01590\n",
      "Training epoch: 1084, train loss: 0.01597, val loss: 0.01618\n",
      "Training epoch: 1085, train loss: 0.01599, val loss: 0.01619\n",
      "Training epoch: 1086, train loss: 0.01606, val loss: 0.01610\n",
      "Training epoch: 1087, train loss: 0.01612, val loss: 0.01625\n",
      "Training epoch: 1088, train loss: 0.01639, val loss: 0.01653\n",
      "Training epoch: 1089, train loss: 0.01608, val loss: 0.01637\n",
      "Training epoch: 1090, train loss: 0.01592, val loss: 0.01598\n",
      "Training epoch: 1091, train loss: 0.01577, val loss: 0.01595\n",
      "Training epoch: 1092, train loss: 0.01566, val loss: 0.01580\n",
      "Training epoch: 1093, train loss: 0.01578, val loss: 0.01597\n",
      "Training epoch: 1094, train loss: 0.01586, val loss: 0.01592\n",
      "Training epoch: 1095, train loss: 0.01569, val loss: 0.01588\n",
      "Training epoch: 1096, train loss: 0.01582, val loss: 0.01597\n",
      "Training epoch: 1097, train loss: 0.01586, val loss: 0.01600\n",
      "Training epoch: 1098, train loss: 0.01579, val loss: 0.01588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1099, train loss: 0.01621, val loss: 0.01642\n",
      "Training epoch: 1100, train loss: 0.01595, val loss: 0.01608\n",
      "Training epoch: 1101, train loss: 0.01612, val loss: 0.01639\n",
      "Training epoch: 1102, train loss: 0.01571, val loss: 0.01586\n",
      "Training epoch: 1103, train loss: 0.01631, val loss: 0.01647\n",
      "Training epoch: 1104, train loss: 0.01568, val loss: 0.01591\n",
      "Training epoch: 1105, train loss: 0.01596, val loss: 0.01611\n",
      "Training epoch: 1106, train loss: 0.01576, val loss: 0.01594\n",
      "Training epoch: 1107, train loss: 0.01591, val loss: 0.01608\n",
      "Training epoch: 1108, train loss: 0.01568, val loss: 0.01584\n",
      "Training epoch: 1109, train loss: 0.01595, val loss: 0.01601\n",
      "Training epoch: 1110, train loss: 0.01587, val loss: 0.01602\n",
      "Training epoch: 1111, train loss: 0.01571, val loss: 0.01583\n",
      "Training epoch: 1112, train loss: 0.01725, val loss: 0.01731\n",
      "Training epoch: 1113, train loss: 0.01571, val loss: 0.01594\n",
      "Training epoch: 1114, train loss: 0.01595, val loss: 0.01603\n",
      "Training epoch: 1115, train loss: 0.01635, val loss: 0.01658\n",
      "Training epoch: 1116, train loss: 0.01593, val loss: 0.01608\n",
      "Training epoch: 1117, train loss: 0.01570, val loss: 0.01596\n",
      "Training epoch: 1118, train loss: 0.01585, val loss: 0.01597\n",
      "Training epoch: 1119, train loss: 0.01593, val loss: 0.01613\n",
      "Training epoch: 1120, train loss: 0.01576, val loss: 0.01581\n",
      "Training epoch: 1121, train loss: 0.01628, val loss: 0.01640\n",
      "Training epoch: 1122, train loss: 0.01630, val loss: 0.01650\n",
      "Training epoch: 1123, train loss: 0.01585, val loss: 0.01604\n",
      "Training epoch: 1124, train loss: 0.01651, val loss: 0.01661\n",
      "Training epoch: 1125, train loss: 0.01612, val loss: 0.01633\n",
      "Training epoch: 1126, train loss: 0.01718, val loss: 0.01723\n",
      "Training epoch: 1127, train loss: 0.01580, val loss: 0.01585\n",
      "Training epoch: 1128, train loss: 0.01633, val loss: 0.01644\n",
      "Training epoch: 1129, train loss: 0.01588, val loss: 0.01600\n",
      "Training epoch: 1130, train loss: 0.01649, val loss: 0.01675\n",
      "Training epoch: 1131, train loss: 0.01580, val loss: 0.01586\n",
      "Training epoch: 1132, train loss: 0.01630, val loss: 0.01644\n",
      "Training epoch: 1133, train loss: 0.01581, val loss: 0.01606\n",
      "Training epoch: 1134, train loss: 0.01567, val loss: 0.01572\n",
      "Training epoch: 1135, train loss: 0.01593, val loss: 0.01608\n",
      "Training epoch: 1136, train loss: 0.01601, val loss: 0.01614\n",
      "Training epoch: 1137, train loss: 0.01600, val loss: 0.01617\n",
      "Training epoch: 1138, train loss: 0.01684, val loss: 0.01698\n",
      "Training epoch: 1139, train loss: 0.01653, val loss: 0.01660\n",
      "Training epoch: 1140, train loss: 0.01591, val loss: 0.01617\n",
      "Training epoch: 1141, train loss: 0.01641, val loss: 0.01655\n",
      "Training epoch: 1142, train loss: 0.01647, val loss: 0.01691\n",
      "Training epoch: 1143, train loss: 0.01632, val loss: 0.01646\n",
      "Training epoch: 1144, train loss: 0.01573, val loss: 0.01599\n",
      "Training epoch: 1145, train loss: 0.01574, val loss: 0.01584\n",
      "Training epoch: 1146, train loss: 0.01587, val loss: 0.01600\n",
      "Training epoch: 1147, train loss: 0.01581, val loss: 0.01599\n",
      "Training epoch: 1148, train loss: 0.01598, val loss: 0.01621\n",
      "Training epoch: 1149, train loss: 0.01594, val loss: 0.01614\n",
      "Training epoch: 1150, train loss: 0.01635, val loss: 0.01640\n",
      "Training epoch: 1151, train loss: 0.01653, val loss: 0.01665\n",
      "Training epoch: 1152, train loss: 0.01573, val loss: 0.01601\n",
      "Training epoch: 1153, train loss: 0.01591, val loss: 0.01607\n",
      "Training epoch: 1154, train loss: 0.01585, val loss: 0.01598\n",
      "Training epoch: 1155, train loss: 0.01602, val loss: 0.01608\n",
      "Training epoch: 1156, train loss: 0.01599, val loss: 0.01619\n",
      "Training epoch: 1157, train loss: 0.01559, val loss: 0.01576\n",
      "Training epoch: 1158, train loss: 0.01564, val loss: 0.01575\n",
      "Training epoch: 1159, train loss: 0.01576, val loss: 0.01594\n",
      "Training epoch: 1160, train loss: 0.01570, val loss: 0.01578\n",
      "Training epoch: 1161, train loss: 0.01629, val loss: 0.01643\n",
      "Training epoch: 1162, train loss: 0.01617, val loss: 0.01643\n",
      "Training epoch: 1163, train loss: 0.01607, val loss: 0.01609\n",
      "Training epoch: 1164, train loss: 0.01582, val loss: 0.01602\n",
      "Training epoch: 1165, train loss: 0.01670, val loss: 0.01682\n",
      "Training epoch: 1166, train loss: 0.01553, val loss: 0.01577\n",
      "Training epoch: 1167, train loss: 0.01703, val loss: 0.01716\n",
      "Training epoch: 1168, train loss: 0.01599, val loss: 0.01621\n",
      "Training epoch: 1169, train loss: 0.01604, val loss: 0.01626\n",
      "Training epoch: 1170, train loss: 0.01566, val loss: 0.01575\n",
      "Training epoch: 1171, train loss: 0.01582, val loss: 0.01597\n",
      "Training epoch: 1172, train loss: 0.01597, val loss: 0.01614\n",
      "Training epoch: 1173, train loss: 0.01603, val loss: 0.01614\n",
      "Training epoch: 1174, train loss: 0.01570, val loss: 0.01591\n",
      "Training epoch: 1175, train loss: 0.01593, val loss: 0.01595\n",
      "Training epoch: 1176, train loss: 0.01567, val loss: 0.01588\n",
      "Training epoch: 1177, train loss: 0.01618, val loss: 0.01619\n",
      "Training epoch: 1178, train loss: 0.01710, val loss: 0.01739\n",
      "Training epoch: 1179, train loss: 0.01589, val loss: 0.01602\n",
      "Training epoch: 1180, train loss: 0.01584, val loss: 0.01601\n",
      "Training epoch: 1181, train loss: 0.01634, val loss: 0.01643\n",
      "Training epoch: 1182, train loss: 0.01571, val loss: 0.01595\n",
      "Training epoch: 1183, train loss: 0.01604, val loss: 0.01618\n",
      "Training epoch: 1184, train loss: 0.01628, val loss: 0.01653\n",
      "Training epoch: 1185, train loss: 0.01630, val loss: 0.01645\n",
      "Training epoch: 1186, train loss: 0.01590, val loss: 0.01603\n",
      "Training epoch: 1187, train loss: 0.01588, val loss: 0.01605\n",
      "Training epoch: 1188, train loss: 0.01566, val loss: 0.01588\n",
      "Training epoch: 1189, train loss: 0.01605, val loss: 0.01612\n",
      "Training epoch: 1190, train loss: 0.01667, val loss: 0.01695\n",
      "Training epoch: 1191, train loss: 0.01600, val loss: 0.01627\n",
      "Training epoch: 1192, train loss: 0.01582, val loss: 0.01598\n",
      "Training epoch: 1193, train loss: 0.01597, val loss: 0.01613\n",
      "Training epoch: 1194, train loss: 0.01588, val loss: 0.01612\n",
      "Training epoch: 1195, train loss: 0.01598, val loss: 0.01620\n",
      "Training epoch: 1196, train loss: 0.01612, val loss: 0.01632\n",
      "Training epoch: 1197, train loss: 0.01609, val loss: 0.01611\n",
      "Training epoch: 1198, train loss: 0.01582, val loss: 0.01594\n",
      "Training epoch: 1199, train loss: 0.01625, val loss: 0.01647\n",
      "Training epoch: 1200, train loss: 0.01584, val loss: 0.01599\n",
      "Training epoch: 1201, train loss: 0.01615, val loss: 0.01633\n",
      "Training epoch: 1202, train loss: 0.01581, val loss: 0.01587\n",
      "Training epoch: 1203, train loss: 0.01603, val loss: 0.01610\n",
      "Training epoch: 1204, train loss: 0.01590, val loss: 0.01601\n",
      "Training epoch: 1205, train loss: 0.01573, val loss: 0.01590\n",
      "Training epoch: 1206, train loss: 0.01563, val loss: 0.01575\n",
      "Training epoch: 1207, train loss: 0.01709, val loss: 0.01739\n",
      "Training epoch: 1208, train loss: 0.01588, val loss: 0.01611\n",
      "Training epoch: 1209, train loss: 0.01615, val loss: 0.01622\n",
      "Training epoch: 1210, train loss: 0.01628, val loss: 0.01642\n",
      "Training epoch: 1211, train loss: 0.01591, val loss: 0.01615\n",
      "Training epoch: 1212, train loss: 0.01585, val loss: 0.01599\n",
      "Training epoch: 1213, train loss: 0.01599, val loss: 0.01609\n",
      "Training epoch: 1214, train loss: 0.01576, val loss: 0.01584\n",
      "Training epoch: 1215, train loss: 0.01601, val loss: 0.01626\n",
      "Training epoch: 1216, train loss: 0.01638, val loss: 0.01664\n",
      "Training epoch: 1217, train loss: 0.01596, val loss: 0.01609\n",
      "Training epoch: 1218, train loss: 0.01625, val loss: 0.01649\n",
      "Training epoch: 1219, train loss: 0.01599, val loss: 0.01608\n",
      "Training epoch: 1220, train loss: 0.01604, val loss: 0.01619\n",
      "Training epoch: 1221, train loss: 0.01608, val loss: 0.01638\n",
      "Training epoch: 1222, train loss: 0.01649, val loss: 0.01661\n",
      "Training epoch: 1223, train loss: 0.01656, val loss: 0.01662\n",
      "Training epoch: 1224, train loss: 0.01709, val loss: 0.01723\n",
      "Training epoch: 1225, train loss: 0.01580, val loss: 0.01600\n",
      "Training epoch: 1226, train loss: 0.01613, val loss: 0.01633\n",
      "Training epoch: 1227, train loss: 0.01610, val loss: 0.01625\n",
      "Training epoch: 1228, train loss: 0.01620, val loss: 0.01636\n",
      "Training epoch: 1229, train loss: 0.01629, val loss: 0.01636\n",
      "Training epoch: 1230, train loss: 0.01640, val loss: 0.01643\n",
      "Training epoch: 1231, train loss: 0.01571, val loss: 0.01596\n",
      "Training epoch: 1232, train loss: 0.01588, val loss: 0.01596\n",
      "Training epoch: 1233, train loss: 0.01615, val loss: 0.01642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1234, train loss: 0.01572, val loss: 0.01589\n",
      "Training epoch: 1235, train loss: 0.01567, val loss: 0.01586\n",
      "Training epoch: 1236, train loss: 0.01631, val loss: 0.01634\n",
      "Training epoch: 1237, train loss: 0.01627, val loss: 0.01660\n",
      "Training epoch: 1238, train loss: 0.01600, val loss: 0.01605\n",
      "Training epoch: 1239, train loss: 0.01574, val loss: 0.01586\n",
      "Training epoch: 1240, train loss: 0.01588, val loss: 0.01598\n",
      "Training epoch: 1241, train loss: 0.01565, val loss: 0.01592\n",
      "Training epoch: 1242, train loss: 0.01597, val loss: 0.01607\n",
      "Training epoch: 1243, train loss: 0.01588, val loss: 0.01609\n",
      "Training epoch: 1244, train loss: 0.01619, val loss: 0.01630\n",
      "Training epoch: 1245, train loss: 0.01645, val loss: 0.01662\n",
      "Training epoch: 1246, train loss: 0.01570, val loss: 0.01576\n",
      "Training epoch: 1247, train loss: 0.01572, val loss: 0.01580\n",
      "Training epoch: 1248, train loss: 0.01586, val loss: 0.01604\n",
      "Training epoch: 1249, train loss: 0.01560, val loss: 0.01569\n",
      "Training epoch: 1250, train loss: 0.01585, val loss: 0.01596\n",
      "Training epoch: 1251, train loss: 0.01558, val loss: 0.01567\n",
      "Training epoch: 1252, train loss: 0.01595, val loss: 0.01619\n",
      "Training epoch: 1253, train loss: 0.01609, val loss: 0.01620\n",
      "Training epoch: 1254, train loss: 0.01612, val loss: 0.01614\n",
      "Training epoch: 1255, train loss: 0.01626, val loss: 0.01654\n",
      "Training epoch: 1256, train loss: 0.01579, val loss: 0.01591\n",
      "Training epoch: 1257, train loss: 0.01626, val loss: 0.01653\n",
      "Training epoch: 1258, train loss: 0.01608, val loss: 0.01617\n",
      "Training epoch: 1259, train loss: 0.01591, val loss: 0.01599\n",
      "Training epoch: 1260, train loss: 0.01581, val loss: 0.01596\n",
      "Training epoch: 1261, train loss: 0.01588, val loss: 0.01593\n",
      "Training epoch: 1262, train loss: 0.01591, val loss: 0.01612\n",
      "Training epoch: 1263, train loss: 0.01562, val loss: 0.01578\n",
      "Training epoch: 1264, train loss: 0.01674, val loss: 0.01690\n",
      "Training epoch: 1265, train loss: 0.01624, val loss: 0.01642\n",
      "Training epoch: 1266, train loss: 0.01643, val loss: 0.01660\n",
      "Training epoch: 1267, train loss: 0.01685, val loss: 0.01710\n",
      "Training epoch: 1268, train loss: 0.01572, val loss: 0.01579\n",
      "Training epoch: 1269, train loss: 0.01646, val loss: 0.01677\n",
      "Training epoch: 1270, train loss: 0.01652, val loss: 0.01659\n",
      "Training epoch: 1271, train loss: 0.01652, val loss: 0.01686\n",
      "Training epoch: 1272, train loss: 0.01597, val loss: 0.01614\n",
      "Training epoch: 1273, train loss: 0.01563, val loss: 0.01578\n",
      "Training epoch: 1274, train loss: 0.01569, val loss: 0.01588\n",
      "Training epoch: 1275, train loss: 0.01572, val loss: 0.01581\n",
      "Training epoch: 1276, train loss: 0.01672, val loss: 0.01700\n",
      "Training epoch: 1277, train loss: 0.01627, val loss: 0.01636\n",
      "Training epoch: 1278, train loss: 0.01573, val loss: 0.01584\n",
      "Training epoch: 1279, train loss: 0.01576, val loss: 0.01587\n",
      "Training epoch: 1280, train loss: 0.01555, val loss: 0.01566\n",
      "Training epoch: 1281, train loss: 0.01586, val loss: 0.01606\n",
      "Training epoch: 1282, train loss: 0.01583, val loss: 0.01598\n",
      "Training epoch: 1283, train loss: 0.01573, val loss: 0.01586\n",
      "Training epoch: 1284, train loss: 0.01573, val loss: 0.01588\n",
      "Training epoch: 1285, train loss: 0.01588, val loss: 0.01601\n",
      "Training epoch: 1286, train loss: 0.01591, val loss: 0.01602\n",
      "Training epoch: 1287, train loss: 0.01602, val loss: 0.01606\n",
      "Training epoch: 1288, train loss: 0.01566, val loss: 0.01586\n",
      "Training epoch: 1289, train loss: 0.01585, val loss: 0.01595\n",
      "Training epoch: 1290, train loss: 0.01631, val loss: 0.01646\n",
      "Training epoch: 1291, train loss: 0.01604, val loss: 0.01621\n",
      "Training epoch: 1292, train loss: 0.01574, val loss: 0.01586\n",
      "Training epoch: 1293, train loss: 0.01595, val loss: 0.01616\n",
      "Training epoch: 1294, train loss: 0.01581, val loss: 0.01603\n",
      "Training epoch: 1295, train loss: 0.01579, val loss: 0.01592\n",
      "Training epoch: 1296, train loss: 0.01615, val loss: 0.01638\n",
      "Training epoch: 1297, train loss: 0.01583, val loss: 0.01601\n",
      "Training epoch: 1298, train loss: 0.01589, val loss: 0.01599\n",
      "Training epoch: 1299, train loss: 0.01571, val loss: 0.01588\n",
      "Training epoch: 1300, train loss: 0.01609, val loss: 0.01619\n",
      "Training epoch: 1301, train loss: 0.01598, val loss: 0.01625\n",
      "Training epoch: 1302, train loss: 0.01623, val loss: 0.01644\n",
      "Training epoch: 1303, train loss: 0.01589, val loss: 0.01615\n",
      "Training epoch: 1304, train loss: 0.01594, val loss: 0.01599\n",
      "Training epoch: 1305, train loss: 0.01564, val loss: 0.01586\n",
      "Training epoch: 1306, train loss: 0.01577, val loss: 0.01589\n",
      "Training epoch: 1307, train loss: 0.01586, val loss: 0.01612\n",
      "Training epoch: 1308, train loss: 0.01687, val loss: 0.01704\n",
      "Training epoch: 1309, train loss: 0.01591, val loss: 0.01611\n",
      "Training epoch: 1310, train loss: 0.01638, val loss: 0.01676\n",
      "Training epoch: 1311, train loss: 0.01584, val loss: 0.01587\n",
      "Training epoch: 1312, train loss: 0.01583, val loss: 0.01600\n",
      "Training epoch: 1313, train loss: 0.01585, val loss: 0.01599\n",
      "Training epoch: 1314, train loss: 0.01579, val loss: 0.01592\n",
      "Training epoch: 1315, train loss: 0.01702, val loss: 0.01714\n",
      "Training epoch: 1316, train loss: 0.01595, val loss: 0.01604\n",
      "Training epoch: 1317, train loss: 0.01579, val loss: 0.01600\n",
      "Training epoch: 1318, train loss: 0.01578, val loss: 0.01594\n",
      "Training epoch: 1319, train loss: 0.01581, val loss: 0.01595\n",
      "Training epoch: 1320, train loss: 0.01566, val loss: 0.01577\n",
      "Training epoch: 1321, train loss: 0.01582, val loss: 0.01603\n",
      "Training epoch: 1322, train loss: 0.01591, val loss: 0.01600\n",
      "Training epoch: 1323, train loss: 0.01572, val loss: 0.01584\n",
      "Training epoch: 1324, train loss: 0.01586, val loss: 0.01596\n",
      "Training epoch: 1325, train loss: 0.01611, val loss: 0.01618\n",
      "Training epoch: 1326, train loss: 0.01595, val loss: 0.01607\n",
      "Training epoch: 1327, train loss: 0.01581, val loss: 0.01602\n",
      "Training epoch: 1328, train loss: 0.01608, val loss: 0.01623\n",
      "Training epoch: 1329, train loss: 0.01596, val loss: 0.01607\n",
      "Training epoch: 1330, train loss: 0.01632, val loss: 0.01652\n",
      "Training epoch: 1331, train loss: 0.01628, val loss: 0.01642\n",
      "Training epoch: 1332, train loss: 0.01710, val loss: 0.01730\n",
      "Training epoch: 1333, train loss: 0.01577, val loss: 0.01583\n",
      "Training epoch: 1334, train loss: 0.01596, val loss: 0.01620\n",
      "Training epoch: 1335, train loss: 0.01605, val loss: 0.01610\n",
      "Training epoch: 1336, train loss: 0.01631, val loss: 0.01657\n",
      "Training epoch: 1337, train loss: 0.01650, val loss: 0.01660\n",
      "Training epoch: 1338, train loss: 0.01559, val loss: 0.01570\n",
      "Training epoch: 1339, train loss: 0.01574, val loss: 0.01589\n",
      "Training epoch: 1340, train loss: 0.01575, val loss: 0.01587\n",
      "Training epoch: 1341, train loss: 0.01561, val loss: 0.01581\n",
      "Training epoch: 1342, train loss: 0.01637, val loss: 0.01645\n",
      "Training epoch: 1343, train loss: 0.01629, val loss: 0.01658\n",
      "Training epoch: 1344, train loss: 0.01594, val loss: 0.01609\n",
      "Training epoch: 1345, train loss: 0.01603, val loss: 0.01611\n",
      "Training epoch: 1346, train loss: 0.01565, val loss: 0.01585\n",
      "Training epoch: 1347, train loss: 0.01587, val loss: 0.01599\n",
      "Training epoch: 1348, train loss: 0.01582, val loss: 0.01602\n",
      "Training epoch: 1349, train loss: 0.01581, val loss: 0.01599\n",
      "Training epoch: 1350, train loss: 0.01584, val loss: 0.01601\n",
      "Training epoch: 1351, train loss: 0.01587, val loss: 0.01608\n",
      "Training epoch: 1352, train loss: 0.01580, val loss: 0.01596\n",
      "Training epoch: 1353, train loss: 0.01569, val loss: 0.01593\n",
      "Training epoch: 1354, train loss: 0.01558, val loss: 0.01573\n",
      "Training epoch: 1355, train loss: 0.01582, val loss: 0.01591\n",
      "Training epoch: 1356, train loss: 0.01670, val loss: 0.01696\n",
      "Training epoch: 1357, train loss: 0.01638, val loss: 0.01652\n",
      "Training epoch: 1358, train loss: 0.01574, val loss: 0.01587\n",
      "Training epoch: 1359, train loss: 0.01569, val loss: 0.01581\n",
      "Training epoch: 1360, train loss: 0.01665, val loss: 0.01684\n",
      "Training epoch: 1361, train loss: 0.01631, val loss: 0.01639\n",
      "Training epoch: 1362, train loss: 0.01582, val loss: 0.01601\n",
      "Training epoch: 1363, train loss: 0.01569, val loss: 0.01588\n",
      "Training epoch: 1364, train loss: 0.01553, val loss: 0.01571\n",
      "Training epoch: 1365, train loss: 0.01578, val loss: 0.01592\n",
      "Training epoch: 1366, train loss: 0.01561, val loss: 0.01575\n",
      "Training epoch: 1367, train loss: 0.01560, val loss: 0.01568\n",
      "Training epoch: 1368, train loss: 0.01605, val loss: 0.01626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1369, train loss: 0.01563, val loss: 0.01573\n",
      "Training epoch: 1370, train loss: 0.01560, val loss: 0.01576\n",
      "Training epoch: 1371, train loss: 0.01566, val loss: 0.01583\n",
      "Training epoch: 1372, train loss: 0.01591, val loss: 0.01601\n",
      "Training epoch: 1373, train loss: 0.01595, val loss: 0.01613\n",
      "Training epoch: 1374, train loss: 0.01606, val loss: 0.01612\n",
      "Training epoch: 1375, train loss: 0.01551, val loss: 0.01568\n",
      "Training epoch: 1376, train loss: 0.01563, val loss: 0.01570\n",
      "Training epoch: 1377, train loss: 0.01571, val loss: 0.01585\n",
      "Training epoch: 1378, train loss: 0.01594, val loss: 0.01624\n",
      "Training epoch: 1379, train loss: 0.01615, val loss: 0.01622\n",
      "Training epoch: 1380, train loss: 0.01803, val loss: 0.01842\n",
      "Training epoch: 1381, train loss: 0.01595, val loss: 0.01611\n",
      "Training epoch: 1382, train loss: 0.01608, val loss: 0.01633\n",
      "Training epoch: 1383, train loss: 0.01574, val loss: 0.01591\n",
      "Training epoch: 1384, train loss: 0.01587, val loss: 0.01591\n",
      "Training epoch: 1385, train loss: 0.01580, val loss: 0.01585\n",
      "Training epoch: 1386, train loss: 0.01601, val loss: 0.01625\n",
      "Training epoch: 1387, train loss: 0.01574, val loss: 0.01598\n",
      "Training epoch: 1388, train loss: 0.01584, val loss: 0.01592\n",
      "Training epoch: 1389, train loss: 0.01617, val loss: 0.01641\n",
      "Training epoch: 1390, train loss: 0.01708, val loss: 0.01733\n",
      "Training epoch: 1391, train loss: 0.01578, val loss: 0.01592\n",
      "Training epoch: 1392, train loss: 0.01553, val loss: 0.01576\n",
      "Training epoch: 1393, train loss: 0.01588, val loss: 0.01598\n",
      "Training epoch: 1394, train loss: 0.01607, val loss: 0.01626\n",
      "Training epoch: 1395, train loss: 0.01650, val loss: 0.01683\n",
      "Training epoch: 1396, train loss: 0.01575, val loss: 0.01580\n",
      "Training epoch: 1397, train loss: 0.01586, val loss: 0.01604\n",
      "Training epoch: 1398, train loss: 0.01606, val loss: 0.01623\n",
      "Training epoch: 1399, train loss: 0.01682, val loss: 0.01713\n",
      "Training epoch: 1400, train loss: 0.01573, val loss: 0.01593\n",
      "Training epoch: 1401, train loss: 0.01586, val loss: 0.01590\n",
      "Training epoch: 1402, train loss: 0.01590, val loss: 0.01613\n",
      "Training epoch: 1403, train loss: 0.01643, val loss: 0.01665\n",
      "Training epoch: 1404, train loss: 0.01615, val loss: 0.01629\n",
      "Training epoch: 1405, train loss: 0.01611, val loss: 0.01626\n",
      "Training epoch: 1406, train loss: 0.01576, val loss: 0.01587\n",
      "Training epoch: 1407, train loss: 0.01640, val loss: 0.01653\n",
      "Training epoch: 1408, train loss: 0.01569, val loss: 0.01577\n",
      "Training epoch: 1409, train loss: 0.01586, val loss: 0.01604\n",
      "Training epoch: 1410, train loss: 0.01587, val loss: 0.01604\n",
      "Training epoch: 1411, train loss: 0.01602, val loss: 0.01614\n",
      "Training epoch: 1412, train loss: 0.01641, val loss: 0.01661\n",
      "Training epoch: 1413, train loss: 0.01701, val loss: 0.01728\n",
      "Training epoch: 1414, train loss: 0.01580, val loss: 0.01608\n",
      "Training epoch: 1415, train loss: 0.01580, val loss: 0.01595\n",
      "Training epoch: 1416, train loss: 0.01626, val loss: 0.01646\n",
      "Training epoch: 1417, train loss: 0.01565, val loss: 0.01582\n",
      "Training epoch: 1418, train loss: 0.01555, val loss: 0.01566\n",
      "Training epoch: 1419, train loss: 0.01636, val loss: 0.01658\n",
      "Training epoch: 1420, train loss: 0.01562, val loss: 0.01569\n",
      "Training epoch: 1421, train loss: 0.01594, val loss: 0.01601\n",
      "Training epoch: 1422, train loss: 0.01608, val loss: 0.01613\n",
      "Training epoch: 1423, train loss: 0.01560, val loss: 0.01574\n",
      "Training epoch: 1424, train loss: 0.01581, val loss: 0.01584\n",
      "Training epoch: 1425, train loss: 0.01579, val loss: 0.01599\n",
      "Training epoch: 1426, train loss: 0.01561, val loss: 0.01576\n",
      "Training epoch: 1427, train loss: 0.01599, val loss: 0.01614\n",
      "Training epoch: 1428, train loss: 0.01633, val loss: 0.01634\n",
      "Training epoch: 1429, train loss: 0.01593, val loss: 0.01603\n",
      "Training epoch: 1430, train loss: 0.01577, val loss: 0.01597\n",
      "Training epoch: 1431, train loss: 0.01596, val loss: 0.01615\n",
      "Training epoch: 1432, train loss: 0.01590, val loss: 0.01614\n",
      "Training epoch: 1433, train loss: 0.01647, val loss: 0.01646\n",
      "Training epoch: 1434, train loss: 0.01572, val loss: 0.01584\n",
      "Training epoch: 1435, train loss: 0.01585, val loss: 0.01604\n",
      "Training epoch: 1436, train loss: 0.01622, val loss: 0.01625\n",
      "Training epoch: 1437, train loss: 0.01579, val loss: 0.01590\n",
      "Training epoch: 1438, train loss: 0.01557, val loss: 0.01580\n",
      "Training epoch: 1439, train loss: 0.01589, val loss: 0.01601\n",
      "Training epoch: 1440, train loss: 0.01564, val loss: 0.01575\n",
      "Training epoch: 1441, train loss: 0.01571, val loss: 0.01592\n",
      "Training epoch: 1442, train loss: 0.01598, val loss: 0.01605\n",
      "Training epoch: 1443, train loss: 0.01587, val loss: 0.01601\n",
      "Training epoch: 1444, train loss: 0.01616, val loss: 0.01632\n",
      "Training epoch: 1445, train loss: 0.01603, val loss: 0.01609\n",
      "Training epoch: 1446, train loss: 0.01625, val loss: 0.01636\n",
      "Training epoch: 1447, train loss: 0.01555, val loss: 0.01571\n",
      "Training epoch: 1448, train loss: 0.01628, val loss: 0.01656\n",
      "Training epoch: 1449, train loss: 0.01631, val loss: 0.01630\n",
      "Training epoch: 1450, train loss: 0.01578, val loss: 0.01592\n",
      "Training epoch: 1451, train loss: 0.01597, val loss: 0.01621\n",
      "Training epoch: 1452, train loss: 0.01567, val loss: 0.01577\n",
      "Training epoch: 1453, train loss: 0.01567, val loss: 0.01581\n",
      "Training epoch: 1454, train loss: 0.01597, val loss: 0.01620\n",
      "Training epoch: 1455, train loss: 0.01553, val loss: 0.01568\n",
      "Training epoch: 1456, train loss: 0.01587, val loss: 0.01596\n",
      "Training epoch: 1457, train loss: 0.01619, val loss: 0.01622\n",
      "Training epoch: 1458, train loss: 0.01606, val loss: 0.01623\n",
      "Training epoch: 1459, train loss: 0.01632, val loss: 0.01637\n",
      "Training epoch: 1460, train loss: 0.01596, val loss: 0.01615\n",
      "Training epoch: 1461, train loss: 0.01662, val loss: 0.01687\n",
      "Training epoch: 1462, train loss: 0.01604, val loss: 0.01616\n",
      "Training epoch: 1463, train loss: 0.01596, val loss: 0.01610\n",
      "Training epoch: 1464, train loss: 0.01600, val loss: 0.01603\n",
      "Training epoch: 1465, train loss: 0.01568, val loss: 0.01589\n",
      "Training epoch: 1466, train loss: 0.01603, val loss: 0.01636\n",
      "Training epoch: 1467, train loss: 0.01598, val loss: 0.01614\n",
      "Training epoch: 1468, train loss: 0.01595, val loss: 0.01603\n",
      "Training epoch: 1469, train loss: 0.01628, val loss: 0.01632\n",
      "Training epoch: 1470, train loss: 0.01562, val loss: 0.01582\n",
      "Training epoch: 1471, train loss: 0.01573, val loss: 0.01591\n",
      "Training epoch: 1472, train loss: 0.01623, val loss: 0.01638\n",
      "Training epoch: 1473, train loss: 0.01559, val loss: 0.01574\n",
      "Training epoch: 1474, train loss: 0.01555, val loss: 0.01571\n",
      "Training epoch: 1475, train loss: 0.01570, val loss: 0.01596\n",
      "Training epoch: 1476, train loss: 0.01573, val loss: 0.01590\n",
      "Training epoch: 1477, train loss: 0.01580, val loss: 0.01602\n",
      "Training epoch: 1478, train loss: 0.01586, val loss: 0.01590\n",
      "Training epoch: 1479, train loss: 0.01578, val loss: 0.01593\n",
      "Training epoch: 1480, train loss: 0.01566, val loss: 0.01581\n",
      "Training epoch: 1481, train loss: 0.01615, val loss: 0.01625\n",
      "Training epoch: 1482, train loss: 0.01556, val loss: 0.01570\n",
      "Training epoch: 1483, train loss: 0.01589, val loss: 0.01609\n",
      "Training epoch: 1484, train loss: 0.01586, val loss: 0.01595\n",
      "Training epoch: 1485, train loss: 0.01576, val loss: 0.01590\n",
      "Training epoch: 1486, train loss: 0.01577, val loss: 0.01579\n",
      "Training epoch: 1487, train loss: 0.01584, val loss: 0.01605\n",
      "Training epoch: 1488, train loss: 0.01590, val loss: 0.01613\n",
      "Training epoch: 1489, train loss: 0.01668, val loss: 0.01689\n",
      "Training epoch: 1490, train loss: 0.01560, val loss: 0.01574\n",
      "Training epoch: 1491, train loss: 0.01582, val loss: 0.01597\n",
      "Training epoch: 1492, train loss: 0.01561, val loss: 0.01574\n",
      "Training epoch: 1493, train loss: 0.01591, val loss: 0.01605\n",
      "Training epoch: 1494, train loss: 0.01566, val loss: 0.01573\n",
      "Training epoch: 1495, train loss: 0.01606, val loss: 0.01619\n",
      "Training epoch: 1496, train loss: 0.01575, val loss: 0.01594\n",
      "Training epoch: 1497, train loss: 0.01645, val loss: 0.01662\n",
      "Training epoch: 1498, train loss: 0.01619, val loss: 0.01621\n",
      "Training epoch: 1499, train loss: 0.01589, val loss: 0.01597\n",
      "Training epoch: 1500, train loss: 0.01616, val loss: 0.01626\n",
      "Training epoch: 1501, train loss: 0.01595, val loss: 0.01612\n",
      "Training epoch: 1502, train loss: 0.01570, val loss: 0.01575\n",
      "Training epoch: 1503, train loss: 0.01586, val loss: 0.01602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1504, train loss: 0.01646, val loss: 0.01669\n",
      "Training epoch: 1505, train loss: 0.01625, val loss: 0.01651\n",
      "Training epoch: 1506, train loss: 0.01590, val loss: 0.01602\n",
      "Training epoch: 1507, train loss: 0.01587, val loss: 0.01601\n",
      "Training epoch: 1508, train loss: 0.01561, val loss: 0.01575\n",
      "Training epoch: 1509, train loss: 0.01587, val loss: 0.01596\n",
      "Training epoch: 1510, train loss: 0.01627, val loss: 0.01649\n",
      "Training epoch: 1511, train loss: 0.01662, val loss: 0.01675\n",
      "Training epoch: 1512, train loss: 0.01615, val loss: 0.01626\n",
      "Training epoch: 1513, train loss: 0.01569, val loss: 0.01587\n",
      "Training epoch: 1514, train loss: 0.01610, val loss: 0.01618\n",
      "Training epoch: 1515, train loss: 0.01549, val loss: 0.01574\n",
      "Training epoch: 1516, train loss: 0.01579, val loss: 0.01606\n",
      "Training epoch: 1517, train loss: 0.01641, val loss: 0.01662\n",
      "Training epoch: 1518, train loss: 0.01637, val loss: 0.01655\n",
      "Training epoch: 1519, train loss: 0.01592, val loss: 0.01599\n",
      "Training epoch: 1520, train loss: 0.01549, val loss: 0.01570\n",
      "Training epoch: 1521, train loss: 0.01612, val loss: 0.01616\n",
      "Training epoch: 1522, train loss: 0.01605, val loss: 0.01609\n",
      "Training epoch: 1523, train loss: 0.01551, val loss: 0.01572\n",
      "Training epoch: 1524, train loss: 0.01566, val loss: 0.01577\n",
      "Training epoch: 1525, train loss: 0.01658, val loss: 0.01679\n",
      "Training epoch: 1526, train loss: 0.01572, val loss: 0.01580\n",
      "Training epoch: 1527, train loss: 0.01629, val loss: 0.01654\n",
      "Training epoch: 1528, train loss: 0.01576, val loss: 0.01593\n",
      "Training epoch: 1529, train loss: 0.01559, val loss: 0.01571\n",
      "Training epoch: 1530, train loss: 0.01557, val loss: 0.01577\n",
      "Training epoch: 1531, train loss: 0.01567, val loss: 0.01581\n",
      "Training epoch: 1532, train loss: 0.01621, val loss: 0.01635\n",
      "Training epoch: 1533, train loss: 0.01575, val loss: 0.01577\n",
      "Training epoch: 1534, train loss: 0.01577, val loss: 0.01596\n",
      "Training epoch: 1535, train loss: 0.01564, val loss: 0.01580\n",
      "Training epoch: 1536, train loss: 0.01576, val loss: 0.01588\n",
      "Training epoch: 1537, train loss: 0.01662, val loss: 0.01669\n",
      "Training epoch: 1538, train loss: 0.01581, val loss: 0.01590\n",
      "Training epoch: 1539, train loss: 0.01558, val loss: 0.01569\n",
      "Training epoch: 1540, train loss: 0.01560, val loss: 0.01574\n",
      "Training epoch: 1541, train loss: 0.01580, val loss: 0.01603\n",
      "Training epoch: 1542, train loss: 0.01591, val loss: 0.01604\n",
      "Training epoch: 1543, train loss: 0.01563, val loss: 0.01566\n",
      "Training epoch: 1544, train loss: 0.01590, val loss: 0.01608\n",
      "Training epoch: 1545, train loss: 0.01588, val loss: 0.01591\n",
      "Training epoch: 1546, train loss: 0.01570, val loss: 0.01594\n",
      "Training epoch: 1547, train loss: 0.01607, val loss: 0.01610\n",
      "Training epoch: 1548, train loss: 0.01561, val loss: 0.01571\n",
      "Training epoch: 1549, train loss: 0.01560, val loss: 0.01579\n",
      "Training epoch: 1550, train loss: 0.01571, val loss: 0.01587\n",
      "Training epoch: 1551, train loss: 0.01571, val loss: 0.01577\n",
      "Training epoch: 1552, train loss: 0.01563, val loss: 0.01584\n",
      "Training epoch: 1553, train loss: 0.01596, val loss: 0.01609\n",
      "Training epoch: 1554, train loss: 0.01698, val loss: 0.01720\n",
      "Training epoch: 1555, train loss: 0.01565, val loss: 0.01571\n",
      "Training epoch: 1556, train loss: 0.01570, val loss: 0.01584\n",
      "Training epoch: 1557, train loss: 0.01571, val loss: 0.01595\n",
      "Training epoch: 1558, train loss: 0.01575, val loss: 0.01583\n",
      "Training epoch: 1559, train loss: 0.01627, val loss: 0.01650\n",
      "Training epoch: 1560, train loss: 0.01584, val loss: 0.01586\n",
      "Training epoch: 1561, train loss: 0.01676, val loss: 0.01685\n",
      "Training epoch: 1562, train loss: 0.01613, val loss: 0.01625\n",
      "Training epoch: 1563, train loss: 0.01611, val loss: 0.01629\n",
      "Training epoch: 1564, train loss: 0.01586, val loss: 0.01605\n",
      "Training epoch: 1565, train loss: 0.01659, val loss: 0.01679\n",
      "Training epoch: 1566, train loss: 0.01558, val loss: 0.01576\n",
      "Training epoch: 1567, train loss: 0.01634, val loss: 0.01639\n",
      "Training epoch: 1568, train loss: 0.01550, val loss: 0.01572\n",
      "Training epoch: 1569, train loss: 0.01563, val loss: 0.01576\n",
      "Training epoch: 1570, train loss: 0.01570, val loss: 0.01584\n",
      "Training epoch: 1571, train loss: 0.01665, val loss: 0.01685\n",
      "Training epoch: 1572, train loss: 0.01601, val loss: 0.01612\n",
      "Training epoch: 1573, train loss: 0.01558, val loss: 0.01571\n",
      "Training epoch: 1574, train loss: 0.01586, val loss: 0.01607\n",
      "Training epoch: 1575, train loss: 0.01590, val loss: 0.01611\n",
      "Training epoch: 1576, train loss: 0.01611, val loss: 0.01630\n",
      "Training epoch: 1577, train loss: 0.01571, val loss: 0.01590\n",
      "Training epoch: 1578, train loss: 0.01565, val loss: 0.01565\n",
      "Training epoch: 1579, train loss: 0.01574, val loss: 0.01585\n",
      "Training epoch: 1580, train loss: 0.01578, val loss: 0.01592\n",
      "Training epoch: 1581, train loss: 0.01606, val loss: 0.01610\n",
      "Training epoch: 1582, train loss: 0.01589, val loss: 0.01598\n",
      "Training epoch: 1583, train loss: 0.01598, val loss: 0.01629\n",
      "Training epoch: 1584, train loss: 0.01587, val loss: 0.01595\n",
      "Training epoch: 1585, train loss: 0.01595, val loss: 0.01612\n",
      "Training epoch: 1586, train loss: 0.01559, val loss: 0.01573\n",
      "Training epoch: 1587, train loss: 0.01577, val loss: 0.01589\n",
      "Training epoch: 1588, train loss: 0.01592, val loss: 0.01599\n",
      "Training epoch: 1589, train loss: 0.01575, val loss: 0.01597\n",
      "Training epoch: 1590, train loss: 0.01570, val loss: 0.01571\n",
      "Training epoch: 1591, train loss: 0.01596, val loss: 0.01623\n",
      "Training epoch: 1592, train loss: 0.01580, val loss: 0.01584\n",
      "Training epoch: 1593, train loss: 0.01581, val loss: 0.01589\n",
      "Training epoch: 1594, train loss: 0.01575, val loss: 0.01581\n",
      "Training epoch: 1595, train loss: 0.01755, val loss: 0.01760\n",
      "Training epoch: 1596, train loss: 0.01570, val loss: 0.01582\n",
      "Training epoch: 1597, train loss: 0.01607, val loss: 0.01622\n",
      "Training epoch: 1598, train loss: 0.01596, val loss: 0.01611\n",
      "Training epoch: 1599, train loss: 0.01561, val loss: 0.01573\n",
      "Training epoch: 1600, train loss: 0.01579, val loss: 0.01589\n",
      "Training epoch: 1601, train loss: 0.01651, val loss: 0.01661\n",
      "Training epoch: 1602, train loss: 0.01552, val loss: 0.01568\n",
      "Training epoch: 1603, train loss: 0.01653, val loss: 0.01665\n",
      "Training epoch: 1604, train loss: 0.01607, val loss: 0.01626\n",
      "Training epoch: 1605, train loss: 0.01606, val loss: 0.01630\n",
      "Training epoch: 1606, train loss: 0.01610, val loss: 0.01634\n",
      "Training epoch: 1607, train loss: 0.01626, val loss: 0.01639\n",
      "Training epoch: 1608, train loss: 0.01576, val loss: 0.01582\n",
      "Training epoch: 1609, train loss: 0.01594, val loss: 0.01609\n",
      "Training epoch: 1610, train loss: 0.01563, val loss: 0.01577\n",
      "Training epoch: 1611, train loss: 0.01592, val loss: 0.01618\n",
      "Training epoch: 1612, train loss: 0.01600, val loss: 0.01618\n",
      "Training epoch: 1613, train loss: 0.01564, val loss: 0.01570\n",
      "Training epoch: 1614, train loss: 0.01556, val loss: 0.01570\n",
      "Training epoch: 1615, train loss: 0.01608, val loss: 0.01621\n",
      "Training epoch: 1616, train loss: 0.01595, val loss: 0.01613\n",
      "Training epoch: 1617, train loss: 0.01563, val loss: 0.01576\n",
      "Training epoch: 1618, train loss: 0.01590, val loss: 0.01595\n",
      "Training epoch: 1619, train loss: 0.01567, val loss: 0.01579\n",
      "Training epoch: 1620, train loss: 0.01600, val loss: 0.01608\n",
      "Training epoch: 1621, train loss: 0.01554, val loss: 0.01570\n",
      "Training epoch: 1622, train loss: 0.01595, val loss: 0.01615\n",
      "Training epoch: 1623, train loss: 0.01659, val loss: 0.01672\n",
      "Training epoch: 1624, train loss: 0.01572, val loss: 0.01591\n",
      "Training epoch: 1625, train loss: 0.01602, val loss: 0.01609\n",
      "Training epoch: 1626, train loss: 0.01621, val loss: 0.01641\n",
      "Training epoch: 1627, train loss: 0.01561, val loss: 0.01576\n",
      "Training epoch: 1628, train loss: 0.01562, val loss: 0.01576\n",
      "Training epoch: 1629, train loss: 0.01561, val loss: 0.01591\n",
      "Training epoch: 1630, train loss: 0.01576, val loss: 0.01584\n",
      "Training epoch: 1631, train loss: 0.01587, val loss: 0.01603\n",
      "Training epoch: 1632, train loss: 0.01580, val loss: 0.01588\n",
      "Training epoch: 1633, train loss: 0.01603, val loss: 0.01622\n",
      "Training epoch: 1634, train loss: 0.01558, val loss: 0.01567\n",
      "Training epoch: 1635, train loss: 0.01582, val loss: 0.01592\n",
      "Training epoch: 1636, train loss: 0.01605, val loss: 0.01623\n",
      "Training epoch: 1637, train loss: 0.01574, val loss: 0.01589\n",
      "Training epoch: 1638, train loss: 0.01653, val loss: 0.01672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1639, train loss: 0.01547, val loss: 0.01565\n",
      "Training epoch: 1640, train loss: 0.01574, val loss: 0.01592\n",
      "Training epoch: 1641, train loss: 0.01578, val loss: 0.01590\n",
      "Training epoch: 1642, train loss: 0.01573, val loss: 0.01588\n",
      "Training epoch: 1643, train loss: 0.01559, val loss: 0.01571\n",
      "Training epoch: 1644, train loss: 0.01593, val loss: 0.01612\n",
      "Training epoch: 1645, train loss: 0.01555, val loss: 0.01568\n",
      "Training epoch: 1646, train loss: 0.01581, val loss: 0.01589\n",
      "Training epoch: 1647, train loss: 0.01579, val loss: 0.01599\n",
      "Training epoch: 1648, train loss: 0.01731, val loss: 0.01744\n",
      "Training epoch: 1649, train loss: 0.01815, val loss: 0.01817\n",
      "Training epoch: 1650, train loss: 0.01576, val loss: 0.01580\n",
      "Training epoch: 1651, train loss: 0.01615, val loss: 0.01625\n",
      "Training epoch: 1652, train loss: 0.01570, val loss: 0.01576\n",
      "Training epoch: 1653, train loss: 0.01559, val loss: 0.01572\n",
      "Training epoch: 1654, train loss: 0.01610, val loss: 0.01622\n",
      "Training epoch: 1655, train loss: 0.01582, val loss: 0.01591\n",
      "Training epoch: 1656, train loss: 0.01555, val loss: 0.01569\n",
      "Training epoch: 1657, train loss: 0.01559, val loss: 0.01571\n",
      "Training epoch: 1658, train loss: 0.01570, val loss: 0.01588\n",
      "Training epoch: 1659, train loss: 0.01622, val loss: 0.01621\n",
      "Training epoch: 1660, train loss: 0.01555, val loss: 0.01577\n",
      "Training epoch: 1661, train loss: 0.01574, val loss: 0.01584\n",
      "Training epoch: 1662, train loss: 0.01583, val loss: 0.01599\n",
      "Training epoch: 1663, train loss: 0.01605, val loss: 0.01623\n",
      "Training epoch: 1664, train loss: 0.01571, val loss: 0.01585\n",
      "Training epoch: 1665, train loss: 0.01584, val loss: 0.01600\n",
      "Training epoch: 1666, train loss: 0.01555, val loss: 0.01571\n",
      "Training epoch: 1667, train loss: 0.01568, val loss: 0.01575\n",
      "Training epoch: 1668, train loss: 0.01567, val loss: 0.01590\n",
      "Training epoch: 1669, train loss: 0.01639, val loss: 0.01647\n",
      "Training epoch: 1670, train loss: 0.01668, val loss: 0.01696\n",
      "Training epoch: 1671, train loss: 0.01595, val loss: 0.01604\n",
      "Training epoch: 1672, train loss: 0.01622, val loss: 0.01641\n",
      "Training epoch: 1673, train loss: 0.01595, val loss: 0.01605\n",
      "Training epoch: 1674, train loss: 0.01580, val loss: 0.01596\n",
      "Training epoch: 1675, train loss: 0.01583, val loss: 0.01586\n",
      "Training epoch: 1676, train loss: 0.01668, val loss: 0.01694\n",
      "Training epoch: 1677, train loss: 0.01566, val loss: 0.01577\n",
      "Training epoch: 1678, train loss: 0.01585, val loss: 0.01611\n",
      "Training epoch: 1679, train loss: 0.01591, val loss: 0.01604\n",
      "Training epoch: 1680, train loss: 0.01570, val loss: 0.01585\n",
      "Training epoch: 1681, train loss: 0.01599, val loss: 0.01613\n",
      "Training epoch: 1682, train loss: 0.01583, val loss: 0.01597\n",
      "Training epoch: 1683, train loss: 0.01563, val loss: 0.01579\n",
      "Training epoch: 1684, train loss: 0.01556, val loss: 0.01568\n",
      "Training epoch: 1685, train loss: 0.01595, val loss: 0.01605\n",
      "Training epoch: 1686, train loss: 0.01571, val loss: 0.01589\n",
      "Training epoch: 1687, train loss: 0.01579, val loss: 0.01595\n",
      "Training epoch: 1688, train loss: 0.01556, val loss: 0.01572\n",
      "Training epoch: 1689, train loss: 0.01570, val loss: 0.01584\n",
      "Training epoch: 1690, train loss: 0.01575, val loss: 0.01583\n",
      "Training epoch: 1691, train loss: 0.01591, val loss: 0.01595\n",
      "Training epoch: 1692, train loss: 0.01614, val loss: 0.01633\n",
      "Training epoch: 1693, train loss: 0.01654, val loss: 0.01679\n",
      "Training epoch: 1694, train loss: 0.01606, val loss: 0.01621\n",
      "Training epoch: 1695, train loss: 0.01577, val loss: 0.01592\n",
      "Training epoch: 1696, train loss: 0.01589, val loss: 0.01601\n",
      "Training epoch: 1697, train loss: 0.01576, val loss: 0.01592\n",
      "Training epoch: 1698, train loss: 0.01558, val loss: 0.01576\n",
      "Training epoch: 1699, train loss: 0.01596, val loss: 0.01615\n",
      "Training epoch: 1700, train loss: 0.01614, val loss: 0.01625\n",
      "Training epoch: 1701, train loss: 0.01608, val loss: 0.01626\n",
      "Training epoch: 1702, train loss: 0.01597, val loss: 0.01613\n",
      "Training epoch: 1703, train loss: 0.01594, val loss: 0.01597\n",
      "Training epoch: 1704, train loss: 0.01578, val loss: 0.01603\n",
      "Training epoch: 1705, train loss: 0.01585, val loss: 0.01599\n",
      "Training epoch: 1706, train loss: 0.01575, val loss: 0.01600\n",
      "Training epoch: 1707, train loss: 0.01558, val loss: 0.01574\n",
      "Training epoch: 1708, train loss: 0.01668, val loss: 0.01669\n",
      "Training epoch: 1709, train loss: 0.01654, val loss: 0.01659\n",
      "Training epoch: 1710, train loss: 0.01557, val loss: 0.01573\n",
      "Training epoch: 1711, train loss: 0.01569, val loss: 0.01587\n",
      "Training epoch: 1712, train loss: 0.01584, val loss: 0.01596\n",
      "Training epoch: 1713, train loss: 0.01579, val loss: 0.01592\n",
      "Training epoch: 1714, train loss: 0.01651, val loss: 0.01670\n",
      "Training epoch: 1715, train loss: 0.01554, val loss: 0.01572\n",
      "Training epoch: 1716, train loss: 0.01564, val loss: 0.01587\n",
      "Training epoch: 1717, train loss: 0.01606, val loss: 0.01602\n",
      "Training epoch: 1718, train loss: 0.01610, val loss: 0.01626\n",
      "Training epoch: 1719, train loss: 0.01571, val loss: 0.01588\n",
      "Training epoch: 1720, train loss: 0.01630, val loss: 0.01661\n",
      "Training epoch: 1721, train loss: 0.01561, val loss: 0.01581\n",
      "Training epoch: 1722, train loss: 0.01561, val loss: 0.01575\n",
      "Training epoch: 1723, train loss: 0.01579, val loss: 0.01598\n",
      "Training epoch: 1724, train loss: 0.01565, val loss: 0.01581\n",
      "Training epoch: 1725, train loss: 0.01598, val loss: 0.01620\n",
      "Training epoch: 1726, train loss: 0.01594, val loss: 0.01610\n",
      "Training epoch: 1727, train loss: 0.01614, val loss: 0.01644\n",
      "Training epoch: 1728, train loss: 0.01629, val loss: 0.01647\n",
      "Training epoch: 1729, train loss: 0.01632, val loss: 0.01651\n",
      "Training epoch: 1730, train loss: 0.01602, val loss: 0.01618\n",
      "Training epoch: 1731, train loss: 0.01582, val loss: 0.01596\n",
      "Training epoch: 1732, train loss: 0.01594, val loss: 0.01603\n",
      "Training epoch: 1733, train loss: 0.01584, val loss: 0.01596\n",
      "Training epoch: 1734, train loss: 0.01561, val loss: 0.01574\n",
      "Training epoch: 1735, train loss: 0.01572, val loss: 0.01580\n",
      "Training epoch: 1736, train loss: 0.01631, val loss: 0.01669\n",
      "Training epoch: 1737, train loss: 0.01573, val loss: 0.01579\n",
      "Training epoch: 1738, train loss: 0.01688, val loss: 0.01698\n",
      "Training epoch: 1739, train loss: 0.01610, val loss: 0.01622\n",
      "Training epoch: 1740, train loss: 0.01579, val loss: 0.01588\n",
      "Training epoch: 1741, train loss: 0.01575, val loss: 0.01588\n",
      "Training epoch: 1742, train loss: 0.01581, val loss: 0.01589\n",
      "Training epoch: 1743, train loss: 0.01581, val loss: 0.01589\n",
      "Training epoch: 1744, train loss: 0.01580, val loss: 0.01598\n",
      "Training epoch: 1745, train loss: 0.01703, val loss: 0.01728\n",
      "Training epoch: 1746, train loss: 0.01584, val loss: 0.01597\n",
      "Training epoch: 1747, train loss: 0.01593, val loss: 0.01604\n",
      "Training epoch: 1748, train loss: 0.01557, val loss: 0.01577\n",
      "Training epoch: 1749, train loss: 0.01588, val loss: 0.01604\n",
      "Training epoch: 1750, train loss: 0.01628, val loss: 0.01649\n",
      "Training epoch: 1751, train loss: 0.01630, val loss: 0.01645\n",
      "Training epoch: 1752, train loss: 0.01687, val loss: 0.01719\n",
      "Training epoch: 1753, train loss: 0.01590, val loss: 0.01601\n",
      "Training epoch: 1754, train loss: 0.01586, val loss: 0.01607\n",
      "Training epoch: 1755, train loss: 0.01617, val loss: 0.01633\n",
      "Training epoch: 1756, train loss: 0.01564, val loss: 0.01581\n",
      "Training epoch: 1757, train loss: 0.01574, val loss: 0.01583\n",
      "Training epoch: 1758, train loss: 0.01589, val loss: 0.01600\n",
      "Training epoch: 1759, train loss: 0.01568, val loss: 0.01590\n",
      "Training epoch: 1760, train loss: 0.01570, val loss: 0.01576\n",
      "Training epoch: 1761, train loss: 0.01569, val loss: 0.01586\n",
      "Training epoch: 1762, train loss: 0.01587, val loss: 0.01590\n",
      "Training epoch: 1763, train loss: 0.01565, val loss: 0.01577\n",
      "Training epoch: 1764, train loss: 0.01570, val loss: 0.01579\n",
      "Training epoch: 1765, train loss: 0.01565, val loss: 0.01579\n",
      "Training epoch: 1766, train loss: 0.01584, val loss: 0.01601\n",
      "Training epoch: 1767, train loss: 0.01629, val loss: 0.01636\n",
      "Training epoch: 1768, train loss: 0.01552, val loss: 0.01564\n",
      "Training epoch: 1769, train loss: 0.01577, val loss: 0.01596\n",
      "Training epoch: 1770, train loss: 0.01590, val loss: 0.01602\n",
      "Training epoch: 1771, train loss: 0.01563, val loss: 0.01585\n",
      "Training epoch: 1772, train loss: 0.01572, val loss: 0.01590\n",
      "Training epoch: 1773, train loss: 0.01582, val loss: 0.01596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1774, train loss: 0.01618, val loss: 0.01643\n",
      "Training epoch: 1775, train loss: 0.01584, val loss: 0.01590\n",
      "Training epoch: 1776, train loss: 0.01616, val loss: 0.01632\n",
      "Training epoch: 1777, train loss: 0.01566, val loss: 0.01575\n",
      "Training epoch: 1778, train loss: 0.01603, val loss: 0.01616\n",
      "Training epoch: 1779, train loss: 0.01612, val loss: 0.01619\n",
      "Training epoch: 1780, train loss: 0.01567, val loss: 0.01575\n",
      "Training epoch: 1781, train loss: 0.01638, val loss: 0.01644\n",
      "Training epoch: 1782, train loss: 0.01591, val loss: 0.01609\n",
      "Training epoch: 1783, train loss: 0.01611, val loss: 0.01621\n",
      "Training epoch: 1784, train loss: 0.01596, val loss: 0.01610\n",
      "Training epoch: 1785, train loss: 0.01610, val loss: 0.01620\n",
      "Training epoch: 1786, train loss: 0.01567, val loss: 0.01584\n",
      "Training epoch: 1787, train loss: 0.01566, val loss: 0.01581\n",
      "Training epoch: 1788, train loss: 0.01577, val loss: 0.01594\n",
      "Training epoch: 1789, train loss: 0.01565, val loss: 0.01581\n",
      "Training epoch: 1790, train loss: 0.01592, val loss: 0.01613\n",
      "Training epoch: 1791, train loss: 0.01579, val loss: 0.01594\n",
      "Training epoch: 1792, train loss: 0.01636, val loss: 0.01658\n",
      "Training epoch: 1793, train loss: 0.01586, val loss: 0.01606\n",
      "Training epoch: 1794, train loss: 0.01579, val loss: 0.01599\n",
      "Training epoch: 1795, train loss: 0.01589, val loss: 0.01607\n",
      "Training epoch: 1796, train loss: 0.01571, val loss: 0.01585\n",
      "Training epoch: 1797, train loss: 0.01590, val loss: 0.01606\n",
      "Training epoch: 1798, train loss: 0.01551, val loss: 0.01565\n",
      "Training epoch: 1799, train loss: 0.01771, val loss: 0.01774\n",
      "Training epoch: 1800, train loss: 0.01582, val loss: 0.01594\n",
      "Training epoch: 1801, train loss: 0.01563, val loss: 0.01575\n",
      "Training epoch: 1802, train loss: 0.01695, val loss: 0.01714\n",
      "Training epoch: 1803, train loss: 0.01620, val loss: 0.01630\n",
      "Training epoch: 1804, train loss: 0.01620, val loss: 0.01633\n",
      "Training epoch: 1805, train loss: 0.01602, val loss: 0.01624\n",
      "Training epoch: 1806, train loss: 0.01563, val loss: 0.01571\n",
      "Training epoch: 1807, train loss: 0.01584, val loss: 0.01596\n",
      "Training epoch: 1808, train loss: 0.01594, val loss: 0.01613\n",
      "Training epoch: 1809, train loss: 0.01608, val loss: 0.01626\n",
      "Training epoch: 1810, train loss: 0.01614, val loss: 0.01630\n",
      "Training epoch: 1811, train loss: 0.01583, val loss: 0.01591\n",
      "Training epoch: 1812, train loss: 0.01565, val loss: 0.01584\n",
      "Training epoch: 1813, train loss: 0.01558, val loss: 0.01575\n",
      "Training epoch: 1814, train loss: 0.01576, val loss: 0.01598\n",
      "Training epoch: 1815, train loss: 0.01563, val loss: 0.01576\n",
      "Training epoch: 1816, train loss: 0.01555, val loss: 0.01570\n",
      "Training epoch: 1817, train loss: 0.01653, val loss: 0.01676\n",
      "Training epoch: 1818, train loss: 0.01598, val loss: 0.01615\n",
      "Training epoch: 1819, train loss: 0.01553, val loss: 0.01567\n",
      "Training epoch: 1820, train loss: 0.01568, val loss: 0.01571\n",
      "Training epoch: 1821, train loss: 0.01672, val loss: 0.01697\n",
      "Training epoch: 1822, train loss: 0.01555, val loss: 0.01565\n",
      "Training epoch: 1823, train loss: 0.01603, val loss: 0.01614\n",
      "Training epoch: 1824, train loss: 0.01590, val loss: 0.01604\n",
      "Training epoch: 1825, train loss: 0.01649, val loss: 0.01669\n",
      "Training epoch: 1826, train loss: 0.01581, val loss: 0.01586\n",
      "Training epoch: 1827, train loss: 0.01583, val loss: 0.01594\n",
      "Training epoch: 1828, train loss: 0.01567, val loss: 0.01585\n",
      "Training epoch: 1829, train loss: 0.01575, val loss: 0.01588\n",
      "Training epoch: 1830, train loss: 0.01590, val loss: 0.01599\n",
      "Training epoch: 1831, train loss: 0.01580, val loss: 0.01597\n",
      "Training epoch: 1832, train loss: 0.01581, val loss: 0.01596\n",
      "Training epoch: 1833, train loss: 0.01594, val loss: 0.01600\n",
      "Training epoch: 1834, train loss: 0.01566, val loss: 0.01574\n",
      "Training epoch: 1835, train loss: 0.01613, val loss: 0.01616\n",
      "Training epoch: 1836, train loss: 0.01570, val loss: 0.01575\n",
      "Training epoch: 1837, train loss: 0.01557, val loss: 0.01571\n",
      "Training epoch: 1838, train loss: 0.01651, val loss: 0.01662\n",
      "Training epoch: 1839, train loss: 0.01565, val loss: 0.01585\n",
      "Training epoch: 1840, train loss: 0.01584, val loss: 0.01590\n",
      "Training epoch: 1841, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 1842, train loss: 0.01598, val loss: 0.01618\n",
      "Training epoch: 1843, train loss: 0.01576, val loss: 0.01586\n",
      "Training epoch: 1844, train loss: 0.01572, val loss: 0.01603\n",
      "Training epoch: 1845, train loss: 0.01565, val loss: 0.01580\n",
      "Training epoch: 1846, train loss: 0.01590, val loss: 0.01600\n",
      "Training epoch: 1847, train loss: 0.01593, val loss: 0.01619\n",
      "Training epoch: 1848, train loss: 0.01584, val loss: 0.01597\n",
      "Training epoch: 1849, train loss: 0.01560, val loss: 0.01571\n",
      "Training epoch: 1850, train loss: 0.01581, val loss: 0.01600\n",
      "Training epoch: 1851, train loss: 0.01598, val loss: 0.01605\n",
      "Training epoch: 1852, train loss: 0.01626, val loss: 0.01633\n",
      "Training epoch: 1853, train loss: 0.01607, val loss: 0.01627\n",
      "Training epoch: 1854, train loss: 0.01591, val loss: 0.01609\n",
      "Training epoch: 1855, train loss: 0.01659, val loss: 0.01680\n",
      "Training epoch: 1856, train loss: 0.01591, val loss: 0.01605\n",
      "Training epoch: 1857, train loss: 0.01566, val loss: 0.01584\n",
      "Training epoch: 1858, train loss: 0.01584, val loss: 0.01604\n",
      "Training epoch: 1859, train loss: 0.01571, val loss: 0.01584\n",
      "Training epoch: 1860, train loss: 0.01567, val loss: 0.01575\n",
      "Training epoch: 1861, train loss: 0.01597, val loss: 0.01614\n",
      "Training epoch: 1862, train loss: 0.01561, val loss: 0.01566\n",
      "Training epoch: 1863, train loss: 0.01570, val loss: 0.01578\n",
      "Training epoch: 1864, train loss: 0.01602, val loss: 0.01631\n",
      "Training epoch: 1865, train loss: 0.01614, val loss: 0.01628\n",
      "Training epoch: 1866, train loss: 0.01623, val loss: 0.01647\n",
      "Training epoch: 1867, train loss: 0.01577, val loss: 0.01578\n",
      "Training epoch: 1868, train loss: 0.01666, val loss: 0.01697\n",
      "Training epoch: 1869, train loss: 0.01558, val loss: 0.01567\n",
      "Training epoch: 1870, train loss: 0.01617, val loss: 0.01621\n",
      "Training epoch: 1871, train loss: 0.01629, val loss: 0.01650\n",
      "Training epoch: 1872, train loss: 0.01560, val loss: 0.01572\n",
      "Training epoch: 1873, train loss: 0.01628, val loss: 0.01655\n",
      "Training epoch: 1874, train loss: 0.01559, val loss: 0.01572\n",
      "Training epoch: 1875, train loss: 0.01586, val loss: 0.01599\n",
      "Training epoch: 1876, train loss: 0.01582, val loss: 0.01601\n",
      "Training epoch: 1877, train loss: 0.01572, val loss: 0.01588\n",
      "Training epoch: 1878, train loss: 0.01581, val loss: 0.01597\n",
      "Training epoch: 1879, train loss: 0.01565, val loss: 0.01575\n",
      "Training epoch: 1880, train loss: 0.01567, val loss: 0.01582\n",
      "Training epoch: 1881, train loss: 0.01605, val loss: 0.01621\n",
      "Training epoch: 1882, train loss: 0.01571, val loss: 0.01582\n",
      "Training epoch: 1883, train loss: 0.01561, val loss: 0.01576\n",
      "Training epoch: 1884, train loss: 0.01590, val loss: 0.01609\n",
      "Training epoch: 1885, train loss: 0.01573, val loss: 0.01587\n",
      "Training epoch: 1886, train loss: 0.01570, val loss: 0.01581\n",
      "Training epoch: 1887, train loss: 0.01560, val loss: 0.01586\n",
      "Training epoch: 1888, train loss: 0.01578, val loss: 0.01584\n",
      "Training epoch: 1889, train loss: 0.01574, val loss: 0.01581\n",
      "Training epoch: 1890, train loss: 0.01591, val loss: 0.01618\n",
      "Training epoch: 1891, train loss: 0.01567, val loss: 0.01580\n",
      "Training epoch: 1892, train loss: 0.01638, val loss: 0.01659\n",
      "Training epoch: 1893, train loss: 0.01554, val loss: 0.01570\n",
      "Training epoch: 1894, train loss: 0.01568, val loss: 0.01575\n",
      "Training epoch: 1895, train loss: 0.01619, val loss: 0.01636\n",
      "Training epoch: 1896, train loss: 0.01554, val loss: 0.01562\n",
      "Training epoch: 1897, train loss: 0.01548, val loss: 0.01571\n",
      "Training epoch: 1898, train loss: 0.01565, val loss: 0.01580\n",
      "Training epoch: 1899, train loss: 0.01571, val loss: 0.01579\n",
      "Training epoch: 1900, train loss: 0.01589, val loss: 0.01613\n",
      "Training epoch: 1901, train loss: 0.01595, val loss: 0.01605\n",
      "Training epoch: 1902, train loss: 0.01562, val loss: 0.01578\n",
      "Training epoch: 1903, train loss: 0.01607, val loss: 0.01611\n",
      "Training epoch: 1904, train loss: 0.01565, val loss: 0.01584\n",
      "Training epoch: 1905, train loss: 0.01557, val loss: 0.01570\n",
      "Training epoch: 1906, train loss: 0.01570, val loss: 0.01585\n",
      "Training epoch: 1907, train loss: 0.01560, val loss: 0.01572\n",
      "Training epoch: 1908, train loss: 0.01555, val loss: 0.01571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1909, train loss: 0.01654, val loss: 0.01675\n",
      "Training epoch: 1910, train loss: 0.01557, val loss: 0.01572\n",
      "Training epoch: 1911, train loss: 0.01565, val loss: 0.01587\n",
      "Training epoch: 1912, train loss: 0.01592, val loss: 0.01600\n",
      "Training epoch: 1913, train loss: 0.01567, val loss: 0.01585\n",
      "Training epoch: 1914, train loss: 0.01577, val loss: 0.01583\n",
      "Training epoch: 1915, train loss: 0.01596, val loss: 0.01590\n",
      "Training epoch: 1916, train loss: 0.01621, val loss: 0.01621\n",
      "Training epoch: 1917, train loss: 0.01633, val loss: 0.01639\n",
      "Training epoch: 1918, train loss: 0.01601, val loss: 0.01623\n",
      "Training epoch: 1919, train loss: 0.01574, val loss: 0.01599\n",
      "Training epoch: 1920, train loss: 0.01566, val loss: 0.01592\n",
      "Training epoch: 1921, train loss: 0.01570, val loss: 0.01582\n",
      "Training epoch: 1922, train loss: 0.01587, val loss: 0.01590\n",
      "Training epoch: 1923, train loss: 0.01587, val loss: 0.01605\n",
      "Training epoch: 1924, train loss: 0.01593, val loss: 0.01615\n",
      "Training epoch: 1925, train loss: 0.01581, val loss: 0.01614\n",
      "Training epoch: 1926, train loss: 0.01565, val loss: 0.01585\n",
      "Training epoch: 1927, train loss: 0.01566, val loss: 0.01572\n",
      "Training epoch: 1928, train loss: 0.01586, val loss: 0.01591\n",
      "Training epoch: 1929, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 1930, train loss: 0.01593, val loss: 0.01610\n",
      "Training epoch: 1931, train loss: 0.01600, val loss: 0.01610\n",
      "Training epoch: 1932, train loss: 0.01621, val loss: 0.01648\n",
      "Training epoch: 1933, train loss: 0.01554, val loss: 0.01581\n",
      "Training epoch: 1934, train loss: 0.01557, val loss: 0.01570\n",
      "Training epoch: 1935, train loss: 0.01557, val loss: 0.01572\n",
      "Training epoch: 1936, train loss: 0.01576, val loss: 0.01586\n",
      "Training epoch: 1937, train loss: 0.01560, val loss: 0.01570\n",
      "Training epoch: 1938, train loss: 0.01664, val loss: 0.01689\n",
      "Training epoch: 1939, train loss: 0.01566, val loss: 0.01575\n",
      "Training epoch: 1940, train loss: 0.01551, val loss: 0.01564\n",
      "Training epoch: 1941, train loss: 0.01593, val loss: 0.01612\n",
      "Training epoch: 1942, train loss: 0.01593, val loss: 0.01619\n",
      "Training epoch: 1943, train loss: 0.01565, val loss: 0.01576\n",
      "Training epoch: 1944, train loss: 0.01588, val loss: 0.01598\n",
      "Training epoch: 1945, train loss: 0.01557, val loss: 0.01570\n",
      "Training epoch: 1946, train loss: 0.01578, val loss: 0.01593\n",
      "Training epoch: 1947, train loss: 0.01572, val loss: 0.01588\n",
      "Training epoch: 1948, train loss: 0.01594, val loss: 0.01604\n",
      "Training epoch: 1949, train loss: 0.01691, val loss: 0.01691\n",
      "Training epoch: 1950, train loss: 0.01563, val loss: 0.01570\n",
      "Training epoch: 1951, train loss: 0.01615, val loss: 0.01630\n",
      "Training epoch: 1952, train loss: 0.01663, val loss: 0.01673\n",
      "Training epoch: 1953, train loss: 0.01665, val loss: 0.01681\n",
      "Training epoch: 1954, train loss: 0.01592, val loss: 0.01612\n",
      "Training epoch: 1955, train loss: 0.01585, val loss: 0.01597\n",
      "Training epoch: 1956, train loss: 0.01581, val loss: 0.01598\n",
      "Training epoch: 1957, train loss: 0.01556, val loss: 0.01566\n",
      "Training epoch: 1958, train loss: 0.01569, val loss: 0.01584\n",
      "Training epoch: 1959, train loss: 0.01576, val loss: 0.01592\n",
      "Training epoch: 1960, train loss: 0.01640, val loss: 0.01640\n",
      "Training epoch: 1961, train loss: 0.01580, val loss: 0.01597\n",
      "Training epoch: 1962, train loss: 0.01578, val loss: 0.01594\n",
      "Training epoch: 1963, train loss: 0.01565, val loss: 0.01582\n",
      "Training epoch: 1964, train loss: 0.01553, val loss: 0.01571\n",
      "Training epoch: 1965, train loss: 0.01568, val loss: 0.01583\n",
      "Training epoch: 1966, train loss: 0.01576, val loss: 0.01591\n",
      "Training epoch: 1967, train loss: 0.01562, val loss: 0.01579\n",
      "Training epoch: 1968, train loss: 0.01567, val loss: 0.01581\n",
      "Training epoch: 1969, train loss: 0.01578, val loss: 0.01585\n",
      "Training epoch: 1970, train loss: 0.01558, val loss: 0.01568\n",
      "Training epoch: 1971, train loss: 0.01566, val loss: 0.01580\n",
      "Training epoch: 1972, train loss: 0.01567, val loss: 0.01580\n",
      "Training epoch: 1973, train loss: 0.01538, val loss: 0.01561\n",
      "Training epoch: 1974, train loss: 0.01664, val loss: 0.01696\n",
      "Training epoch: 1975, train loss: 0.01568, val loss: 0.01577\n",
      "Training epoch: 1976, train loss: 0.01584, val loss: 0.01598\n",
      "Training epoch: 1977, train loss: 0.01582, val loss: 0.01604\n",
      "Training epoch: 1978, train loss: 0.01590, val loss: 0.01607\n",
      "Training epoch: 1979, train loss: 0.01584, val loss: 0.01593\n",
      "Training epoch: 1980, train loss: 0.01555, val loss: 0.01567\n",
      "Training epoch: 1981, train loss: 0.01555, val loss: 0.01570\n",
      "Training epoch: 1982, train loss: 0.01622, val loss: 0.01633\n",
      "Training epoch: 1983, train loss: 0.01579, val loss: 0.01589\n",
      "Training epoch: 1984, train loss: 0.01583, val loss: 0.01592\n",
      "Training epoch: 1985, train loss: 0.01556, val loss: 0.01567\n",
      "Training epoch: 1986, train loss: 0.01558, val loss: 0.01568\n",
      "Training epoch: 1987, train loss: 0.01569, val loss: 0.01585\n",
      "Training epoch: 1988, train loss: 0.01597, val loss: 0.01610\n",
      "Training epoch: 1989, train loss: 0.01557, val loss: 0.01569\n",
      "Training epoch: 1990, train loss: 0.01611, val loss: 0.01630\n",
      "Training epoch: 1991, train loss: 0.01581, val loss: 0.01592\n",
      "Training epoch: 1992, train loss: 0.01548, val loss: 0.01568\n",
      "Training epoch: 1993, train loss: 0.01591, val loss: 0.01602\n",
      "Training epoch: 1994, train loss: 0.01587, val loss: 0.01599\n",
      "Training epoch: 1995, train loss: 0.01553, val loss: 0.01570\n",
      "Training epoch: 1996, train loss: 0.01570, val loss: 0.01578\n",
      "Training epoch: 1997, train loss: 0.01588, val loss: 0.01599\n",
      "Training epoch: 1998, train loss: 0.01556, val loss: 0.01575\n",
      "Training epoch: 1999, train loss: 0.01721, val loss: 0.01730\n",
      "Training epoch: 2000, train loss: 0.01741, val loss: 0.01747\n",
      "Training epoch: 2001, train loss: 0.01601, val loss: 0.01618\n",
      "Training epoch: 2002, train loss: 0.01550, val loss: 0.01575\n",
      "Training epoch: 2003, train loss: 0.01598, val loss: 0.01610\n",
      "Training epoch: 2004, train loss: 0.01549, val loss: 0.01560\n",
      "Training epoch: 2005, train loss: 0.01557, val loss: 0.01575\n",
      "Training epoch: 2006, train loss: 0.01587, val loss: 0.01607\n",
      "Training epoch: 2007, train loss: 0.01601, val loss: 0.01616\n",
      "Training epoch: 2008, train loss: 0.01564, val loss: 0.01578\n",
      "Training epoch: 2009, train loss: 0.01558, val loss: 0.01571\n",
      "Training epoch: 2010, train loss: 0.01565, val loss: 0.01576\n",
      "Training epoch: 2011, train loss: 0.01566, val loss: 0.01589\n",
      "Training epoch: 2012, train loss: 0.01593, val loss: 0.01603\n",
      "Training epoch: 2013, train loss: 0.01564, val loss: 0.01585\n",
      "Training epoch: 2014, train loss: 0.01546, val loss: 0.01560\n",
      "Training epoch: 2015, train loss: 0.01558, val loss: 0.01572\n",
      "Training epoch: 2016, train loss: 0.01569, val loss: 0.01587\n",
      "Training epoch: 2017, train loss: 0.01584, val loss: 0.01610\n",
      "Training epoch: 2018, train loss: 0.01599, val loss: 0.01610\n",
      "Training epoch: 2019, train loss: 0.01556, val loss: 0.01565\n",
      "Training epoch: 2020, train loss: 0.01577, val loss: 0.01589\n",
      "Training epoch: 2021, train loss: 0.01565, val loss: 0.01586\n",
      "Training epoch: 2022, train loss: 0.01602, val loss: 0.01613\n",
      "Training epoch: 2023, train loss: 0.01586, val loss: 0.01603\n",
      "Training epoch: 2024, train loss: 0.01616, val loss: 0.01634\n",
      "Training epoch: 2025, train loss: 0.01578, val loss: 0.01597\n",
      "Training epoch: 2026, train loss: 0.01604, val loss: 0.01629\n",
      "Training epoch: 2027, train loss: 0.01577, val loss: 0.01594\n",
      "Training epoch: 2028, train loss: 0.01617, val loss: 0.01634\n",
      "Training epoch: 2029, train loss: 0.01610, val loss: 0.01638\n",
      "Training epoch: 2030, train loss: 0.01579, val loss: 0.01589\n",
      "Training epoch: 2031, train loss: 0.01605, val loss: 0.01622\n",
      "Training epoch: 2032, train loss: 0.01611, val loss: 0.01631\n",
      "Training epoch: 2033, train loss: 0.01558, val loss: 0.01568\n",
      "Training epoch: 2034, train loss: 0.01572, val loss: 0.01570\n",
      "Training epoch: 2035, train loss: 0.01581, val loss: 0.01592\n",
      "Training epoch: 2036, train loss: 0.01564, val loss: 0.01575\n",
      "Training epoch: 2037, train loss: 0.01559, val loss: 0.01570\n",
      "Training epoch: 2038, train loss: 0.01563, val loss: 0.01576\n",
      "Training epoch: 2039, train loss: 0.01586, val loss: 0.01595\n",
      "Training epoch: 2040, train loss: 0.01592, val loss: 0.01615\n",
      "Training epoch: 2041, train loss: 0.01573, val loss: 0.01577\n",
      "Training epoch: 2042, train loss: 0.01551, val loss: 0.01567\n",
      "Training epoch: 2043, train loss: 0.01565, val loss: 0.01576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2044, train loss: 0.01583, val loss: 0.01592\n",
      "Training epoch: 2045, train loss: 0.01550, val loss: 0.01564\n",
      "Training epoch: 2046, train loss: 0.01617, val loss: 0.01627\n",
      "Training epoch: 2047, train loss: 0.01656, val loss: 0.01674\n",
      "Training epoch: 2048, train loss: 0.01598, val loss: 0.01616\n",
      "Training epoch: 2049, train loss: 0.01565, val loss: 0.01586\n",
      "Training epoch: 2050, train loss: 0.01598, val loss: 0.01610\n",
      "Training epoch: 2051, train loss: 0.01590, val loss: 0.01610\n",
      "Training epoch: 2052, train loss: 0.01564, val loss: 0.01578\n",
      "Training epoch: 2053, train loss: 0.01571, val loss: 0.01584\n",
      "Training epoch: 2054, train loss: 0.01575, val loss: 0.01593\n",
      "Training epoch: 2055, train loss: 0.01569, val loss: 0.01580\n",
      "Training epoch: 2056, train loss: 0.01597, val loss: 0.01603\n",
      "Training epoch: 2057, train loss: 0.01564, val loss: 0.01578\n",
      "Training epoch: 2058, train loss: 0.01581, val loss: 0.01592\n",
      "Training epoch: 2059, train loss: 0.01578, val loss: 0.01599\n",
      "Training epoch: 2060, train loss: 0.01597, val loss: 0.01620\n",
      "Training epoch: 2061, train loss: 0.01555, val loss: 0.01572\n",
      "Training epoch: 2062, train loss: 0.01594, val loss: 0.01603\n",
      "Training epoch: 2063, train loss: 0.01570, val loss: 0.01578\n",
      "Training epoch: 2064, train loss: 0.01553, val loss: 0.01565\n",
      "Training epoch: 2065, train loss: 0.01604, val loss: 0.01626\n",
      "Training epoch: 2066, train loss: 0.01649, val loss: 0.01676\n",
      "Training epoch: 2067, train loss: 0.01563, val loss: 0.01580\n",
      "Training epoch: 2068, train loss: 0.01598, val loss: 0.01615\n",
      "Training epoch: 2069, train loss: 0.01571, val loss: 0.01574\n",
      "Training epoch: 2070, train loss: 0.01578, val loss: 0.01596\n",
      "Training epoch: 2071, train loss: 0.01637, val loss: 0.01659\n",
      "Training epoch: 2072, train loss: 0.01652, val loss: 0.01671\n",
      "Training epoch: 2073, train loss: 0.01561, val loss: 0.01584\n",
      "Training epoch: 2074, train loss: 0.01556, val loss: 0.01566\n",
      "Training epoch: 2075, train loss: 0.01571, val loss: 0.01587\n",
      "Training epoch: 2076, train loss: 0.01587, val loss: 0.01607\n",
      "Training epoch: 2077, train loss: 0.01552, val loss: 0.01569\n",
      "Training epoch: 2078, train loss: 0.01567, val loss: 0.01589\n",
      "Training epoch: 2079, train loss: 0.01579, val loss: 0.01594\n",
      "Training epoch: 2080, train loss: 0.01590, val loss: 0.01595\n",
      "Training epoch: 2081, train loss: 0.01659, val loss: 0.01683\n",
      "Training epoch: 2082, train loss: 0.01556, val loss: 0.01577\n",
      "Training epoch: 2083, train loss: 0.01568, val loss: 0.01581\n",
      "Training epoch: 2084, train loss: 0.01558, val loss: 0.01573\n",
      "Training epoch: 2085, train loss: 0.01562, val loss: 0.01568\n",
      "Training epoch: 2086, train loss: 0.01584, val loss: 0.01604\n",
      "Training epoch: 2087, train loss: 0.01579, val loss: 0.01598\n",
      "Training epoch: 2088, train loss: 0.01559, val loss: 0.01569\n",
      "Training epoch: 2089, train loss: 0.01566, val loss: 0.01580\n",
      "Training epoch: 2090, train loss: 0.01578, val loss: 0.01600\n",
      "Training epoch: 2091, train loss: 0.01565, val loss: 0.01588\n",
      "Training epoch: 2092, train loss: 0.01547, val loss: 0.01563\n",
      "Training epoch: 2093, train loss: 0.01561, val loss: 0.01583\n",
      "Training epoch: 2094, train loss: 0.01586, val loss: 0.01598\n",
      "Training epoch: 2095, train loss: 0.01584, val loss: 0.01601\n",
      "Training epoch: 2096, train loss: 0.01623, val loss: 0.01648\n",
      "Training epoch: 2097, train loss: 0.01569, val loss: 0.01580\n",
      "Training epoch: 2098, train loss: 0.01580, val loss: 0.01590\n",
      "Training epoch: 2099, train loss: 0.01626, val loss: 0.01648\n",
      "Training epoch: 2100, train loss: 0.01584, val loss: 0.01592\n",
      "Training epoch: 2101, train loss: 0.01557, val loss: 0.01578\n",
      "Training epoch: 2102, train loss: 0.01567, val loss: 0.01568\n",
      "Training epoch: 2103, train loss: 0.01793, val loss: 0.01801\n",
      "Training epoch: 2104, train loss: 0.01561, val loss: 0.01576\n",
      "Training epoch: 2105, train loss: 0.01580, val loss: 0.01601\n",
      "Training epoch: 2106, train loss: 0.01582, val loss: 0.01601\n",
      "Training epoch: 2107, train loss: 0.01557, val loss: 0.01583\n",
      "Training epoch: 2108, train loss: 0.01587, val loss: 0.01599\n",
      "Training epoch: 2109, train loss: 0.01567, val loss: 0.01579\n",
      "Training epoch: 2110, train loss: 0.01549, val loss: 0.01561\n",
      "Training epoch: 2111, train loss: 0.01617, val loss: 0.01627\n",
      "Training epoch: 2112, train loss: 0.01638, val loss: 0.01646\n",
      "Training epoch: 2113, train loss: 0.01591, val loss: 0.01613\n",
      "Training epoch: 2114, train loss: 0.01589, val loss: 0.01595\n",
      "Training epoch: 2115, train loss: 0.01604, val loss: 0.01619\n",
      "Training epoch: 2116, train loss: 0.01579, val loss: 0.01593\n",
      "Training epoch: 2117, train loss: 0.01598, val loss: 0.01611\n",
      "Training epoch: 2118, train loss: 0.01597, val loss: 0.01616\n",
      "Training epoch: 2119, train loss: 0.01574, val loss: 0.01588\n",
      "Training epoch: 2120, train loss: 0.01579, val loss: 0.01588\n",
      "Training epoch: 2121, train loss: 0.01569, val loss: 0.01577\n",
      "Training epoch: 2122, train loss: 0.01541, val loss: 0.01562\n",
      "Training epoch: 2123, train loss: 0.01566, val loss: 0.01581\n",
      "Training epoch: 2124, train loss: 0.01558, val loss: 0.01588\n",
      "Training epoch: 2125, train loss: 0.01594, val loss: 0.01619\n",
      "Training epoch: 2126, train loss: 0.01577, val loss: 0.01592\n",
      "Training epoch: 2127, train loss: 0.01643, val loss: 0.01649\n",
      "Training epoch: 2128, train loss: 0.01603, val loss: 0.01608\n",
      "Training epoch: 2129, train loss: 0.01574, val loss: 0.01590\n",
      "Training epoch: 2130, train loss: 0.01551, val loss: 0.01567\n",
      "Training epoch: 2131, train loss: 0.01574, val loss: 0.01598\n",
      "Training epoch: 2132, train loss: 0.01561, val loss: 0.01575\n",
      "Training epoch: 2133, train loss: 0.01639, val loss: 0.01662\n",
      "Training epoch: 2134, train loss: 0.01712, val loss: 0.01743\n",
      "Training epoch: 2135, train loss: 0.01568, val loss: 0.01580\n",
      "Training epoch: 2136, train loss: 0.01567, val loss: 0.01571\n",
      "Training epoch: 2137, train loss: 0.01588, val loss: 0.01603\n",
      "Training epoch: 2138, train loss: 0.01562, val loss: 0.01579\n",
      "Training epoch: 2139, train loss: 0.01557, val loss: 0.01569\n",
      "Training epoch: 2140, train loss: 0.01594, val loss: 0.01590\n",
      "Training epoch: 2141, train loss: 0.01562, val loss: 0.01578\n",
      "Training epoch: 2142, train loss: 0.01566, val loss: 0.01576\n",
      "Training epoch: 2143, train loss: 0.01587, val loss: 0.01604\n",
      "Training epoch: 2144, train loss: 0.01595, val loss: 0.01602\n",
      "Training epoch: 2145, train loss: 0.01578, val loss: 0.01580\n",
      "Training epoch: 2146, train loss: 0.01583, val loss: 0.01600\n",
      "Training epoch: 2147, train loss: 0.01556, val loss: 0.01570\n",
      "Training epoch: 2148, train loss: 0.01569, val loss: 0.01587\n",
      "Training epoch: 2149, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 2150, train loss: 0.01582, val loss: 0.01590\n",
      "Training epoch: 2151, train loss: 0.01551, val loss: 0.01563\n",
      "Training epoch: 2152, train loss: 0.01582, val loss: 0.01598\n",
      "Training epoch: 2153, train loss: 0.01559, val loss: 0.01572\n",
      "Training epoch: 2154, train loss: 0.01582, val loss: 0.01606\n",
      "Training epoch: 2155, train loss: 0.01571, val loss: 0.01593\n",
      "Training epoch: 2156, train loss: 0.01575, val loss: 0.01585\n",
      "Training epoch: 2157, train loss: 0.01560, val loss: 0.01570\n",
      "Training epoch: 2158, train loss: 0.01544, val loss: 0.01561\n",
      "Training epoch: 2159, train loss: 0.01577, val loss: 0.01593\n",
      "Training epoch: 2160, train loss: 0.01574, val loss: 0.01591\n",
      "Training epoch: 2161, train loss: 0.01571, val loss: 0.01592\n",
      "Training epoch: 2162, train loss: 0.01589, val loss: 0.01616\n",
      "Training epoch: 2163, train loss: 0.01632, val loss: 0.01650\n",
      "Training epoch: 2164, train loss: 0.01559, val loss: 0.01565\n",
      "Training epoch: 2165, train loss: 0.01545, val loss: 0.01559\n",
      "Training epoch: 2166, train loss: 0.01550, val loss: 0.01573\n",
      "Training epoch: 2167, train loss: 0.01555, val loss: 0.01566\n",
      "Training epoch: 2168, train loss: 0.01643, val loss: 0.01676\n",
      "Training epoch: 2169, train loss: 0.01580, val loss: 0.01600\n",
      "Training epoch: 2170, train loss: 0.01571, val loss: 0.01587\n",
      "Training epoch: 2171, train loss: 0.01635, val loss: 0.01653\n",
      "Training epoch: 2172, train loss: 0.01623, val loss: 0.01631\n",
      "Training epoch: 2173, train loss: 0.01598, val loss: 0.01613\n",
      "Training epoch: 2174, train loss: 0.01570, val loss: 0.01593\n",
      "Training epoch: 2175, train loss: 0.01571, val loss: 0.01584\n",
      "Training epoch: 2176, train loss: 0.01570, val loss: 0.01583\n",
      "Training epoch: 2177, train loss: 0.01568, val loss: 0.01591\n",
      "Training epoch: 2178, train loss: 0.01564, val loss: 0.01576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2179, train loss: 0.01615, val loss: 0.01625\n",
      "Training epoch: 2180, train loss: 0.01590, val loss: 0.01609\n",
      "Training epoch: 2181, train loss: 0.01580, val loss: 0.01599\n",
      "Training epoch: 2182, train loss: 0.01550, val loss: 0.01564\n",
      "Training epoch: 2183, train loss: 0.01567, val loss: 0.01582\n",
      "Training epoch: 2184, train loss: 0.01625, val loss: 0.01632\n",
      "Training epoch: 2185, train loss: 0.01560, val loss: 0.01585\n",
      "Training epoch: 2186, train loss: 0.01555, val loss: 0.01563\n",
      "Training epoch: 2187, train loss: 0.01608, val loss: 0.01622\n",
      "Training epoch: 2188, train loss: 0.01573, val loss: 0.01586\n",
      "Training epoch: 2189, train loss: 0.01584, val loss: 0.01597\n",
      "Training epoch: 2190, train loss: 0.01593, val loss: 0.01612\n",
      "Training epoch: 2191, train loss: 0.01570, val loss: 0.01581\n",
      "Training epoch: 2192, train loss: 0.01605, val loss: 0.01619\n",
      "Training epoch: 2193, train loss: 0.01589, val loss: 0.01603\n",
      "Training epoch: 2194, train loss: 0.01608, val loss: 0.01633\n",
      "Training epoch: 2195, train loss: 0.01585, val loss: 0.01604\n",
      "Training epoch: 2196, train loss: 0.01553, val loss: 0.01574\n",
      "Training epoch: 2197, train loss: 0.01594, val loss: 0.01599\n",
      "Training epoch: 2198, train loss: 0.01570, val loss: 0.01587\n",
      "Training epoch: 2199, train loss: 0.01563, val loss: 0.01582\n",
      "Training epoch: 2200, train loss: 0.01559, val loss: 0.01579\n",
      "Training epoch: 2201, train loss: 0.01650, val loss: 0.01677\n",
      "Training epoch: 2202, train loss: 0.01614, val loss: 0.01637\n",
      "Training epoch: 2203, train loss: 0.01586, val loss: 0.01599\n",
      "Training epoch: 2204, train loss: 0.01597, val loss: 0.01616\n",
      "Training epoch: 2205, train loss: 0.01561, val loss: 0.01570\n",
      "Training epoch: 2206, train loss: 0.01597, val loss: 0.01608\n",
      "Training epoch: 2207, train loss: 0.01565, val loss: 0.01583\n",
      "Training epoch: 2208, train loss: 0.01548, val loss: 0.01574\n",
      "Training epoch: 2209, train loss: 0.01589, val loss: 0.01606\n",
      "Training epoch: 2210, train loss: 0.01583, val loss: 0.01598\n",
      "Training epoch: 2211, train loss: 0.01553, val loss: 0.01569\n",
      "Training epoch: 2212, train loss: 0.01559, val loss: 0.01573\n",
      "Training epoch: 2213, train loss: 0.01566, val loss: 0.01577\n",
      "Training epoch: 2214, train loss: 0.01560, val loss: 0.01575\n",
      "Training epoch: 2215, train loss: 0.01570, val loss: 0.01591\n",
      "Training epoch: 2216, train loss: 0.01566, val loss: 0.01588\n",
      "Training epoch: 2217, train loss: 0.01629, val loss: 0.01641\n",
      "Training epoch: 2218, train loss: 0.01581, val loss: 0.01593\n",
      "Training epoch: 2219, train loss: 0.01562, val loss: 0.01582\n",
      "Training epoch: 2220, train loss: 0.01568, val loss: 0.01580\n",
      "Training epoch: 2221, train loss: 0.01592, val loss: 0.01614\n",
      "Training epoch: 2222, train loss: 0.01625, val loss: 0.01648\n",
      "Training epoch: 2223, train loss: 0.01575, val loss: 0.01591\n",
      "Training epoch: 2224, train loss: 0.01655, val loss: 0.01678\n",
      "Training epoch: 2225, train loss: 0.01568, val loss: 0.01577\n",
      "Training epoch: 2226, train loss: 0.01583, val loss: 0.01599\n",
      "Training epoch: 2227, train loss: 0.01570, val loss: 0.01583\n",
      "Training epoch: 2228, train loss: 0.01591, val loss: 0.01600\n",
      "Training epoch: 2229, train loss: 0.01563, val loss: 0.01576\n",
      "Training epoch: 2230, train loss: 0.01544, val loss: 0.01567\n",
      "Training epoch: 2231, train loss: 0.01568, val loss: 0.01573\n",
      "Training epoch: 2232, train loss: 0.01600, val loss: 0.01608\n",
      "Training epoch: 2233, train loss: 0.01546, val loss: 0.01563\n",
      "Training epoch: 2234, train loss: 0.01565, val loss: 0.01582\n",
      "Training epoch: 2235, train loss: 0.01599, val loss: 0.01607\n",
      "Training epoch: 2236, train loss: 0.01591, val loss: 0.01625\n",
      "Training epoch: 2237, train loss: 0.01557, val loss: 0.01580\n",
      "Training epoch: 2238, train loss: 0.01591, val loss: 0.01615\n",
      "Training epoch: 2239, train loss: 0.01635, val loss: 0.01657\n",
      "Training epoch: 2240, train loss: 0.01568, val loss: 0.01578\n",
      "Training epoch: 2241, train loss: 0.01581, val loss: 0.01589\n",
      "Training epoch: 2242, train loss: 0.01550, val loss: 0.01558\n",
      "Training epoch: 2243, train loss: 0.01683, val loss: 0.01689\n",
      "Training epoch: 2244, train loss: 0.01558, val loss: 0.01573\n",
      "Training epoch: 2245, train loss: 0.01563, val loss: 0.01575\n",
      "Training epoch: 2246, train loss: 0.01574, val loss: 0.01594\n",
      "Training epoch: 2247, train loss: 0.01606, val loss: 0.01621\n",
      "Training epoch: 2248, train loss: 0.01559, val loss: 0.01580\n",
      "Training epoch: 2249, train loss: 0.01567, val loss: 0.01583\n",
      "Training epoch: 2250, train loss: 0.01559, val loss: 0.01579\n",
      "Training epoch: 2251, train loss: 0.01557, val loss: 0.01571\n",
      "Training epoch: 2252, train loss: 0.01590, val loss: 0.01615\n",
      "Training epoch: 2253, train loss: 0.01575, val loss: 0.01590\n",
      "Training epoch: 2254, train loss: 0.01582, val loss: 0.01596\n",
      "Training epoch: 2255, train loss: 0.01625, val loss: 0.01645\n",
      "Training epoch: 2256, train loss: 0.01617, val loss: 0.01637\n",
      "Training epoch: 2257, train loss: 0.01552, val loss: 0.01569\n",
      "Training epoch: 2258, train loss: 0.01574, val loss: 0.01578\n",
      "Training epoch: 2259, train loss: 0.01606, val loss: 0.01630\n",
      "Training epoch: 2260, train loss: 0.01575, val loss: 0.01592\n",
      "Training epoch: 2261, train loss: 0.01575, val loss: 0.01585\n",
      "Training epoch: 2262, train loss: 0.01602, val loss: 0.01624\n",
      "Training epoch: 2263, train loss: 0.01547, val loss: 0.01566\n",
      "Training epoch: 2264, train loss: 0.01577, val loss: 0.01599\n",
      "Training epoch: 2265, train loss: 0.01581, val loss: 0.01592\n",
      "Training epoch: 2266, train loss: 0.01614, val loss: 0.01638\n",
      "Training epoch: 2267, train loss: 0.01565, val loss: 0.01572\n",
      "Training epoch: 2268, train loss: 0.01564, val loss: 0.01578\n",
      "Training epoch: 2269, train loss: 0.01543, val loss: 0.01562\n",
      "Training epoch: 2270, train loss: 0.01572, val loss: 0.01586\n",
      "Training epoch: 2271, train loss: 0.01651, val loss: 0.01663\n",
      "Training epoch: 2272, train loss: 0.01548, val loss: 0.01563\n",
      "Training epoch: 2273, train loss: 0.01551, val loss: 0.01570\n",
      "Training epoch: 2274, train loss: 0.01572, val loss: 0.01592\n",
      "Training epoch: 2275, train loss: 0.01614, val loss: 0.01641\n",
      "Training epoch: 2276, train loss: 0.01578, val loss: 0.01588\n",
      "Training epoch: 2277, train loss: 0.01632, val loss: 0.01654\n",
      "Training epoch: 2278, train loss: 0.01617, val loss: 0.01639\n",
      "Training epoch: 2279, train loss: 0.01563, val loss: 0.01571\n",
      "Training epoch: 2280, train loss: 0.01590, val loss: 0.01611\n",
      "Training epoch: 2281, train loss: 0.01603, val loss: 0.01623\n",
      "Training epoch: 2282, train loss: 0.01554, val loss: 0.01566\n",
      "Training epoch: 2283, train loss: 0.01551, val loss: 0.01568\n",
      "Training epoch: 2284, train loss: 0.01569, val loss: 0.01587\n",
      "Training epoch: 2285, train loss: 0.01546, val loss: 0.01563\n",
      "Training epoch: 2286, train loss: 0.01593, val loss: 0.01598\n",
      "Training epoch: 2287, train loss: 0.01554, val loss: 0.01562\n",
      "Training epoch: 2288, train loss: 0.01549, val loss: 0.01562\n",
      "Training epoch: 2289, train loss: 0.01559, val loss: 0.01571\n",
      "Training epoch: 2290, train loss: 0.01548, val loss: 0.01563\n",
      "Training epoch: 2291, train loss: 0.01584, val loss: 0.01597\n",
      "Training epoch: 2292, train loss: 0.01586, val loss: 0.01598\n",
      "Training epoch: 2293, train loss: 0.01577, val loss: 0.01591\n",
      "Training epoch: 2294, train loss: 0.01571, val loss: 0.01587\n",
      "Training epoch: 2295, train loss: 0.01554, val loss: 0.01561\n",
      "Training epoch: 2296, train loss: 0.01547, val loss: 0.01557\n",
      "Training epoch: 2297, train loss: 0.01577, val loss: 0.01589\n",
      "Training epoch: 2298, train loss: 0.01611, val loss: 0.01628\n",
      "Training epoch: 2299, train loss: 0.01616, val loss: 0.01637\n",
      "Training epoch: 2300, train loss: 0.01603, val loss: 0.01626\n",
      "Training epoch: 2301, train loss: 0.01558, val loss: 0.01574\n",
      "Training epoch: 2302, train loss: 0.01550, val loss: 0.01562\n",
      "Training epoch: 2303, train loss: 0.01579, val loss: 0.01593\n",
      "Training epoch: 2304, train loss: 0.01585, val loss: 0.01597\n",
      "Training epoch: 2305, train loss: 0.01611, val loss: 0.01633\n",
      "Training epoch: 2306, train loss: 0.01656, val loss: 0.01672\n",
      "Training epoch: 2307, train loss: 0.01596, val loss: 0.01602\n",
      "Training epoch: 2308, train loss: 0.01569, val loss: 0.01586\n",
      "Training epoch: 2309, train loss: 0.01567, val loss: 0.01577\n",
      "Training epoch: 2310, train loss: 0.01546, val loss: 0.01561\n",
      "Training epoch: 2311, train loss: 0.01555, val loss: 0.01568\n",
      "Training epoch: 2312, train loss: 0.01575, val loss: 0.01595\n",
      "Training epoch: 2313, train loss: 0.01567, val loss: 0.01587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2314, train loss: 0.01572, val loss: 0.01583\n",
      "Training epoch: 2315, train loss: 0.01560, val loss: 0.01574\n",
      "Training epoch: 2316, train loss: 0.01564, val loss: 0.01578\n",
      "Training epoch: 2317, train loss: 0.01554, val loss: 0.01566\n",
      "Training epoch: 2318, train loss: 0.01569, val loss: 0.01590\n",
      "Training epoch: 2319, train loss: 0.01552, val loss: 0.01567\n",
      "Training epoch: 2320, train loss: 0.01569, val loss: 0.01581\n",
      "Training epoch: 2321, train loss: 0.01568, val loss: 0.01595\n",
      "Training epoch: 2322, train loss: 0.01631, val loss: 0.01650\n",
      "Training epoch: 2323, train loss: 0.01573, val loss: 0.01592\n",
      "Training epoch: 2324, train loss: 0.01578, val loss: 0.01600\n",
      "Training epoch: 2325, train loss: 0.01602, val loss: 0.01617\n",
      "Training epoch: 2326, train loss: 0.01567, val loss: 0.01588\n",
      "Training epoch: 2327, train loss: 0.01606, val loss: 0.01619\n",
      "Training epoch: 2328, train loss: 0.01634, val loss: 0.01634\n",
      "Training epoch: 2329, train loss: 0.01581, val loss: 0.01595\n",
      "Training epoch: 2330, train loss: 0.01555, val loss: 0.01567\n",
      "Training epoch: 2331, train loss: 0.01562, val loss: 0.01579\n",
      "Training epoch: 2332, train loss: 0.01572, val loss: 0.01593\n",
      "Training epoch: 2333, train loss: 0.01574, val loss: 0.01593\n",
      "Training epoch: 2334, train loss: 0.01551, val loss: 0.01563\n",
      "Training epoch: 2335, train loss: 0.01547, val loss: 0.01559\n",
      "Training epoch: 2336, train loss: 0.01575, val loss: 0.01591\n",
      "Training epoch: 2337, train loss: 0.01574, val loss: 0.01573\n",
      "Training epoch: 2338, train loss: 0.01621, val loss: 0.01649\n",
      "Training epoch: 2339, train loss: 0.01565, val loss: 0.01587\n",
      "Training epoch: 2340, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 2341, train loss: 0.01565, val loss: 0.01584\n",
      "Training epoch: 2342, train loss: 0.01590, val loss: 0.01612\n",
      "Training epoch: 2343, train loss: 0.01554, val loss: 0.01563\n",
      "Training epoch: 2344, train loss: 0.01559, val loss: 0.01570\n",
      "Training epoch: 2345, train loss: 0.01557, val loss: 0.01568\n",
      "Training epoch: 2346, train loss: 0.01591, val loss: 0.01599\n",
      "Training epoch: 2347, train loss: 0.01583, val loss: 0.01596\n",
      "Training epoch: 2348, train loss: 0.01598, val loss: 0.01614\n",
      "Training epoch: 2349, train loss: 0.01553, val loss: 0.01576\n",
      "Training epoch: 2350, train loss: 0.01593, val loss: 0.01613\n",
      "Training epoch: 2351, train loss: 0.01586, val loss: 0.01601\n",
      "Training epoch: 2352, train loss: 0.01544, val loss: 0.01562\n",
      "Training epoch: 2353, train loss: 0.01552, val loss: 0.01576\n",
      "Training epoch: 2354, train loss: 0.01616, val loss: 0.01627\n",
      "Training epoch: 2355, train loss: 0.01615, val loss: 0.01637\n",
      "Training epoch: 2356, train loss: 0.01555, val loss: 0.01566\n",
      "Training epoch: 2357, train loss: 0.01561, val loss: 0.01575\n",
      "Training epoch: 2358, train loss: 0.01558, val loss: 0.01573\n",
      "Training epoch: 2359, train loss: 0.01582, val loss: 0.01594\n",
      "Training epoch: 2360, train loss: 0.01564, val loss: 0.01577\n",
      "Training epoch: 2361, train loss: 0.01589, val loss: 0.01614\n",
      "Training epoch: 2362, train loss: 0.01573, val loss: 0.01590\n",
      "Training epoch: 2363, train loss: 0.01565, val loss: 0.01578\n",
      "Training epoch: 2364, train loss: 0.01578, val loss: 0.01597\n",
      "Training epoch: 2365, train loss: 0.01561, val loss: 0.01578\n",
      "Training epoch: 2366, train loss: 0.01569, val loss: 0.01589\n",
      "Training epoch: 2367, train loss: 0.01583, val loss: 0.01587\n",
      "Training epoch: 2368, train loss: 0.01599, val loss: 0.01620\n",
      "Training epoch: 2369, train loss: 0.01571, val loss: 0.01588\n",
      "Training epoch: 2370, train loss: 0.01611, val loss: 0.01639\n",
      "Training epoch: 2371, train loss: 0.01588, val loss: 0.01613\n",
      "Training epoch: 2372, train loss: 0.01659, val loss: 0.01677\n",
      "Training epoch: 2373, train loss: 0.01625, val loss: 0.01640\n",
      "Training epoch: 2374, train loss: 0.01581, val loss: 0.01594\n",
      "Training epoch: 2375, train loss: 0.01558, val loss: 0.01566\n",
      "Training epoch: 2376, train loss: 0.01569, val loss: 0.01576\n",
      "Training epoch: 2377, train loss: 0.01585, val loss: 0.01588\n",
      "Training epoch: 2378, train loss: 0.01588, val loss: 0.01603\n",
      "Training epoch: 2379, train loss: 0.01559, val loss: 0.01569\n",
      "Training epoch: 2380, train loss: 0.01571, val loss: 0.01583\n",
      "Training epoch: 2381, train loss: 0.01556, val loss: 0.01572\n",
      "Training epoch: 2382, train loss: 0.01581, val loss: 0.01582\n",
      "Training epoch: 2383, train loss: 0.01544, val loss: 0.01563\n",
      "Training epoch: 2384, train loss: 0.01572, val loss: 0.01579\n",
      "Training epoch: 2385, train loss: 0.01619, val loss: 0.01642\n",
      "Training epoch: 2386, train loss: 0.01574, val loss: 0.01582\n",
      "Training epoch: 2387, train loss: 0.01561, val loss: 0.01572\n",
      "Training epoch: 2388, train loss: 0.01624, val loss: 0.01640\n",
      "Training epoch: 2389, train loss: 0.01559, val loss: 0.01576\n",
      "Training epoch: 2390, train loss: 0.01560, val loss: 0.01568\n",
      "Training epoch: 2391, train loss: 0.01589, val loss: 0.01595\n",
      "Training epoch: 2392, train loss: 0.01559, val loss: 0.01566\n",
      "Training epoch: 2393, train loss: 0.01549, val loss: 0.01557\n",
      "Training epoch: 2394, train loss: 0.01580, val loss: 0.01583\n",
      "Training epoch: 2395, train loss: 0.01564, val loss: 0.01577\n",
      "Training epoch: 2396, train loss: 0.01602, val loss: 0.01623\n",
      "Training epoch: 2397, train loss: 0.01589, val loss: 0.01607\n",
      "Training epoch: 2398, train loss: 0.01612, val loss: 0.01645\n",
      "Training epoch: 2399, train loss: 0.01548, val loss: 0.01568\n",
      "Training epoch: 2400, train loss: 0.01584, val loss: 0.01607\n",
      "Training epoch: 2401, train loss: 0.01571, val loss: 0.01592\n",
      "Training epoch: 2402, train loss: 0.01564, val loss: 0.01581\n",
      "Training epoch: 2403, train loss: 0.01558, val loss: 0.01577\n",
      "Training epoch: 2404, train loss: 0.01555, val loss: 0.01567\n",
      "Training epoch: 2405, train loss: 0.01583, val loss: 0.01591\n",
      "Training epoch: 2406, train loss: 0.01580, val loss: 0.01597\n",
      "Training epoch: 2407, train loss: 0.01568, val loss: 0.01582\n",
      "Training epoch: 2408, train loss: 0.01584, val loss: 0.01590\n",
      "Training epoch: 2409, train loss: 0.01590, val loss: 0.01597\n",
      "Training epoch: 2410, train loss: 0.01583, val loss: 0.01612\n",
      "Training epoch: 2411, train loss: 0.01588, val loss: 0.01591\n",
      "Training epoch: 2412, train loss: 0.01582, val loss: 0.01603\n",
      "Training epoch: 2413, train loss: 0.01570, val loss: 0.01585\n",
      "Training epoch: 2414, train loss: 0.01553, val loss: 0.01568\n",
      "Training epoch: 2415, train loss: 0.01558, val loss: 0.01575\n",
      "Training epoch: 2416, train loss: 0.01583, val loss: 0.01607\n",
      "Training epoch: 2417, train loss: 0.01567, val loss: 0.01589\n",
      "Training epoch: 2418, train loss: 0.01569, val loss: 0.01580\n",
      "Training epoch: 2419, train loss: 0.01615, val loss: 0.01625\n",
      "Training epoch: 2420, train loss: 0.01596, val loss: 0.01611\n",
      "Training epoch: 2421, train loss: 0.01585, val loss: 0.01606\n",
      "Training epoch: 2422, train loss: 0.01557, val loss: 0.01572\n",
      "Training epoch: 2423, train loss: 0.01615, val loss: 0.01629\n",
      "Training epoch: 2424, train loss: 0.01578, val loss: 0.01597\n",
      "Training epoch: 2425, train loss: 0.01572, val loss: 0.01582\n",
      "Training epoch: 2426, train loss: 0.01577, val loss: 0.01587\n",
      "Training epoch: 2427, train loss: 0.01606, val loss: 0.01628\n",
      "Training epoch: 2428, train loss: 0.01565, val loss: 0.01578\n",
      "Training epoch: 2429, train loss: 0.01549, val loss: 0.01560\n",
      "Training epoch: 2430, train loss: 0.01548, val loss: 0.01572\n",
      "Training epoch: 2431, train loss: 0.01581, val loss: 0.01587\n",
      "Training epoch: 2432, train loss: 0.01582, val loss: 0.01594\n",
      "Training epoch: 2433, train loss: 0.01547, val loss: 0.01574\n",
      "Training epoch: 2434, train loss: 0.01562, val loss: 0.01585\n",
      "Training epoch: 2435, train loss: 0.01613, val loss: 0.01627\n",
      "Training epoch: 2436, train loss: 0.01672, val loss: 0.01688\n",
      "Training epoch: 2437, train loss: 0.01750, val loss: 0.01776\n",
      "Training epoch: 2438, train loss: 0.01668, val loss: 0.01694\n",
      "Training epoch: 2439, train loss: 0.01564, val loss: 0.01588\n",
      "Training epoch: 2440, train loss: 0.01595, val loss: 0.01615\n",
      "Training epoch: 2441, train loss: 0.01625, val loss: 0.01640\n",
      "Training epoch: 2442, train loss: 0.01573, val loss: 0.01587\n",
      "Training epoch: 2443, train loss: 0.01570, val loss: 0.01576\n",
      "Training epoch: 2444, train loss: 0.01551, val loss: 0.01570\n",
      "Training epoch: 2445, train loss: 0.01558, val loss: 0.01565\n",
      "Training epoch: 2446, train loss: 0.01550, val loss: 0.01567\n",
      "Training epoch: 2447, train loss: 0.01566, val loss: 0.01595\n",
      "Training epoch: 2448, train loss: 0.01550, val loss: 0.01564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2449, train loss: 0.01640, val loss: 0.01665\n",
      "Training epoch: 2450, train loss: 0.01622, val loss: 0.01644\n",
      "Training epoch: 2451, train loss: 0.01592, val loss: 0.01609\n",
      "Training epoch: 2452, train loss: 0.01607, val loss: 0.01631\n",
      "Training epoch: 2453, train loss: 0.01554, val loss: 0.01569\n",
      "Training epoch: 2454, train loss: 0.01594, val loss: 0.01612\n",
      "Training epoch: 2455, train loss: 0.01601, val loss: 0.01620\n",
      "Training epoch: 2456, train loss: 0.01543, val loss: 0.01554\n",
      "Training epoch: 2457, train loss: 0.01585, val loss: 0.01604\n",
      "Training epoch: 2458, train loss: 0.01594, val loss: 0.01612\n",
      "Training epoch: 2459, train loss: 0.01582, val loss: 0.01597\n",
      "Training epoch: 2460, train loss: 0.01558, val loss: 0.01578\n",
      "Training epoch: 2461, train loss: 0.01584, val loss: 0.01600\n",
      "Training epoch: 2462, train loss: 0.01548, val loss: 0.01565\n",
      "Training epoch: 2463, train loss: 0.01580, val loss: 0.01602\n",
      "Training epoch: 2464, train loss: 0.01565, val loss: 0.01581\n",
      "Training epoch: 2465, train loss: 0.01572, val loss: 0.01596\n",
      "Training epoch: 2466, train loss: 0.01577, val loss: 0.01591\n",
      "Training epoch: 2467, train loss: 0.01593, val loss: 0.01614\n",
      "Training epoch: 2468, train loss: 0.01586, val loss: 0.01605\n",
      "Training epoch: 2469, train loss: 0.01568, val loss: 0.01584\n",
      "Training epoch: 2470, train loss: 0.01591, val loss: 0.01613\n",
      "Training epoch: 2471, train loss: 0.01608, val loss: 0.01626\n",
      "Training epoch: 2472, train loss: 0.01556, val loss: 0.01571\n",
      "Training epoch: 2473, train loss: 0.01593, val loss: 0.01605\n",
      "Training epoch: 2474, train loss: 0.01546, val loss: 0.01564\n",
      "Training epoch: 2475, train loss: 0.01578, val loss: 0.01596\n",
      "Training epoch: 2476, train loss: 0.01565, val loss: 0.01579\n",
      "Training epoch: 2477, train loss: 0.01593, val loss: 0.01595\n",
      "Training epoch: 2478, train loss: 0.01555, val loss: 0.01574\n",
      "Training epoch: 2479, train loss: 0.01572, val loss: 0.01593\n",
      "Training epoch: 2480, train loss: 0.01575, val loss: 0.01585\n",
      "Training epoch: 2481, train loss: 0.01563, val loss: 0.01577\n",
      "Training epoch: 2482, train loss: 0.01571, val loss: 0.01578\n",
      "Training epoch: 2483, train loss: 0.01588, val loss: 0.01607\n",
      "Training epoch: 2484, train loss: 0.01565, val loss: 0.01587\n",
      "Training epoch: 2485, train loss: 0.01557, val loss: 0.01575\n",
      "Training epoch: 2486, train loss: 0.01589, val loss: 0.01613\n",
      "Training epoch: 2487, train loss: 0.01594, val loss: 0.01619\n",
      "Training epoch: 2488, train loss: 0.01564, val loss: 0.01577\n",
      "Training epoch: 2489, train loss: 0.01566, val loss: 0.01590\n",
      "Training epoch: 2490, train loss: 0.01572, val loss: 0.01586\n",
      "Training epoch: 2491, train loss: 0.01583, val loss: 0.01600\n",
      "Training epoch: 2492, train loss: 0.01570, val loss: 0.01584\n",
      "Training epoch: 2493, train loss: 0.01567, val loss: 0.01594\n",
      "Training epoch: 2494, train loss: 0.01573, val loss: 0.01589\n",
      "Training epoch: 2495, train loss: 0.01614, val loss: 0.01638\n",
      "Training epoch: 2496, train loss: 0.01556, val loss: 0.01569\n",
      "Training epoch: 2497, train loss: 0.01544, val loss: 0.01561\n",
      "Training epoch: 2498, train loss: 0.01556, val loss: 0.01562\n",
      "Training epoch: 2499, train loss: 0.01551, val loss: 0.01570\n",
      "Training epoch: 2500, train loss: 0.01580, val loss: 0.01594\n",
      "Training epoch: 2501, train loss: 0.01574, val loss: 0.01595\n",
      "Training epoch: 2502, train loss: 0.01580, val loss: 0.01594\n",
      "Training epoch: 2503, train loss: 0.01606, val loss: 0.01614\n",
      "Training epoch: 2504, train loss: 0.01594, val loss: 0.01613\n",
      "Training epoch: 2505, train loss: 0.01569, val loss: 0.01589\n",
      "Training epoch: 2506, train loss: 0.01550, val loss: 0.01571\n",
      "Training epoch: 2507, train loss: 0.01552, val loss: 0.01561\n",
      "Training epoch: 2508, train loss: 0.01570, val loss: 0.01580\n",
      "Training epoch: 2509, train loss: 0.01575, val loss: 0.01587\n",
      "Training epoch: 2510, train loss: 0.01548, val loss: 0.01568\n",
      "Training epoch: 2511, train loss: 0.01544, val loss: 0.01559\n",
      "Training epoch: 2512, train loss: 0.01567, val loss: 0.01592\n",
      "Training epoch: 2513, train loss: 0.01550, val loss: 0.01561\n",
      "Training epoch: 2514, train loss: 0.01574, val loss: 0.01591\n",
      "Training epoch: 2515, train loss: 0.01561, val loss: 0.01577\n",
      "Training epoch: 2516, train loss: 0.01573, val loss: 0.01594\n",
      "Training epoch: 2517, train loss: 0.01554, val loss: 0.01573\n",
      "Training epoch: 2518, train loss: 0.01595, val loss: 0.01617\n",
      "Training epoch: 2519, train loss: 0.01564, val loss: 0.01571\n",
      "Training epoch: 2520, train loss: 0.01587, val loss: 0.01604\n",
      "Training epoch: 2521, train loss: 0.01583, val loss: 0.01601\n",
      "Training epoch: 2522, train loss: 0.01591, val loss: 0.01612\n",
      "Training epoch: 2523, train loss: 0.01593, val loss: 0.01602\n",
      "Training epoch: 2524, train loss: 0.01576, val loss: 0.01591\n",
      "Training epoch: 2525, train loss: 0.01606, val loss: 0.01623\n",
      "Training epoch: 2526, train loss: 0.01582, val loss: 0.01608\n",
      "Training epoch: 2527, train loss: 0.01583, val loss: 0.01604\n",
      "Training epoch: 2528, train loss: 0.01577, val loss: 0.01594\n",
      "Training epoch: 2529, train loss: 0.01637, val loss: 0.01671\n",
      "Training epoch: 2530, train loss: 0.01626, val loss: 0.01645\n",
      "Training epoch: 2531, train loss: 0.01562, val loss: 0.01568\n",
      "Training epoch: 2532, train loss: 0.01570, val loss: 0.01588\n",
      "Training epoch: 2533, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 2534, train loss: 0.01573, val loss: 0.01600\n",
      "Training epoch: 2535, train loss: 0.01608, val loss: 0.01622\n",
      "Training epoch: 2536, train loss: 0.01576, val loss: 0.01595\n",
      "Training epoch: 2537, train loss: 0.01563, val loss: 0.01582\n",
      "Training epoch: 2538, train loss: 0.01594, val loss: 0.01622\n",
      "Training epoch: 2539, train loss: 0.01663, val loss: 0.01677\n",
      "Training epoch: 2540, train loss: 0.01561, val loss: 0.01576\n",
      "Training epoch: 2541, train loss: 0.01585, val loss: 0.01602\n",
      "Training epoch: 2542, train loss: 0.01583, val loss: 0.01601\n",
      "Training epoch: 2543, train loss: 0.01566, val loss: 0.01579\n",
      "Training epoch: 2544, train loss: 0.01559, val loss: 0.01571\n",
      "Training epoch: 2545, train loss: 0.01588, val loss: 0.01593\n",
      "Training epoch: 2546, train loss: 0.01570, val loss: 0.01586\n",
      "Training epoch: 2547, train loss: 0.01561, val loss: 0.01586\n",
      "Training epoch: 2548, train loss: 0.01586, val loss: 0.01604\n",
      "Training epoch: 2549, train loss: 0.01571, val loss: 0.01602\n",
      "Training epoch: 2550, train loss: 0.01553, val loss: 0.01579\n",
      "Training epoch: 2551, train loss: 0.01580, val loss: 0.01605\n",
      "Training epoch: 2552, train loss: 0.01541, val loss: 0.01563\n",
      "Training epoch: 2553, train loss: 0.01624, val loss: 0.01642\n",
      "Training epoch: 2554, train loss: 0.01556, val loss: 0.01578\n",
      "Training epoch: 2555, train loss: 0.01552, val loss: 0.01574\n",
      "Training epoch: 2556, train loss: 0.01585, val loss: 0.01595\n",
      "Training epoch: 2557, train loss: 0.01586, val loss: 0.01599\n",
      "Training epoch: 2558, train loss: 0.01568, val loss: 0.01583\n",
      "Training epoch: 2559, train loss: 0.01569, val loss: 0.01582\n",
      "Training epoch: 2560, train loss: 0.01562, val loss: 0.01582\n",
      "Training epoch: 2561, train loss: 0.01578, val loss: 0.01603\n",
      "Training epoch: 2562, train loss: 0.01600, val loss: 0.01622\n",
      "Training epoch: 2563, train loss: 0.01612, val loss: 0.01626\n",
      "Training epoch: 2564, train loss: 0.01569, val loss: 0.01592\n",
      "Training epoch: 2565, train loss: 0.01572, val loss: 0.01587\n",
      "Training epoch: 2566, train loss: 0.01579, val loss: 0.01588\n",
      "Training epoch: 2567, train loss: 0.01639, val loss: 0.01652\n",
      "Training epoch: 2568, train loss: 0.01554, val loss: 0.01566\n",
      "Training epoch: 2569, train loss: 0.01553, val loss: 0.01568\n",
      "Training epoch: 2570, train loss: 0.01573, val loss: 0.01590\n",
      "Training epoch: 2571, train loss: 0.01583, val loss: 0.01597\n",
      "Training epoch: 2572, train loss: 0.01576, val loss: 0.01591\n",
      "Training epoch: 2573, train loss: 0.01612, val loss: 0.01629\n",
      "Training epoch: 2574, train loss: 0.01624, val loss: 0.01630\n",
      "Training epoch: 2575, train loss: 0.01564, val loss: 0.01583\n",
      "Training epoch: 2576, train loss: 0.01574, val loss: 0.01590\n",
      "Training epoch: 2577, train loss: 0.01592, val loss: 0.01610\n",
      "Training epoch: 2578, train loss: 0.01631, val loss: 0.01659\n",
      "Training epoch: 2579, train loss: 0.01559, val loss: 0.01573\n",
      "Training epoch: 2580, train loss: 0.01624, val loss: 0.01644\n",
      "Training epoch: 2581, train loss: 0.01554, val loss: 0.01578\n",
      "Training epoch: 2582, train loss: 0.01571, val loss: 0.01576\n",
      "Training epoch: 2583, train loss: 0.01561, val loss: 0.01582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2584, train loss: 0.01611, val loss: 0.01634\n",
      "Training epoch: 2585, train loss: 0.01568, val loss: 0.01583\n",
      "Training epoch: 2586, train loss: 0.01571, val loss: 0.01584\n",
      "Training epoch: 2587, train loss: 0.01564, val loss: 0.01571\n",
      "Training epoch: 2588, train loss: 0.01553, val loss: 0.01563\n",
      "Training epoch: 2589, train loss: 0.01577, val loss: 0.01590\n",
      "Training epoch: 2590, train loss: 0.01558, val loss: 0.01583\n",
      "Training epoch: 2591, train loss: 0.01571, val loss: 0.01585\n",
      "Training epoch: 2592, train loss: 0.01580, val loss: 0.01593\n",
      "Training epoch: 2593, train loss: 0.01577, val loss: 0.01592\n",
      "Training epoch: 2594, train loss: 0.01578, val loss: 0.01589\n",
      "Training epoch: 2595, train loss: 0.01598, val loss: 0.01607\n",
      "Training epoch: 2596, train loss: 0.01558, val loss: 0.01570\n",
      "Training epoch: 2597, train loss: 0.01629, val loss: 0.01654\n",
      "Training epoch: 2598, train loss: 0.01592, val loss: 0.01616\n",
      "Training epoch: 2599, train loss: 0.01594, val loss: 0.01603\n",
      "Training epoch: 2600, train loss: 0.01574, val loss: 0.01601\n",
      "Training epoch: 2601, train loss: 0.01572, val loss: 0.01592\n",
      "Training epoch: 2602, train loss: 0.01567, val loss: 0.01585\n",
      "Training epoch: 2603, train loss: 0.01585, val loss: 0.01599\n",
      "Training epoch: 2604, train loss: 0.01557, val loss: 0.01575\n",
      "Training epoch: 2605, train loss: 0.01570, val loss: 0.01579\n",
      "Training epoch: 2606, train loss: 0.01566, val loss: 0.01577\n",
      "Training epoch: 2607, train loss: 0.01580, val loss: 0.01590\n",
      "Training epoch: 2608, train loss: 0.01577, val loss: 0.01593\n",
      "Training epoch: 2609, train loss: 0.01583, val loss: 0.01604\n",
      "Training epoch: 2610, train loss: 0.01575, val loss: 0.01595\n",
      "Training epoch: 2611, train loss: 0.01560, val loss: 0.01573\n",
      "Training epoch: 2612, train loss: 0.01561, val loss: 0.01573\n",
      "Training epoch: 2613, train loss: 0.01561, val loss: 0.01576\n",
      "Training epoch: 2614, train loss: 0.01557, val loss: 0.01564\n",
      "Training epoch: 2615, train loss: 0.01576, val loss: 0.01585\n",
      "Training epoch: 2616, train loss: 0.01559, val loss: 0.01570\n",
      "Training epoch: 2617, train loss: 0.01595, val loss: 0.01617\n",
      "Training epoch: 2618, train loss: 0.01546, val loss: 0.01566\n",
      "Training epoch: 2619, train loss: 0.01570, val loss: 0.01586\n",
      "Training epoch: 2620, train loss: 0.01622, val loss: 0.01638\n",
      "Training epoch: 2621, train loss: 0.01589, val loss: 0.01615\n",
      "Training epoch: 2622, train loss: 0.01584, val loss: 0.01593\n",
      "Training epoch: 2623, train loss: 0.01579, val loss: 0.01585\n",
      "Training epoch: 2624, train loss: 0.01582, val loss: 0.01596\n",
      "Training epoch: 2625, train loss: 0.01559, val loss: 0.01583\n",
      "Training epoch: 2626, train loss: 0.01567, val loss: 0.01570\n",
      "Training epoch: 2627, train loss: 0.01560, val loss: 0.01573\n",
      "Training epoch: 2628, train loss: 0.01578, val loss: 0.01589\n",
      "Training epoch: 2629, train loss: 0.01548, val loss: 0.01572\n",
      "Training epoch: 2630, train loss: 0.01570, val loss: 0.01582\n",
      "Training epoch: 2631, train loss: 0.01559, val loss: 0.01571\n",
      "Training epoch: 2632, train loss: 0.01582, val loss: 0.01585\n",
      "Training epoch: 2633, train loss: 0.01581, val loss: 0.01596\n",
      "Training epoch: 2634, train loss: 0.01564, val loss: 0.01576\n",
      "Training epoch: 2635, train loss: 0.01571, val loss: 0.01586\n",
      "Training epoch: 2636, train loss: 0.01553, val loss: 0.01573\n",
      "Training epoch: 2637, train loss: 0.01559, val loss: 0.01572\n",
      "Training epoch: 2638, train loss: 0.01549, val loss: 0.01563\n",
      "Training epoch: 2639, train loss: 0.01618, val loss: 0.01634\n",
      "Training epoch: 2640, train loss: 0.01576, val loss: 0.01578\n",
      "Training epoch: 2641, train loss: 0.01574, val loss: 0.01581\n",
      "Training epoch: 2642, train loss: 0.01571, val loss: 0.01593\n",
      "Training epoch: 2643, train loss: 0.01571, val loss: 0.01581\n",
      "Training epoch: 2644, train loss: 0.01561, val loss: 0.01579\n",
      "Training epoch: 2645, train loss: 0.01599, val loss: 0.01620\n",
      "Training epoch: 2646, train loss: 0.01551, val loss: 0.01563\n",
      "Training epoch: 2647, train loss: 0.01563, val loss: 0.01588\n",
      "Training epoch: 2648, train loss: 0.01595, val loss: 0.01608\n",
      "Training epoch: 2649, train loss: 0.01583, val loss: 0.01602\n",
      "Training epoch: 2650, train loss: 0.01607, val loss: 0.01620\n",
      "Training epoch: 2651, train loss: 0.01544, val loss: 0.01556\n",
      "Training epoch: 2652, train loss: 0.01625, val loss: 0.01646\n",
      "Training epoch: 2653, train loss: 0.01553, val loss: 0.01562\n",
      "Training epoch: 2654, train loss: 0.01568, val loss: 0.01581\n",
      "Training epoch: 2655, train loss: 0.01546, val loss: 0.01558\n",
      "Training epoch: 2656, train loss: 0.01561, val loss: 0.01579\n",
      "Training epoch: 2657, train loss: 0.01559, val loss: 0.01578\n",
      "Training epoch: 2658, train loss: 0.01558, val loss: 0.01577\n",
      "Training epoch: 2659, train loss: 0.01556, val loss: 0.01566\n",
      "Training epoch: 2660, train loss: 0.01561, val loss: 0.01575\n",
      "Training epoch: 2661, train loss: 0.01574, val loss: 0.01583\n",
      "Training epoch: 2662, train loss: 0.01576, val loss: 0.01601\n",
      "Training epoch: 2663, train loss: 0.01616, val loss: 0.01626\n",
      "Training epoch: 2664, train loss: 0.01617, val loss: 0.01632\n",
      "Training epoch: 2665, train loss: 0.01631, val loss: 0.01658\n",
      "Training epoch: 2666, train loss: 0.01594, val loss: 0.01598\n",
      "Training epoch: 2667, train loss: 0.01608, val loss: 0.01626\n",
      "Training epoch: 2668, train loss: 0.01596, val loss: 0.01621\n",
      "Training epoch: 2669, train loss: 0.01567, val loss: 0.01576\n",
      "Training epoch: 2670, train loss: 0.01548, val loss: 0.01557\n",
      "Training epoch: 2671, train loss: 0.01570, val loss: 0.01590\n",
      "Training epoch: 2672, train loss: 0.01564, val loss: 0.01575\n",
      "Training epoch: 2673, train loss: 0.01574, val loss: 0.01589\n",
      "Training epoch: 2674, train loss: 0.01570, val loss: 0.01590\n",
      "Training epoch: 2675, train loss: 0.01554, val loss: 0.01572\n",
      "Training epoch: 2676, train loss: 0.01551, val loss: 0.01569\n",
      "Training epoch: 2677, train loss: 0.01559, val loss: 0.01568\n",
      "Training epoch: 2678, train loss: 0.01561, val loss: 0.01568\n",
      "Training epoch: 2679, train loss: 0.01556, val loss: 0.01582\n",
      "Training epoch: 2680, train loss: 0.01559, val loss: 0.01576\n",
      "Training epoch: 2681, train loss: 0.01580, val loss: 0.01596\n",
      "Training epoch: 2682, train loss: 0.01579, val loss: 0.01594\n",
      "Training epoch: 2683, train loss: 0.01570, val loss: 0.01589\n",
      "Training epoch: 2684, train loss: 0.01559, val loss: 0.01575\n",
      "Training epoch: 2685, train loss: 0.01569, val loss: 0.01581\n",
      "Training epoch: 2686, train loss: 0.01572, val loss: 0.01576\n",
      "Training epoch: 2687, train loss: 0.01587, val loss: 0.01606\n",
      "Training epoch: 2688, train loss: 0.01568, val loss: 0.01582\n",
      "Training epoch: 2689, train loss: 0.01573, val loss: 0.01595\n",
      "Training epoch: 2690, train loss: 0.01564, val loss: 0.01575\n",
      "Training epoch: 2691, train loss: 0.01557, val loss: 0.01570\n",
      "Training epoch: 2692, train loss: 0.01602, val loss: 0.01608\n",
      "Training epoch: 2693, train loss: 0.01566, val loss: 0.01583\n",
      "Training epoch: 2694, train loss: 0.01579, val loss: 0.01595\n",
      "Training epoch: 2695, train loss: 0.01578, val loss: 0.01600\n",
      "Training epoch: 2696, train loss: 0.01564, val loss: 0.01585\n",
      "Training epoch: 2697, train loss: 0.01562, val loss: 0.01581\n",
      "Training epoch: 2698, train loss: 0.01572, val loss: 0.01585\n",
      "Training epoch: 2699, train loss: 0.01568, val loss: 0.01583\n",
      "Training epoch: 2700, train loss: 0.01599, val loss: 0.01620\n",
      "Training epoch: 2701, train loss: 0.01602, val loss: 0.01618\n",
      "Training epoch: 2702, train loss: 0.01648, val loss: 0.01665\n",
      "Training epoch: 2703, train loss: 0.01563, val loss: 0.01589\n",
      "Training epoch: 2704, train loss: 0.01622, val loss: 0.01641\n",
      "Training epoch: 2705, train loss: 0.01591, val loss: 0.01607\n",
      "Training epoch: 2706, train loss: 0.01555, val loss: 0.01577\n",
      "Training epoch: 2707, train loss: 0.01596, val loss: 0.01598\n",
      "Training epoch: 2708, train loss: 0.01572, val loss: 0.01593\n",
      "Training epoch: 2709, train loss: 0.01600, val loss: 0.01619\n",
      "Training epoch: 2710, train loss: 0.01573, val loss: 0.01597\n",
      "Training epoch: 2711, train loss: 0.01596, val loss: 0.01616\n",
      "Training epoch: 2712, train loss: 0.01563, val loss: 0.01582\n",
      "Training epoch: 2713, train loss: 0.01584, val loss: 0.01593\n",
      "Training epoch: 2714, train loss: 0.01552, val loss: 0.01571\n",
      "Training epoch: 2715, train loss: 0.01567, val loss: 0.01570\n",
      "Training epoch: 2716, train loss: 0.01559, val loss: 0.01579\n",
      "Training epoch: 2717, train loss: 0.01563, val loss: 0.01570\n",
      "Training epoch: 2718, train loss: 0.01592, val loss: 0.01606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2719, train loss: 0.01558, val loss: 0.01580\n",
      "Training epoch: 2720, train loss: 0.01569, val loss: 0.01593\n",
      "Training epoch: 2721, train loss: 0.01562, val loss: 0.01575\n",
      "Training epoch: 2722, train loss: 0.01548, val loss: 0.01563\n",
      "Training epoch: 2723, train loss: 0.01604, val loss: 0.01627\n",
      "Training epoch: 2724, train loss: 0.01594, val loss: 0.01606\n",
      "Training epoch: 2725, train loss: 0.01586, val loss: 0.01608\n",
      "Training epoch: 2726, train loss: 0.01549, val loss: 0.01568\n",
      "Training epoch: 2727, train loss: 0.01562, val loss: 0.01580\n",
      "Training epoch: 2728, train loss: 0.01570, val loss: 0.01580\n",
      "Training epoch: 2729, train loss: 0.01550, val loss: 0.01558\n",
      "Training epoch: 2730, train loss: 0.01577, val loss: 0.01592\n",
      "Training epoch: 2731, train loss: 0.01555, val loss: 0.01568\n",
      "Training epoch: 2732, train loss: 0.01554, val loss: 0.01572\n",
      "Training epoch: 2733, train loss: 0.01551, val loss: 0.01563\n",
      "Training epoch: 2734, train loss: 0.01571, val loss: 0.01587\n",
      "Training epoch: 2735, train loss: 0.01563, val loss: 0.01573\n",
      "Training epoch: 2736, train loss: 0.01583, val loss: 0.01603\n",
      "Training epoch: 2737, train loss: 0.01575, val loss: 0.01585\n",
      "Training epoch: 2738, train loss: 0.01583, val loss: 0.01593\n",
      "Training epoch: 2739, train loss: 0.01564, val loss: 0.01576\n",
      "Training epoch: 2740, train loss: 0.01582, val loss: 0.01592\n",
      "Training epoch: 2741, train loss: 0.01560, val loss: 0.01573\n",
      "Training epoch: 2742, train loss: 0.01577, val loss: 0.01597\n",
      "Training epoch: 2743, train loss: 0.01578, val loss: 0.01595\n",
      "Training epoch: 2744, train loss: 0.01563, val loss: 0.01581\n",
      "Training epoch: 2745, train loss: 0.01576, val loss: 0.01590\n",
      "Training epoch: 2746, train loss: 0.01655, val loss: 0.01662\n",
      "Training epoch: 2747, train loss: 0.01557, val loss: 0.01564\n",
      "Training epoch: 2748, train loss: 0.01558, val loss: 0.01572\n",
      "Training epoch: 2749, train loss: 0.01557, val loss: 0.01565\n",
      "Training epoch: 2750, train loss: 0.01591, val loss: 0.01613\n",
      "Training epoch: 2751, train loss: 0.01567, val loss: 0.01577\n",
      "Training epoch: 2752, train loss: 0.01555, val loss: 0.01562\n",
      "Training epoch: 2753, train loss: 0.01550, val loss: 0.01571\n",
      "Training epoch: 2754, train loss: 0.01562, val loss: 0.01583\n",
      "Training epoch: 2755, train loss: 0.01570, val loss: 0.01584\n",
      "Training epoch: 2756, train loss: 0.01590, val loss: 0.01601\n",
      "Training epoch: 2757, train loss: 0.01577, val loss: 0.01586\n",
      "Training epoch: 2758, train loss: 0.01556, val loss: 0.01572\n",
      "Training epoch: 2759, train loss: 0.01551, val loss: 0.01573\n",
      "Training epoch: 2760, train loss: 0.01550, val loss: 0.01561\n",
      "Training epoch: 2761, train loss: 0.01564, val loss: 0.01570\n",
      "Training epoch: 2762, train loss: 0.01556, val loss: 0.01571\n",
      "Training epoch: 2763, train loss: 0.01563, val loss: 0.01581\n",
      "Training epoch: 2764, train loss: 0.01604, val loss: 0.01623\n",
      "Training epoch: 2765, train loss: 0.01555, val loss: 0.01575\n",
      "Training epoch: 2766, train loss: 0.01543, val loss: 0.01556\n",
      "Training epoch: 2767, train loss: 0.01565, val loss: 0.01567\n",
      "Training epoch: 2768, train loss: 0.01581, val loss: 0.01598\n",
      "Training epoch: 2769, train loss: 0.01587, val loss: 0.01589\n",
      "Training epoch: 2770, train loss: 0.01565, val loss: 0.01568\n",
      "Training epoch: 2771, train loss: 0.01551, val loss: 0.01563\n",
      "Training epoch: 2772, train loss: 0.01543, val loss: 0.01562\n",
      "Training epoch: 2773, train loss: 0.01547, val loss: 0.01560\n",
      "Training epoch: 2774, train loss: 0.01581, val loss: 0.01594\n",
      "Training epoch: 2775, train loss: 0.01602, val loss: 0.01626\n",
      "Training epoch: 2776, train loss: 0.01620, val loss: 0.01655\n",
      "Training epoch: 2777, train loss: 0.01601, val loss: 0.01613\n",
      "Training epoch: 2778, train loss: 0.01598, val loss: 0.01610\n",
      "Training epoch: 2779, train loss: 0.01563, val loss: 0.01575\n",
      "Training epoch: 2780, train loss: 0.01560, val loss: 0.01576\n",
      "Training epoch: 2781, train loss: 0.01567, val loss: 0.01581\n",
      "Training epoch: 2782, train loss: 0.01591, val loss: 0.01606\n",
      "Training epoch: 2783, train loss: 0.01561, val loss: 0.01578\n",
      "Training epoch: 2784, train loss: 0.01562, val loss: 0.01578\n",
      "Training epoch: 2785, train loss: 0.01571, val loss: 0.01589\n",
      "Training epoch: 2786, train loss: 0.01577, val loss: 0.01596\n",
      "Training epoch: 2787, train loss: 0.01556, val loss: 0.01576\n",
      "Training epoch: 2788, train loss: 0.01543, val loss: 0.01560\n",
      "Training epoch: 2789, train loss: 0.01600, val loss: 0.01618\n",
      "Training epoch: 2790, train loss: 0.01546, val loss: 0.01565\n",
      "Training epoch: 2791, train loss: 0.01559, val loss: 0.01573\n",
      "Training epoch: 2792, train loss: 0.01599, val loss: 0.01602\n",
      "Training epoch: 2793, train loss: 0.01566, val loss: 0.01583\n",
      "Training epoch: 2794, train loss: 0.01557, val loss: 0.01562\n",
      "Training epoch: 2795, train loss: 0.01549, val loss: 0.01562\n",
      "Training epoch: 2796, train loss: 0.01572, val loss: 0.01598\n",
      "Training epoch: 2797, train loss: 0.01595, val loss: 0.01601\n",
      "Training epoch: 2798, train loss: 0.01570, val loss: 0.01582\n",
      "Training epoch: 2799, train loss: 0.01563, val loss: 0.01572\n",
      "Training epoch: 2800, train loss: 0.01565, val loss: 0.01574\n",
      "Training epoch: 2801, train loss: 0.01569, val loss: 0.01587\n",
      "Training epoch: 2802, train loss: 0.01556, val loss: 0.01576\n",
      "Training epoch: 2803, train loss: 0.01563, val loss: 0.01581\n",
      "Training epoch: 2804, train loss: 0.01564, val loss: 0.01584\n",
      "Training epoch: 2805, train loss: 0.01571, val loss: 0.01595\n",
      "Training epoch: 2806, train loss: 0.01556, val loss: 0.01569\n",
      "Training epoch: 2807, train loss: 0.01559, val loss: 0.01568\n",
      "Training epoch: 2808, train loss: 0.01573, val loss: 0.01584\n",
      "Training epoch: 2809, train loss: 0.01591, val loss: 0.01623\n",
      "Training epoch: 2810, train loss: 0.01573, val loss: 0.01579\n",
      "Training epoch: 2811, train loss: 0.01567, val loss: 0.01582\n",
      "Training epoch: 2812, train loss: 0.01548, val loss: 0.01562\n",
      "Training epoch: 2813, train loss: 0.01560, val loss: 0.01573\n",
      "Training epoch: 2814, train loss: 0.01709, val loss: 0.01706\n",
      "Training epoch: 2815, train loss: 0.01570, val loss: 0.01583\n",
      "Training epoch: 2816, train loss: 0.01550, val loss: 0.01567\n",
      "Training epoch: 2817, train loss: 0.01565, val loss: 0.01580\n",
      "Training epoch: 2818, train loss: 0.01587, val loss: 0.01596\n",
      "Training epoch: 2819, train loss: 0.01569, val loss: 0.01581\n",
      "Training epoch: 2820, train loss: 0.01585, val loss: 0.01598\n",
      "Training epoch: 2821, train loss: 0.01558, val loss: 0.01566\n",
      "Training epoch: 2822, train loss: 0.01576, val loss: 0.01587\n",
      "Training epoch: 2823, train loss: 0.01669, val loss: 0.01699\n",
      "Training epoch: 2824, train loss: 0.01598, val loss: 0.01615\n",
      "Training epoch: 2825, train loss: 0.01556, val loss: 0.01566\n",
      "Training epoch: 2826, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 2827, train loss: 0.01566, val loss: 0.01581\n",
      "Training epoch: 2828, train loss: 0.01561, val loss: 0.01574\n",
      "Training epoch: 2829, train loss: 0.01607, val loss: 0.01633\n",
      "Training epoch: 2830, train loss: 0.01580, val loss: 0.01593\n",
      "Training epoch: 2831, train loss: 0.01584, val loss: 0.01598\n",
      "Training epoch: 2832, train loss: 0.01599, val loss: 0.01604\n",
      "Training epoch: 2833, train loss: 0.01605, val loss: 0.01611\n",
      "Training epoch: 2834, train loss: 0.01582, val loss: 0.01607\n",
      "Training epoch: 2835, train loss: 0.01560, val loss: 0.01581\n",
      "Training epoch: 2836, train loss: 0.01554, val loss: 0.01568\n",
      "Training epoch: 2837, train loss: 0.01591, val loss: 0.01608\n",
      "Training epoch: 2838, train loss: 0.01548, val loss: 0.01557\n",
      "Training epoch: 2839, train loss: 0.01589, val loss: 0.01598\n",
      "Training epoch: 2840, train loss: 0.01556, val loss: 0.01569\n",
      "Training epoch: 2841, train loss: 0.01557, val loss: 0.01564\n",
      "Training epoch: 2842, train loss: 0.01574, val loss: 0.01592\n",
      "Training epoch: 2843, train loss: 0.01594, val loss: 0.01619\n",
      "Training epoch: 2844, train loss: 0.01553, val loss: 0.01574\n",
      "Training epoch: 2845, train loss: 0.01572, val loss: 0.01587\n",
      "Training epoch: 2846, train loss: 0.01550, val loss: 0.01560\n",
      "Training epoch: 2847, train loss: 0.01578, val loss: 0.01589\n",
      "Training epoch: 2848, train loss: 0.01560, val loss: 0.01576\n",
      "Training epoch: 2849, train loss: 0.01660, val loss: 0.01659\n",
      "Training epoch: 2850, train loss: 0.01576, val loss: 0.01605\n",
      "Training epoch: 2851, train loss: 0.01600, val loss: 0.01615\n",
      "Training epoch: 2852, train loss: 0.01561, val loss: 0.01582\n",
      "Training epoch: 2853, train loss: 0.01573, val loss: 0.01600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2854, train loss: 0.01604, val loss: 0.01611\n",
      "Training epoch: 2855, train loss: 0.01562, val loss: 0.01579\n",
      "Training epoch: 2856, train loss: 0.01551, val loss: 0.01562\n",
      "Training epoch: 2857, train loss: 0.01563, val loss: 0.01592\n",
      "Training epoch: 2858, train loss: 0.01545, val loss: 0.01564\n",
      "Training epoch: 2859, train loss: 0.01562, val loss: 0.01573\n",
      "Training epoch: 2860, train loss: 0.01555, val loss: 0.01571\n",
      "Training epoch: 2861, train loss: 0.01571, val loss: 0.01577\n",
      "Training epoch: 2862, train loss: 0.01557, val loss: 0.01567\n",
      "Training epoch: 2863, train loss: 0.01570, val loss: 0.01578\n",
      "Training epoch: 2864, train loss: 0.01590, val loss: 0.01613\n",
      "Training epoch: 2865, train loss: 0.01656, val loss: 0.01685\n",
      "Training epoch: 2866, train loss: 0.01559, val loss: 0.01573\n",
      "Training epoch: 2867, train loss: 0.01589, val loss: 0.01591\n",
      "Training epoch: 2868, train loss: 0.01558, val loss: 0.01571\n",
      "Training epoch: 2869, train loss: 0.01575, val loss: 0.01586\n",
      "Training epoch: 2870, train loss: 0.01570, val loss: 0.01586\n",
      "Training epoch: 2871, train loss: 0.01565, val loss: 0.01577\n",
      "Training epoch: 2872, train loss: 0.01598, val loss: 0.01608\n",
      "Training epoch: 2873, train loss: 0.01573, val loss: 0.01584\n",
      "Training epoch: 2874, train loss: 0.01569, val loss: 0.01581\n",
      "Training epoch: 2875, train loss: 0.01547, val loss: 0.01559\n",
      "Training epoch: 2876, train loss: 0.01561, val loss: 0.01571\n",
      "Training epoch: 2877, train loss: 0.01571, val loss: 0.01584\n",
      "Training epoch: 2878, train loss: 0.01546, val loss: 0.01555\n",
      "Training epoch: 2879, train loss: 0.01547, val loss: 0.01566\n",
      "Training epoch: 2880, train loss: 0.01601, val loss: 0.01622\n",
      "Training epoch: 2881, train loss: 0.01569, val loss: 0.01594\n",
      "Training epoch: 2882, train loss: 0.01588, val loss: 0.01606\n",
      "Training epoch: 2883, train loss: 0.01557, val loss: 0.01575\n",
      "Training epoch: 2884, train loss: 0.01594, val loss: 0.01610\n",
      "Training epoch: 2885, train loss: 0.01606, val loss: 0.01636\n",
      "Training epoch: 2886, train loss: 0.01613, val loss: 0.01626\n",
      "Training epoch: 2887, train loss: 0.01553, val loss: 0.01562\n",
      "Training epoch: 2888, train loss: 0.01555, val loss: 0.01572\n",
      "Training epoch: 2889, train loss: 0.01618, val loss: 0.01630\n",
      "Training epoch: 2890, train loss: 0.01546, val loss: 0.01562\n",
      "Training epoch: 2891, train loss: 0.01583, val loss: 0.01608\n",
      "Training epoch: 2892, train loss: 0.01554, val loss: 0.01567\n",
      "Training epoch: 2893, train loss: 0.01563, val loss: 0.01586\n",
      "Training epoch: 2894, train loss: 0.01569, val loss: 0.01583\n",
      "Training epoch: 2895, train loss: 0.01574, val loss: 0.01585\n",
      "Training epoch: 2896, train loss: 0.01568, val loss: 0.01575\n",
      "Training epoch: 2897, train loss: 0.01568, val loss: 0.01574\n",
      "Training epoch: 2898, train loss: 0.01568, val loss: 0.01585\n",
      "Training epoch: 2899, train loss: 0.01568, val loss: 0.01583\n",
      "Training epoch: 2900, train loss: 0.01583, val loss: 0.01587\n",
      "Training epoch: 2901, train loss: 0.01606, val loss: 0.01611\n",
      "Training epoch: 2902, train loss: 0.01557, val loss: 0.01567\n",
      "Training epoch: 2903, train loss: 0.01558, val loss: 0.01565\n",
      "Training epoch: 2904, train loss: 0.01564, val loss: 0.01586\n",
      "Training epoch: 2905, train loss: 0.01622, val loss: 0.01646\n",
      "Training epoch: 2906, train loss: 0.01567, val loss: 0.01584\n",
      "Training epoch: 2907, train loss: 0.01561, val loss: 0.01572\n",
      "Training epoch: 2908, train loss: 0.01552, val loss: 0.01566\n",
      "Training epoch: 2909, train loss: 0.01550, val loss: 0.01563\n",
      "Training epoch: 2910, train loss: 0.01558, val loss: 0.01566\n",
      "Training epoch: 2911, train loss: 0.01553, val loss: 0.01563\n",
      "Training epoch: 2912, train loss: 0.01566, val loss: 0.01581\n",
      "Training epoch: 2913, train loss: 0.01544, val loss: 0.01553\n",
      "Training epoch: 2914, train loss: 0.01551, val loss: 0.01566\n",
      "Training epoch: 2915, train loss: 0.01582, val loss: 0.01599\n",
      "Training epoch: 2916, train loss: 0.01545, val loss: 0.01560\n",
      "Training epoch: 2917, train loss: 0.01614, val loss: 0.01625\n",
      "Training epoch: 2918, train loss: 0.01680, val loss: 0.01710\n",
      "Training epoch: 2919, train loss: 0.01549, val loss: 0.01572\n",
      "Training epoch: 2920, train loss: 0.01562, val loss: 0.01588\n",
      "Training epoch: 2921, train loss: 0.01569, val loss: 0.01587\n",
      "Training epoch: 2922, train loss: 0.01562, val loss: 0.01582\n",
      "Training epoch: 2923, train loss: 0.01658, val loss: 0.01680\n",
      "Training epoch: 2924, train loss: 0.01553, val loss: 0.01569\n",
      "Training epoch: 2925, train loss: 0.01564, val loss: 0.01578\n",
      "Training epoch: 2926, train loss: 0.01595, val loss: 0.01608\n",
      "Training epoch: 2927, train loss: 0.01544, val loss: 0.01571\n",
      "Training epoch: 2928, train loss: 0.01607, val loss: 0.01613\n",
      "Training epoch: 2929, train loss: 0.01557, val loss: 0.01569\n",
      "Training epoch: 2930, train loss: 0.01560, val loss: 0.01565\n",
      "Training epoch: 2931, train loss: 0.01584, val loss: 0.01595\n",
      "Training epoch: 2932, train loss: 0.01571, val loss: 0.01585\n",
      "Training epoch: 2933, train loss: 0.01600, val loss: 0.01625\n",
      "Training epoch: 2934, train loss: 0.01593, val loss: 0.01612\n",
      "Training epoch: 2935, train loss: 0.01567, val loss: 0.01574\n",
      "Training epoch: 2936, train loss: 0.01559, val loss: 0.01572\n",
      "Training epoch: 2937, train loss: 0.01559, val loss: 0.01573\n",
      "Training epoch: 2938, train loss: 0.01572, val loss: 0.01579\n",
      "Training epoch: 2939, train loss: 0.01564, val loss: 0.01578\n",
      "Training epoch: 2940, train loss: 0.01556, val loss: 0.01571\n",
      "Training epoch: 2941, train loss: 0.01559, val loss: 0.01578\n",
      "Training epoch: 2942, train loss: 0.01559, val loss: 0.01565\n",
      "Training epoch: 2943, train loss: 0.01569, val loss: 0.01584\n",
      "Training epoch: 2944, train loss: 0.01580, val loss: 0.01585\n",
      "Training epoch: 2945, train loss: 0.01553, val loss: 0.01577\n",
      "Training epoch: 2946, train loss: 0.01552, val loss: 0.01569\n",
      "Training epoch: 2947, train loss: 0.01560, val loss: 0.01577\n",
      "Training epoch: 2948, train loss: 0.01563, val loss: 0.01569\n",
      "Training epoch: 2949, train loss: 0.01567, val loss: 0.01580\n",
      "Training epoch: 2950, train loss: 0.01589, val loss: 0.01603\n",
      "Training epoch: 2951, train loss: 0.01548, val loss: 0.01563\n",
      "Training epoch: 2952, train loss: 0.01560, val loss: 0.01575\n",
      "Training epoch: 2953, train loss: 0.01554, val loss: 0.01561\n",
      "Training epoch: 2954, train loss: 0.01560, val loss: 0.01573\n",
      "Training epoch: 2955, train loss: 0.01553, val loss: 0.01563\n",
      "Training epoch: 2956, train loss: 0.01555, val loss: 0.01573\n",
      "Training epoch: 2957, train loss: 0.01563, val loss: 0.01573\n",
      "Training epoch: 2958, train loss: 0.01589, val loss: 0.01606\n",
      "Training epoch: 2959, train loss: 0.01564, val loss: 0.01577\n",
      "Training epoch: 2960, train loss: 0.01584, val loss: 0.01589\n",
      "Training epoch: 2961, train loss: 0.01586, val loss: 0.01597\n",
      "Training epoch: 2962, train loss: 0.01566, val loss: 0.01585\n",
      "Training epoch: 2963, train loss: 0.01574, val loss: 0.01591\n",
      "Training epoch: 2964, train loss: 0.01552, val loss: 0.01569\n",
      "Training epoch: 2965, train loss: 0.01572, val loss: 0.01585\n",
      "Training epoch: 2966, train loss: 0.01603, val loss: 0.01632\n",
      "Training epoch: 2967, train loss: 0.01553, val loss: 0.01564\n",
      "Training epoch: 2968, train loss: 0.01574, val loss: 0.01584\n",
      "Training epoch: 2969, train loss: 0.01589, val loss: 0.01611\n",
      "Training epoch: 2970, train loss: 0.01569, val loss: 0.01592\n",
      "Training epoch: 2971, train loss: 0.01552, val loss: 0.01576\n",
      "Training epoch: 2972, train loss: 0.01578, val loss: 0.01594\n",
      "Training epoch: 2973, train loss: 0.01558, val loss: 0.01574\n",
      "Training epoch: 2974, train loss: 0.01582, val loss: 0.01597\n",
      "Training epoch: 2975, train loss: 0.01570, val loss: 0.01579\n",
      "Training epoch: 2976, train loss: 0.01564, val loss: 0.01580\n",
      "Training epoch: 2977, train loss: 0.01558, val loss: 0.01581\n",
      "Training epoch: 2978, train loss: 0.01578, val loss: 0.01600\n",
      "Training epoch: 2979, train loss: 0.01561, val loss: 0.01561\n",
      "Training epoch: 2980, train loss: 0.01569, val loss: 0.01580\n",
      "Training epoch: 2981, train loss: 0.01546, val loss: 0.01562\n",
      "Training epoch: 2982, train loss: 0.01567, val loss: 0.01594\n",
      "Training epoch: 2983, train loss: 0.01577, val loss: 0.01596\n",
      "Training epoch: 2984, train loss: 0.01586, val loss: 0.01599\n",
      "Training epoch: 2985, train loss: 0.01566, val loss: 0.01580\n",
      "Training epoch: 2986, train loss: 0.01575, val loss: 0.01593\n",
      "Training epoch: 2987, train loss: 0.01569, val loss: 0.01576\n",
      "Training epoch: 2988, train loss: 0.01594, val loss: 0.01616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2989, train loss: 0.01605, val loss: 0.01620\n",
      "Training epoch: 2990, train loss: 0.01605, val loss: 0.01618\n",
      "Training epoch: 2991, train loss: 0.01589, val loss: 0.01605\n",
      "Training epoch: 2992, train loss: 0.01598, val loss: 0.01607\n",
      "Training epoch: 2993, train loss: 0.01569, val loss: 0.01574\n",
      "Training epoch: 2994, train loss: 0.01608, val loss: 0.01640\n",
      "Training epoch: 2995, train loss: 0.01552, val loss: 0.01570\n",
      "Training epoch: 2996, train loss: 0.01557, val loss: 0.01566\n",
      "Training epoch: 2997, train loss: 0.01576, val loss: 0.01594\n",
      "Training epoch: 2998, train loss: 0.01577, val loss: 0.01585\n",
      "Training epoch: 2999, train loss: 0.01547, val loss: 0.01560\n",
      "Training epoch: 3000, train loss: 0.01570, val loss: 0.01576\n",
      "Training epoch: 3001, train loss: 0.01541, val loss: 0.01559\n",
      "Training epoch: 3002, train loss: 0.01577, val loss: 0.01587\n",
      "Training epoch: 3003, train loss: 0.01550, val loss: 0.01562\n",
      "Training epoch: 3004, train loss: 0.01616, val loss: 0.01620\n",
      "Training epoch: 3005, train loss: 0.01561, val loss: 0.01578\n",
      "Training epoch: 3006, train loss: 0.01575, val loss: 0.01598\n",
      "Training epoch: 3007, train loss: 0.01607, val loss: 0.01624\n",
      "Training epoch: 3008, train loss: 0.01552, val loss: 0.01571\n",
      "Training epoch: 3009, train loss: 0.01572, val loss: 0.01586\n",
      "Training epoch: 3010, train loss: 0.01562, val loss: 0.01579\n",
      "Training epoch: 3011, train loss: 0.01569, val loss: 0.01576\n",
      "Training epoch: 3012, train loss: 0.01564, val loss: 0.01583\n",
      "Training epoch: 3013, train loss: 0.01554, val loss: 0.01574\n",
      "Training epoch: 3014, train loss: 0.01567, val loss: 0.01575\n",
      "Training epoch: 3015, train loss: 0.01559, val loss: 0.01567\n",
      "Training epoch: 3016, train loss: 0.01551, val loss: 0.01565\n",
      "Training epoch: 3017, train loss: 0.01558, val loss: 0.01572\n",
      "Training epoch: 3018, train loss: 0.01588, val loss: 0.01603\n",
      "Training epoch: 3019, train loss: 0.01580, val loss: 0.01599\n",
      "Training epoch: 3020, train loss: 0.01582, val loss: 0.01598\n",
      "Training epoch: 3021, train loss: 0.01557, val loss: 0.01584\n",
      "Training epoch: 3022, train loss: 0.01585, val loss: 0.01589\n",
      "Training epoch: 3023, train loss: 0.01582, val loss: 0.01591\n",
      "Training epoch: 3024, train loss: 0.01561, val loss: 0.01574\n",
      "Training epoch: 3025, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 3026, train loss: 0.01574, val loss: 0.01589\n",
      "Training epoch: 3027, train loss: 0.01564, val loss: 0.01582\n",
      "Training epoch: 3028, train loss: 0.01564, val loss: 0.01578\n",
      "Training epoch: 3029, train loss: 0.01586, val loss: 0.01601\n",
      "Training epoch: 3030, train loss: 0.01592, val loss: 0.01610\n",
      "Training epoch: 3031, train loss: 0.01557, val loss: 0.01571\n",
      "Training epoch: 3032, train loss: 0.01587, val loss: 0.01607\n",
      "Training epoch: 3033, train loss: 0.01564, val loss: 0.01569\n",
      "Training epoch: 3034, train loss: 0.01553, val loss: 0.01569\n",
      "Training epoch: 3035, train loss: 0.01589, val loss: 0.01608\n",
      "Training epoch: 3036, train loss: 0.01544, val loss: 0.01565\n",
      "Training epoch: 3037, train loss: 0.01555, val loss: 0.01565\n",
      "Training epoch: 3038, train loss: 0.01593, val loss: 0.01606\n",
      "Training epoch: 3039, train loss: 0.01550, val loss: 0.01561\n",
      "Training epoch: 3040, train loss: 0.01559, val loss: 0.01578\n",
      "Training epoch: 3041, train loss: 0.01568, val loss: 0.01580\n",
      "Training epoch: 3042, train loss: 0.01554, val loss: 0.01565\n",
      "Training epoch: 3043, train loss: 0.01548, val loss: 0.01568\n",
      "Training epoch: 3044, train loss: 0.01570, val loss: 0.01587\n",
      "Training epoch: 3045, train loss: 0.01608, val loss: 0.01618\n",
      "Training epoch: 3046, train loss: 0.01582, val loss: 0.01609\n",
      "Training epoch: 3047, train loss: 0.01569, val loss: 0.01586\n",
      "Training epoch: 3048, train loss: 0.01554, val loss: 0.01570\n",
      "Training epoch: 3049, train loss: 0.01573, val loss: 0.01584\n",
      "Training epoch: 3050, train loss: 0.01547, val loss: 0.01558\n",
      "Training epoch: 3051, train loss: 0.01574, val loss: 0.01593\n",
      "Training epoch: 3052, train loss: 0.01591, val loss: 0.01603\n",
      "Training epoch: 3053, train loss: 0.01625, val loss: 0.01652\n",
      "Training epoch: 3054, train loss: 0.01657, val loss: 0.01681\n",
      "Training epoch: 3055, train loss: 0.01560, val loss: 0.01574\n",
      "Training epoch: 3056, train loss: 0.01547, val loss: 0.01557\n",
      "Training epoch: 3057, train loss: 0.01563, val loss: 0.01571\n",
      "Training epoch: 3058, train loss: 0.01599, val loss: 0.01609\n",
      "Training epoch: 3059, train loss: 0.01560, val loss: 0.01576\n",
      "Training epoch: 3060, train loss: 0.01593, val loss: 0.01603\n",
      "Training epoch: 3061, train loss: 0.01632, val loss: 0.01655\n",
      "Training epoch: 3062, train loss: 0.01562, val loss: 0.01567\n",
      "Training epoch: 3063, train loss: 0.01585, val loss: 0.01607\n",
      "Training epoch: 3064, train loss: 0.01543, val loss: 0.01563\n",
      "Training epoch: 3065, train loss: 0.01568, val loss: 0.01580\n",
      "Training epoch: 3066, train loss: 0.01594, val loss: 0.01608\n",
      "Training epoch: 3067, train loss: 0.01558, val loss: 0.01563\n",
      "Training epoch: 3068, train loss: 0.01588, val loss: 0.01598\n",
      "Training epoch: 3069, train loss: 0.01581, val loss: 0.01602\n",
      "Training epoch: 3070, train loss: 0.01569, val loss: 0.01592\n",
      "Training epoch: 3071, train loss: 0.01543, val loss: 0.01558\n",
      "Training epoch: 3072, train loss: 0.01585, val loss: 0.01598\n",
      "Training epoch: 3073, train loss: 0.01574, val loss: 0.01590\n",
      "Training epoch: 3074, train loss: 0.01546, val loss: 0.01561\n",
      "Training epoch: 3075, train loss: 0.01596, val loss: 0.01615\n",
      "Training epoch: 3076, train loss: 0.01608, val loss: 0.01623\n",
      "Training epoch: 3077, train loss: 0.01586, val loss: 0.01600\n",
      "Training epoch: 3078, train loss: 0.01585, val loss: 0.01601\n",
      "Training epoch: 3079, train loss: 0.01553, val loss: 0.01565\n",
      "Training epoch: 3080, train loss: 0.01551, val loss: 0.01554\n",
      "Training epoch: 3081, train loss: 0.01546, val loss: 0.01561\n",
      "Training epoch: 3082, train loss: 0.01577, val loss: 0.01590\n",
      "Training epoch: 3083, train loss: 0.01550, val loss: 0.01570\n",
      "Training epoch: 3084, train loss: 0.01547, val loss: 0.01561\n",
      "Training epoch: 3085, train loss: 0.01549, val loss: 0.01564\n",
      "Training epoch: 3086, train loss: 0.01547, val loss: 0.01561\n",
      "Training epoch: 3087, train loss: 0.01550, val loss: 0.01563\n",
      "Training epoch: 3088, train loss: 0.01578, val loss: 0.01589\n",
      "Training epoch: 3089, train loss: 0.01547, val loss: 0.01559\n",
      "Training epoch: 3090, train loss: 0.01620, val loss: 0.01642\n",
      "Training epoch: 3091, train loss: 0.01544, val loss: 0.01565\n",
      "Training epoch: 3092, train loss: 0.01582, val loss: 0.01601\n",
      "Training epoch: 3093, train loss: 0.01543, val loss: 0.01560\n",
      "Training epoch: 3094, train loss: 0.01559, val loss: 0.01575\n",
      "Training epoch: 3095, train loss: 0.01585, val loss: 0.01603\n",
      "Training epoch: 3096, train loss: 0.01572, val loss: 0.01580\n",
      "Training epoch: 3097, train loss: 0.01597, val loss: 0.01617\n",
      "Training epoch: 3098, train loss: 0.01560, val loss: 0.01582\n",
      "Training epoch: 3099, train loss: 0.01544, val loss: 0.01558\n",
      "Training epoch: 3100, train loss: 0.01558, val loss: 0.01575\n",
      "Training epoch: 3101, train loss: 0.01601, val loss: 0.01620\n",
      "Training epoch: 3102, train loss: 0.01572, val loss: 0.01586\n",
      "Training epoch: 3103, train loss: 0.01617, val loss: 0.01635\n",
      "Training epoch: 3104, train loss: 0.01547, val loss: 0.01563\n",
      "Training epoch: 3105, train loss: 0.01566, val loss: 0.01577\n",
      "Training epoch: 3106, train loss: 0.01560, val loss: 0.01564\n",
      "Training epoch: 3107, train loss: 0.01597, val loss: 0.01616\n",
      "Training epoch: 3108, train loss: 0.01588, val loss: 0.01606\n",
      "Training epoch: 3109, train loss: 0.01588, val loss: 0.01603\n",
      "Training epoch: 3110, train loss: 0.01604, val loss: 0.01616\n",
      "Training epoch: 3111, train loss: 0.01552, val loss: 0.01568\n",
      "Training epoch: 3112, train loss: 0.01593, val loss: 0.01597\n",
      "Training epoch: 3113, train loss: 0.01645, val loss: 0.01672\n",
      "Training epoch: 3114, train loss: 0.01588, val loss: 0.01606\n",
      "Training epoch: 3115, train loss: 0.01570, val loss: 0.01579\n",
      "Training epoch: 3116, train loss: 0.01541, val loss: 0.01557\n",
      "Training epoch: 3117, train loss: 0.01565, val loss: 0.01576\n",
      "Training epoch: 3118, train loss: 0.01570, val loss: 0.01575\n",
      "Training epoch: 3119, train loss: 0.01558, val loss: 0.01571\n",
      "Training epoch: 3120, train loss: 0.01559, val loss: 0.01574\n",
      "Training epoch: 3121, train loss: 0.01566, val loss: 0.01580\n",
      "Training epoch: 3122, train loss: 0.01576, val loss: 0.01587\n",
      "Training epoch: 3123, train loss: 0.01551, val loss: 0.01563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3124, train loss: 0.01556, val loss: 0.01564\n",
      "Training epoch: 3125, train loss: 0.01600, val loss: 0.01614\n",
      "Training epoch: 3126, train loss: 0.01566, val loss: 0.01592\n",
      "Training epoch: 3127, train loss: 0.01556, val loss: 0.01574\n",
      "Training epoch: 3128, train loss: 0.01568, val loss: 0.01583\n",
      "Training epoch: 3129, train loss: 0.01558, val loss: 0.01577\n",
      "Training epoch: 3130, train loss: 0.01561, val loss: 0.01567\n",
      "Training epoch: 3131, train loss: 0.01556, val loss: 0.01566\n",
      "Training epoch: 3132, train loss: 0.01570, val loss: 0.01573\n",
      "Training epoch: 3133, train loss: 0.01549, val loss: 0.01562\n",
      "Training epoch: 3134, train loss: 0.01576, val loss: 0.01598\n",
      "Training epoch: 3135, train loss: 0.01604, val loss: 0.01627\n",
      "Training epoch: 3136, train loss: 0.01586, val loss: 0.01598\n",
      "Training epoch: 3137, train loss: 0.01554, val loss: 0.01567\n",
      "Training epoch: 3138, train loss: 0.01555, val loss: 0.01569\n",
      "Training epoch: 3139, train loss: 0.01553, val loss: 0.01567\n",
      "Training epoch: 3140, train loss: 0.01545, val loss: 0.01562\n",
      "Training epoch: 3141, train loss: 0.01594, val loss: 0.01607\n",
      "Training epoch: 3142, train loss: 0.01542, val loss: 0.01558\n",
      "Training epoch: 3143, train loss: 0.01622, val loss: 0.01624\n",
      "Training epoch: 3144, train loss: 0.01568, val loss: 0.01579\n",
      "Training epoch: 3145, train loss: 0.01563, val loss: 0.01572\n",
      "Training epoch: 3146, train loss: 0.01545, val loss: 0.01563\n",
      "Training epoch: 3147, train loss: 0.01553, val loss: 0.01568\n",
      "Training epoch: 3148, train loss: 0.01570, val loss: 0.01571\n",
      "Training epoch: 3149, train loss: 0.01598, val loss: 0.01616\n",
      "Training epoch: 3150, train loss: 0.01634, val loss: 0.01661\n",
      "Training epoch: 3151, train loss: 0.01551, val loss: 0.01572\n",
      "Training epoch: 3152, train loss: 0.01590, val loss: 0.01595\n",
      "Training epoch: 3153, train loss: 0.01577, val loss: 0.01586\n",
      "Training epoch: 3154, train loss: 0.01551, val loss: 0.01566\n",
      "Training epoch: 3155, train loss: 0.01638, val loss: 0.01661\n",
      "Training epoch: 3156, train loss: 0.01559, val loss: 0.01565\n",
      "Training epoch: 3157, train loss: 0.01587, val loss: 0.01598\n",
      "Training epoch: 3158, train loss: 0.01551, val loss: 0.01569\n",
      "Training epoch: 3159, train loss: 0.01554, val loss: 0.01571\n",
      "Training epoch: 3160, train loss: 0.01595, val loss: 0.01600\n",
      "Training epoch: 3161, train loss: 0.01549, val loss: 0.01570\n",
      "Training epoch: 3162, train loss: 0.01600, val loss: 0.01599\n",
      "Training epoch: 3163, train loss: 0.01566, val loss: 0.01577\n",
      "Training epoch: 3164, train loss: 0.01558, val loss: 0.01564\n",
      "Training epoch: 3165, train loss: 0.01548, val loss: 0.01565\n",
      "Training epoch: 3166, train loss: 0.01564, val loss: 0.01577\n",
      "Training epoch: 3167, train loss: 0.01537, val loss: 0.01555\n",
      "Training epoch: 3168, train loss: 0.01596, val loss: 0.01611\n",
      "Training epoch: 3169, train loss: 0.01547, val loss: 0.01557\n",
      "Training epoch: 3170, train loss: 0.01573, val loss: 0.01587\n",
      "Training epoch: 3171, train loss: 0.01548, val loss: 0.01566\n",
      "Training epoch: 3172, train loss: 0.01585, val loss: 0.01594\n",
      "Training epoch: 3173, train loss: 0.01561, val loss: 0.01574\n",
      "Training epoch: 3174, train loss: 0.01542, val loss: 0.01555\n",
      "Training epoch: 3175, train loss: 0.01586, val loss: 0.01598\n",
      "Training epoch: 3176, train loss: 0.01587, val loss: 0.01606\n",
      "Training epoch: 3177, train loss: 0.01577, val loss: 0.01594\n",
      "Training epoch: 3178, train loss: 0.01572, val loss: 0.01581\n",
      "Training epoch: 3179, train loss: 0.01558, val loss: 0.01569\n",
      "Training epoch: 3180, train loss: 0.01548, val loss: 0.01566\n",
      "Training epoch: 3181, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 3182, train loss: 0.01579, val loss: 0.01608\n",
      "Training epoch: 3183, train loss: 0.01564, val loss: 0.01574\n",
      "Training epoch: 3184, train loss: 0.01567, val loss: 0.01584\n",
      "Training epoch: 3185, train loss: 0.01575, val loss: 0.01586\n",
      "Training epoch: 3186, train loss: 0.01553, val loss: 0.01565\n",
      "Training epoch: 3187, train loss: 0.01597, val loss: 0.01625\n",
      "Training epoch: 3188, train loss: 0.01554, val loss: 0.01569\n",
      "Training epoch: 3189, train loss: 0.01664, val loss: 0.01680\n",
      "Training epoch: 3190, train loss: 0.01554, val loss: 0.01583\n",
      "Training epoch: 3191, train loss: 0.01573, val loss: 0.01587\n",
      "Training epoch: 3192, train loss: 0.01627, val loss: 0.01634\n",
      "Training epoch: 3193, train loss: 0.01559, val loss: 0.01566\n",
      "Training epoch: 3194, train loss: 0.01561, val loss: 0.01569\n",
      "Training epoch: 3195, train loss: 0.01550, val loss: 0.01566\n",
      "Training epoch: 3196, train loss: 0.01570, val loss: 0.01586\n",
      "Training epoch: 3197, train loss: 0.01550, val loss: 0.01565\n",
      "Training epoch: 3198, train loss: 0.01545, val loss: 0.01558\n",
      "Training epoch: 3199, train loss: 0.01545, val loss: 0.01557\n",
      "Training epoch: 3200, train loss: 0.01593, val loss: 0.01594\n",
      "Training epoch: 3201, train loss: 0.01664, val loss: 0.01694\n",
      "Training epoch: 3202, train loss: 0.01608, val loss: 0.01629\n",
      "Training epoch: 3203, train loss: 0.01571, val loss: 0.01591\n",
      "Training epoch: 3204, train loss: 0.01562, val loss: 0.01581\n",
      "Training epoch: 3205, train loss: 0.01567, val loss: 0.01582\n",
      "Training epoch: 3206, train loss: 0.01566, val loss: 0.01578\n",
      "Training epoch: 3207, train loss: 0.01550, val loss: 0.01560\n",
      "Training epoch: 3208, train loss: 0.01585, val loss: 0.01594\n",
      "Training epoch: 3209, train loss: 0.01566, val loss: 0.01586\n",
      "Training epoch: 3210, train loss: 0.01558, val loss: 0.01571\n",
      "Training epoch: 3211, train loss: 0.01557, val loss: 0.01572\n",
      "Training epoch: 3212, train loss: 0.01569, val loss: 0.01573\n",
      "Training epoch: 3213, train loss: 0.01553, val loss: 0.01562\n",
      "Training epoch: 3214, train loss: 0.01605, val loss: 0.01607\n",
      "Training epoch: 3215, train loss: 0.01553, val loss: 0.01566\n",
      "Training epoch: 3216, train loss: 0.01598, val loss: 0.01625\n",
      "Training epoch: 3217, train loss: 0.01593, val loss: 0.01614\n",
      "Training epoch: 3218, train loss: 0.01579, val loss: 0.01600\n",
      "Training epoch: 3219, train loss: 0.01558, val loss: 0.01589\n",
      "Training epoch: 3220, train loss: 0.01587, val loss: 0.01586\n",
      "Training epoch: 3221, train loss: 0.01593, val loss: 0.01609\n",
      "Training epoch: 3222, train loss: 0.01587, val loss: 0.01600\n",
      "Training epoch: 3223, train loss: 0.01590, val loss: 0.01605\n",
      "Training epoch: 3224, train loss: 0.01550, val loss: 0.01564\n",
      "Training epoch: 3225, train loss: 0.01572, val loss: 0.01587\n",
      "Training epoch: 3226, train loss: 0.01569, val loss: 0.01579\n",
      "Training epoch: 3227, train loss: 0.01569, val loss: 0.01587\n",
      "Training epoch: 3228, train loss: 0.01560, val loss: 0.01575\n",
      "Training epoch: 3229, train loss: 0.01573, val loss: 0.01584\n",
      "Training epoch: 3230, train loss: 0.01566, val loss: 0.01576\n",
      "Training epoch: 3231, train loss: 0.01564, val loss: 0.01569\n",
      "Training epoch: 3232, train loss: 0.01604, val loss: 0.01625\n",
      "Training epoch: 3233, train loss: 0.01565, val loss: 0.01584\n",
      "Training epoch: 3234, train loss: 0.01592, val loss: 0.01612\n",
      "Training epoch: 3235, train loss: 0.01550, val loss: 0.01565\n",
      "Training epoch: 3236, train loss: 0.01577, val loss: 0.01603\n",
      "Training epoch: 3237, train loss: 0.01546, val loss: 0.01565\n",
      "Training epoch: 3238, train loss: 0.01563, val loss: 0.01574\n",
      "Training epoch: 3239, train loss: 0.01634, val loss: 0.01665\n",
      "Training epoch: 3240, train loss: 0.01573, val loss: 0.01591\n",
      "Training epoch: 3241, train loss: 0.01582, val loss: 0.01601\n",
      "Training epoch: 3242, train loss: 0.01588, val loss: 0.01604\n",
      "Training epoch: 3243, train loss: 0.01551, val loss: 0.01573\n",
      "Training epoch: 3244, train loss: 0.01555, val loss: 0.01573\n",
      "Training epoch: 3245, train loss: 0.01611, val loss: 0.01627\n",
      "Training epoch: 3246, train loss: 0.01596, val loss: 0.01601\n",
      "Training epoch: 3247, train loss: 0.01569, val loss: 0.01588\n",
      "Training epoch: 3248, train loss: 0.01553, val loss: 0.01567\n",
      "Training epoch: 3249, train loss: 0.01556, val loss: 0.01562\n",
      "Training epoch: 3250, train loss: 0.01634, val loss: 0.01646\n",
      "Training epoch: 3251, train loss: 0.01631, val loss: 0.01654\n",
      "Training epoch: 3252, train loss: 0.01589, val loss: 0.01607\n",
      "Training epoch: 3253, train loss: 0.01562, val loss: 0.01574\n",
      "Training epoch: 3254, train loss: 0.01596, val loss: 0.01623\n",
      "Training epoch: 3255, train loss: 0.01604, val loss: 0.01618\n",
      "Training epoch: 3256, train loss: 0.01591, val loss: 0.01613\n",
      "Training epoch: 3257, train loss: 0.01558, val loss: 0.01583\n",
      "Training epoch: 3258, train loss: 0.01567, val loss: 0.01575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3259, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 3260, train loss: 0.01549, val loss: 0.01565\n",
      "Training epoch: 3261, train loss: 0.01551, val loss: 0.01562\n",
      "Training epoch: 3262, train loss: 0.01547, val loss: 0.01560\n",
      "Training epoch: 3263, train loss: 0.01597, val loss: 0.01612\n",
      "Training epoch: 3264, train loss: 0.01652, val loss: 0.01679\n",
      "Training epoch: 3265, train loss: 0.01540, val loss: 0.01558\n",
      "Training epoch: 3266, train loss: 0.01555, val loss: 0.01561\n",
      "Training epoch: 3267, train loss: 0.01574, val loss: 0.01589\n",
      "Training epoch: 3268, train loss: 0.01591, val loss: 0.01609\n",
      "Training epoch: 3269, train loss: 0.01541, val loss: 0.01549\n",
      "Training epoch: 3270, train loss: 0.01586, val loss: 0.01588\n",
      "Training epoch: 3271, train loss: 0.01576, val loss: 0.01576\n",
      "Training epoch: 3272, train loss: 0.01551, val loss: 0.01563\n",
      "Training epoch: 3273, train loss: 0.01556, val loss: 0.01563\n",
      "Training epoch: 3274, train loss: 0.01634, val loss: 0.01636\n",
      "Training epoch: 3275, train loss: 0.01573, val loss: 0.01576\n",
      "Training epoch: 3276, train loss: 0.01552, val loss: 0.01563\n",
      "Training epoch: 3277, train loss: 0.01561, val loss: 0.01574\n",
      "Training epoch: 3278, train loss: 0.01555, val loss: 0.01562\n",
      "Training epoch: 3279, train loss: 0.01585, val loss: 0.01604\n",
      "Training epoch: 3280, train loss: 0.01542, val loss: 0.01559\n",
      "Training epoch: 3281, train loss: 0.01547, val loss: 0.01560\n",
      "Training epoch: 3282, train loss: 0.01548, val loss: 0.01559\n",
      "Training epoch: 3283, train loss: 0.01556, val loss: 0.01574\n",
      "Training epoch: 3284, train loss: 0.01591, val loss: 0.01611\n",
      "Training epoch: 3285, train loss: 0.01558, val loss: 0.01578\n",
      "Training epoch: 3286, train loss: 0.01596, val loss: 0.01621\n",
      "Training epoch: 3287, train loss: 0.01573, val loss: 0.01584\n",
      "Training epoch: 3288, train loss: 0.01610, val loss: 0.01620\n",
      "Training epoch: 3289, train loss: 0.01564, val loss: 0.01579\n",
      "Training epoch: 3290, train loss: 0.01581, val loss: 0.01593\n",
      "Training epoch: 3291, train loss: 0.01546, val loss: 0.01561\n",
      "Training epoch: 3292, train loss: 0.01603, val loss: 0.01610\n",
      "Training epoch: 3293, train loss: 0.01563, val loss: 0.01586\n",
      "Training epoch: 3294, train loss: 0.01552, val loss: 0.01562\n",
      "Training epoch: 3295, train loss: 0.01555, val loss: 0.01569\n",
      "Training epoch: 3296, train loss: 0.01541, val loss: 0.01556\n",
      "Training epoch: 3297, train loss: 0.01569, val loss: 0.01583\n",
      "Training epoch: 3298, train loss: 0.01579, val loss: 0.01586\n",
      "Training epoch: 3299, train loss: 0.01547, val loss: 0.01562\n",
      "Training epoch: 3300, train loss: 0.01586, val loss: 0.01603\n",
      "Training epoch: 3301, train loss: 0.01556, val loss: 0.01564\n",
      "Training epoch: 3302, train loss: 0.01585, val loss: 0.01601\n",
      "Training epoch: 3303, train loss: 0.01553, val loss: 0.01565\n",
      "Training epoch: 3304, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 3305, train loss: 0.01591, val loss: 0.01607\n",
      "Training epoch: 3306, train loss: 0.01563, val loss: 0.01567\n",
      "Training epoch: 3307, train loss: 0.01569, val loss: 0.01583\n",
      "Training epoch: 3308, train loss: 0.01573, val loss: 0.01585\n",
      "Training epoch: 3309, train loss: 0.01550, val loss: 0.01563\n",
      "Training epoch: 3310, train loss: 0.01562, val loss: 0.01579\n",
      "Training epoch: 3311, train loss: 0.01563, val loss: 0.01575\n",
      "Training epoch: 3312, train loss: 0.01594, val loss: 0.01614\n",
      "Training epoch: 3313, train loss: 0.01562, val loss: 0.01577\n",
      "Training epoch: 3314, train loss: 0.01556, val loss: 0.01573\n",
      "Training epoch: 3315, train loss: 0.01546, val loss: 0.01559\n",
      "Training epoch: 3316, train loss: 0.01568, val loss: 0.01578\n",
      "Training epoch: 3317, train loss: 0.01553, val loss: 0.01567\n",
      "Training epoch: 3318, train loss: 0.01588, val loss: 0.01600\n",
      "Training epoch: 3319, train loss: 0.01577, val loss: 0.01601\n",
      "Training epoch: 3320, train loss: 0.01546, val loss: 0.01561\n",
      "Training epoch: 3321, train loss: 0.01559, val loss: 0.01567\n",
      "Training epoch: 3322, train loss: 0.01549, val loss: 0.01559\n",
      "Training epoch: 3323, train loss: 0.01566, val loss: 0.01571\n",
      "Training epoch: 3324, train loss: 0.01627, val loss: 0.01652\n",
      "Training epoch: 3325, train loss: 0.01606, val loss: 0.01634\n",
      "Training epoch: 3326, train loss: 0.01600, val loss: 0.01615\n",
      "Training epoch: 3327, train loss: 0.01555, val loss: 0.01566\n",
      "Training epoch: 3328, train loss: 0.01551, val loss: 0.01562\n",
      "Training epoch: 3329, train loss: 0.01551, val loss: 0.01562\n",
      "Training epoch: 3330, train loss: 0.01560, val loss: 0.01570\n",
      "Training epoch: 3331, train loss: 0.01554, val loss: 0.01557\n",
      "Training epoch: 3332, train loss: 0.01540, val loss: 0.01565\n",
      "Training epoch: 3333, train loss: 0.01553, val loss: 0.01568\n",
      "Training epoch: 3334, train loss: 0.01576, val loss: 0.01587\n",
      "Training epoch: 3335, train loss: 0.01558, val loss: 0.01569\n",
      "Training epoch: 3336, train loss: 0.01563, val loss: 0.01573\n",
      "Training epoch: 3337, train loss: 0.01548, val loss: 0.01564\n",
      "Training epoch: 3338, train loss: 0.01578, val loss: 0.01595\n",
      "Training epoch: 3339, train loss: 0.01543, val loss: 0.01551\n",
      "Training epoch: 3340, train loss: 0.01557, val loss: 0.01569\n",
      "Training epoch: 3341, train loss: 0.01546, val loss: 0.01559\n",
      "Training epoch: 3342, train loss: 0.01542, val loss: 0.01552\n",
      "Training epoch: 3343, train loss: 0.01543, val loss: 0.01554\n",
      "Training epoch: 3344, train loss: 0.01561, val loss: 0.01571\n",
      "Training epoch: 3345, train loss: 0.01557, val loss: 0.01564\n",
      "Training epoch: 3346, train loss: 0.01553, val loss: 0.01562\n",
      "Training epoch: 3347, train loss: 0.01549, val loss: 0.01572\n",
      "Training epoch: 3348, train loss: 0.01577, val loss: 0.01603\n",
      "Training epoch: 3349, train loss: 0.01568, val loss: 0.01590\n",
      "Training epoch: 3350, train loss: 0.01572, val loss: 0.01570\n",
      "Training epoch: 3351, train loss: 0.01597, val loss: 0.01602\n",
      "Training epoch: 3352, train loss: 0.01606, val loss: 0.01622\n",
      "Training epoch: 3353, train loss: 0.01560, val loss: 0.01572\n",
      "Training epoch: 3354, train loss: 0.01594, val loss: 0.01616\n",
      "Training epoch: 3355, train loss: 0.01558, val loss: 0.01575\n",
      "Training epoch: 3356, train loss: 0.01566, val loss: 0.01575\n",
      "Training epoch: 3357, train loss: 0.01554, val loss: 0.01561\n",
      "Training epoch: 3358, train loss: 0.01619, val loss: 0.01638\n",
      "Training epoch: 3359, train loss: 0.01579, val loss: 0.01592\n",
      "Training epoch: 3360, train loss: 0.01564, val loss: 0.01578\n",
      "Training epoch: 3361, train loss: 0.01619, val loss: 0.01644\n",
      "Training epoch: 3362, train loss: 0.01555, val loss: 0.01580\n",
      "Training epoch: 3363, train loss: 0.01565, val loss: 0.01574\n",
      "Training epoch: 3364, train loss: 0.01582, val loss: 0.01596\n",
      "Training epoch: 3365, train loss: 0.01576, val loss: 0.01585\n",
      "Training epoch: 3366, train loss: 0.01576, val loss: 0.01592\n",
      "Training epoch: 3367, train loss: 0.01581, val loss: 0.01585\n",
      "Training epoch: 3368, train loss: 0.01575, val loss: 0.01576\n",
      "Training epoch: 3369, train loss: 0.01564, val loss: 0.01565\n",
      "Training epoch: 3370, train loss: 0.01569, val loss: 0.01589\n",
      "Training epoch: 3371, train loss: 0.01550, val loss: 0.01563\n",
      "Training epoch: 3372, train loss: 0.01540, val loss: 0.01556\n",
      "Training epoch: 3373, train loss: 0.01577, val loss: 0.01594\n",
      "Training epoch: 3374, train loss: 0.01578, val loss: 0.01589\n",
      "Training epoch: 3375, train loss: 0.01588, val loss: 0.01600\n",
      "Training epoch: 3376, train loss: 0.01567, val loss: 0.01583\n",
      "Training epoch: 3377, train loss: 0.01562, val loss: 0.01573\n",
      "Training epoch: 3378, train loss: 0.01545, val loss: 0.01557\n",
      "Training epoch: 3379, train loss: 0.01563, val loss: 0.01583\n",
      "Training epoch: 3380, train loss: 0.01569, val loss: 0.01586\n",
      "Training epoch: 3381, train loss: 0.01554, val loss: 0.01560\n",
      "Training epoch: 3382, train loss: 0.01562, val loss: 0.01566\n",
      "Training epoch: 3383, train loss: 0.01617, val loss: 0.01634\n",
      "Training epoch: 3384, train loss: 0.01611, val loss: 0.01631\n",
      "Training epoch: 3385, train loss: 0.01545, val loss: 0.01558\n",
      "Training epoch: 3386, train loss: 0.01565, val loss: 0.01566\n",
      "Training epoch: 3387, train loss: 0.01547, val loss: 0.01560\n",
      "Training epoch: 3388, train loss: 0.01569, val loss: 0.01580\n",
      "Training epoch: 3389, train loss: 0.01555, val loss: 0.01563\n",
      "Training epoch: 3390, train loss: 0.01552, val loss: 0.01570\n",
      "Training epoch: 3391, train loss: 0.01543, val loss: 0.01555\n",
      "Training epoch: 3392, train loss: 0.01549, val loss: 0.01554\n",
      "Training epoch: 3393, train loss: 0.01561, val loss: 0.01570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3394, train loss: 0.01658, val loss: 0.01670\n",
      "Training epoch: 3395, train loss: 0.01603, val loss: 0.01619\n",
      "Training epoch: 3396, train loss: 0.01587, val loss: 0.01598\n",
      "Training epoch: 3397, train loss: 0.01565, val loss: 0.01572\n",
      "Training epoch: 3398, train loss: 0.01584, val loss: 0.01600\n",
      "Training epoch: 3399, train loss: 0.01555, val loss: 0.01567\n",
      "Training epoch: 3400, train loss: 0.01570, val loss: 0.01588\n",
      "Training epoch: 3401, train loss: 0.01571, val loss: 0.01592\n",
      "Training epoch: 3402, train loss: 0.01595, val loss: 0.01612\n",
      "Training epoch: 3403, train loss: 0.01577, val loss: 0.01579\n",
      "Training epoch: 3404, train loss: 0.01565, val loss: 0.01575\n",
      "Training epoch: 3405, train loss: 0.01593, val loss: 0.01602\n",
      "Training epoch: 3406, train loss: 0.01568, val loss: 0.01588\n",
      "Training epoch: 3407, train loss: 0.01576, val loss: 0.01587\n",
      "Training epoch: 3408, train loss: 0.01569, val loss: 0.01593\n",
      "Training epoch: 3409, train loss: 0.01612, val loss: 0.01614\n",
      "Training epoch: 3410, train loss: 0.01600, val loss: 0.01614\n",
      "Training epoch: 3411, train loss: 0.01565, val loss: 0.01578\n",
      "Training epoch: 3412, train loss: 0.01559, val loss: 0.01565\n",
      "Training epoch: 3413, train loss: 0.01554, val loss: 0.01564\n",
      "Training epoch: 3414, train loss: 0.01567, val loss: 0.01582\n",
      "Training epoch: 3415, train loss: 0.01581, val loss: 0.01589\n",
      "Training epoch: 3416, train loss: 0.01561, val loss: 0.01566\n",
      "Training epoch: 3417, train loss: 0.01564, val loss: 0.01578\n",
      "Training epoch: 3418, train loss: 0.01572, val loss: 0.01578\n",
      "Training epoch: 3419, train loss: 0.01555, val loss: 0.01576\n",
      "Training epoch: 3420, train loss: 0.01575, val loss: 0.01584\n",
      "Training epoch: 3421, train loss: 0.01647, val loss: 0.01677\n",
      "Training epoch: 3422, train loss: 0.01584, val loss: 0.01604\n",
      "Training epoch: 3423, train loss: 0.01548, val loss: 0.01568\n",
      "Training epoch: 3424, train loss: 0.01581, val loss: 0.01602\n",
      "Training epoch: 3425, train loss: 0.01555, val loss: 0.01586\n",
      "Training epoch: 3426, train loss: 0.01566, val loss: 0.01580\n",
      "Training epoch: 3427, train loss: 0.01552, val loss: 0.01565\n",
      "Training epoch: 3428, train loss: 0.01575, val loss: 0.01597\n",
      "Training epoch: 3429, train loss: 0.01620, val loss: 0.01625\n",
      "Training epoch: 3430, train loss: 0.01574, val loss: 0.01581\n",
      "Training epoch: 3431, train loss: 0.01566, val loss: 0.01579\n",
      "Training epoch: 3432, train loss: 0.01565, val loss: 0.01583\n",
      "Training epoch: 3433, train loss: 0.01564, val loss: 0.01575\n",
      "Training epoch: 3434, train loss: 0.01602, val loss: 0.01616\n",
      "Training epoch: 3435, train loss: 0.01567, val loss: 0.01573\n",
      "Training epoch: 3436, train loss: 0.01541, val loss: 0.01554\n",
      "Training epoch: 3437, train loss: 0.01596, val loss: 0.01609\n",
      "Training epoch: 3438, train loss: 0.01603, val loss: 0.01629\n",
      "Training epoch: 3439, train loss: 0.01593, val loss: 0.01601\n",
      "Training epoch: 3440, train loss: 0.01548, val loss: 0.01558\n",
      "Training epoch: 3441, train loss: 0.01561, val loss: 0.01572\n",
      "Training epoch: 3442, train loss: 0.01555, val loss: 0.01569\n",
      "Training epoch: 3443, train loss: 0.01607, val loss: 0.01629\n",
      "Training epoch: 3444, train loss: 0.01550, val loss: 0.01559\n",
      "Training epoch: 3445, train loss: 0.01584, val loss: 0.01600\n",
      "Training epoch: 3446, train loss: 0.01559, val loss: 0.01576\n",
      "Training epoch: 3447, train loss: 0.01586, val loss: 0.01595\n",
      "Training epoch: 3448, train loss: 0.01562, val loss: 0.01580\n",
      "Training epoch: 3449, train loss: 0.01575, val loss: 0.01588\n",
      "Training epoch: 3450, train loss: 0.01570, val loss: 0.01593\n",
      "Training epoch: 3451, train loss: 0.01557, val loss: 0.01571\n",
      "Training epoch: 3452, train loss: 0.01572, val loss: 0.01584\n",
      "Training epoch: 3453, train loss: 0.01572, val loss: 0.01573\n",
      "Training epoch: 3454, train loss: 0.01562, val loss: 0.01577\n",
      "Training epoch: 3455, train loss: 0.01558, val loss: 0.01565\n",
      "Training epoch: 3456, train loss: 0.01565, val loss: 0.01580\n",
      "Training epoch: 3457, train loss: 0.01552, val loss: 0.01561\n",
      "Training epoch: 3458, train loss: 0.01574, val loss: 0.01583\n",
      "Training epoch: 3459, train loss: 0.01561, val loss: 0.01575\n",
      "Training epoch: 3460, train loss: 0.01541, val loss: 0.01559\n",
      "Training epoch: 3461, train loss: 0.01578, val loss: 0.01583\n",
      "Training epoch: 3462, train loss: 0.01594, val loss: 0.01609\n",
      "Training epoch: 3463, train loss: 0.01563, val loss: 0.01574\n",
      "Training epoch: 3464, train loss: 0.01566, val loss: 0.01575\n",
      "Training epoch: 3465, train loss: 0.01560, val loss: 0.01571\n",
      "Training epoch: 3466, train loss: 0.01552, val loss: 0.01567\n",
      "Training epoch: 3467, train loss: 0.01554, val loss: 0.01572\n",
      "Training epoch: 3468, train loss: 0.01549, val loss: 0.01560\n",
      "Training epoch: 3469, train loss: 0.01544, val loss: 0.01553\n",
      "Training epoch: 3470, train loss: 0.01575, val loss: 0.01585\n",
      "Training epoch: 3471, train loss: 0.01552, val loss: 0.01562\n",
      "Training epoch: 3472, train loss: 0.01554, val loss: 0.01570\n",
      "Training epoch: 3473, train loss: 0.01610, val loss: 0.01631\n",
      "Training epoch: 3474, train loss: 0.01574, val loss: 0.01585\n",
      "Training epoch: 3475, train loss: 0.01570, val loss: 0.01578\n",
      "Training epoch: 3476, train loss: 0.01552, val loss: 0.01562\n",
      "Training epoch: 3477, train loss: 0.01579, val loss: 0.01584\n",
      "Training epoch: 3478, train loss: 0.01550, val loss: 0.01562\n",
      "Training epoch: 3479, train loss: 0.01596, val loss: 0.01616\n",
      "Training epoch: 3480, train loss: 0.01542, val loss: 0.01555\n",
      "Training epoch: 3481, train loss: 0.01555, val loss: 0.01561\n",
      "Training epoch: 3482, train loss: 0.01574, val loss: 0.01589\n",
      "Training epoch: 3483, train loss: 0.01604, val loss: 0.01632\n",
      "Training epoch: 3484, train loss: 0.01561, val loss: 0.01577\n",
      "Training epoch: 3485, train loss: 0.01586, val loss: 0.01594\n",
      "Training epoch: 3486, train loss: 0.01570, val loss: 0.01595\n",
      "Training epoch: 3487, train loss: 0.01575, val loss: 0.01585\n",
      "Training epoch: 3488, train loss: 0.01561, val loss: 0.01572\n",
      "Training epoch: 3489, train loss: 0.01589, val loss: 0.01608\n",
      "Training epoch: 3490, train loss: 0.01589, val loss: 0.01598\n",
      "Training epoch: 3491, train loss: 0.01593, val loss: 0.01600\n",
      "Training epoch: 3492, train loss: 0.01557, val loss: 0.01563\n",
      "Training epoch: 3493, train loss: 0.01620, val loss: 0.01617\n",
      "Training epoch: 3494, train loss: 0.01551, val loss: 0.01559\n",
      "Training epoch: 3495, train loss: 0.01569, val loss: 0.01579\n",
      "Training epoch: 3496, train loss: 0.01553, val loss: 0.01564\n",
      "Training epoch: 3497, train loss: 0.01596, val loss: 0.01592\n",
      "Training epoch: 3498, train loss: 0.01543, val loss: 0.01554\n",
      "Training epoch: 3499, train loss: 0.01576, val loss: 0.01579\n",
      "Training epoch: 3500, train loss: 0.01547, val loss: 0.01554\n",
      "Training epoch: 3501, train loss: 0.01584, val loss: 0.01602\n",
      "Training epoch: 3502, train loss: 0.01572, val loss: 0.01586\n",
      "Training epoch: 3503, train loss: 0.01536, val loss: 0.01553\n",
      "Training epoch: 3504, train loss: 0.01573, val loss: 0.01585\n",
      "Training epoch: 3505, train loss: 0.01573, val loss: 0.01587\n",
      "Training epoch: 3506, train loss: 0.01563, val loss: 0.01577\n",
      "Training epoch: 3507, train loss: 0.01560, val loss: 0.01567\n",
      "Training epoch: 3508, train loss: 0.01554, val loss: 0.01562\n",
      "Training epoch: 3509, train loss: 0.01559, val loss: 0.01572\n",
      "Training epoch: 3510, train loss: 0.01570, val loss: 0.01582\n",
      "Training epoch: 3511, train loss: 0.01568, val loss: 0.01594\n",
      "Training epoch: 3512, train loss: 0.01612, val loss: 0.01605\n",
      "Training epoch: 3513, train loss: 0.01576, val loss: 0.01598\n",
      "Training epoch: 3514, train loss: 0.01565, val loss: 0.01587\n",
      "Training epoch: 3515, train loss: 0.01565, val loss: 0.01576\n",
      "Training epoch: 3516, train loss: 0.01561, val loss: 0.01571\n",
      "Training epoch: 3517, train loss: 0.01537, val loss: 0.01554\n",
      "Training epoch: 3518, train loss: 0.01564, val loss: 0.01567\n",
      "Training epoch: 3519, train loss: 0.01575, val loss: 0.01585\n",
      "Training epoch: 3520, train loss: 0.01547, val loss: 0.01562\n",
      "Training epoch: 3521, train loss: 0.01543, val loss: 0.01558\n",
      "Training epoch: 3522, train loss: 0.01559, val loss: 0.01568\n",
      "Training epoch: 3523, train loss: 0.01605, val loss: 0.01611\n",
      "Training epoch: 3524, train loss: 0.01562, val loss: 0.01580\n",
      "Training epoch: 3525, train loss: 0.01570, val loss: 0.01584\n",
      "Training epoch: 3526, train loss: 0.01566, val loss: 0.01579\n",
      "Training epoch: 3527, train loss: 0.01578, val loss: 0.01585\n",
      "Training epoch: 3528, train loss: 0.01555, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3529, train loss: 0.01576, val loss: 0.01597\n",
      "Training epoch: 3530, train loss: 0.01549, val loss: 0.01563\n",
      "Training epoch: 3531, train loss: 0.01547, val loss: 0.01569\n",
      "Training epoch: 3532, train loss: 0.01572, val loss: 0.01572\n",
      "Training epoch: 3533, train loss: 0.01559, val loss: 0.01580\n",
      "Training epoch: 3534, train loss: 0.01572, val loss: 0.01589\n",
      "Training epoch: 3535, train loss: 0.01582, val loss: 0.01598\n",
      "Training epoch: 3536, train loss: 0.01568, val loss: 0.01573\n",
      "Training epoch: 3537, train loss: 0.01629, val loss: 0.01650\n",
      "Training epoch: 3538, train loss: 0.01577, val loss: 0.01596\n",
      "Training epoch: 3539, train loss: 0.01601, val loss: 0.01616\n",
      "Training epoch: 3540, train loss: 0.01580, val loss: 0.01597\n",
      "Training epoch: 3541, train loss: 0.01585, val loss: 0.01595\n",
      "Training epoch: 3542, train loss: 0.01549, val loss: 0.01565\n",
      "Training epoch: 3543, train loss: 0.01569, val loss: 0.01587\n",
      "Training epoch: 3544, train loss: 0.01556, val loss: 0.01569\n",
      "Training epoch: 3545, train loss: 0.01597, val loss: 0.01622\n",
      "Training epoch: 3546, train loss: 0.01553, val loss: 0.01569\n",
      "Training epoch: 3547, train loss: 0.01575, val loss: 0.01588\n",
      "Training epoch: 3548, train loss: 0.01548, val loss: 0.01559\n",
      "Training epoch: 3549, train loss: 0.01616, val loss: 0.01645\n",
      "Training epoch: 3550, train loss: 0.01556, val loss: 0.01568\n",
      "Training epoch: 3551, train loss: 0.01562, val loss: 0.01577\n",
      "Training epoch: 3552, train loss: 0.01557, val loss: 0.01575\n",
      "Training epoch: 3553, train loss: 0.01567, val loss: 0.01578\n",
      "Training epoch: 3554, train loss: 0.01566, val loss: 0.01571\n",
      "Training epoch: 3555, train loss: 0.01596, val loss: 0.01593\n",
      "Training epoch: 3556, train loss: 0.01572, val loss: 0.01577\n",
      "Training epoch: 3557, train loss: 0.01572, val loss: 0.01588\n",
      "Training epoch: 3558, train loss: 0.01588, val loss: 0.01602\n",
      "Training epoch: 3559, train loss: 0.01558, val loss: 0.01578\n",
      "Training epoch: 3560, train loss: 0.01550, val loss: 0.01565\n",
      "Training epoch: 3561, train loss: 0.01564, val loss: 0.01589\n",
      "Training epoch: 3562, train loss: 0.01613, val loss: 0.01617\n",
      "Training epoch: 3563, train loss: 0.01552, val loss: 0.01556\n",
      "Training epoch: 3564, train loss: 0.01617, val loss: 0.01645\n",
      "Training epoch: 3565, train loss: 0.01575, val loss: 0.01584\n",
      "Training epoch: 3566, train loss: 0.01541, val loss: 0.01561\n",
      "Training epoch: 3567, train loss: 0.01556, val loss: 0.01569\n",
      "Training epoch: 3568, train loss: 0.01579, val loss: 0.01593\n",
      "Training epoch: 3569, train loss: 0.01563, val loss: 0.01559\n",
      "Training epoch: 3570, train loss: 0.01563, val loss: 0.01573\n",
      "Training epoch: 3571, train loss: 0.01590, val loss: 0.01607\n",
      "Training epoch: 3572, train loss: 0.01584, val loss: 0.01598\n",
      "Training epoch: 3573, train loss: 0.01592, val loss: 0.01596\n",
      "Training epoch: 3574, train loss: 0.01554, val loss: 0.01561\n",
      "Training epoch: 3575, train loss: 0.01565, val loss: 0.01588\n",
      "Training epoch: 3576, train loss: 0.01584, val loss: 0.01589\n",
      "Training epoch: 3577, train loss: 0.01564, val loss: 0.01580\n",
      "Training epoch: 3578, train loss: 0.01569, val loss: 0.01576\n",
      "Training epoch: 3579, train loss: 0.01589, val loss: 0.01602\n",
      "Training epoch: 3580, train loss: 0.01583, val loss: 0.01599\n",
      "Training epoch: 3581, train loss: 0.01553, val loss: 0.01570\n",
      "Training epoch: 3582, train loss: 0.01573, val loss: 0.01593\n",
      "Training epoch: 3583, train loss: 0.01537, val loss: 0.01561\n",
      "Training epoch: 3584, train loss: 0.01567, val loss: 0.01579\n",
      "Training epoch: 3585, train loss: 0.01549, val loss: 0.01568\n",
      "Training epoch: 3586, train loss: 0.01590, val loss: 0.01607\n",
      "Training epoch: 3587, train loss: 0.01583, val loss: 0.01594\n",
      "Training epoch: 3588, train loss: 0.01562, val loss: 0.01561\n",
      "Training epoch: 3589, train loss: 0.01550, val loss: 0.01558\n",
      "Training epoch: 3590, train loss: 0.01578, val loss: 0.01580\n",
      "Training epoch: 3591, train loss: 0.01571, val loss: 0.01590\n",
      "Training epoch: 3592, train loss: 0.01572, val loss: 0.01580\n",
      "Training epoch: 3593, train loss: 0.01586, val loss: 0.01603\n",
      "Training epoch: 3594, train loss: 0.01568, val loss: 0.01575\n",
      "Training epoch: 3595, train loss: 0.01555, val loss: 0.01571\n",
      "Training epoch: 3596, train loss: 0.01625, val loss: 0.01617\n",
      "Training epoch: 3597, train loss: 0.01571, val loss: 0.01587\n",
      "Training epoch: 3598, train loss: 0.01561, val loss: 0.01568\n",
      "Training epoch: 3599, train loss: 0.01548, val loss: 0.01567\n",
      "Training epoch: 3600, train loss: 0.01562, val loss: 0.01562\n",
      "Training epoch: 3601, train loss: 0.01564, val loss: 0.01579\n",
      "Training epoch: 3602, train loss: 0.01572, val loss: 0.01587\n",
      "Training epoch: 3603, train loss: 0.01610, val loss: 0.01617\n",
      "Training epoch: 3604, train loss: 0.01604, val loss: 0.01623\n",
      "Training epoch: 3605, train loss: 0.01558, val loss: 0.01568\n",
      "Training epoch: 3606, train loss: 0.01562, val loss: 0.01576\n",
      "Training epoch: 3607, train loss: 0.01594, val loss: 0.01612\n",
      "Training epoch: 3608, train loss: 0.01549, val loss: 0.01561\n",
      "Training epoch: 3609, train loss: 0.01605, val loss: 0.01617\n",
      "Training epoch: 3610, train loss: 0.01557, val loss: 0.01575\n",
      "Training epoch: 3611, train loss: 0.01575, val loss: 0.01586\n",
      "Training epoch: 3612, train loss: 0.01619, val loss: 0.01646\n",
      "Training epoch: 3613, train loss: 0.01562, val loss: 0.01577\n",
      "Training epoch: 3614, train loss: 0.01582, val loss: 0.01593\n",
      "Training epoch: 3615, train loss: 0.01556, val loss: 0.01578\n",
      "Training epoch: 3616, train loss: 0.01595, val loss: 0.01614\n",
      "Training epoch: 3617, train loss: 0.01548, val loss: 0.01562\n",
      "Training epoch: 3618, train loss: 0.01580, val loss: 0.01583\n",
      "Training epoch: 3619, train loss: 0.01556, val loss: 0.01572\n",
      "Training epoch: 3620, train loss: 0.01534, val loss: 0.01560\n",
      "Training epoch: 3621, train loss: 0.01562, val loss: 0.01583\n",
      "Training epoch: 3622, train loss: 0.01542, val loss: 0.01557\n",
      "Training epoch: 3623, train loss: 0.01609, val loss: 0.01629\n",
      "Training epoch: 3624, train loss: 0.01550, val loss: 0.01557\n",
      "Training epoch: 3625, train loss: 0.01584, val loss: 0.01599\n",
      "Training epoch: 3626, train loss: 0.01542, val loss: 0.01558\n",
      "Training epoch: 3627, train loss: 0.01554, val loss: 0.01569\n",
      "Training epoch: 3628, train loss: 0.01560, val loss: 0.01572\n",
      "Training epoch: 3629, train loss: 0.01544, val loss: 0.01555\n",
      "Training epoch: 3630, train loss: 0.01552, val loss: 0.01567\n",
      "Training epoch: 3631, train loss: 0.01555, val loss: 0.01565\n",
      "Training epoch: 3632, train loss: 0.01575, val loss: 0.01591\n",
      "Training epoch: 3633, train loss: 0.01571, val loss: 0.01591\n",
      "Training epoch: 3634, train loss: 0.01555, val loss: 0.01583\n",
      "Training epoch: 3635, train loss: 0.01543, val loss: 0.01563\n",
      "Training epoch: 3636, train loss: 0.01564, val loss: 0.01565\n",
      "Training epoch: 3637, train loss: 0.01572, val loss: 0.01588\n",
      "Training epoch: 3638, train loss: 0.01564, val loss: 0.01571\n",
      "Training epoch: 3639, train loss: 0.01611, val loss: 0.01623\n",
      "Training epoch: 3640, train loss: 0.01539, val loss: 0.01558\n",
      "Training epoch: 3641, train loss: 0.01561, val loss: 0.01579\n",
      "Training epoch: 3642, train loss: 0.01562, val loss: 0.01572\n",
      "Training epoch: 3643, train loss: 0.01545, val loss: 0.01560\n",
      "Training epoch: 3644, train loss: 0.01566, val loss: 0.01581\n",
      "Training epoch: 3645, train loss: 0.01562, val loss: 0.01574\n",
      "Training epoch: 3646, train loss: 0.01569, val loss: 0.01585\n",
      "Training epoch: 3647, train loss: 0.01544, val loss: 0.01564\n",
      "Training epoch: 3648, train loss: 0.01552, val loss: 0.01564\n",
      "Training epoch: 3649, train loss: 0.01576, val loss: 0.01599\n",
      "Training epoch: 3650, train loss: 0.01565, val loss: 0.01581\n",
      "Training epoch: 3651, train loss: 0.01567, val loss: 0.01584\n",
      "Training epoch: 3652, train loss: 0.01577, val loss: 0.01589\n",
      "Training epoch: 3653, train loss: 0.01542, val loss: 0.01564\n",
      "Training epoch: 3654, train loss: 0.01614, val loss: 0.01627\n",
      "Training epoch: 3655, train loss: 0.01581, val loss: 0.01602\n",
      "Training epoch: 3656, train loss: 0.01559, val loss: 0.01572\n",
      "Training epoch: 3657, train loss: 0.01546, val loss: 0.01564\n",
      "Training epoch: 3658, train loss: 0.01574, val loss: 0.01588\n",
      "Training epoch: 3659, train loss: 0.01561, val loss: 0.01575\n",
      "Training epoch: 3660, train loss: 0.01563, val loss: 0.01580\n",
      "Training epoch: 3661, train loss: 0.01557, val loss: 0.01570\n",
      "Training epoch: 3662, train loss: 0.01621, val loss: 0.01635\n",
      "Training epoch: 3663, train loss: 0.01543, val loss: 0.01555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3664, train loss: 0.01578, val loss: 0.01585\n",
      "Training epoch: 3665, train loss: 0.01588, val loss: 0.01590\n",
      "Training epoch: 3666, train loss: 0.01583, val loss: 0.01605\n",
      "Training epoch: 3667, train loss: 0.01579, val loss: 0.01599\n",
      "Training epoch: 3668, train loss: 0.01569, val loss: 0.01585\n",
      "Training epoch: 3669, train loss: 0.01583, val loss: 0.01589\n",
      "Training epoch: 3670, train loss: 0.01557, val loss: 0.01567\n",
      "Training epoch: 3671, train loss: 0.01558, val loss: 0.01578\n",
      "Training epoch: 3672, train loss: 0.01568, val loss: 0.01586\n",
      "Training epoch: 3673, train loss: 0.01564, val loss: 0.01570\n",
      "Training epoch: 3674, train loss: 0.01582, val loss: 0.01580\n",
      "Training epoch: 3675, train loss: 0.01557, val loss: 0.01570\n",
      "Training epoch: 3676, train loss: 0.01545, val loss: 0.01569\n",
      "Training epoch: 3677, train loss: 0.01567, val loss: 0.01582\n",
      "Training epoch: 3678, train loss: 0.01543, val loss: 0.01560\n",
      "Training epoch: 3679, train loss: 0.01559, val loss: 0.01573\n",
      "Training epoch: 3680, train loss: 0.01610, val loss: 0.01612\n",
      "Training epoch: 3681, train loss: 0.01573, val loss: 0.01573\n",
      "Training epoch: 3682, train loss: 0.01556, val loss: 0.01572\n",
      "Training epoch: 3683, train loss: 0.01553, val loss: 0.01562\n",
      "Training epoch: 3684, train loss: 0.01560, val loss: 0.01568\n",
      "Training epoch: 3685, train loss: 0.01568, val loss: 0.01577\n",
      "Training epoch: 3686, train loss: 0.01560, val loss: 0.01569\n",
      "Training epoch: 3687, train loss: 0.01560, val loss: 0.01565\n",
      "Training epoch: 3688, train loss: 0.01569, val loss: 0.01587\n",
      "Training epoch: 3689, train loss: 0.01558, val loss: 0.01582\n",
      "Training epoch: 3690, train loss: 0.01554, val loss: 0.01567\n",
      "Training epoch: 3691, train loss: 0.01553, val loss: 0.01559\n",
      "Training epoch: 3692, train loss: 0.01583, val loss: 0.01600\n",
      "Training epoch: 3693, train loss: 0.01619, val loss: 0.01629\n",
      "Training epoch: 3694, train loss: 0.01539, val loss: 0.01552\n",
      "Training epoch: 3695, train loss: 0.01544, val loss: 0.01564\n",
      "Training epoch: 3696, train loss: 0.01571, val loss: 0.01582\n",
      "Training epoch: 3697, train loss: 0.01542, val loss: 0.01553\n",
      "Training epoch: 3698, train loss: 0.01552, val loss: 0.01564\n",
      "Training epoch: 3699, train loss: 0.01556, val loss: 0.01557\n",
      "Training epoch: 3700, train loss: 0.01551, val loss: 0.01559\n",
      "Training epoch: 3701, train loss: 0.01551, val loss: 0.01566\n",
      "Training epoch: 3702, train loss: 0.01585, val loss: 0.01600\n",
      "Training epoch: 3703, train loss: 0.01615, val loss: 0.01629\n",
      "Training epoch: 3704, train loss: 0.01593, val loss: 0.01602\n",
      "Training epoch: 3705, train loss: 0.01626, val loss: 0.01649\n",
      "Training epoch: 3706, train loss: 0.01570, val loss: 0.01574\n",
      "Training epoch: 3707, train loss: 0.01540, val loss: 0.01559\n",
      "Training epoch: 3708, train loss: 0.01556, val loss: 0.01574\n",
      "Training epoch: 3709, train loss: 0.01598, val loss: 0.01620\n",
      "Training epoch: 3710, train loss: 0.01552, val loss: 0.01566\n",
      "Training epoch: 3711, train loss: 0.01573, val loss: 0.01592\n",
      "Training epoch: 3712, train loss: 0.01544, val loss: 0.01561\n",
      "Training epoch: 3713, train loss: 0.01545, val loss: 0.01556\n",
      "Training epoch: 3714, train loss: 0.01565, val loss: 0.01575\n",
      "Training epoch: 3715, train loss: 0.01598, val loss: 0.01618\n",
      "Training epoch: 3716, train loss: 0.01568, val loss: 0.01582\n",
      "Training epoch: 3717, train loss: 0.01557, val loss: 0.01563\n",
      "Training epoch: 3718, train loss: 0.01594, val loss: 0.01607\n",
      "Training epoch: 3719, train loss: 0.01552, val loss: 0.01570\n",
      "Training epoch: 3720, train loss: 0.01557, val loss: 0.01570\n",
      "Training epoch: 3721, train loss: 0.01544, val loss: 0.01558\n",
      "Training epoch: 3722, train loss: 0.01572, val loss: 0.01583\n",
      "Training epoch: 3723, train loss: 0.01567, val loss: 0.01575\n",
      "Training epoch: 3724, train loss: 0.01571, val loss: 0.01588\n",
      "Training epoch: 3725, train loss: 0.01559, val loss: 0.01571\n",
      "Training epoch: 3726, train loss: 0.01571, val loss: 0.01588\n",
      "Training epoch: 3727, train loss: 0.01574, val loss: 0.01586\n",
      "Training epoch: 3728, train loss: 0.01549, val loss: 0.01566\n",
      "Training epoch: 3729, train loss: 0.01577, val loss: 0.01587\n",
      "Training epoch: 3730, train loss: 0.01539, val loss: 0.01560\n",
      "Training epoch: 3731, train loss: 0.01560, val loss: 0.01573\n",
      "Training epoch: 3732, train loss: 0.01667, val loss: 0.01683\n",
      "Training epoch: 3733, train loss: 0.01598, val loss: 0.01617\n",
      "Training epoch: 3734, train loss: 0.01596, val loss: 0.01601\n",
      "Training epoch: 3735, train loss: 0.01572, val loss: 0.01585\n",
      "Training epoch: 3736, train loss: 0.01584, val loss: 0.01603\n",
      "Training epoch: 3737, train loss: 0.01557, val loss: 0.01559\n",
      "Training epoch: 3738, train loss: 0.01566, val loss: 0.01576\n",
      "Training epoch: 3739, train loss: 0.01572, val loss: 0.01577\n",
      "Training epoch: 3740, train loss: 0.01556, val loss: 0.01569\n",
      "Training epoch: 3741, train loss: 0.01573, val loss: 0.01577\n",
      "Training epoch: 3742, train loss: 0.01589, val loss: 0.01585\n",
      "Training epoch: 3743, train loss: 0.01567, val loss: 0.01588\n",
      "Training epoch: 3744, train loss: 0.01566, val loss: 0.01576\n",
      "Training epoch: 3745, train loss: 0.01561, val loss: 0.01581\n",
      "Training epoch: 3746, train loss: 0.01556, val loss: 0.01564\n",
      "Training epoch: 3747, train loss: 0.01566, val loss: 0.01573\n",
      "Training epoch: 3748, train loss: 0.01557, val loss: 0.01570\n",
      "Training epoch: 3749, train loss: 0.01564, val loss: 0.01583\n",
      "Training epoch: 3750, train loss: 0.01657, val loss: 0.01673\n",
      "Training epoch: 3751, train loss: 0.01563, val loss: 0.01590\n",
      "Training epoch: 3752, train loss: 0.01557, val loss: 0.01575\n",
      "Training epoch: 3753, train loss: 0.01546, val loss: 0.01558\n",
      "Training epoch: 3754, train loss: 0.01568, val loss: 0.01573\n",
      "Training epoch: 3755, train loss: 0.01586, val loss: 0.01594\n",
      "Training epoch: 3756, train loss: 0.01571, val loss: 0.01576\n",
      "Training epoch: 3757, train loss: 0.01640, val loss: 0.01664\n",
      "Training epoch: 3758, train loss: 0.01581, val loss: 0.01594\n",
      "Training epoch: 3759, train loss: 0.01554, val loss: 0.01579\n",
      "Training epoch: 3760, train loss: 0.01555, val loss: 0.01570\n",
      "Training epoch: 3761, train loss: 0.01575, val loss: 0.01589\n",
      "Training epoch: 3762, train loss: 0.01548, val loss: 0.01565\n",
      "Training epoch: 3763, train loss: 0.01561, val loss: 0.01578\n",
      "Training epoch: 3764, train loss: 0.01567, val loss: 0.01583\n",
      "Training epoch: 3765, train loss: 0.01569, val loss: 0.01579\n",
      "Training epoch: 3766, train loss: 0.01577, val loss: 0.01587\n",
      "Training epoch: 3767, train loss: 0.01572, val loss: 0.01572\n",
      "Training epoch: 3768, train loss: 0.01608, val loss: 0.01630\n",
      "Training epoch: 3769, train loss: 0.01571, val loss: 0.01589\n",
      "Training epoch: 3770, train loss: 0.01553, val loss: 0.01567\n",
      "Early stop at epoch 3770, With Testing Error: 0.01567\n",
      "Subnetwork pruning.\n",
      "Fine tuning.\n",
      "[1.03083 1.03994 1.05088]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1584 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = SOSxNN(input_num=10, input_dummy_num=0, subnet_num=10, subnet_arch=[10, 6], task=\"Regression\",\n",
    "               activation_func=tf.tanh, batch_size=1000, training_epochs=5000, lr_bp=0.001, lr_cl=0.1,\n",
    "               beta_threshold=0.01, tuning_epochs=0, l1_proj=0.001, l1_subnet = 0.01, smooth_lambda=10**(-5),\n",
    "               verbose=True, val_ratio=0.2, early_stop_thres=500)\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"test\", train_x)\n",
    "\n",
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "sosxnn_mse_stat = np.hstack([np.round(np.mean((scaler_y.inverse_transform(tr_pred) - scaler_y.inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(val_pred) - scaler_y.inverse_transform(model.val_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(pred_test) - scaler_y.inverse_transform(test_y))**2),5)])\n",
    "print(sosxnn_mse_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n",
      "Training epoch: 1, train loss: 0.06170, val loss: 0.06134\n",
      "Training epoch: 2, train loss: 0.05215, val loss: 0.05046\n",
      "Training epoch: 3, train loss: 0.04400, val loss: 0.04358\n",
      "Training epoch: 4, train loss: 0.03982, val loss: 0.03917\n",
      "Training epoch: 5, train loss: 0.03740, val loss: 0.03685\n",
      "Training epoch: 6, train loss: 0.03247, val loss: 0.03200\n",
      "Training epoch: 7, train loss: 0.03132, val loss: 0.03095\n",
      "Training epoch: 8, train loss: 0.02966, val loss: 0.02916\n",
      "Training epoch: 9, train loss: 0.02844, val loss: 0.02798\n",
      "Training epoch: 10, train loss: 0.02705, val loss: 0.02688\n",
      "Training epoch: 11, train loss: 0.02938, val loss: 0.02920\n",
      "Training epoch: 12, train loss: 0.02370, val loss: 0.02357\n",
      "Training epoch: 13, train loss: 0.02399, val loss: 0.02403\n",
      "Training epoch: 14, train loss: 0.02217, val loss: 0.02214\n",
      "Training epoch: 15, train loss: 0.02233, val loss: 0.02231\n",
      "Training epoch: 16, train loss: 0.02205, val loss: 0.02205\n",
      "Training epoch: 17, train loss: 0.02175, val loss: 0.02173\n",
      "Training epoch: 18, train loss: 0.02190, val loss: 0.02192\n",
      "Training epoch: 19, train loss: 0.02154, val loss: 0.02168\n",
      "Training epoch: 20, train loss: 0.02172, val loss: 0.02187\n",
      "Training epoch: 21, train loss: 0.02140, val loss: 0.02180\n",
      "Training epoch: 22, train loss: 0.02156, val loss: 0.02177\n",
      "Training epoch: 23, train loss: 0.02116, val loss: 0.02115\n",
      "Training epoch: 24, train loss: 0.02124, val loss: 0.02139\n",
      "Training epoch: 25, train loss: 0.02104, val loss: 0.02105\n",
      "Training epoch: 26, train loss: 0.02309, val loss: 0.02349\n",
      "Training epoch: 27, train loss: 0.02159, val loss: 0.02165\n",
      "Training epoch: 28, train loss: 0.02090, val loss: 0.02107\n",
      "Training epoch: 29, train loss: 0.02069, val loss: 0.02097\n",
      "Training epoch: 30, train loss: 0.02170, val loss: 0.02236\n",
      "Training epoch: 31, train loss: 0.02092, val loss: 0.02153\n",
      "Training epoch: 32, train loss: 0.02159, val loss: 0.02180\n",
      "Training epoch: 33, train loss: 0.02054, val loss: 0.02083\n",
      "Training epoch: 34, train loss: 0.01988, val loss: 0.02007\n",
      "Training epoch: 35, train loss: 0.02010, val loss: 0.02044\n",
      "Training epoch: 36, train loss: 0.01969, val loss: 0.01985\n",
      "Training epoch: 37, train loss: 0.02015, val loss: 0.02046\n",
      "Training epoch: 38, train loss: 0.02075, val loss: 0.02100\n",
      "Training epoch: 39, train loss: 0.01959, val loss: 0.01980\n",
      "Training epoch: 40, train loss: 0.02037, val loss: 0.02076\n",
      "Training epoch: 41, train loss: 0.02044, val loss: 0.02057\n",
      "Training epoch: 42, train loss: 0.01953, val loss: 0.01989\n",
      "Training epoch: 43, train loss: 0.01996, val loss: 0.02019\n",
      "Training epoch: 44, train loss: 0.02433, val loss: 0.02504\n",
      "Training epoch: 45, train loss: 0.02079, val loss: 0.02074\n",
      "Training epoch: 46, train loss: 0.01949, val loss: 0.01981\n",
      "Training epoch: 47, train loss: 0.01999, val loss: 0.02004\n",
      "Training epoch: 48, train loss: 0.02061, val loss: 0.02125\n",
      "Training epoch: 49, train loss: 0.01941, val loss: 0.01950\n",
      "Training epoch: 50, train loss: 0.01905, val loss: 0.01920\n",
      "Training epoch: 51, train loss: 0.01965, val loss: 0.01991\n",
      "Training epoch: 52, train loss: 0.02028, val loss: 0.02032\n",
      "Training epoch: 53, train loss: 0.01936, val loss: 0.01960\n",
      "Training epoch: 54, train loss: 0.01959, val loss: 0.01982\n",
      "Training epoch: 55, train loss: 0.01934, val loss: 0.01957\n",
      "Training epoch: 56, train loss: 0.01902, val loss: 0.01923\n",
      "Training epoch: 57, train loss: 0.01984, val loss: 0.02010\n",
      "Training epoch: 58, train loss: 0.01968, val loss: 0.02024\n",
      "Training epoch: 59, train loss: 0.02391, val loss: 0.02378\n",
      "Training epoch: 60, train loss: 0.02149, val loss: 0.02232\n",
      "Training epoch: 61, train loss: 0.02100, val loss: 0.02098\n",
      "Training epoch: 62, train loss: 0.01889, val loss: 0.01916\n",
      "Training epoch: 63, train loss: 0.01960, val loss: 0.01973\n",
      "Training epoch: 64, train loss: 0.02018, val loss: 0.02025\n",
      "Training epoch: 65, train loss: 0.01841, val loss: 0.01867\n",
      "Training epoch: 66, train loss: 0.01993, val loss: 0.01989\n",
      "Training epoch: 67, train loss: 0.01905, val loss: 0.01913\n",
      "Training epoch: 68, train loss: 0.01859, val loss: 0.01861\n",
      "Training epoch: 69, train loss: 0.01860, val loss: 0.01898\n",
      "Training epoch: 70, train loss: 0.02004, val loss: 0.02014\n",
      "Training epoch: 71, train loss: 0.01838, val loss: 0.01878\n",
      "Training epoch: 72, train loss: 0.01954, val loss: 0.01969\n",
      "Training epoch: 73, train loss: 0.01872, val loss: 0.01875\n",
      "Training epoch: 74, train loss: 0.01857, val loss: 0.01880\n",
      "Training epoch: 75, train loss: 0.01879, val loss: 0.01882\n",
      "Training epoch: 76, train loss: 0.01878, val loss: 0.01890\n",
      "Training epoch: 77, train loss: 0.01892, val loss: 0.01924\n",
      "Training epoch: 78, train loss: 0.01868, val loss: 0.01865\n",
      "Training epoch: 79, train loss: 0.01972, val loss: 0.02022\n",
      "Training epoch: 80, train loss: 0.01913, val loss: 0.01925\n",
      "Training epoch: 81, train loss: 0.01986, val loss: 0.01993\n",
      "Training epoch: 82, train loss: 0.01803, val loss: 0.01810\n",
      "Training epoch: 83, train loss: 0.01840, val loss: 0.01870\n",
      "Training epoch: 84, train loss: 0.01831, val loss: 0.01839\n",
      "Training epoch: 85, train loss: 0.01913, val loss: 0.01922\n",
      "Training epoch: 86, train loss: 0.01794, val loss: 0.01824\n",
      "Training epoch: 87, train loss: 0.01879, val loss: 0.01882\n",
      "Training epoch: 88, train loss: 0.01799, val loss: 0.01823\n",
      "Training epoch: 89, train loss: 0.01838, val loss: 0.01842\n",
      "Training epoch: 90, train loss: 0.02073, val loss: 0.02111\n",
      "Training epoch: 91, train loss: 0.01868, val loss: 0.01874\n",
      "Training epoch: 92, train loss: 0.01925, val loss: 0.01923\n",
      "Training epoch: 93, train loss: 0.01807, val loss: 0.01819\n",
      "Training epoch: 94, train loss: 0.01829, val loss: 0.01854\n",
      "Training epoch: 95, train loss: 0.02112, val loss: 0.02119\n",
      "Training epoch: 96, train loss: 0.01808, val loss: 0.01813\n",
      "Training epoch: 97, train loss: 0.01783, val loss: 0.01792\n",
      "Training epoch: 98, train loss: 0.01805, val loss: 0.01806\n",
      "Training epoch: 99, train loss: 0.01789, val loss: 0.01809\n",
      "Training epoch: 100, train loss: 0.01808, val loss: 0.01820\n",
      "Training epoch: 101, train loss: 0.01888, val loss: 0.01888\n",
      "Training epoch: 102, train loss: 0.01789, val loss: 0.01814\n",
      "Training epoch: 103, train loss: 0.01867, val loss: 0.01868\n",
      "Training epoch: 104, train loss: 0.01906, val loss: 0.01951\n",
      "Training epoch: 105, train loss: 0.01858, val loss: 0.01855\n",
      "Training epoch: 106, train loss: 0.01773, val loss: 0.01799\n",
      "Training epoch: 107, train loss: 0.01846, val loss: 0.01847\n",
      "Training epoch: 108, train loss: 0.01798, val loss: 0.01801\n",
      "Training epoch: 109, train loss: 0.01885, val loss: 0.01909\n",
      "Training epoch: 110, train loss: 0.01807, val loss: 0.01812\n",
      "Training epoch: 111, train loss: 0.01771, val loss: 0.01790\n",
      "Training epoch: 112, train loss: 0.01843, val loss: 0.01840\n",
      "Training epoch: 113, train loss: 0.01809, val loss: 0.01819\n",
      "Training epoch: 114, train loss: 0.01816, val loss: 0.01829\n",
      "Training epoch: 115, train loss: 0.01761, val loss: 0.01781\n",
      "Training epoch: 116, train loss: 0.02050, val loss: 0.02049\n",
      "Training epoch: 117, train loss: 0.01776, val loss: 0.01779\n",
      "Training epoch: 118, train loss: 0.01776, val loss: 0.01795\n",
      "Training epoch: 119, train loss: 0.01898, val loss: 0.01888\n",
      "Training epoch: 120, train loss: 0.01955, val loss: 0.01917\n",
      "Training epoch: 121, train loss: 0.01826, val loss: 0.01851\n",
      "Training epoch: 122, train loss: 0.01789, val loss: 0.01824\n",
      "Training epoch: 123, train loss: 0.01837, val loss: 0.01850\n",
      "Training epoch: 124, train loss: 0.01828, val loss: 0.01862\n",
      "Training epoch: 125, train loss: 0.01880, val loss: 0.01897\n",
      "Training epoch: 126, train loss: 0.02074, val loss: 0.02118\n",
      "Training epoch: 127, train loss: 0.01789, val loss: 0.01779\n",
      "Training epoch: 128, train loss: 0.01856, val loss: 0.01876\n",
      "Training epoch: 129, train loss: 0.01949, val loss: 0.01950\n",
      "Training epoch: 130, train loss: 0.02177, val loss: 0.02226\n",
      "Training epoch: 131, train loss: 0.01763, val loss: 0.01773\n",
      "Training epoch: 132, train loss: 0.01812, val loss: 0.01814\n",
      "Training epoch: 133, train loss: 0.01762, val loss: 0.01801\n",
      "Training epoch: 134, train loss: 0.01815, val loss: 0.01805\n",
      "Training epoch: 135, train loss: 0.02059, val loss: 0.02084\n",
      "Training epoch: 136, train loss: 0.01768, val loss: 0.01760\n",
      "Training epoch: 137, train loss: 0.01721, val loss: 0.01743\n",
      "Training epoch: 138, train loss: 0.01756, val loss: 0.01748\n",
      "Training epoch: 139, train loss: 0.01788, val loss: 0.01799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 140, train loss: 0.02001, val loss: 0.01987\n",
      "Training epoch: 141, train loss: 0.01836, val loss: 0.01857\n",
      "Training epoch: 142, train loss: 0.01740, val loss: 0.01739\n",
      "Training epoch: 143, train loss: 0.01746, val loss: 0.01771\n",
      "Training epoch: 144, train loss: 0.01835, val loss: 0.01838\n",
      "Training epoch: 145, train loss: 0.02131, val loss: 0.02190\n",
      "Training epoch: 146, train loss: 0.01958, val loss: 0.01926\n",
      "Training epoch: 147, train loss: 0.01833, val loss: 0.01834\n",
      "Training epoch: 148, train loss: 0.01771, val loss: 0.01757\n",
      "Training epoch: 149, train loss: 0.01734, val loss: 0.01731\n",
      "Training epoch: 150, train loss: 0.01756, val loss: 0.01766\n",
      "Training epoch: 151, train loss: 0.01959, val loss: 0.01960\n",
      "Training epoch: 152, train loss: 0.01845, val loss: 0.01832\n",
      "Training epoch: 153, train loss: 0.01766, val loss: 0.01769\n",
      "Training epoch: 154, train loss: 0.01723, val loss: 0.01721\n",
      "Training epoch: 155, train loss: 0.01748, val loss: 0.01727\n",
      "Training epoch: 156, train loss: 0.01783, val loss: 0.01798\n",
      "Training epoch: 157, train loss: 0.02046, val loss: 0.02088\n",
      "Training epoch: 158, train loss: 0.01778, val loss: 0.01762\n",
      "Training epoch: 159, train loss: 0.01724, val loss: 0.01735\n",
      "Training epoch: 160, train loss: 0.01763, val loss: 0.01764\n",
      "Training epoch: 161, train loss: 0.01792, val loss: 0.01800\n",
      "Training epoch: 162, train loss: 0.01717, val loss: 0.01718\n",
      "Training epoch: 163, train loss: 0.01918, val loss: 0.01909\n",
      "Training epoch: 164, train loss: 0.02268, val loss: 0.02305\n",
      "Training epoch: 165, train loss: 0.01720, val loss: 0.01742\n",
      "Training epoch: 166, train loss: 0.01873, val loss: 0.01852\n",
      "Training epoch: 167, train loss: 0.01788, val loss: 0.01822\n",
      "Training epoch: 168, train loss: 0.01915, val loss: 0.01898\n",
      "Training epoch: 169, train loss: 0.01928, val loss: 0.01971\n",
      "Training epoch: 170, train loss: 0.01870, val loss: 0.01872\n",
      "Training epoch: 171, train loss: 0.01708, val loss: 0.01705\n",
      "Training epoch: 172, train loss: 0.02060, val loss: 0.02083\n",
      "Training epoch: 173, train loss: 0.01750, val loss: 0.01734\n",
      "Training epoch: 174, train loss: 0.01951, val loss: 0.01981\n",
      "Training epoch: 175, train loss: 0.01807, val loss: 0.01796\n",
      "Training epoch: 176, train loss: 0.01856, val loss: 0.01859\n",
      "Training epoch: 177, train loss: 0.01848, val loss: 0.01827\n",
      "Training epoch: 178, train loss: 0.01723, val loss: 0.01722\n",
      "Training epoch: 179, train loss: 0.01722, val loss: 0.01729\n",
      "Training epoch: 180, train loss: 0.02148, val loss: 0.02153\n",
      "Training epoch: 181, train loss: 0.01729, val loss: 0.01725\n",
      "Training epoch: 182, train loss: 0.01699, val loss: 0.01693\n",
      "Training epoch: 183, train loss: 0.01715, val loss: 0.01696\n",
      "Training epoch: 184, train loss: 0.01741, val loss: 0.01739\n",
      "Training epoch: 185, train loss: 0.01704, val loss: 0.01728\n",
      "Training epoch: 186, train loss: 0.01723, val loss: 0.01718\n",
      "Training epoch: 187, train loss: 0.01736, val loss: 0.01754\n",
      "Training epoch: 188, train loss: 0.01660, val loss: 0.01664\n",
      "Training epoch: 189, train loss: 0.01820, val loss: 0.01811\n",
      "Training epoch: 190, train loss: 0.02410, val loss: 0.02428\n",
      "Training epoch: 191, train loss: 0.01810, val loss: 0.01820\n",
      "Training epoch: 192, train loss: 0.01838, val loss: 0.01840\n",
      "Training epoch: 193, train loss: 0.01705, val loss: 0.01713\n",
      "Training epoch: 194, train loss: 0.01693, val loss: 0.01698\n",
      "Training epoch: 195, train loss: 0.01736, val loss: 0.01743\n",
      "Training epoch: 196, train loss: 0.01980, val loss: 0.01946\n",
      "Training epoch: 197, train loss: 0.01805, val loss: 0.01792\n",
      "Training epoch: 198, train loss: 0.01693, val loss: 0.01681\n",
      "Training epoch: 199, train loss: 0.02006, val loss: 0.02008\n",
      "Training epoch: 200, train loss: 0.01695, val loss: 0.01687\n",
      "Training epoch: 201, train loss: 0.01784, val loss: 0.01776\n",
      "Training epoch: 202, train loss: 0.01839, val loss: 0.01897\n",
      "Training epoch: 203, train loss: 0.01810, val loss: 0.01840\n",
      "Training epoch: 204, train loss: 0.02129, val loss: 0.02197\n",
      "Training epoch: 205, train loss: 0.01955, val loss: 0.01939\n",
      "Training epoch: 206, train loss: 0.01867, val loss: 0.01858\n",
      "Training epoch: 207, train loss: 0.01751, val loss: 0.01748\n",
      "Training epoch: 208, train loss: 0.01679, val loss: 0.01672\n",
      "Training epoch: 209, train loss: 0.01774, val loss: 0.01783\n",
      "Training epoch: 210, train loss: 0.01805, val loss: 0.01793\n",
      "Training epoch: 211, train loss: 0.01720, val loss: 0.01726\n",
      "Training epoch: 212, train loss: 0.01706, val loss: 0.01726\n",
      "Training epoch: 213, train loss: 0.01757, val loss: 0.01749\n",
      "Training epoch: 214, train loss: 0.01912, val loss: 0.01919\n",
      "Training epoch: 215, train loss: 0.01686, val loss: 0.01704\n",
      "Training epoch: 216, train loss: 0.01743, val loss: 0.01731\n",
      "Training epoch: 217, train loss: 0.02468, val loss: 0.02473\n",
      "Training epoch: 218, train loss: 0.01811, val loss: 0.01802\n",
      "Training epoch: 219, train loss: 0.01802, val loss: 0.01827\n",
      "Training epoch: 220, train loss: 0.01738, val loss: 0.01716\n",
      "Training epoch: 221, train loss: 0.01907, val loss: 0.01890\n",
      "Training epoch: 222, train loss: 0.01777, val loss: 0.01804\n",
      "Training epoch: 223, train loss: 0.02282, val loss: 0.02275\n",
      "Training epoch: 224, train loss: 0.01754, val loss: 0.01739\n",
      "Training epoch: 225, train loss: 0.01708, val loss: 0.01717\n",
      "Training epoch: 226, train loss: 0.01696, val loss: 0.01704\n",
      "Training epoch: 227, train loss: 0.01805, val loss: 0.01812\n",
      "Training epoch: 228, train loss: 0.01653, val loss: 0.01668\n",
      "Training epoch: 229, train loss: 0.01727, val loss: 0.01728\n",
      "Training epoch: 230, train loss: 0.01698, val loss: 0.01700\n",
      "Training epoch: 231, train loss: 0.01695, val loss: 0.01694\n",
      "Training epoch: 232, train loss: 0.01758, val loss: 0.01752\n",
      "Training epoch: 233, train loss: 0.01726, val loss: 0.01725\n",
      "Training epoch: 234, train loss: 0.01835, val loss: 0.01832\n",
      "Training epoch: 235, train loss: 0.01809, val loss: 0.01839\n",
      "Training epoch: 236, train loss: 0.01676, val loss: 0.01666\n",
      "Training epoch: 237, train loss: 0.01765, val loss: 0.01766\n",
      "Training epoch: 238, train loss: 0.01678, val loss: 0.01695\n",
      "Training epoch: 239, train loss: 0.01715, val loss: 0.01703\n",
      "Training epoch: 240, train loss: 0.01666, val loss: 0.01655\n",
      "Training epoch: 241, train loss: 0.01714, val loss: 0.01727\n",
      "Training epoch: 242, train loss: 0.01917, val loss: 0.01894\n",
      "Training epoch: 243, train loss: 0.01690, val loss: 0.01693\n",
      "Training epoch: 244, train loss: 0.01721, val loss: 0.01716\n",
      "Training epoch: 245, train loss: 0.01747, val loss: 0.01737\n",
      "Training epoch: 246, train loss: 0.01807, val loss: 0.01830\n",
      "Training epoch: 247, train loss: 0.02011, val loss: 0.02008\n",
      "Training epoch: 248, train loss: 0.01738, val loss: 0.01718\n",
      "Training epoch: 249, train loss: 0.01712, val loss: 0.01699\n",
      "Training epoch: 250, train loss: 0.01730, val loss: 0.01730\n",
      "Training epoch: 251, train loss: 0.01983, val loss: 0.01976\n",
      "Training epoch: 252, train loss: 0.01677, val loss: 0.01652\n",
      "Training epoch: 253, train loss: 0.01894, val loss: 0.01894\n",
      "Training epoch: 254, train loss: 0.01730, val loss: 0.01727\n",
      "Training epoch: 255, train loss: 0.01762, val loss: 0.01761\n",
      "Training epoch: 256, train loss: 0.01721, val loss: 0.01714\n",
      "Training epoch: 257, train loss: 0.01770, val loss: 0.01767\n",
      "Training epoch: 258, train loss: 0.01756, val loss: 0.01749\n",
      "Training epoch: 259, train loss: 0.01678, val loss: 0.01720\n",
      "Training epoch: 260, train loss: 0.02309, val loss: 0.02285\n",
      "Training epoch: 261, train loss: 0.01672, val loss: 0.01688\n",
      "Training epoch: 262, train loss: 0.01716, val loss: 0.01713\n",
      "Training epoch: 263, train loss: 0.01840, val loss: 0.01832\n",
      "Training epoch: 264, train loss: 0.01717, val loss: 0.01712\n",
      "Training epoch: 265, train loss: 0.01673, val loss: 0.01679\n",
      "Training epoch: 266, train loss: 0.01687, val loss: 0.01671\n",
      "Training epoch: 267, train loss: 0.01707, val loss: 0.01715\n",
      "Training epoch: 268, train loss: 0.01833, val loss: 0.01848\n",
      "Training epoch: 269, train loss: 0.01774, val loss: 0.01764\n",
      "Training epoch: 270, train loss: 0.01747, val loss: 0.01730\n",
      "Training epoch: 271, train loss: 0.01752, val loss: 0.01760\n",
      "Training epoch: 272, train loss: 0.01789, val loss: 0.01792\n",
      "Training epoch: 273, train loss: 0.01760, val loss: 0.01734\n",
      "Training epoch: 274, train loss: 0.01856, val loss: 0.01867\n",
      "Training epoch: 275, train loss: 0.01852, val loss: 0.01835\n",
      "Training epoch: 276, train loss: 0.01776, val loss: 0.01764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 277, train loss: 0.01953, val loss: 0.01953\n",
      "Training epoch: 278, train loss: 0.01700, val loss: 0.01701\n",
      "Training epoch: 279, train loss: 0.01958, val loss: 0.02007\n",
      "Training epoch: 280, train loss: 0.01866, val loss: 0.01871\n",
      "Training epoch: 281, train loss: 0.01715, val loss: 0.01730\n",
      "Training epoch: 282, train loss: 0.01660, val loss: 0.01667\n",
      "Training epoch: 283, train loss: 0.01694, val loss: 0.01694\n",
      "Training epoch: 284, train loss: 0.01800, val loss: 0.01835\n",
      "Training epoch: 285, train loss: 0.02091, val loss: 0.02077\n",
      "Training epoch: 286, train loss: 0.01897, val loss: 0.01887\n",
      "Training epoch: 287, train loss: 0.01749, val loss: 0.01717\n",
      "Training epoch: 288, train loss: 0.01838, val loss: 0.01833\n",
      "Training epoch: 289, train loss: 0.01670, val loss: 0.01677\n",
      "Training epoch: 290, train loss: 0.01736, val loss: 0.01737\n",
      "Training epoch: 291, train loss: 0.01807, val loss: 0.01797\n",
      "Training epoch: 292, train loss: 0.01787, val loss: 0.01834\n",
      "Training epoch: 293, train loss: 0.01729, val loss: 0.01740\n",
      "Training epoch: 294, train loss: 0.01683, val loss: 0.01677\n",
      "Training epoch: 295, train loss: 0.01661, val loss: 0.01671\n",
      "Training epoch: 296, train loss: 0.01641, val loss: 0.01644\n",
      "Training epoch: 297, train loss: 0.01789, val loss: 0.01771\n",
      "Training epoch: 298, train loss: 0.01939, val loss: 0.01909\n",
      "Training epoch: 299, train loss: 0.01756, val loss: 0.01769\n",
      "Training epoch: 300, train loss: 0.01722, val loss: 0.01726\n",
      "Training epoch: 301, train loss: 0.01725, val loss: 0.01732\n",
      "Training epoch: 302, train loss: 0.01707, val loss: 0.01692\n",
      "Training epoch: 303, train loss: 0.01666, val loss: 0.01659\n",
      "Training epoch: 304, train loss: 0.01697, val loss: 0.01703\n",
      "Training epoch: 305, train loss: 0.01662, val loss: 0.01687\n",
      "Training epoch: 306, train loss: 0.01646, val loss: 0.01641\n",
      "Training epoch: 307, train loss: 0.01696, val loss: 0.01688\n",
      "Training epoch: 308, train loss: 0.01706, val loss: 0.01702\n",
      "Training epoch: 309, train loss: 0.01840, val loss: 0.01863\n",
      "Training epoch: 310, train loss: 0.01660, val loss: 0.01676\n",
      "Training epoch: 311, train loss: 0.01799, val loss: 0.01779\n",
      "Training epoch: 312, train loss: 0.01694, val loss: 0.01703\n",
      "Training epoch: 313, train loss: 0.01669, val loss: 0.01674\n",
      "Training epoch: 314, train loss: 0.02024, val loss: 0.02050\n",
      "Training epoch: 315, train loss: 0.02100, val loss: 0.02156\n",
      "Training epoch: 316, train loss: 0.01873, val loss: 0.01903\n",
      "Training epoch: 317, train loss: 0.01858, val loss: 0.01867\n",
      "Training epoch: 318, train loss: 0.01906, val loss: 0.01941\n",
      "Training epoch: 319, train loss: 0.01784, val loss: 0.01789\n",
      "Training epoch: 320, train loss: 0.01860, val loss: 0.01878\n",
      "Training epoch: 321, train loss: 0.01766, val loss: 0.01743\n",
      "Training epoch: 322, train loss: 0.01746, val loss: 0.01763\n",
      "Training epoch: 323, train loss: 0.01710, val loss: 0.01736\n",
      "Training epoch: 324, train loss: 0.01724, val loss: 0.01724\n",
      "Training epoch: 325, train loss: 0.01634, val loss: 0.01643\n",
      "Training epoch: 326, train loss: 0.01687, val loss: 0.01697\n",
      "Training epoch: 327, train loss: 0.01887, val loss: 0.01903\n",
      "Training epoch: 328, train loss: 0.02039, val loss: 0.02011\n",
      "Training epoch: 329, train loss: 0.01697, val loss: 0.01726\n",
      "Training epoch: 330, train loss: 0.01665, val loss: 0.01672\n",
      "Training epoch: 331, train loss: 0.01750, val loss: 0.01754\n",
      "Training epoch: 332, train loss: 0.01668, val loss: 0.01689\n",
      "Training epoch: 333, train loss: 0.01746, val loss: 0.01744\n",
      "Training epoch: 334, train loss: 0.01681, val loss: 0.01687\n",
      "Training epoch: 335, train loss: 0.01816, val loss: 0.01840\n",
      "Training epoch: 336, train loss: 0.01697, val loss: 0.01703\n",
      "Training epoch: 337, train loss: 0.01639, val loss: 0.01658\n",
      "Training epoch: 338, train loss: 0.01754, val loss: 0.01746\n",
      "Training epoch: 339, train loss: 0.01803, val loss: 0.01799\n",
      "Training epoch: 340, train loss: 0.01799, val loss: 0.01803\n",
      "Training epoch: 341, train loss: 0.01824, val loss: 0.01839\n",
      "Training epoch: 342, train loss: 0.01675, val loss: 0.01664\n",
      "Training epoch: 343, train loss: 0.01715, val loss: 0.01729\n",
      "Training epoch: 344, train loss: 0.01673, val loss: 0.01696\n",
      "Training epoch: 345, train loss: 0.01830, val loss: 0.01827\n",
      "Training epoch: 346, train loss: 0.01796, val loss: 0.01808\n",
      "Training epoch: 347, train loss: 0.01744, val loss: 0.01794\n",
      "Training epoch: 348, train loss: 0.02293, val loss: 0.02267\n",
      "Training epoch: 349, train loss: 0.01764, val loss: 0.01787\n",
      "Training epoch: 350, train loss: 0.01725, val loss: 0.01729\n",
      "Training epoch: 351, train loss: 0.01757, val loss: 0.01760\n",
      "Training epoch: 352, train loss: 0.01962, val loss: 0.01971\n",
      "Training epoch: 353, train loss: 0.01647, val loss: 0.01671\n",
      "Training epoch: 354, train loss: 0.02035, val loss: 0.02042\n",
      "Training epoch: 355, train loss: 0.01706, val loss: 0.01721\n",
      "Training epoch: 356, train loss: 0.01691, val loss: 0.01704\n",
      "Training epoch: 357, train loss: 0.01738, val loss: 0.01780\n",
      "Training epoch: 358, train loss: 0.01679, val loss: 0.01684\n",
      "Training epoch: 359, train loss: 0.01738, val loss: 0.01746\n",
      "Training epoch: 360, train loss: 0.01955, val loss: 0.01953\n",
      "Training epoch: 361, train loss: 0.01704, val loss: 0.01746\n",
      "Training epoch: 362, train loss: 0.02201, val loss: 0.02203\n",
      "Training epoch: 363, train loss: 0.01706, val loss: 0.01712\n",
      "Training epoch: 364, train loss: 0.01716, val loss: 0.01751\n",
      "Training epoch: 365, train loss: 0.01694, val loss: 0.01680\n",
      "Training epoch: 366, train loss: 0.01723, val loss: 0.01745\n",
      "Training epoch: 367, train loss: 0.01747, val loss: 0.01761\n",
      "Training epoch: 368, train loss: 0.01721, val loss: 0.01726\n",
      "Training epoch: 369, train loss: 0.01655, val loss: 0.01689\n",
      "Training epoch: 370, train loss: 0.01690, val loss: 0.01688\n",
      "Training epoch: 371, train loss: 0.01723, val loss: 0.01782\n",
      "Training epoch: 372, train loss: 0.01706, val loss: 0.01690\n",
      "Training epoch: 373, train loss: 0.01635, val loss: 0.01657\n",
      "Training epoch: 374, train loss: 0.01663, val loss: 0.01692\n",
      "Training epoch: 375, train loss: 0.01919, val loss: 0.01919\n",
      "Training epoch: 376, train loss: 0.01711, val loss: 0.01734\n",
      "Training epoch: 377, train loss: 0.01678, val loss: 0.01680\n",
      "Training epoch: 378, train loss: 0.02068, val loss: 0.02104\n",
      "Training epoch: 379, train loss: 0.01654, val loss: 0.01684\n",
      "Training epoch: 380, train loss: 0.01688, val loss: 0.01688\n",
      "Training epoch: 381, train loss: 0.01941, val loss: 0.01971\n",
      "Training epoch: 382, train loss: 0.01891, val loss: 0.01883\n",
      "Training epoch: 383, train loss: 0.01667, val loss: 0.01688\n",
      "Training epoch: 384, train loss: 0.01657, val loss: 0.01677\n",
      "Training epoch: 385, train loss: 0.01702, val loss: 0.01706\n",
      "Training epoch: 386, train loss: 0.01766, val loss: 0.01781\n",
      "Training epoch: 387, train loss: 0.01805, val loss: 0.01838\n",
      "Training epoch: 388, train loss: 0.01721, val loss: 0.01729\n",
      "Training epoch: 389, train loss: 0.01712, val loss: 0.01719\n",
      "Training epoch: 390, train loss: 0.01683, val loss: 0.01725\n",
      "Training epoch: 391, train loss: 0.01669, val loss: 0.01676\n",
      "Training epoch: 392, train loss: 0.02049, val loss: 0.02061\n",
      "Training epoch: 393, train loss: 0.01673, val loss: 0.01708\n",
      "Training epoch: 394, train loss: 0.01719, val loss: 0.01744\n",
      "Training epoch: 395, train loss: 0.01921, val loss: 0.01934\n",
      "Training epoch: 396, train loss: 0.01948, val loss: 0.01968\n",
      "Training epoch: 397, train loss: 0.01699, val loss: 0.01702\n",
      "Training epoch: 398, train loss: 0.01817, val loss: 0.01826\n",
      "Training epoch: 399, train loss: 0.01828, val loss: 0.01876\n",
      "Training epoch: 400, train loss: 0.01843, val loss: 0.01895\n",
      "Training epoch: 401, train loss: 0.01717, val loss: 0.01724\n",
      "Training epoch: 402, train loss: 0.01691, val loss: 0.01728\n",
      "Training epoch: 403, train loss: 0.01660, val loss: 0.01654\n",
      "Training epoch: 404, train loss: 0.01727, val loss: 0.01735\n",
      "Training epoch: 405, train loss: 0.02291, val loss: 0.02351\n",
      "Training epoch: 406, train loss: 0.01925, val loss: 0.01981\n",
      "Training epoch: 407, train loss: 0.01801, val loss: 0.01783\n",
      "Training epoch: 408, train loss: 0.01730, val loss: 0.01759\n",
      "Training epoch: 409, train loss: 0.01834, val loss: 0.01848\n",
      "Training epoch: 410, train loss: 0.01875, val loss: 0.01868\n",
      "Training epoch: 411, train loss: 0.01733, val loss: 0.01739\n",
      "Training epoch: 412, train loss: 0.01734, val loss: 0.01748\n",
      "Training epoch: 413, train loss: 0.01723, val loss: 0.01757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 414, train loss: 0.01714, val loss: 0.01722\n",
      "Training epoch: 415, train loss: 0.01665, val loss: 0.01697\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = SOSxNN(input_num=10, input_dummy_num=0, subnet_num=10, subnet_arch=[10, 6], task=\"Regression\",\n",
    "               activation_func=tf.tanh, batch_size=1000, training_epochs=5000, lr_bp=0.001, lr_cl=0.1,\n",
    "               beta_threshold=0.01, tuning_epochs=0, l1_proj=0.001, l1_subnet = 0.01, smooth_lambda=10**(-4),\n",
    "               verbose=True, val_ratio=0.2, early_stop_thres=500)\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"test\", train_x)\n",
    "\n",
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "sosxnn_mse_stat = np.hstack([np.round(np.mean((scaler_y.inverse_transform(tr_pred) - scaler_y.inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(val_pred) - scaler_y.inverse_transform(model.val_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(pred_test) - scaler_y.inverse_transform(test_y))**2),5)])\n",
    "print(sosxnn_mse_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n",
      "Training epoch: 1, train loss: 0.06137, val loss: 0.06088\n",
      "Training epoch: 2, train loss: 0.04753, val loss: 0.04731\n",
      "Training epoch: 3, train loss: 0.04208, val loss: 0.04122\n",
      "Training epoch: 4, train loss: 0.04031, val loss: 0.03954\n",
      "Training epoch: 5, train loss: 0.03597, val loss: 0.03514\n",
      "Training epoch: 6, train loss: 0.03247, val loss: 0.03187\n",
      "Training epoch: 7, train loss: 0.03047, val loss: 0.02990\n",
      "Training epoch: 8, train loss: 0.03006, val loss: 0.02890\n",
      "Training epoch: 9, train loss: 0.03512, val loss: 0.03617\n",
      "Training epoch: 10, train loss: 0.02596, val loss: 0.02589\n",
      "Training epoch: 11, train loss: 0.02954, val loss: 0.02935\n",
      "Training epoch: 12, train loss: 0.02411, val loss: 0.02400\n",
      "Training epoch: 13, train loss: 0.02643, val loss: 0.02618\n",
      "Training epoch: 14, train loss: 0.02371, val loss: 0.02360\n",
      "Training epoch: 15, train loss: 0.02213, val loss: 0.02212\n",
      "Training epoch: 16, train loss: 0.02386, val loss: 0.02360\n",
      "Training epoch: 17, train loss: 0.02322, val loss: 0.02318\n",
      "Training epoch: 18, train loss: 0.02685, val loss: 0.02662\n",
      "Training epoch: 19, train loss: 0.02301, val loss: 0.02326\n",
      "Training epoch: 20, train loss: 0.02248, val loss: 0.02310\n",
      "Training epoch: 21, train loss: 0.02279, val loss: 0.02291\n",
      "Training epoch: 22, train loss: 0.02221, val loss: 0.02249\n",
      "Training epoch: 23, train loss: 0.02203, val loss: 0.02213\n",
      "Training epoch: 24, train loss: 0.02224, val loss: 0.02240\n",
      "Training epoch: 25, train loss: 0.02454, val loss: 0.02396\n",
      "Training epoch: 26, train loss: 0.02332, val loss: 0.02388\n",
      "Training epoch: 27, train loss: 0.02226, val loss: 0.02225\n",
      "Training epoch: 28, train loss: 0.02422, val loss: 0.02445\n",
      "Training epoch: 29, train loss: 0.02199, val loss: 0.02220\n",
      "Training epoch: 30, train loss: 0.02902, val loss: 0.03054\n",
      "Training epoch: 31, train loss: 0.02118, val loss: 0.02192\n",
      "Training epoch: 32, train loss: 0.02377, val loss: 0.02396\n",
      "Training epoch: 33, train loss: 0.02435, val loss: 0.02484\n",
      "Training epoch: 34, train loss: 0.02123, val loss: 0.02131\n",
      "Training epoch: 35, train loss: 0.02029, val loss: 0.02070\n",
      "Training epoch: 36, train loss: 0.02586, val loss: 0.02752\n",
      "Training epoch: 37, train loss: 0.02098, val loss: 0.02117\n",
      "Training epoch: 38, train loss: 0.02210, val loss: 0.02211\n",
      "Training epoch: 39, train loss: 0.02140, val loss: 0.02207\n",
      "Training epoch: 40, train loss: 0.02254, val loss: 0.02344\n",
      "Training epoch: 41, train loss: 0.02620, val loss: 0.02607\n",
      "Training epoch: 42, train loss: 0.02271, val loss: 0.02335\n",
      "Training epoch: 43, train loss: 0.02084, val loss: 0.02111\n",
      "Training epoch: 44, train loss: 0.02629, val loss: 0.02658\n",
      "Training epoch: 45, train loss: 0.02072, val loss: 0.02085\n",
      "Training epoch: 46, train loss: 0.02077, val loss: 0.02083\n",
      "Training epoch: 47, train loss: 0.02155, val loss: 0.02149\n",
      "Training epoch: 48, train loss: 0.02056, val loss: 0.02094\n",
      "Training epoch: 49, train loss: 0.01993, val loss: 0.02043\n",
      "Training epoch: 50, train loss: 0.03386, val loss: 0.03642\n",
      "Training epoch: 51, train loss: 0.02138, val loss: 0.02166\n",
      "Training epoch: 52, train loss: 0.02183, val loss: 0.02227\n",
      "Training epoch: 53, train loss: 0.01994, val loss: 0.01991\n",
      "Training epoch: 54, train loss: 0.02347, val loss: 0.02356\n",
      "Training epoch: 55, train loss: 0.02222, val loss: 0.02256\n",
      "Training epoch: 56, train loss: 0.05662, val loss: 0.06073\n",
      "Training epoch: 57, train loss: 0.02478, val loss: 0.02612\n",
      "Training epoch: 58, train loss: 0.02175, val loss: 0.02269\n",
      "Training epoch: 59, train loss: 0.02073, val loss: 0.02149\n",
      "Training epoch: 60, train loss: 0.01985, val loss: 0.02072\n",
      "Training epoch: 61, train loss: 0.01992, val loss: 0.02075\n",
      "Training epoch: 62, train loss: 0.02181, val loss: 0.02221\n",
      "Training epoch: 63, train loss: 0.01981, val loss: 0.02054\n",
      "Training epoch: 64, train loss: 0.02286, val loss: 0.02344\n",
      "Training epoch: 65, train loss: 0.02044, val loss: 0.02138\n",
      "Training epoch: 66, train loss: 0.02424, val loss: 0.02482\n",
      "Training epoch: 67, train loss: 0.03105, val loss: 0.03285\n",
      "Training epoch: 68, train loss: 0.02092, val loss: 0.02160\n",
      "Training epoch: 69, train loss: 0.02188, val loss: 0.02202\n",
      "Training epoch: 70, train loss: 0.01991, val loss: 0.02015\n",
      "Training epoch: 71, train loss: 0.02016, val loss: 0.02026\n",
      "Training epoch: 72, train loss: 0.02402, val loss: 0.02392\n",
      "Training epoch: 73, train loss: 0.01988, val loss: 0.01997\n",
      "Training epoch: 74, train loss: 0.02147, val loss: 0.02146\n",
      "Training epoch: 75, train loss: 0.01961, val loss: 0.01979\n",
      "Training epoch: 76, train loss: 0.01928, val loss: 0.01943\n",
      "Training epoch: 77, train loss: 0.01982, val loss: 0.02017\n",
      "Training epoch: 78, train loss: 0.01947, val loss: 0.01996\n",
      "Training epoch: 79, train loss: 0.02293, val loss: 0.02400\n",
      "Training epoch: 80, train loss: 0.03257, val loss: 0.03221\n",
      "Training epoch: 81, train loss: 0.02106, val loss: 0.02161\n",
      "Training epoch: 82, train loss: 0.02124, val loss: 0.02158\n",
      "Training epoch: 83, train loss: 0.01991, val loss: 0.01987\n",
      "Training epoch: 84, train loss: 0.01928, val loss: 0.01946\n",
      "Training epoch: 85, train loss: 0.01935, val loss: 0.01918\n",
      "Training epoch: 86, train loss: 0.01906, val loss: 0.01921\n",
      "Training epoch: 87, train loss: 0.02638, val loss: 0.02603\n",
      "Training epoch: 88, train loss: 0.02029, val loss: 0.02046\n",
      "Training epoch: 89, train loss: 0.02190, val loss: 0.02278\n",
      "Training epoch: 90, train loss: 0.01969, val loss: 0.02047\n",
      "Training epoch: 91, train loss: 0.01978, val loss: 0.01991\n",
      "Training epoch: 92, train loss: 0.01956, val loss: 0.01963\n",
      "Training epoch: 93, train loss: 0.01885, val loss: 0.01932\n",
      "Training epoch: 94, train loss: 0.01886, val loss: 0.01909\n",
      "Training epoch: 95, train loss: 0.01932, val loss: 0.01931\n",
      "Training epoch: 96, train loss: 0.01886, val loss: 0.01884\n",
      "Training epoch: 97, train loss: 0.02233, val loss: 0.02304\n",
      "Training epoch: 98, train loss: 0.02013, val loss: 0.02021\n",
      "Training epoch: 99, train loss: 0.02001, val loss: 0.02001\n",
      "Training epoch: 100, train loss: 0.36315, val loss: 0.38599\n",
      "Training epoch: 101, train loss: 0.02070, val loss: 0.02174\n",
      "Training epoch: 102, train loss: 0.02036, val loss: 0.02103\n",
      "Training epoch: 103, train loss: 0.02165, val loss: 0.02278\n",
      "Training epoch: 104, train loss: 0.01922, val loss: 0.01999\n",
      "Training epoch: 105, train loss: 0.02138, val loss: 0.02218\n",
      "Training epoch: 106, train loss: 0.02815, val loss: 0.02897\n",
      "Training epoch: 107, train loss: 0.02090, val loss: 0.02175\n",
      "Training epoch: 108, train loss: 0.01934, val loss: 0.02000\n",
      "Training epoch: 109, train loss: 0.01870, val loss: 0.01946\n",
      "Training epoch: 110, train loss: 0.02016, val loss: 0.02048\n",
      "Training epoch: 111, train loss: 0.02074, val loss: 0.02053\n",
      "Training epoch: 112, train loss: 0.01884, val loss: 0.01880\n",
      "Training epoch: 113, train loss: 0.02033, val loss: 0.02059\n",
      "Training epoch: 114, train loss: 0.02215, val loss: 0.02263\n",
      "Training epoch: 115, train loss: 0.01816, val loss: 0.01852\n",
      "Training epoch: 116, train loss: 0.02597, val loss: 0.02640\n",
      "Training epoch: 117, train loss: 0.01898, val loss: 0.01918\n",
      "Training epoch: 118, train loss: 0.01850, val loss: 0.01857\n",
      "Training epoch: 119, train loss: 0.01986, val loss: 0.01966\n",
      "Training epoch: 120, train loss: 0.02046, val loss: 0.02088\n",
      "Training epoch: 121, train loss: 0.02613, val loss: 0.02649\n",
      "Training epoch: 122, train loss: 0.01863, val loss: 0.01899\n",
      "Training epoch: 123, train loss: 0.01809, val loss: 0.01856\n",
      "Training epoch: 124, train loss: 0.02060, val loss: 0.02130\n",
      "Training epoch: 125, train loss: 0.02490, val loss: 0.02596\n",
      "Training epoch: 126, train loss: 0.03050, val loss: 0.03250\n",
      "Training epoch: 127, train loss: 0.02161, val loss: 0.02119\n",
      "Training epoch: 128, train loss: 0.01901, val loss: 0.01956\n",
      "Training epoch: 129, train loss: 0.04115, val loss: 0.04133\n",
      "Training epoch: 130, train loss: 0.03008, val loss: 0.03097\n",
      "Training epoch: 131, train loss: 0.01818, val loss: 0.01833\n",
      "Training epoch: 132, train loss: 0.02099, val loss: 0.02127\n",
      "Training epoch: 133, train loss: 0.02738, val loss: 0.02795\n",
      "Training epoch: 134, train loss: 0.02153, val loss: 0.02151\n",
      "Training epoch: 135, train loss: 0.02482, val loss: 0.02525\n",
      "Training epoch: 136, train loss: 0.01951, val loss: 0.01998\n",
      "Training epoch: 137, train loss: 0.03188, val loss: 0.03271\n",
      "Training epoch: 138, train loss: 0.01964, val loss: 0.01935\n",
      "Training epoch: 139, train loss: 0.01905, val loss: 0.01957\n",
      "Training epoch: 140, train loss: 0.01924, val loss: 0.01935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 141, train loss: 0.02790, val loss: 0.02798\n",
      "Training epoch: 142, train loss: 0.02007, val loss: 0.02042\n",
      "Training epoch: 143, train loss: 0.01919, val loss: 0.01965\n",
      "Training epoch: 144, train loss: 0.02413, val loss: 0.02423\n",
      "Training epoch: 145, train loss: 0.01941, val loss: 0.01972\n",
      "Training epoch: 146, train loss: 0.02868, val loss: 0.02863\n",
      "Training epoch: 147, train loss: 0.05466, val loss: 0.05847\n",
      "Training epoch: 148, train loss: 0.02871, val loss: 0.02922\n",
      "Training epoch: 149, train loss: 0.02084, val loss: 0.02112\n",
      "Training epoch: 150, train loss: 0.02062, val loss: 0.02107\n",
      "Training epoch: 151, train loss: 0.01972, val loss: 0.01965\n",
      "Training epoch: 152, train loss: 0.01871, val loss: 0.01878\n",
      "Training epoch: 153, train loss: 0.01799, val loss: 0.01835\n",
      "Training epoch: 154, train loss: 0.01740, val loss: 0.01772\n",
      "Training epoch: 155, train loss: 0.01784, val loss: 0.01797\n",
      "Training epoch: 156, train loss: 0.01885, val loss: 0.01894\n",
      "Training epoch: 157, train loss: 0.01825, val loss: 0.01843\n",
      "Training epoch: 158, train loss: 0.01808, val loss: 0.01865\n",
      "Training epoch: 159, train loss: 0.02886, val loss: 0.02891\n",
      "Training epoch: 160, train loss: 0.02689, val loss: 0.02694\n",
      "Training epoch: 161, train loss: 0.02789, val loss: 0.02878\n",
      "Training epoch: 162, train loss: 0.01847, val loss: 0.01823\n",
      "Training epoch: 163, train loss: 0.02259, val loss: 0.02286\n",
      "Training epoch: 164, train loss: 0.02702, val loss: 0.02688\n",
      "Training epoch: 165, train loss: 0.01825, val loss: 0.01833\n",
      "Training epoch: 166, train loss: 0.01964, val loss: 0.01991\n",
      "Training epoch: 167, train loss: 0.01962, val loss: 0.02028\n",
      "Training epoch: 168, train loss: 0.02121, val loss: 0.02189\n",
      "Training epoch: 169, train loss: 0.02037, val loss: 0.02048\n",
      "Training epoch: 170, train loss: 0.03006, val loss: 0.02994\n",
      "Training epoch: 171, train loss: 0.01753, val loss: 0.01787\n",
      "Training epoch: 172, train loss: 0.01823, val loss: 0.01852\n",
      "Training epoch: 173, train loss: 0.02198, val loss: 0.02207\n",
      "Training epoch: 174, train loss: 0.01899, val loss: 0.01920\n",
      "Training epoch: 175, train loss: 0.03806, val loss: 0.03826\n",
      "Training epoch: 176, train loss: 0.04184, val loss: 0.04182\n",
      "Training epoch: 177, train loss: 0.01934, val loss: 0.01944\n",
      "Training epoch: 178, train loss: 0.01864, val loss: 0.01835\n",
      "Training epoch: 179, train loss: 0.02492, val loss: 0.02492\n",
      "Training epoch: 180, train loss: 0.02511, val loss: 0.02542\n",
      "Training epoch: 181, train loss: 0.02029, val loss: 0.02098\n",
      "Training epoch: 182, train loss: 0.01951, val loss: 0.01983\n",
      "Training epoch: 183, train loss: 0.02275, val loss: 0.02289\n",
      "Training epoch: 184, train loss: 0.02641, val loss: 0.02641\n",
      "Training epoch: 185, train loss: 0.02823, val loss: 0.02902\n",
      "Training epoch: 186, train loss: 0.05392, val loss: 0.05291\n",
      "Training epoch: 187, train loss: 0.02182, val loss: 0.02193\n",
      "Training epoch: 188, train loss: 0.03350, val loss: 0.03500\n",
      "Training epoch: 189, train loss: 0.02072, val loss: 0.02064\n",
      "Training epoch: 190, train loss: 0.01906, val loss: 0.01879\n",
      "Training epoch: 191, train loss: 0.02532, val loss: 0.02555\n",
      "Training epoch: 192, train loss: 0.01889, val loss: 0.01889\n",
      "Training epoch: 193, train loss: 0.02092, val loss: 0.02137\n",
      "Training epoch: 194, train loss: 0.01855, val loss: 0.01886\n",
      "Training epoch: 195, train loss: 0.02721, val loss: 0.02733\n",
      "Training epoch: 196, train loss: 0.02284, val loss: 0.02269\n",
      "Training epoch: 197, train loss: 0.02870, val loss: 0.02871\n",
      "Training epoch: 198, train loss: 0.01841, val loss: 0.01861\n",
      "Training epoch: 199, train loss: 0.01954, val loss: 0.01971\n",
      "Training epoch: 200, train loss: 0.02130, val loss: 0.02142\n",
      "Training epoch: 201, train loss: 0.01912, val loss: 0.01905\n",
      "Training epoch: 202, train loss: 0.01764, val loss: 0.01779\n",
      "Training epoch: 203, train loss: 0.02467, val loss: 0.02460\n",
      "Training epoch: 204, train loss: 0.02102, val loss: 0.02261\n",
      "Training epoch: 205, train loss: 0.01865, val loss: 0.01953\n",
      "Training epoch: 206, train loss: 0.01968, val loss: 0.02051\n",
      "Training epoch: 207, train loss: 0.01920, val loss: 0.02024\n",
      "Training epoch: 208, train loss: 0.01921, val loss: 0.01994\n",
      "Training epoch: 209, train loss: 0.02070, val loss: 0.02141\n",
      "Training epoch: 210, train loss: 0.01954, val loss: 0.02014\n",
      "Training epoch: 211, train loss: 0.01915, val loss: 0.01961\n",
      "Training epoch: 212, train loss: 0.01900, val loss: 0.01940\n",
      "Training epoch: 213, train loss: 0.01711, val loss: 0.01728\n",
      "Training epoch: 214, train loss: 0.01972, val loss: 0.01982\n",
      "Training epoch: 215, train loss: 0.01768, val loss: 0.01772\n",
      "Training epoch: 216, train loss: 0.01848, val loss: 0.01852\n",
      "Training epoch: 217, train loss: 0.02356, val loss: 0.02384\n",
      "Training epoch: 218, train loss: 0.03334, val loss: 0.03347\n",
      "Training epoch: 219, train loss: 0.01866, val loss: 0.01921\n",
      "Training epoch: 220, train loss: 0.01881, val loss: 0.01838\n",
      "Training epoch: 221, train loss: 0.02148, val loss: 0.02169\n",
      "Training epoch: 222, train loss: 0.03279, val loss: 0.03253\n",
      "Training epoch: 223, train loss: 0.02608, val loss: 0.02608\n",
      "Training epoch: 224, train loss: 0.02383, val loss: 0.02416\n",
      "Training epoch: 225, train loss: 0.02472, val loss: 0.02473\n",
      "Training epoch: 226, train loss: 0.01720, val loss: 0.01771\n",
      "Training epoch: 227, train loss: 0.01864, val loss: 0.01862\n",
      "Training epoch: 228, train loss: 0.01930, val loss: 0.01930\n",
      "Training epoch: 229, train loss: 0.01985, val loss: 0.02061\n",
      "Training epoch: 230, train loss: 0.01772, val loss: 0.01775\n",
      "Training epoch: 231, train loss: 0.01749, val loss: 0.01762\n",
      "Training epoch: 232, train loss: 0.02269, val loss: 0.02364\n",
      "Training epoch: 233, train loss: 0.01950, val loss: 0.01958\n",
      "Training epoch: 234, train loss: 0.01894, val loss: 0.01923\n",
      "Training epoch: 235, train loss: 0.02188, val loss: 0.02192\n",
      "Training epoch: 236, train loss: 0.48005, val loss: 0.48338\n",
      "Training epoch: 237, train loss: 0.01955, val loss: 0.02062\n",
      "Training epoch: 238, train loss: 0.02542, val loss: 0.02609\n",
      "Training epoch: 239, train loss: 0.01858, val loss: 0.01956\n",
      "Training epoch: 240, train loss: 0.01878, val loss: 0.01988\n",
      "Training epoch: 241, train loss: 0.01885, val loss: 0.01968\n",
      "Training epoch: 242, train loss: 0.04347, val loss: 0.04409\n",
      "Training epoch: 243, train loss: 0.01791, val loss: 0.01871\n",
      "Training epoch: 244, train loss: 0.01897, val loss: 0.02002\n",
      "Training epoch: 245, train loss: 0.01961, val loss: 0.02066\n",
      "Training epoch: 246, train loss: 0.02280, val loss: 0.02346\n",
      "Training epoch: 247, train loss: 0.03026, val loss: 0.03050\n",
      "Training epoch: 248, train loss: 0.03052, val loss: 0.03012\n",
      "Training epoch: 249, train loss: 0.02390, val loss: 0.02424\n",
      "Training epoch: 250, train loss: 0.02039, val loss: 0.02074\n",
      "Training epoch: 251, train loss: 0.01880, val loss: 0.01878\n",
      "Training epoch: 252, train loss: 0.02139, val loss: 0.02133\n",
      "Training epoch: 253, train loss: 0.01922, val loss: 0.01916\n",
      "Training epoch: 254, train loss: 0.01954, val loss: 0.01958\n",
      "Training epoch: 255, train loss: 0.01983, val loss: 0.01990\n",
      "Training epoch: 256, train loss: 0.01772, val loss: 0.01811\n",
      "Training epoch: 257, train loss: 0.01742, val loss: 0.01763\n",
      "Training epoch: 258, train loss: 0.02209, val loss: 0.02218\n",
      "Training epoch: 259, train loss: 0.01937, val loss: 0.01935\n",
      "Training epoch: 260, train loss: 0.01713, val loss: 0.01739\n",
      "Training epoch: 261, train loss: 0.01765, val loss: 0.01809\n",
      "Training epoch: 262, train loss: 0.01813, val loss: 0.01931\n",
      "Training epoch: 263, train loss: 0.01970, val loss: 0.02039\n",
      "Training epoch: 264, train loss: 0.03572, val loss: 0.03632\n",
      "Training epoch: 265, train loss: 0.01837, val loss: 0.01840\n",
      "Training epoch: 266, train loss: 0.01782, val loss: 0.01803\n",
      "Training epoch: 267, train loss: 0.01739, val loss: 0.01738\n",
      "Training epoch: 268, train loss: 0.01712, val loss: 0.01748\n",
      "Training epoch: 269, train loss: 0.01843, val loss: 0.01839\n",
      "Training epoch: 270, train loss: 0.02074, val loss: 0.02063\n",
      "Training epoch: 271, train loss: 0.02473, val loss: 0.02461\n",
      "Training epoch: 272, train loss: 0.02071, val loss: 0.02137\n",
      "Training epoch: 273, train loss: 0.01990, val loss: 0.02019\n",
      "Training epoch: 274, train loss: 0.02048, val loss: 0.02014\n",
      "Training epoch: 275, train loss: 0.02535, val loss: 0.02543\n",
      "Training epoch: 276, train loss: 0.03266, val loss: 0.03301\n",
      "Training epoch: 277, train loss: 0.02479, val loss: 0.02473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 278, train loss: 0.01791, val loss: 0.01830\n",
      "Training epoch: 279, train loss: 0.01769, val loss: 0.01807\n",
      "Training epoch: 280, train loss: 0.01748, val loss: 0.01781\n",
      "Training epoch: 281, train loss: 0.01967, val loss: 0.01971\n",
      "Training epoch: 282, train loss: 0.01766, val loss: 0.01828\n",
      "Training epoch: 283, train loss: 0.01939, val loss: 0.01931\n",
      "Training epoch: 284, train loss: 0.01914, val loss: 0.01958\n",
      "Training epoch: 285, train loss: 0.01736, val loss: 0.01795\n",
      "Training epoch: 286, train loss: 0.01813, val loss: 0.01862\n",
      "Training epoch: 287, train loss: 0.03491, val loss: 0.03530\n",
      "Training epoch: 288, train loss: 0.02141, val loss: 0.02191\n",
      "Training epoch: 289, train loss: 0.02283, val loss: 0.02236\n",
      "Training epoch: 290, train loss: 0.02020, val loss: 0.02071\n",
      "Training epoch: 291, train loss: 0.02335, val loss: 0.02342\n",
      "Training epoch: 292, train loss: 0.01858, val loss: 0.01868\n",
      "Training epoch: 293, train loss: 0.02229, val loss: 0.02288\n",
      "Training epoch: 294, train loss: 0.01860, val loss: 0.01912\n",
      "Training epoch: 295, train loss: 0.04394, val loss: 0.04458\n",
      "Training epoch: 296, train loss: 0.01761, val loss: 0.01758\n",
      "Training epoch: 297, train loss: 0.01992, val loss: 0.02037\n",
      "Training epoch: 298, train loss: 0.02529, val loss: 0.02611\n",
      "Training epoch: 299, train loss: 0.01953, val loss: 0.02069\n",
      "Training epoch: 300, train loss: 0.02098, val loss: 0.02235\n",
      "Training epoch: 301, train loss: 0.02084, val loss: 0.02145\n",
      "Training epoch: 302, train loss: 0.01894, val loss: 0.01934\n",
      "Training epoch: 303, train loss: 0.01743, val loss: 0.01764\n",
      "Training epoch: 304, train loss: 0.01830, val loss: 0.01846\n",
      "Training epoch: 305, train loss: 0.01709, val loss: 0.01732\n",
      "Training epoch: 306, train loss: 0.01703, val loss: 0.01742\n",
      "Training epoch: 307, train loss: 0.02195, val loss: 0.02210\n",
      "Training epoch: 308, train loss: 0.01894, val loss: 0.01905\n",
      "Training epoch: 309, train loss: 0.01983, val loss: 0.02010\n",
      "Training epoch: 310, train loss: 0.01853, val loss: 0.01914\n",
      "Training epoch: 311, train loss: 0.02242, val loss: 0.02264\n",
      "Training epoch: 312, train loss: 0.01924, val loss: 0.01978\n",
      "Training epoch: 313, train loss: 0.02004, val loss: 0.02109\n",
      "Training epoch: 314, train loss: 0.02016, val loss: 0.02100\n",
      "Training epoch: 315, train loss: 0.02388, val loss: 0.02512\n",
      "Training epoch: 316, train loss: 0.02096, val loss: 0.02217\n",
      "Training epoch: 317, train loss: 0.01872, val loss: 0.01924\n",
      "Training epoch: 318, train loss: 0.02896, val loss: 0.02885\n",
      "Training epoch: 319, train loss: 0.02206, val loss: 0.02221\n",
      "Training epoch: 320, train loss: 0.01690, val loss: 0.01709\n",
      "Training epoch: 321, train loss: 0.01681, val loss: 0.01709\n",
      "Training epoch: 322, train loss: 0.01831, val loss: 0.01853\n",
      "Training epoch: 323, train loss: 0.03416, val loss: 0.03479\n",
      "Training epoch: 324, train loss: 0.01719, val loss: 0.01719\n",
      "Training epoch: 325, train loss: 0.01928, val loss: 0.01965\n",
      "Training epoch: 326, train loss: 0.02066, val loss: 0.02109\n",
      "Training epoch: 327, train loss: 0.01914, val loss: 0.02009\n",
      "Training epoch: 328, train loss: 0.02050, val loss: 0.02204\n",
      "Training epoch: 329, train loss: 0.01830, val loss: 0.01915\n",
      "Training epoch: 330, train loss: 0.01826, val loss: 0.01881\n",
      "Training epoch: 331, train loss: 0.02343, val loss: 0.02355\n",
      "Training epoch: 332, train loss: 0.04098, val loss: 0.04146\n",
      "Training epoch: 333, train loss: 0.02246, val loss: 0.02278\n",
      "Training epoch: 334, train loss: 0.02558, val loss: 0.02543\n",
      "Training epoch: 335, train loss: 0.02845, val loss: 0.02840\n",
      "Training epoch: 336, train loss: 0.01903, val loss: 0.01952\n",
      "Training epoch: 337, train loss: 0.02051, val loss: 0.02083\n",
      "Training epoch: 338, train loss: 0.01945, val loss: 0.01971\n",
      "Training epoch: 339, train loss: 0.03573, val loss: 0.03589\n",
      "Training epoch: 340, train loss: 0.01950, val loss: 0.02024\n",
      "Training epoch: 341, train loss: 0.01883, val loss: 0.01906\n",
      "Training epoch: 342, train loss: 0.01917, val loss: 0.01970\n",
      "Training epoch: 343, train loss: 0.01832, val loss: 0.01822\n",
      "Training epoch: 344, train loss: 0.01940, val loss: 0.01973\n",
      "Training epoch: 345, train loss: 0.02192, val loss: 0.02175\n",
      "Training epoch: 346, train loss: 0.01830, val loss: 0.01855\n",
      "Training epoch: 347, train loss: 0.01819, val loss: 0.01879\n",
      "Training epoch: 348, train loss: 0.01858, val loss: 0.01852\n",
      "Training epoch: 349, train loss: 0.02384, val loss: 0.02406\n",
      "Training epoch: 350, train loss: 0.01660, val loss: 0.01700\n",
      "Training epoch: 351, train loss: 0.02432, val loss: 0.02450\n",
      "Training epoch: 352, train loss: 0.03131, val loss: 0.03215\n",
      "Training epoch: 353, train loss: 0.03779, val loss: 0.03775\n",
      "Training epoch: 354, train loss: 0.01997, val loss: 0.02016\n",
      "Training epoch: 355, train loss: 0.02019, val loss: 0.02067\n",
      "Training epoch: 356, train loss: 0.01950, val loss: 0.02005\n",
      "Training epoch: 357, train loss: 0.02078, val loss: 0.02167\n",
      "Training epoch: 358, train loss: 0.01853, val loss: 0.01860\n",
      "Training epoch: 359, train loss: 0.01971, val loss: 0.02035\n",
      "Training epoch: 360, train loss: 0.02016, val loss: 0.02071\n",
      "Training epoch: 361, train loss: 0.01772, val loss: 0.01792\n",
      "Training epoch: 362, train loss: 0.01728, val loss: 0.01751\n",
      "Training epoch: 363, train loss: 0.01961, val loss: 0.01952\n",
      "Training epoch: 364, train loss: 0.01772, val loss: 0.01822\n",
      "Training epoch: 365, train loss: 0.02717, val loss: 0.02929\n",
      "Training epoch: 366, train loss: 0.01887, val loss: 0.01974\n",
      "Training epoch: 367, train loss: 0.01919, val loss: 0.01946\n",
      "Training epoch: 368, train loss: 0.01801, val loss: 0.01843\n",
      "Training epoch: 369, train loss: 0.01772, val loss: 0.01820\n",
      "Training epoch: 370, train loss: 0.01789, val loss: 0.01812\n",
      "Training epoch: 371, train loss: 0.01754, val loss: 0.01798\n",
      "Training epoch: 372, train loss: 0.01857, val loss: 0.01899\n",
      "Training epoch: 373, train loss: 0.01885, val loss: 0.01899\n",
      "Training epoch: 374, train loss: 0.03751, val loss: 0.03814\n",
      "Training epoch: 375, train loss: 0.02017, val loss: 0.02006\n",
      "Training epoch: 376, train loss: 0.01932, val loss: 0.01990\n",
      "Training epoch: 377, train loss: 0.03001, val loss: 0.03005\n",
      "Training epoch: 378, train loss: 0.03242, val loss: 0.03299\n",
      "Training epoch: 379, train loss: 0.01850, val loss: 0.01912\n",
      "Training epoch: 380, train loss: 0.01793, val loss: 0.01879\n",
      "Training epoch: 381, train loss: 0.02045, val loss: 0.02026\n",
      "Training epoch: 382, train loss: 0.01885, val loss: 0.01904\n",
      "Training epoch: 383, train loss: 0.01946, val loss: 0.01962\n",
      "Training epoch: 384, train loss: 0.01678, val loss: 0.01728\n",
      "Training epoch: 385, train loss: 0.02362, val loss: 0.02466\n",
      "Training epoch: 386, train loss: 0.01924, val loss: 0.01974\n",
      "Training epoch: 387, train loss: 0.02624, val loss: 0.02662\n",
      "Training epoch: 388, train loss: 0.01824, val loss: 0.01928\n",
      "Training epoch: 389, train loss: 0.02418, val loss: 0.02402\n",
      "Training epoch: 390, train loss: 0.02226, val loss: 0.02369\n",
      "Training epoch: 391, train loss: 0.01728, val loss: 0.01745\n",
      "Training epoch: 392, train loss: 0.01818, val loss: 0.01862\n",
      "Training epoch: 393, train loss: 0.02195, val loss: 0.02239\n",
      "Training epoch: 394, train loss: 0.02117, val loss: 0.02130\n",
      "Training epoch: 395, train loss: 0.01646, val loss: 0.01681\n",
      "Training epoch: 396, train loss: 0.02054, val loss: 0.02103\n",
      "Training epoch: 397, train loss: 0.01823, val loss: 0.01849\n",
      "Training epoch: 398, train loss: 0.01711, val loss: 0.01729\n",
      "Training epoch: 399, train loss: 0.02905, val loss: 0.02910\n",
      "Training epoch: 400, train loss: 0.05866, val loss: 0.06057\n",
      "Training epoch: 401, train loss: 0.01841, val loss: 0.01884\n",
      "Training epoch: 402, train loss: 0.02398, val loss: 0.02488\n",
      "Training epoch: 403, train loss: 0.02386, val loss: 0.02507\n",
      "Training epoch: 404, train loss: 0.01805, val loss: 0.01921\n",
      "Training epoch: 405, train loss: 0.01792, val loss: 0.01886\n",
      "Training epoch: 406, train loss: 0.01866, val loss: 0.01969\n",
      "Training epoch: 407, train loss: 0.01962, val loss: 0.02072\n",
      "Training epoch: 408, train loss: 0.01984, val loss: 0.02060\n",
      "Training epoch: 409, train loss: 0.02466, val loss: 0.02560\n",
      "Training epoch: 410, train loss: 0.02468, val loss: 0.02488\n",
      "Training epoch: 411, train loss: 0.01860, val loss: 0.01899\n",
      "Training epoch: 412, train loss: 0.01790, val loss: 0.01820\n",
      "Training epoch: 413, train loss: 0.01883, val loss: 0.01900\n",
      "Training epoch: 414, train loss: 0.02154, val loss: 0.02199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 415, train loss: 0.01922, val loss: 0.01953\n",
      "Training epoch: 416, train loss: 0.02007, val loss: 0.02059\n",
      "Training epoch: 417, train loss: 0.01738, val loss: 0.01795\n",
      "Training epoch: 418, train loss: 0.01743, val loss: 0.01780\n",
      "Training epoch: 419, train loss: 0.02442, val loss: 0.02450\n",
      "Training epoch: 420, train loss: 0.03895, val loss: 0.03839\n",
      "Training epoch: 421, train loss: 0.01886, val loss: 0.01966\n",
      "Training epoch: 422, train loss: 0.01976, val loss: 0.02086\n",
      "Training epoch: 423, train loss: 0.02306, val loss: 0.02356\n",
      "Training epoch: 424, train loss: 0.02841, val loss: 0.02869\n",
      "Training epoch: 425, train loss: 0.01830, val loss: 0.01890\n",
      "Training epoch: 426, train loss: 0.01667, val loss: 0.01703\n",
      "Training epoch: 427, train loss: 0.01860, val loss: 0.01885\n",
      "Training epoch: 428, train loss: 0.01702, val loss: 0.01753\n",
      "Training epoch: 429, train loss: 0.02052, val loss: 0.02049\n",
      "Training epoch: 430, train loss: 0.01886, val loss: 0.01925\n",
      "Training epoch: 431, train loss: 0.01861, val loss: 0.01897\n",
      "Training epoch: 432, train loss: 0.01750, val loss: 0.01807\n",
      "Training epoch: 433, train loss: 0.01666, val loss: 0.01699\n",
      "Training epoch: 434, train loss: 0.02572, val loss: 0.02571\n",
      "Training epoch: 435, train loss: 0.01764, val loss: 0.01874\n",
      "Training epoch: 436, train loss: 0.02662, val loss: 0.02692\n",
      "Training epoch: 437, train loss: 0.01775, val loss: 0.01797\n",
      "Training epoch: 438, train loss: 0.01781, val loss: 0.01784\n",
      "Training epoch: 439, train loss: 0.01714, val loss: 0.01736\n",
      "Training epoch: 440, train loss: 0.01791, val loss: 0.01841\n",
      "Training epoch: 441, train loss: 0.02386, val loss: 0.02504\n",
      "Training epoch: 442, train loss: 0.02045, val loss: 0.02085\n",
      "Training epoch: 443, train loss: 0.02367, val loss: 0.02416\n",
      "Training epoch: 444, train loss: 0.02050, val loss: 0.02114\n",
      "Training epoch: 445, train loss: 0.01818, val loss: 0.01833\n",
      "Training epoch: 446, train loss: 0.02551, val loss: 0.02574\n",
      "Training epoch: 447, train loss: 0.01852, val loss: 0.01895\n",
      "Training epoch: 448, train loss: 0.01915, val loss: 0.01970\n",
      "Training epoch: 449, train loss: 0.02974, val loss: 0.03022\n",
      "Training epoch: 450, train loss: 0.01891, val loss: 0.02045\n",
      "Training epoch: 451, train loss: 0.02200, val loss: 0.02239\n",
      "Training epoch: 452, train loss: 0.01993, val loss: 0.02071\n",
      "Training epoch: 453, train loss: 0.02498, val loss: 0.02507\n",
      "Training epoch: 454, train loss: 0.01826, val loss: 0.01856\n",
      "Training epoch: 455, train loss: 0.02043, val loss: 0.02084\n",
      "Training epoch: 456, train loss: 0.02036, val loss: 0.02049\n",
      "Training epoch: 457, train loss: 0.01677, val loss: 0.01724\n",
      "Training epoch: 458, train loss: 0.01712, val loss: 0.01756\n",
      "Training epoch: 459, train loss: 0.01692, val loss: 0.01728\n",
      "Training epoch: 460, train loss: 0.01813, val loss: 0.01856\n",
      "Training epoch: 461, train loss: 0.01721, val loss: 0.01801\n",
      "Training epoch: 462, train loss: 0.01830, val loss: 0.01834\n",
      "Training epoch: 463, train loss: 0.02614, val loss: 0.02696\n",
      "Training epoch: 464, train loss: 0.02366, val loss: 0.02378\n",
      "Training epoch: 465, train loss: 0.02831, val loss: 0.02873\n",
      "Training epoch: 466, train loss: 0.01853, val loss: 0.01892\n",
      "Training epoch: 467, train loss: 0.02277, val loss: 0.02259\n",
      "Training epoch: 468, train loss: 0.01804, val loss: 0.01827\n",
      "Training epoch: 469, train loss: 0.02060, val loss: 0.02239\n",
      "Training epoch: 470, train loss: 0.02034, val loss: 0.02126\n",
      "Training epoch: 471, train loss: 0.01845, val loss: 0.01964\n",
      "Training epoch: 472, train loss: 0.01721, val loss: 0.01788\n",
      "Training epoch: 473, train loss: 0.01875, val loss: 0.01875\n",
      "Training epoch: 474, train loss: 0.01869, val loss: 0.01914\n",
      "Training epoch: 475, train loss: 0.01694, val loss: 0.01717\n",
      "Training epoch: 476, train loss: 0.01709, val loss: 0.01744\n",
      "Training epoch: 477, train loss: 0.01700, val loss: 0.01727\n",
      "Training epoch: 478, train loss: 0.01855, val loss: 0.01881\n",
      "Training epoch: 479, train loss: 0.01801, val loss: 0.01822\n",
      "Training epoch: 480, train loss: 0.02163, val loss: 0.02203\n",
      "Training epoch: 481, train loss: 0.01660, val loss: 0.01701\n",
      "Training epoch: 482, train loss: 0.01880, val loss: 0.01931\n",
      "Training epoch: 483, train loss: 0.01901, val loss: 0.01927\n",
      "Training epoch: 484, train loss: 0.02053, val loss: 0.02088\n",
      "Training epoch: 485, train loss: 0.02795, val loss: 0.02832\n",
      "Training epoch: 486, train loss: 0.02793, val loss: 0.02815\n",
      "Training epoch: 487, train loss: 0.01910, val loss: 0.01983\n",
      "Training epoch: 488, train loss: 0.01842, val loss: 0.01891\n",
      "Training epoch: 489, train loss: 0.01826, val loss: 0.01892\n",
      "Training epoch: 490, train loss: 0.01710, val loss: 0.01741\n",
      "Training epoch: 491, train loss: 0.02391, val loss: 0.02414\n",
      "Training epoch: 492, train loss: 0.01805, val loss: 0.01871\n",
      "Training epoch: 493, train loss: 0.01668, val loss: 0.01714\n",
      "Training epoch: 494, train loss: 0.01782, val loss: 0.01834\n",
      "Training epoch: 495, train loss: 0.01695, val loss: 0.01767\n",
      "Training epoch: 496, train loss: 0.01833, val loss: 0.01862\n",
      "Training epoch: 497, train loss: 0.04593, val loss: 0.04663\n",
      "Training epoch: 498, train loss: 0.01989, val loss: 0.02175\n",
      "Training epoch: 499, train loss: 0.02726, val loss: 0.02806\n",
      "Training epoch: 500, train loss: 0.02163, val loss: 0.02346\n",
      "Training epoch: 501, train loss: 0.01899, val loss: 0.02018\n",
      "Training epoch: 502, train loss: 0.01922, val loss: 0.02034\n",
      "Training epoch: 503, train loss: 0.02145, val loss: 0.02237\n",
      "Training epoch: 504, train loss: 0.01806, val loss: 0.01916\n",
      "Training epoch: 505, train loss: 0.01931, val loss: 0.02038\n",
      "Training epoch: 506, train loss: 0.02210, val loss: 0.02300\n",
      "Training epoch: 507, train loss: 0.01975, val loss: 0.02057\n",
      "Training epoch: 508, train loss: 0.02100, val loss: 0.02255\n",
      "Training epoch: 509, train loss: 0.01831, val loss: 0.01965\n",
      "Training epoch: 510, train loss: 0.01923, val loss: 0.02010\n",
      "Training epoch: 511, train loss: 0.01890, val loss: 0.02003\n",
      "Training epoch: 512, train loss: 0.02590, val loss: 0.02648\n",
      "Training epoch: 513, train loss: 0.02491, val loss: 0.02551\n",
      "Training epoch: 514, train loss: 0.01899, val loss: 0.01960\n",
      "Training epoch: 515, train loss: 0.02194, val loss: 0.02220\n",
      "Training epoch: 516, train loss: 0.02821, val loss: 0.02832\n",
      "Training epoch: 517, train loss: 0.05845, val loss: 0.05895\n",
      "Training epoch: 518, train loss: 0.01939, val loss: 0.02008\n",
      "Training epoch: 519, train loss: 0.01787, val loss: 0.01826\n",
      "Training epoch: 520, train loss: 0.02036, val loss: 0.02049\n",
      "Training epoch: 521, train loss: 0.02246, val loss: 0.02237\n",
      "Training epoch: 522, train loss: 0.02577, val loss: 0.02584\n",
      "Training epoch: 523, train loss: 0.02095, val loss: 0.02113\n",
      "Training epoch: 524, train loss: 0.02525, val loss: 0.02548\n",
      "Training epoch: 525, train loss: 0.01908, val loss: 0.01972\n",
      "Training epoch: 526, train loss: 0.01775, val loss: 0.01794\n",
      "Training epoch: 527, train loss: 0.01782, val loss: 0.01802\n",
      "Training epoch: 528, train loss: 0.01678, val loss: 0.01710\n",
      "Training epoch: 529, train loss: 0.01694, val loss: 0.01734\n",
      "Training epoch: 530, train loss: 0.01701, val loss: 0.01748\n",
      "Training epoch: 531, train loss: 0.05723, val loss: 0.05834\n",
      "Training epoch: 532, train loss: 0.02947, val loss: 0.02955\n",
      "Training epoch: 533, train loss: 0.02072, val loss: 0.02140\n",
      "Training epoch: 534, train loss: 0.02105, val loss: 0.02108\n",
      "Training epoch: 535, train loss: 0.02019, val loss: 0.02063\n",
      "Training epoch: 536, train loss: 0.02052, val loss: 0.02083\n",
      "Training epoch: 537, train loss: 0.02125, val loss: 0.02165\n",
      "Training epoch: 538, train loss: 0.02838, val loss: 0.02857\n",
      "Training epoch: 539, train loss: 0.02443, val loss: 0.02548\n",
      "Training epoch: 540, train loss: 0.02238, val loss: 0.02345\n",
      "Training epoch: 541, train loss: 0.01891, val loss: 0.02005\n",
      "Training epoch: 542, train loss: 0.01762, val loss: 0.01832\n",
      "Training epoch: 543, train loss: 0.01867, val loss: 0.01946\n",
      "Training epoch: 544, train loss: 0.04941, val loss: 0.04822\n",
      "Training epoch: 545, train loss: 0.01801, val loss: 0.01817\n",
      "Training epoch: 546, train loss: 0.01902, val loss: 0.01920\n",
      "Training epoch: 547, train loss: 0.01968, val loss: 0.01988\n",
      "Training epoch: 548, train loss: 0.01978, val loss: 0.02012\n",
      "Training epoch: 549, train loss: 0.01846, val loss: 0.01833\n",
      "Training epoch: 550, train loss: 0.01853, val loss: 0.01875\n",
      "Training epoch: 551, train loss: 0.01749, val loss: 0.01777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 552, train loss: 0.02428, val loss: 0.02441\n",
      "Training epoch: 553, train loss: 0.01887, val loss: 0.01911\n",
      "Training epoch: 554, train loss: 0.02307, val loss: 0.02393\n",
      "Training epoch: 555, train loss: 0.01747, val loss: 0.01845\n",
      "Training epoch: 556, train loss: 0.02198, val loss: 0.02195\n",
      "Training epoch: 557, train loss: 0.01899, val loss: 0.01928\n",
      "Training epoch: 558, train loss: 0.01883, val loss: 0.01897\n",
      "Training epoch: 559, train loss: 0.02778, val loss: 0.02832\n",
      "Training epoch: 560, train loss: 0.02563, val loss: 0.02611\n",
      "Training epoch: 561, train loss: 0.01718, val loss: 0.01762\n",
      "Training epoch: 562, train loss: 0.01748, val loss: 0.01844\n",
      "Training epoch: 563, train loss: 0.01835, val loss: 0.01872\n",
      "Training epoch: 564, train loss: 0.01729, val loss: 0.01809\n",
      "Training epoch: 565, train loss: 0.01755, val loss: 0.01770\n",
      "Training epoch: 566, train loss: 0.03667, val loss: 0.03751\n",
      "Training epoch: 567, train loss: 0.02348, val loss: 0.02382\n",
      "Training epoch: 568, train loss: 0.01982, val loss: 0.01987\n",
      "Training epoch: 569, train loss: 0.01767, val loss: 0.01808\n",
      "Training epoch: 570, train loss: 0.01801, val loss: 0.01830\n",
      "Training epoch: 571, train loss: 0.01746, val loss: 0.01774\n",
      "Training epoch: 572, train loss: 0.01680, val loss: 0.01725\n",
      "Training epoch: 573, train loss: 0.02014, val loss: 0.02093\n",
      "Training epoch: 574, train loss: 0.03464, val loss: 0.03499\n",
      "Training epoch: 575, train loss: 0.02259, val loss: 0.02296\n",
      "Training epoch: 576, train loss: 0.02955, val loss: 0.03035\n",
      "Training epoch: 577, train loss: 0.01802, val loss: 0.01876\n",
      "Training epoch: 578, train loss: 0.02797, val loss: 0.02829\n",
      "Training epoch: 579, train loss: 0.02861, val loss: 0.02998\n",
      "Training epoch: 580, train loss: 0.02101, val loss: 0.02135\n",
      "Training epoch: 581, train loss: 0.01629, val loss: 0.01677\n",
      "Training epoch: 582, train loss: 0.02113, val loss: 0.02138\n",
      "Training epoch: 583, train loss: 0.02896, val loss: 0.02840\n",
      "Training epoch: 584, train loss: 0.02247, val loss: 0.02450\n",
      "Training epoch: 585, train loss: 0.01751, val loss: 0.01856\n",
      "Training epoch: 586, train loss: 0.01827, val loss: 0.01900\n",
      "Training epoch: 587, train loss: 0.02025, val loss: 0.02077\n",
      "Training epoch: 588, train loss: 0.01694, val loss: 0.01743\n",
      "Training epoch: 589, train loss: 0.01652, val loss: 0.01667\n",
      "Training epoch: 590, train loss: 0.02383, val loss: 0.02405\n",
      "Training epoch: 591, train loss: 0.01826, val loss: 0.01851\n",
      "Training epoch: 592, train loss: 0.01844, val loss: 0.01855\n",
      "Training epoch: 593, train loss: 0.01860, val loss: 0.01900\n",
      "Training epoch: 594, train loss: 0.01734, val loss: 0.01773\n",
      "Training epoch: 595, train loss: 0.01844, val loss: 0.01873\n",
      "Training epoch: 596, train loss: 0.01725, val loss: 0.01781\n",
      "Training epoch: 597, train loss: 0.01853, val loss: 0.01894\n",
      "Training epoch: 598, train loss: 0.02559, val loss: 0.02685\n",
      "Training epoch: 599, train loss: 0.02212, val loss: 0.02303\n",
      "Training epoch: 600, train loss: 0.01919, val loss: 0.02014\n",
      "Training epoch: 601, train loss: 0.01822, val loss: 0.01861\n",
      "Training epoch: 602, train loss: 0.01738, val loss: 0.01815\n",
      "Training epoch: 603, train loss: 0.01702, val loss: 0.01787\n",
      "Training epoch: 604, train loss: 0.01892, val loss: 0.02015\n",
      "Training epoch: 605, train loss: 0.02319, val loss: 0.02326\n",
      "Training epoch: 606, train loss: 0.02114, val loss: 0.02114\n",
      "Training epoch: 607, train loss: 0.01762, val loss: 0.01817\n",
      "Training epoch: 608, train loss: 0.01802, val loss: 0.01856\n",
      "Training epoch: 609, train loss: 0.01803, val loss: 0.01861\n",
      "Training epoch: 610, train loss: 0.01676, val loss: 0.01701\n",
      "Training epoch: 611, train loss: 0.02010, val loss: 0.02014\n",
      "Training epoch: 612, train loss: 0.01706, val loss: 0.01763\n",
      "Training epoch: 613, train loss: 0.03852, val loss: 0.03885\n",
      "Training epoch: 614, train loss: 0.02542, val loss: 0.02615\n",
      "Training epoch: 615, train loss: 0.02285, val loss: 0.02317\n",
      "Training epoch: 616, train loss: 0.02182, val loss: 0.02193\n",
      "Training epoch: 617, train loss: 0.01995, val loss: 0.02055\n",
      "Training epoch: 618, train loss: 0.01844, val loss: 0.01914\n",
      "Training epoch: 619, train loss: 0.01895, val loss: 0.01904\n",
      "Training epoch: 620, train loss: 0.01716, val loss: 0.01765\n",
      "Training epoch: 621, train loss: 0.01649, val loss: 0.01696\n",
      "Training epoch: 622, train loss: 0.01742, val loss: 0.01767\n",
      "Training epoch: 623, train loss: 0.01730, val loss: 0.01776\n",
      "Training epoch: 624, train loss: 0.02184, val loss: 0.02241\n",
      "Training epoch: 625, train loss: 0.01718, val loss: 0.01814\n",
      "Training epoch: 626, train loss: 0.01986, val loss: 0.02022\n",
      "Training epoch: 627, train loss: 0.01769, val loss: 0.01810\n",
      "Training epoch: 628, train loss: 0.01679, val loss: 0.01722\n",
      "Training epoch: 629, train loss: 0.01798, val loss: 0.01806\n",
      "Training epoch: 630, train loss: 0.01780, val loss: 0.01878\n",
      "Training epoch: 631, train loss: 0.01724, val loss: 0.01777\n",
      "Training epoch: 632, train loss: 0.01706, val loss: 0.01771\n",
      "Training epoch: 633, train loss: 0.01881, val loss: 0.01898\n",
      "Training epoch: 634, train loss: 0.02014, val loss: 0.02061\n",
      "Training epoch: 635, train loss: 0.01887, val loss: 0.01910\n",
      "Training epoch: 636, train loss: 0.01924, val loss: 0.01941\n",
      "Training epoch: 637, train loss: 0.01700, val loss: 0.01770\n",
      "Training epoch: 638, train loss: 0.01981, val loss: 0.02062\n",
      "Training epoch: 639, train loss: 0.01729, val loss: 0.01757\n",
      "Training epoch: 640, train loss: 0.01709, val loss: 0.01726\n",
      "Training epoch: 641, train loss: 0.02109, val loss: 0.02172\n",
      "Training epoch: 642, train loss: 0.02041, val loss: 0.02059\n",
      "Training epoch: 643, train loss: 0.02379, val loss: 0.02409\n",
      "Training epoch: 644, train loss: 0.03850, val loss: 0.04034\n",
      "Training epoch: 645, train loss: 0.02540, val loss: 0.02734\n",
      "Training epoch: 646, train loss: 0.01956, val loss: 0.01998\n",
      "Training epoch: 647, train loss: 0.01803, val loss: 0.01907\n",
      "Training epoch: 648, train loss: 0.01898, val loss: 0.02028\n",
      "Training epoch: 649, train loss: 0.01765, val loss: 0.01864\n",
      "Training epoch: 650, train loss: 0.01779, val loss: 0.01818\n",
      "Training epoch: 651, train loss: 0.01639, val loss: 0.01695\n",
      "Training epoch: 652, train loss: 0.01712, val loss: 0.01751\n",
      "Training epoch: 653, train loss: 0.02009, val loss: 0.02070\n",
      "Training epoch: 654, train loss: 0.02255, val loss: 0.02307\n",
      "Training epoch: 655, train loss: 0.01854, val loss: 0.01890\n",
      "Training epoch: 656, train loss: 0.01850, val loss: 0.01877\n",
      "Training epoch: 657, train loss: 0.01671, val loss: 0.01715\n",
      "Training epoch: 658, train loss: 0.02134, val loss: 0.02156\n",
      "Training epoch: 659, train loss: 0.01739, val loss: 0.01778\n",
      "Training epoch: 660, train loss: 0.01835, val loss: 0.01911\n",
      "Training epoch: 661, train loss: 0.02638, val loss: 0.02668\n",
      "Training epoch: 662, train loss: 0.02656, val loss: 0.02812\n",
      "Training epoch: 663, train loss: 0.02680, val loss: 0.02734\n",
      "Training epoch: 664, train loss: 0.01874, val loss: 0.01922\n",
      "Training epoch: 665, train loss: 0.02405, val loss: 0.02485\n",
      "Training epoch: 666, train loss: 0.02080, val loss: 0.02121\n",
      "Training epoch: 667, train loss: 0.01684, val loss: 0.01739\n",
      "Training epoch: 668, train loss: 0.02030, val loss: 0.02085\n",
      "Training epoch: 669, train loss: 0.02364, val loss: 0.02395\n",
      "Training epoch: 670, train loss: 0.02107, val loss: 0.02133\n",
      "Training epoch: 671, train loss: 0.01705, val loss: 0.01720\n",
      "Training epoch: 672, train loss: 0.02591, val loss: 0.02616\n",
      "Training epoch: 673, train loss: 0.01687, val loss: 0.01742\n",
      "Training epoch: 674, train loss: 0.01869, val loss: 0.01900\n",
      "Training epoch: 675, train loss: 0.01725, val loss: 0.01868\n",
      "Training epoch: 676, train loss: 0.03148, val loss: 0.03115\n",
      "Training epoch: 677, train loss: 0.01926, val loss: 0.01983\n",
      "Training epoch: 678, train loss: 0.01698, val loss: 0.01742\n",
      "Training epoch: 679, train loss: 0.01868, val loss: 0.01899\n",
      "Training epoch: 680, train loss: 0.01902, val loss: 0.01947\n",
      "Training epoch: 681, train loss: 0.02169, val loss: 0.02175\n",
      "Training epoch: 682, train loss: 0.01942, val loss: 0.02010\n",
      "Training epoch: 683, train loss: 0.02120, val loss: 0.02127\n",
      "Training epoch: 684, train loss: 0.01778, val loss: 0.01824\n",
      "Training epoch: 685, train loss: 0.01807, val loss: 0.01822\n",
      "Training epoch: 686, train loss: 0.01648, val loss: 0.01700\n",
      "Training epoch: 687, train loss: 0.01876, val loss: 0.01911\n",
      "Training epoch: 688, train loss: 0.01787, val loss: 0.01860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 689, train loss: 0.01750, val loss: 0.01787\n",
      "Training epoch: 690, train loss: 0.01937, val loss: 0.01960\n",
      "Training epoch: 691, train loss: 0.01658, val loss: 0.01691\n",
      "Training epoch: 692, train loss: 0.01664, val loss: 0.01722\n",
      "Training epoch: 693, train loss: 0.02590, val loss: 0.02766\n",
      "Training epoch: 694, train loss: 0.01766, val loss: 0.01813\n",
      "Training epoch: 695, train loss: 0.02159, val loss: 0.02196\n",
      "Training epoch: 696, train loss: 0.01900, val loss: 0.01972\n",
      "Training epoch: 697, train loss: 0.01793, val loss: 0.01823\n",
      "Training epoch: 698, train loss: 0.01770, val loss: 0.01783\n",
      "Training epoch: 699, train loss: 0.02029, val loss: 0.02090\n",
      "Training epoch: 700, train loss: 0.02427, val loss: 0.02522\n",
      "Training epoch: 701, train loss: 0.02029, val loss: 0.02077\n",
      "Training epoch: 702, train loss: 0.01765, val loss: 0.01840\n",
      "Training epoch: 703, train loss: 0.01650, val loss: 0.01666\n",
      "Training epoch: 704, train loss: 0.01868, val loss: 0.01883\n",
      "Training epoch: 705, train loss: 0.02608, val loss: 0.02632\n",
      "Training epoch: 706, train loss: 0.01884, val loss: 0.01915\n",
      "Training epoch: 707, train loss: 0.01919, val loss: 0.01944\n",
      "Training epoch: 708, train loss: 0.02072, val loss: 0.02095\n",
      "Training epoch: 709, train loss: 0.02007, val loss: 0.02045\n",
      "Training epoch: 710, train loss: 0.01866, val loss: 0.01928\n",
      "Training epoch: 711, train loss: 0.01861, val loss: 0.01893\n",
      "Training epoch: 712, train loss: 0.01767, val loss: 0.01799\n",
      "Training epoch: 713, train loss: 0.01792, val loss: 0.01845\n",
      "Training epoch: 714, train loss: 0.02170, val loss: 0.02192\n",
      "Training epoch: 715, train loss: 0.01824, val loss: 0.01844\n",
      "Training epoch: 716, train loss: 0.01823, val loss: 0.01848\n",
      "Training epoch: 717, train loss: 0.02035, val loss: 0.02069\n",
      "Training epoch: 718, train loss: 0.01724, val loss: 0.01815\n",
      "Training epoch: 719, train loss: 0.01847, val loss: 0.01897\n",
      "Training epoch: 720, train loss: 0.02034, val loss: 0.02052\n",
      "Training epoch: 721, train loss: 0.02408, val loss: 0.02502\n",
      "Training epoch: 722, train loss: 0.01807, val loss: 0.01919\n",
      "Training epoch: 723, train loss: 0.01979, val loss: 0.02003\n",
      "Training epoch: 724, train loss: 0.02187, val loss: 0.02233\n",
      "Training epoch: 725, train loss: 0.02314, val loss: 0.02344\n",
      "Training epoch: 726, train loss: 0.01904, val loss: 0.01931\n",
      "Training epoch: 727, train loss: 0.01817, val loss: 0.01856\n",
      "Training epoch: 728, train loss: 0.01632, val loss: 0.01679\n",
      "Training epoch: 729, train loss: 0.02031, val loss: 0.02059\n",
      "Training epoch: 730, train loss: 0.01721, val loss: 0.01788\n",
      "Training epoch: 731, train loss: 0.01719, val loss: 0.01742\n",
      "Training epoch: 732, train loss: 0.01731, val loss: 0.01747\n",
      "Training epoch: 733, train loss: 0.02308, val loss: 0.02319\n",
      "Training epoch: 734, train loss: 0.01747, val loss: 0.01774\n",
      "Training epoch: 735, train loss: 0.02427, val loss: 0.02482\n",
      "Training epoch: 736, train loss: 0.01693, val loss: 0.01764\n",
      "Training epoch: 737, train loss: 0.01876, val loss: 0.01923\n",
      "Training epoch: 738, train loss: 0.01611, val loss: 0.01645\n",
      "Training epoch: 739, train loss: 0.01808, val loss: 0.01846\n",
      "Training epoch: 740, train loss: 0.01758, val loss: 0.01804\n",
      "Training epoch: 741, train loss: 0.02318, val loss: 0.02386\n",
      "Training epoch: 742, train loss: 0.02820, val loss: 0.02866\n",
      "Training epoch: 743, train loss: 0.01738, val loss: 0.01773\n",
      "Training epoch: 744, train loss: 0.01686, val loss: 0.01721\n",
      "Training epoch: 745, train loss: 0.01614, val loss: 0.01648\n",
      "Training epoch: 746, train loss: 0.01715, val loss: 0.01758\n",
      "Training epoch: 747, train loss: 0.01778, val loss: 0.01814\n",
      "Training epoch: 748, train loss: 0.01781, val loss: 0.01814\n",
      "Training epoch: 749, train loss: 0.02843, val loss: 0.02839\n",
      "Training epoch: 750, train loss: 0.01878, val loss: 0.01894\n",
      "Training epoch: 751, train loss: 0.01700, val loss: 0.01806\n",
      "Training epoch: 752, train loss: 0.01743, val loss: 0.01784\n",
      "Training epoch: 753, train loss: 0.01768, val loss: 0.01814\n",
      "Training epoch: 754, train loss: 0.02165, val loss: 0.02176\n",
      "Training epoch: 755, train loss: 0.01916, val loss: 0.01988\n",
      "Training epoch: 756, train loss: 0.01937, val loss: 0.01954\n",
      "Training epoch: 757, train loss: 0.01744, val loss: 0.01772\n",
      "Training epoch: 758, train loss: 0.02009, val loss: 0.02029\n",
      "Training epoch: 759, train loss: 0.01759, val loss: 0.01835\n",
      "Training epoch: 760, train loss: 0.02036, val loss: 0.02116\n",
      "Training epoch: 761, train loss: 0.01975, val loss: 0.01992\n",
      "Training epoch: 762, train loss: 0.01833, val loss: 0.01874\n",
      "Training epoch: 763, train loss: 0.01668, val loss: 0.01706\n",
      "Training epoch: 764, train loss: 0.02063, val loss: 0.02113\n",
      "Training epoch: 765, train loss: 0.02916, val loss: 0.03069\n",
      "Training epoch: 766, train loss: 0.02682, val loss: 0.02686\n",
      "Training epoch: 767, train loss: 0.01770, val loss: 0.01804\n",
      "Training epoch: 768, train loss: 0.01792, val loss: 0.01837\n",
      "Training epoch: 769, train loss: 0.01717, val loss: 0.01792\n",
      "Training epoch: 770, train loss: 0.01918, val loss: 0.02009\n",
      "Training epoch: 771, train loss: 0.01718, val loss: 0.01760\n",
      "Training epoch: 772, train loss: 0.01711, val loss: 0.01759\n",
      "Training epoch: 773, train loss: 0.06405, val loss: 0.06310\n",
      "Training epoch: 774, train loss: 0.01901, val loss: 0.01948\n",
      "Training epoch: 775, train loss: 0.03149, val loss: 0.03160\n",
      "Training epoch: 776, train loss: 0.02014, val loss: 0.02057\n",
      "Training epoch: 777, train loss: 0.02096, val loss: 0.02194\n",
      "Training epoch: 778, train loss: 0.03296, val loss: 0.03285\n",
      "Training epoch: 779, train loss: 0.01721, val loss: 0.01759\n",
      "Training epoch: 780, train loss: 0.01646, val loss: 0.01700\n",
      "Training epoch: 781, train loss: 0.01771, val loss: 0.01795\n",
      "Training epoch: 782, train loss: 0.02073, val loss: 0.02147\n",
      "Training epoch: 783, train loss: 0.01919, val loss: 0.01948\n",
      "Training epoch: 784, train loss: 0.01838, val loss: 0.01869\n",
      "Training epoch: 785, train loss: 0.01867, val loss: 0.01894\n",
      "Training epoch: 786, train loss: 0.02038, val loss: 0.02071\n",
      "Training epoch: 787, train loss: 0.01682, val loss: 0.01703\n",
      "Training epoch: 788, train loss: 0.01805, val loss: 0.01824\n",
      "Training epoch: 789, train loss: 0.01838, val loss: 0.01883\n",
      "Training epoch: 790, train loss: 0.01682, val loss: 0.01711\n",
      "Training epoch: 791, train loss: 0.01680, val loss: 0.01707\n",
      "Training epoch: 792, train loss: 0.01627, val loss: 0.01677\n",
      "Training epoch: 793, train loss: 0.02066, val loss: 0.02104\n",
      "Training epoch: 794, train loss: 0.01719, val loss: 0.01743\n",
      "Training epoch: 795, train loss: 0.02484, val loss: 0.02488\n",
      "Training epoch: 796, train loss: 0.04827, val loss: 0.04889\n",
      "Training epoch: 797, train loss: 0.02664, val loss: 0.02793\n",
      "Training epoch: 798, train loss: 0.02076, val loss: 0.02257\n",
      "Training epoch: 799, train loss: 0.02042, val loss: 0.02142\n",
      "Training epoch: 800, train loss: 0.02800, val loss: 0.02875\n",
      "Training epoch: 801, train loss: 0.01982, val loss: 0.02072\n",
      "Training epoch: 802, train loss: 0.01876, val loss: 0.01969\n",
      "Training epoch: 803, train loss: 0.01822, val loss: 0.01948\n",
      "Training epoch: 804, train loss: 0.01737, val loss: 0.01840\n",
      "Training epoch: 805, train loss: 0.01828, val loss: 0.01929\n",
      "Training epoch: 806, train loss: 0.01920, val loss: 0.02007\n",
      "Training epoch: 807, train loss: 0.01722, val loss: 0.01803\n",
      "Training epoch: 808, train loss: 0.01728, val loss: 0.01794\n",
      "Training epoch: 809, train loss: 0.01675, val loss: 0.01708\n",
      "Training epoch: 810, train loss: 0.02055, val loss: 0.02086\n",
      "Training epoch: 811, train loss: 0.01631, val loss: 0.01674\n",
      "Training epoch: 812, train loss: 0.01635, val loss: 0.01660\n",
      "Training epoch: 813, train loss: 0.01687, val loss: 0.01726\n",
      "Training epoch: 814, train loss: 0.01661, val loss: 0.01695\n",
      "Training epoch: 815, train loss: 0.01744, val loss: 0.01784\n",
      "Training epoch: 816, train loss: 0.01696, val loss: 0.01741\n",
      "Training epoch: 817, train loss: 0.01661, val loss: 0.01694\n",
      "Training epoch: 818, train loss: 0.08074, val loss: 0.08218\n",
      "Training epoch: 819, train loss: 0.01694, val loss: 0.01722\n",
      "Training epoch: 820, train loss: 0.02173, val loss: 0.02241\n",
      "Training epoch: 821, train loss: 0.01875, val loss: 0.01883\n",
      "Training epoch: 822, train loss: 0.02012, val loss: 0.02099\n",
      "Training epoch: 823, train loss: 0.02167, val loss: 0.02194\n",
      "Training epoch: 824, train loss: 0.01798, val loss: 0.01814\n",
      "Training epoch: 825, train loss: 0.01678, val loss: 0.01720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 826, train loss: 0.02762, val loss: 0.02912\n",
      "Training epoch: 827, train loss: 0.02978, val loss: 0.03000\n",
      "Training epoch: 828, train loss: 0.01737, val loss: 0.01761\n",
      "Training epoch: 829, train loss: 0.03235, val loss: 0.03272\n",
      "Training epoch: 830, train loss: 0.02040, val loss: 0.02076\n",
      "Training epoch: 831, train loss: 0.02745, val loss: 0.02857\n",
      "Training epoch: 832, train loss: 0.01767, val loss: 0.01787\n",
      "Training epoch: 833, train loss: 0.01836, val loss: 0.01893\n",
      "Training epoch: 834, train loss: 0.01701, val loss: 0.01774\n",
      "Training epoch: 835, train loss: 0.02267, val loss: 0.02275\n",
      "Training epoch: 836, train loss: 0.02090, val loss: 0.02145\n",
      "Training epoch: 837, train loss: 0.01667, val loss: 0.01716\n",
      "Training epoch: 838, train loss: 0.01676, val loss: 0.01712\n",
      "Training epoch: 839, train loss: 0.02162, val loss: 0.02211\n",
      "Training epoch: 840, train loss: 0.01961, val loss: 0.02135\n",
      "Training epoch: 841, train loss: 0.01763, val loss: 0.01894\n",
      "Training epoch: 842, train loss: 0.01852, val loss: 0.01944\n",
      "Training epoch: 843, train loss: 0.01926, val loss: 0.02003\n",
      "Training epoch: 844, train loss: 0.01799, val loss: 0.01892\n",
      "Training epoch: 845, train loss: 0.01728, val loss: 0.01776\n",
      "Training epoch: 846, train loss: 0.01723, val loss: 0.01742\n",
      "Training epoch: 847, train loss: 0.01629, val loss: 0.01672\n",
      "Training epoch: 848, train loss: 0.01668, val loss: 0.01693\n",
      "Training epoch: 849, train loss: 0.02419, val loss: 0.02445\n",
      "Training epoch: 850, train loss: 0.02380, val loss: 0.02440\n",
      "Training epoch: 851, train loss: 0.01793, val loss: 0.01837\n",
      "Training epoch: 852, train loss: 0.01836, val loss: 0.01894\n",
      "Training epoch: 853, train loss: 0.01697, val loss: 0.01716\n",
      "Training epoch: 854, train loss: 0.02014, val loss: 0.02031\n",
      "Training epoch: 855, train loss: 0.01958, val loss: 0.01978\n",
      "Training epoch: 856, train loss: 0.02692, val loss: 0.02693\n",
      "Training epoch: 857, train loss: 0.01802, val loss: 0.01847\n",
      "Training epoch: 858, train loss: 0.01853, val loss: 0.01918\n",
      "Training epoch: 859, train loss: 0.01611, val loss: 0.01669\n",
      "Training epoch: 860, train loss: 0.02459, val loss: 0.02473\n",
      "Training epoch: 861, train loss: 0.01720, val loss: 0.01759\n",
      "Training epoch: 862, train loss: 0.02007, val loss: 0.02043\n",
      "Training epoch: 863, train loss: 0.02003, val loss: 0.02100\n",
      "Training epoch: 864, train loss: 0.02198, val loss: 0.02216\n",
      "Training epoch: 865, train loss: 0.01984, val loss: 0.01998\n",
      "Training epoch: 866, train loss: 0.02928, val loss: 0.03027\n",
      "Training epoch: 867, train loss: 0.01662, val loss: 0.01714\n",
      "Training epoch: 868, train loss: 0.01902, val loss: 0.01920\n",
      "Training epoch: 869, train loss: 0.01735, val loss: 0.01764\n",
      "Training epoch: 870, train loss: 0.02640, val loss: 0.02655\n",
      "Training epoch: 871, train loss: 0.02299, val loss: 0.02280\n",
      "Training epoch: 872, train loss: 0.01691, val loss: 0.01799\n",
      "Training epoch: 873, train loss: 0.01853, val loss: 0.01927\n",
      "Training epoch: 874, train loss: 0.02517, val loss: 0.02463\n",
      "Training epoch: 875, train loss: 0.02394, val loss: 0.02458\n",
      "Training epoch: 876, train loss: 0.01746, val loss: 0.01766\n",
      "Training epoch: 877, train loss: 0.01655, val loss: 0.01685\n",
      "Training epoch: 878, train loss: 0.01624, val loss: 0.01647\n",
      "Training epoch: 879, train loss: 0.01631, val loss: 0.01659\n",
      "Training epoch: 880, train loss: 0.01886, val loss: 0.01944\n",
      "Training epoch: 881, train loss: 0.01741, val loss: 0.01736\n",
      "Training epoch: 882, train loss: 0.01730, val loss: 0.01762\n",
      "Training epoch: 883, train loss: 0.01661, val loss: 0.01696\n",
      "Training epoch: 884, train loss: 0.01700, val loss: 0.01744\n",
      "Training epoch: 885, train loss: 0.01636, val loss: 0.01690\n",
      "Training epoch: 886, train loss: 0.01895, val loss: 0.01923\n",
      "Training epoch: 887, train loss: 0.02292, val loss: 0.02313\n",
      "Training epoch: 888, train loss: 0.02589, val loss: 0.02840\n",
      "Training epoch: 889, train loss: 0.02237, val loss: 0.02456\n",
      "Training epoch: 890, train loss: 0.02060, val loss: 0.02239\n",
      "Training epoch: 891, train loss: 0.01920, val loss: 0.02047\n",
      "Training epoch: 892, train loss: 0.01945, val loss: 0.02054\n",
      "Training epoch: 893, train loss: 0.02108, val loss: 0.02200\n",
      "Training epoch: 894, train loss: 0.02765, val loss: 0.02828\n",
      "Training epoch: 895, train loss: 0.01857, val loss: 0.01949\n",
      "Training epoch: 896, train loss: 0.01747, val loss: 0.01866\n",
      "Training epoch: 897, train loss: 0.01773, val loss: 0.01895\n",
      "Training epoch: 898, train loss: 0.01928, val loss: 0.02058\n",
      "Training epoch: 899, train loss: 0.01755, val loss: 0.01861\n",
      "Training epoch: 900, train loss: 0.01773, val loss: 0.01869\n",
      "Training epoch: 901, train loss: 0.01747, val loss: 0.01853\n",
      "Training epoch: 902, train loss: 0.01958, val loss: 0.02041\n",
      "Training epoch: 903, train loss: 0.01745, val loss: 0.01836\n",
      "Training epoch: 904, train loss: 0.01894, val loss: 0.01991\n",
      "Training epoch: 905, train loss: 0.02909, val loss: 0.02986\n",
      "Training epoch: 906, train loss: 0.01937, val loss: 0.02055\n",
      "Training epoch: 907, train loss: 0.01912, val loss: 0.01986\n",
      "Training epoch: 908, train loss: 0.01729, val loss: 0.01819\n",
      "Training epoch: 909, train loss: 0.01820, val loss: 0.01928\n",
      "Training epoch: 910, train loss: 0.01714, val loss: 0.01785\n",
      "Training epoch: 911, train loss: 0.01709, val loss: 0.01787\n",
      "Training epoch: 912, train loss: 0.01652, val loss: 0.01696\n",
      "Training epoch: 913, train loss: 0.01724, val loss: 0.01763\n",
      "Training epoch: 914, train loss: 0.01826, val loss: 0.01849\n",
      "Training epoch: 915, train loss: 0.01724, val loss: 0.01768\n",
      "Training epoch: 916, train loss: 0.01721, val loss: 0.01751\n",
      "Training epoch: 917, train loss: 0.01773, val loss: 0.01797\n",
      "Training epoch: 918, train loss: 0.01739, val loss: 0.01769\n",
      "Training epoch: 919, train loss: 0.01643, val loss: 0.01681\n",
      "Training epoch: 920, train loss: 0.01725, val loss: 0.01764\n",
      "Training epoch: 921, train loss: 0.01671, val loss: 0.01713\n",
      "Training epoch: 922, train loss: 0.02251, val loss: 0.02277\n",
      "Training epoch: 923, train loss: 0.01838, val loss: 0.01864\n",
      "Training epoch: 924, train loss: 0.01681, val loss: 0.01712\n",
      "Training epoch: 925, train loss: 0.02051, val loss: 0.02102\n",
      "Training epoch: 926, train loss: 0.01981, val loss: 0.02004\n",
      "Training epoch: 927, train loss: 0.02180, val loss: 0.02222\n",
      "Training epoch: 928, train loss: 0.01900, val loss: 0.01913\n",
      "Training epoch: 929, train loss: 0.01669, val loss: 0.01713\n",
      "Training epoch: 930, train loss: 0.01763, val loss: 0.01842\n",
      "Training epoch: 931, train loss: 0.01814, val loss: 0.01834\n",
      "Training epoch: 932, train loss: 0.01619, val loss: 0.01669\n",
      "Training epoch: 933, train loss: 0.01588, val loss: 0.01630\n",
      "Training epoch: 934, train loss: 0.01668, val loss: 0.01685\n",
      "Training epoch: 935, train loss: 0.01717, val loss: 0.01761\n",
      "Training epoch: 936, train loss: 0.01742, val loss: 0.01751\n",
      "Training epoch: 937, train loss: 0.01608, val loss: 0.01642\n",
      "Training epoch: 938, train loss: 0.02575, val loss: 0.02588\n",
      "Training epoch: 939, train loss: 0.01991, val loss: 0.02019\n",
      "Training epoch: 940, train loss: 0.01739, val loss: 0.01785\n",
      "Training epoch: 941, train loss: 0.02253, val loss: 0.02271\n",
      "Training epoch: 942, train loss: 0.01709, val loss: 0.01726\n",
      "Training epoch: 943, train loss: 0.01658, val loss: 0.01686\n",
      "Training epoch: 944, train loss: 0.01766, val loss: 0.01804\n",
      "Training epoch: 945, train loss: 0.01675, val loss: 0.01758\n",
      "Training epoch: 946, train loss: 0.01781, val loss: 0.01850\n",
      "Training epoch: 947, train loss: 0.01666, val loss: 0.01694\n",
      "Training epoch: 948, train loss: 0.01646, val loss: 0.01663\n",
      "Training epoch: 949, train loss: 0.02189, val loss: 0.02217\n",
      "Training epoch: 950, train loss: 0.01774, val loss: 0.01776\n",
      "Training epoch: 951, train loss: 0.01880, val loss: 0.01951\n",
      "Training epoch: 952, train loss: 0.01869, val loss: 0.01864\n",
      "Training epoch: 953, train loss: 0.02596, val loss: 0.02611\n",
      "Training epoch: 954, train loss: 0.01829, val loss: 0.01852\n",
      "Training epoch: 955, train loss: 0.01760, val loss: 0.01787\n",
      "Training epoch: 956, train loss: 0.01910, val loss: 0.01937\n",
      "Training epoch: 957, train loss: 0.01744, val loss: 0.01788\n",
      "Training epoch: 958, train loss: 0.01823, val loss: 0.01888\n",
      "Training epoch: 959, train loss: 0.02055, val loss: 0.02175\n",
      "Training epoch: 960, train loss: 0.01775, val loss: 0.01845\n",
      "Training epoch: 961, train loss: 0.01915, val loss: 0.01938\n",
      "Training epoch: 962, train loss: 0.02212, val loss: 0.02311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 963, train loss: 0.01820, val loss: 0.01830\n",
      "Training epoch: 964, train loss: 0.01691, val loss: 0.01762\n",
      "Training epoch: 965, train loss: 0.02187, val loss: 0.02273\n",
      "Training epoch: 966, train loss: 0.02191, val loss: 0.02201\n",
      "Training epoch: 967, train loss: 0.01923, val loss: 0.01983\n",
      "Training epoch: 968, train loss: 0.01985, val loss: 0.01994\n",
      "Training epoch: 969, train loss: 0.01736, val loss: 0.01767\n",
      "Training epoch: 970, train loss: 0.01719, val loss: 0.01733\n",
      "Training epoch: 971, train loss: 0.01922, val loss: 0.01951\n",
      "Training epoch: 972, train loss: 0.01605, val loss: 0.01653\n",
      "Training epoch: 973, train loss: 0.02300, val loss: 0.02392\n",
      "Training epoch: 974, train loss: 0.02307, val loss: 0.02412\n",
      "Training epoch: 975, train loss: 0.02173, val loss: 0.02271\n",
      "Training epoch: 976, train loss: 0.01880, val loss: 0.01999\n",
      "Training epoch: 977, train loss: 0.01828, val loss: 0.01938\n",
      "Training epoch: 978, train loss: 0.01807, val loss: 0.01936\n",
      "Training epoch: 979, train loss: 0.01860, val loss: 0.01997\n",
      "Training epoch: 980, train loss: 0.01798, val loss: 0.01919\n",
      "Training epoch: 981, train loss: 0.01869, val loss: 0.01974\n",
      "Training epoch: 982, train loss: 0.06690, val loss: 0.06767\n",
      "Training epoch: 983, train loss: 0.02892, val loss: 0.03041\n",
      "Training epoch: 984, train loss: 0.01954, val loss: 0.02067\n",
      "Training epoch: 985, train loss: 0.01846, val loss: 0.01953\n",
      "Training epoch: 986, train loss: 0.02051, val loss: 0.02125\n",
      "Training epoch: 987, train loss: 0.01945, val loss: 0.02035\n",
      "Training epoch: 988, train loss: 0.02582, val loss: 0.02647\n",
      "Training epoch: 989, train loss: 0.01777, val loss: 0.01861\n",
      "Training epoch: 990, train loss: 0.01693, val loss: 0.01787\n",
      "Training epoch: 991, train loss: 0.01738, val loss: 0.01806\n",
      "Training epoch: 992, train loss: 0.01846, val loss: 0.01942\n",
      "Training epoch: 993, train loss: 0.01738, val loss: 0.01814\n",
      "Training epoch: 994, train loss: 0.01758, val loss: 0.01802\n",
      "Training epoch: 995, train loss: 0.01675, val loss: 0.01696\n",
      "Training epoch: 996, train loss: 0.02543, val loss: 0.02576\n",
      "Training epoch: 997, train loss: 0.01644, val loss: 0.01684\n",
      "Training epoch: 998, train loss: 0.01600, val loss: 0.01637\n",
      "Training epoch: 999, train loss: 0.01728, val loss: 0.01761\n",
      "Training epoch: 1000, train loss: 0.01769, val loss: 0.01795\n",
      "Training epoch: 1001, train loss: 0.02648, val loss: 0.02658\n",
      "Training epoch: 1002, train loss: 0.02561, val loss: 0.02573\n",
      "Training epoch: 1003, train loss: 0.01571, val loss: 0.01605\n",
      "Training epoch: 1004, train loss: 0.01742, val loss: 0.01763\n",
      "Training epoch: 1005, train loss: 0.01757, val loss: 0.01818\n",
      "Training epoch: 1006, train loss: 0.01707, val loss: 0.01736\n",
      "Training epoch: 1007, train loss: 0.01701, val loss: 0.01755\n",
      "Training epoch: 1008, train loss: 0.01848, val loss: 0.01873\n",
      "Training epoch: 1009, train loss: 0.01781, val loss: 0.01834\n",
      "Training epoch: 1010, train loss: 0.02033, val loss: 0.02058\n",
      "Training epoch: 1011, train loss: 0.01891, val loss: 0.01963\n",
      "Training epoch: 1012, train loss: 0.01837, val loss: 0.01827\n",
      "Training epoch: 1013, train loss: 0.01969, val loss: 0.01998\n",
      "Training epoch: 1014, train loss: 0.01690, val loss: 0.01747\n",
      "Training epoch: 1015, train loss: 0.01758, val loss: 0.01791\n",
      "Training epoch: 1016, train loss: 0.01840, val loss: 0.01837\n",
      "Training epoch: 1017, train loss: 0.01802, val loss: 0.01819\n",
      "Training epoch: 1018, train loss: 0.01768, val loss: 0.01797\n",
      "Training epoch: 1019, train loss: 0.02009, val loss: 0.02056\n",
      "Training epoch: 1020, train loss: 0.01803, val loss: 0.01829\n",
      "Training epoch: 1021, train loss: 0.01700, val loss: 0.01730\n",
      "Training epoch: 1022, train loss: 0.01839, val loss: 0.01875\n",
      "Training epoch: 1023, train loss: 0.01799, val loss: 0.01888\n",
      "Training epoch: 1024, train loss: 0.01959, val loss: 0.01978\n",
      "Training epoch: 1025, train loss: 0.02088, val loss: 0.02149\n",
      "Training epoch: 1026, train loss: 0.01837, val loss: 0.01866\n",
      "Training epoch: 1027, train loss: 0.02111, val loss: 0.02148\n",
      "Training epoch: 1028, train loss: 0.01727, val loss: 0.01765\n",
      "Training epoch: 1029, train loss: 0.01777, val loss: 0.01799\n",
      "Training epoch: 1030, train loss: 0.01677, val loss: 0.01732\n",
      "Training epoch: 1031, train loss: 0.02194, val loss: 0.02212\n",
      "Training epoch: 1032, train loss: 0.01612, val loss: 0.01627\n",
      "Training epoch: 1033, train loss: 0.01599, val loss: 0.01631\n",
      "Training epoch: 1034, train loss: 0.01630, val loss: 0.01653\n",
      "Training epoch: 1035, train loss: 0.01916, val loss: 0.01953\n",
      "Training epoch: 1036, train loss: 0.01589, val loss: 0.01645\n",
      "Training epoch: 1037, train loss: 0.01623, val loss: 0.01652\n",
      "Training epoch: 1038, train loss: 0.01892, val loss: 0.01932\n",
      "Training epoch: 1039, train loss: 0.02517, val loss: 0.02728\n",
      "Training epoch: 1040, train loss: 0.01998, val loss: 0.02070\n",
      "Training epoch: 1041, train loss: 0.01940, val loss: 0.01968\n",
      "Training epoch: 1042, train loss: 0.01638, val loss: 0.01656\n",
      "Training epoch: 1043, train loss: 0.01669, val loss: 0.01715\n",
      "Training epoch: 1044, train loss: 0.02369, val loss: 0.02389\n",
      "Training epoch: 1045, train loss: 0.01689, val loss: 0.01724\n",
      "Training epoch: 1046, train loss: 0.01668, val loss: 0.01694\n",
      "Training epoch: 1047, train loss: 0.01935, val loss: 0.01955\n",
      "Training epoch: 1048, train loss: 0.01762, val loss: 0.01752\n",
      "Training epoch: 1049, train loss: 0.01652, val loss: 0.01686\n",
      "Training epoch: 1050, train loss: 0.02205, val loss: 0.02244\n",
      "Training epoch: 1051, train loss: 0.01787, val loss: 0.01836\n",
      "Training epoch: 1052, train loss: 0.01870, val loss: 0.01959\n",
      "Training epoch: 1053, train loss: 0.01952, val loss: 0.02010\n",
      "Training epoch: 1054, train loss: 0.01847, val loss: 0.01870\n",
      "Training epoch: 1055, train loss: 0.02330, val loss: 0.02360\n",
      "Training epoch: 1056, train loss: 0.01942, val loss: 0.01964\n",
      "Training epoch: 1057, train loss: 0.01657, val loss: 0.01676\n",
      "Training epoch: 1058, train loss: 0.01641, val loss: 0.01682\n",
      "Training epoch: 1059, train loss: 0.01588, val loss: 0.01635\n",
      "Training epoch: 1060, train loss: 0.02093, val loss: 0.02130\n",
      "Training epoch: 1061, train loss: 0.01863, val loss: 0.01914\n",
      "Training epoch: 1062, train loss: 0.02009, val loss: 0.02014\n",
      "Training epoch: 1063, train loss: 0.03521, val loss: 0.03543\n",
      "Training epoch: 1064, train loss: 0.01655, val loss: 0.01717\n",
      "Training epoch: 1065, train loss: 0.01981, val loss: 0.01996\n",
      "Training epoch: 1066, train loss: 0.01680, val loss: 0.01704\n",
      "Training epoch: 1067, train loss: 0.01738, val loss: 0.01758\n",
      "Training epoch: 1068, train loss: 0.01601, val loss: 0.01631\n",
      "Training epoch: 1069, train loss: 0.01706, val loss: 0.01773\n",
      "Training epoch: 1070, train loss: 0.01663, val loss: 0.01686\n",
      "Training epoch: 1071, train loss: 0.01625, val loss: 0.01646\n",
      "Training epoch: 1072, train loss: 0.02474, val loss: 0.02560\n",
      "Training epoch: 1073, train loss: 0.02100, val loss: 0.02216\n",
      "Training epoch: 1074, train loss: 0.01981, val loss: 0.02042\n",
      "Training epoch: 1075, train loss: 0.01925, val loss: 0.01938\n",
      "Training epoch: 1076, train loss: 0.02303, val loss: 0.02319\n",
      "Training epoch: 1077, train loss: 0.02129, val loss: 0.02137\n",
      "Training epoch: 1078, train loss: 0.01697, val loss: 0.01742\n",
      "Training epoch: 1079, train loss: 0.01775, val loss: 0.01786\n",
      "Training epoch: 1080, train loss: 0.01860, val loss: 0.01872\n",
      "Training epoch: 1081, train loss: 0.01932, val loss: 0.01936\n",
      "Training epoch: 1082, train loss: 0.04631, val loss: 0.04628\n",
      "Training epoch: 1083, train loss: 0.03048, val loss: 0.03234\n",
      "Training epoch: 1084, train loss: 0.01869, val loss: 0.01887\n",
      "Training epoch: 1085, train loss: 0.01714, val loss: 0.01752\n",
      "Training epoch: 1086, train loss: 0.01749, val loss: 0.01770\n",
      "Training epoch: 1087, train loss: 0.01718, val loss: 0.01732\n",
      "Training epoch: 1088, train loss: 0.01814, val loss: 0.01811\n",
      "Training epoch: 1089, train loss: 0.02066, val loss: 0.02092\n",
      "Training epoch: 1090, train loss: 0.01640, val loss: 0.01679\n",
      "Training epoch: 1091, train loss: 0.02203, val loss: 0.02293\n",
      "Training epoch: 1092, train loss: 0.01592, val loss: 0.01639\n",
      "Training epoch: 1093, train loss: 0.01697, val loss: 0.01758\n",
      "Training epoch: 1094, train loss: 0.01639, val loss: 0.01685\n",
      "Training epoch: 1095, train loss: 0.01830, val loss: 0.01911\n",
      "Training epoch: 1096, train loss: 0.01919, val loss: 0.02019\n",
      "Training epoch: 1097, train loss: 0.01695, val loss: 0.01791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1098, train loss: 0.01650, val loss: 0.01650\n",
      "Training epoch: 1099, train loss: 0.01758, val loss: 0.01796\n",
      "Training epoch: 1100, train loss: 0.01805, val loss: 0.01864\n",
      "Training epoch: 1101, train loss: 0.02799, val loss: 0.02797\n",
      "Training epoch: 1102, train loss: 0.01625, val loss: 0.01672\n",
      "Training epoch: 1103, train loss: 0.01870, val loss: 0.01891\n",
      "Training epoch: 1104, train loss: 0.01774, val loss: 0.01799\n",
      "Training epoch: 1105, train loss: 0.01741, val loss: 0.01792\n",
      "Training epoch: 1106, train loss: 0.03790, val loss: 0.03884\n",
      "Training epoch: 1107, train loss: 0.02331, val loss: 0.02332\n",
      "Training epoch: 1108, train loss: 0.01722, val loss: 0.01834\n",
      "Training epoch: 1109, train loss: 0.02208, val loss: 0.02261\n",
      "Training epoch: 1110, train loss: 0.01665, val loss: 0.01689\n",
      "Training epoch: 1111, train loss: 0.01737, val loss: 0.01776\n",
      "Training epoch: 1112, train loss: 0.02065, val loss: 0.02096\n",
      "Training epoch: 1113, train loss: 0.01973, val loss: 0.01977\n",
      "Training epoch: 1114, train loss: 0.01962, val loss: 0.02016\n",
      "Training epoch: 1115, train loss: 0.01734, val loss: 0.01783\n",
      "Training epoch: 1116, train loss: 0.01681, val loss: 0.01691\n",
      "Training epoch: 1117, train loss: 0.01707, val loss: 0.01746\n",
      "Training epoch: 1118, train loss: 0.01797, val loss: 0.01845\n",
      "Training epoch: 1119, train loss: 0.01716, val loss: 0.01725\n",
      "Training epoch: 1120, train loss: 0.02009, val loss: 0.02045\n",
      "Training epoch: 1121, train loss: 0.02013, val loss: 0.02044\n",
      "Training epoch: 1122, train loss: 0.01868, val loss: 0.01946\n",
      "Training epoch: 1123, train loss: 0.02255, val loss: 0.02325\n",
      "Training epoch: 1124, train loss: 0.01747, val loss: 0.01765\n",
      "Training epoch: 1125, train loss: 0.01951, val loss: 0.02016\n",
      "Training epoch: 1126, train loss: 0.01766, val loss: 0.01778\n",
      "Training epoch: 1127, train loss: 0.02297, val loss: 0.02326\n",
      "Training epoch: 1128, train loss: 0.02108, val loss: 0.02162\n",
      "Training epoch: 1129, train loss: 0.01763, val loss: 0.01809\n",
      "Training epoch: 1130, train loss: 0.02052, val loss: 0.02050\n",
      "Training epoch: 1131, train loss: 0.01911, val loss: 0.01945\n",
      "Training epoch: 1132, train loss: 0.02461, val loss: 0.02478\n",
      "Training epoch: 1133, train loss: 0.01867, val loss: 0.01915\n",
      "Training epoch: 1134, train loss: 0.01664, val loss: 0.01705\n",
      "Training epoch: 1135, train loss: 0.01802, val loss: 0.01830\n",
      "Training epoch: 1136, train loss: 0.01624, val loss: 0.01669\n",
      "Training epoch: 1137, train loss: 0.01717, val loss: 0.01724\n",
      "Training epoch: 1138, train loss: 0.01800, val loss: 0.01810\n",
      "Training epoch: 1139, train loss: 0.01731, val loss: 0.01742\n",
      "Training epoch: 1140, train loss: 0.01641, val loss: 0.01663\n",
      "Training epoch: 1141, train loss: 0.01570, val loss: 0.01596\n",
      "Training epoch: 1142, train loss: 0.01706, val loss: 0.01731\n",
      "Training epoch: 1143, train loss: 0.01879, val loss: 0.01999\n",
      "Training epoch: 1144, train loss: 0.01695, val loss: 0.01795\n",
      "Training epoch: 1145, train loss: 0.01973, val loss: 0.02031\n",
      "Training epoch: 1146, train loss: 0.01786, val loss: 0.01795\n",
      "Training epoch: 1147, train loss: 0.01714, val loss: 0.01755\n",
      "Training epoch: 1148, train loss: 0.01592, val loss: 0.01625\n",
      "Training epoch: 1149, train loss: 0.01728, val loss: 0.01775\n",
      "Training epoch: 1150, train loss: 0.01750, val loss: 0.01783\n",
      "Training epoch: 1151, train loss: 0.01682, val loss: 0.01698\n",
      "Training epoch: 1152, train loss: 0.01686, val loss: 0.01711\n",
      "Training epoch: 1153, train loss: 0.01613, val loss: 0.01647\n",
      "Training epoch: 1154, train loss: 0.02274, val loss: 0.02281\n",
      "Training epoch: 1155, train loss: 0.01739, val loss: 0.01743\n",
      "Training epoch: 1156, train loss: 0.02025, val loss: 0.02111\n",
      "Training epoch: 1157, train loss: 0.02016, val loss: 0.02134\n",
      "Training epoch: 1158, train loss: 0.04477, val loss: 0.04364\n",
      "Training epoch: 1159, train loss: 0.01838, val loss: 0.01867\n",
      "Training epoch: 1160, train loss: 0.02526, val loss: 0.02563\n",
      "Training epoch: 1161, train loss: 0.01874, val loss: 0.01892\n",
      "Training epoch: 1162, train loss: 0.01672, val loss: 0.01707\n",
      "Training epoch: 1163, train loss: 0.01833, val loss: 0.01878\n",
      "Training epoch: 1164, train loss: 0.01820, val loss: 0.01856\n",
      "Training epoch: 1165, train loss: 0.02083, val loss: 0.02091\n",
      "Training epoch: 1166, train loss: 0.02208, val loss: 0.02221\n",
      "Training epoch: 1167, train loss: 0.02785, val loss: 0.02805\n",
      "Training epoch: 1168, train loss: 0.02224, val loss: 0.02312\n",
      "Training epoch: 1169, train loss: 0.02359, val loss: 0.02596\n",
      "Training epoch: 1170, train loss: 0.01921, val loss: 0.02013\n",
      "Training epoch: 1171, train loss: 0.01796, val loss: 0.01920\n",
      "Training epoch: 1172, train loss: 0.01940, val loss: 0.02018\n",
      "Training epoch: 1173, train loss: 0.01765, val loss: 0.01873\n",
      "Training epoch: 1174, train loss: 0.01715, val loss: 0.01804\n",
      "Training epoch: 1175, train loss: 0.01715, val loss: 0.01806\n",
      "Training epoch: 1176, train loss: 0.01654, val loss: 0.01726\n",
      "Training epoch: 1177, train loss: 0.01811, val loss: 0.01858\n",
      "Training epoch: 1178, train loss: 0.01626, val loss: 0.01648\n",
      "Training epoch: 1179, train loss: 0.01684, val loss: 0.01710\n",
      "Training epoch: 1180, train loss: 0.01696, val loss: 0.01722\n",
      "Training epoch: 1181, train loss: 0.01904, val loss: 0.01925\n",
      "Training epoch: 1182, train loss: 0.02047, val loss: 0.02061\n",
      "Training epoch: 1183, train loss: 0.01661, val loss: 0.01698\n",
      "Training epoch: 1184, train loss: 0.01995, val loss: 0.02027\n",
      "Training epoch: 1185, train loss: 0.01576, val loss: 0.01608\n",
      "Training epoch: 1186, train loss: 0.01778, val loss: 0.01793\n",
      "Training epoch: 1187, train loss: 0.01833, val loss: 0.01854\n",
      "Training epoch: 1188, train loss: 0.01576, val loss: 0.01590\n",
      "Training epoch: 1189, train loss: 0.01618, val loss: 0.01655\n",
      "Training epoch: 1190, train loss: 0.01691, val loss: 0.01745\n",
      "Training epoch: 1191, train loss: 0.03055, val loss: 0.03203\n",
      "Training epoch: 1192, train loss: 0.02535, val loss: 0.02521\n",
      "Training epoch: 1193, train loss: 0.01762, val loss: 0.01782\n",
      "Training epoch: 1194, train loss: 0.02430, val loss: 0.02443\n",
      "Training epoch: 1195, train loss: 0.01638, val loss: 0.01703\n",
      "Training epoch: 1196, train loss: 0.02210, val loss: 0.02324\n",
      "Training epoch: 1197, train loss: 0.01695, val loss: 0.01687\n",
      "Training epoch: 1198, train loss: 0.01875, val loss: 0.01896\n",
      "Training epoch: 1199, train loss: 0.01878, val loss: 0.01909\n",
      "Training epoch: 1200, train loss: 0.02671, val loss: 0.02739\n",
      "Training epoch: 1201, train loss: 0.02019, val loss: 0.02007\n",
      "Training epoch: 1202, train loss: 0.02987, val loss: 0.02960\n",
      "Training epoch: 1203, train loss: 0.02616, val loss: 0.02612\n",
      "Training epoch: 1204, train loss: 0.01817, val loss: 0.01871\n",
      "Training epoch: 1205, train loss: 0.01834, val loss: 0.01879\n",
      "Training epoch: 1206, train loss: 0.01950, val loss: 0.02061\n",
      "Training epoch: 1207, train loss: 0.02715, val loss: 0.02859\n",
      "Training epoch: 1208, train loss: 0.01801, val loss: 0.01788\n",
      "Training epoch: 1209, train loss: 0.01724, val loss: 0.01748\n",
      "Training epoch: 1210, train loss: 0.01653, val loss: 0.01697\n",
      "Training epoch: 1211, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1212, train loss: 0.01970, val loss: 0.02050\n",
      "Training epoch: 1213, train loss: 0.02084, val loss: 0.02092\n",
      "Training epoch: 1214, train loss: 0.01728, val loss: 0.01782\n",
      "Training epoch: 1215, train loss: 0.01587, val loss: 0.01614\n",
      "Training epoch: 1216, train loss: 0.01705, val loss: 0.01729\n",
      "Training epoch: 1217, train loss: 0.01951, val loss: 0.01992\n",
      "Training epoch: 1218, train loss: 0.01990, val loss: 0.02055\n",
      "Training epoch: 1219, train loss: 0.01680, val loss: 0.01694\n",
      "Training epoch: 1220, train loss: 0.01831, val loss: 0.01838\n",
      "Training epoch: 1221, train loss: 0.02161, val loss: 0.02250\n",
      "Training epoch: 1222, train loss: 0.02016, val loss: 0.02094\n",
      "Training epoch: 1223, train loss: 0.01787, val loss: 0.01898\n",
      "Training epoch: 1224, train loss: 0.02456, val loss: 0.02622\n",
      "Training epoch: 1225, train loss: 0.01901, val loss: 0.01980\n",
      "Training epoch: 1226, train loss: 0.01816, val loss: 0.01922\n",
      "Training epoch: 1227, train loss: 0.01674, val loss: 0.01735\n",
      "Training epoch: 1228, train loss: 0.01936, val loss: 0.02018\n",
      "Training epoch: 1229, train loss: 0.02049, val loss: 0.02069\n",
      "Training epoch: 1230, train loss: 0.01716, val loss: 0.01730\n",
      "Training epoch: 1231, train loss: 0.01696, val loss: 0.01748\n",
      "Training epoch: 1232, train loss: 0.02064, val loss: 0.02087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1233, train loss: 0.01734, val loss: 0.01754\n",
      "Training epoch: 1234, train loss: 0.01814, val loss: 0.01847\n",
      "Training epoch: 1235, train loss: 0.01626, val loss: 0.01651\n",
      "Training epoch: 1236, train loss: 0.01626, val loss: 0.01643\n",
      "Training epoch: 1237, train loss: 0.01627, val loss: 0.01665\n",
      "Training epoch: 1238, train loss: 0.01832, val loss: 0.01856\n",
      "Training epoch: 1239, train loss: 0.01904, val loss: 0.01970\n",
      "Training epoch: 1240, train loss: 0.01789, val loss: 0.01859\n",
      "Training epoch: 1241, train loss: 0.01745, val loss: 0.01750\n",
      "Training epoch: 1242, train loss: 0.01830, val loss: 0.01843\n",
      "Training epoch: 1243, train loss: 0.01908, val loss: 0.01926\n",
      "Training epoch: 1244, train loss: 0.01951, val loss: 0.02023\n",
      "Training epoch: 1245, train loss: 0.01839, val loss: 0.01855\n",
      "Training epoch: 1246, train loss: 0.01988, val loss: 0.02062\n",
      "Training epoch: 1247, train loss: 0.01709, val loss: 0.01726\n",
      "Training epoch: 1248, train loss: 0.03289, val loss: 0.03254\n",
      "Training epoch: 1249, train loss: 0.02449, val loss: 0.02443\n",
      "Training epoch: 1250, train loss: 0.01741, val loss: 0.01743\n",
      "Training epoch: 1251, train loss: 0.01644, val loss: 0.01673\n",
      "Training epoch: 1252, train loss: 0.01770, val loss: 0.01795\n",
      "Training epoch: 1253, train loss: 0.01965, val loss: 0.01993\n",
      "Training epoch: 1254, train loss: 0.01995, val loss: 0.02085\n",
      "Training epoch: 1255, train loss: 0.01805, val loss: 0.01827\n",
      "Training epoch: 1256, train loss: 0.03523, val loss: 0.03672\n",
      "Training epoch: 1257, train loss: 0.01670, val loss: 0.01691\n",
      "Training epoch: 1258, train loss: 0.01650, val loss: 0.01679\n",
      "Training epoch: 1259, train loss: 0.01637, val loss: 0.01680\n",
      "Training epoch: 1260, train loss: 0.01670, val loss: 0.01681\n",
      "Training epoch: 1261, train loss: 0.02522, val loss: 0.02537\n",
      "Training epoch: 1262, train loss: 0.02256, val loss: 0.02274\n",
      "Training epoch: 1263, train loss: 0.01656, val loss: 0.01689\n",
      "Training epoch: 1264, train loss: 0.02176, val loss: 0.02210\n",
      "Training epoch: 1265, train loss: 0.01686, val loss: 0.01704\n",
      "Training epoch: 1266, train loss: 0.01637, val loss: 0.01660\n",
      "Training epoch: 1267, train loss: 0.01641, val loss: 0.01698\n",
      "Training epoch: 1268, train loss: 0.06171, val loss: 0.06254\n",
      "Training epoch: 1269, train loss: 0.01716, val loss: 0.01813\n",
      "Training epoch: 1270, train loss: 0.01814, val loss: 0.01893\n",
      "Training epoch: 1271, train loss: 0.01873, val loss: 0.01882\n",
      "Training epoch: 1272, train loss: 0.03009, val loss: 0.03085\n",
      "Training epoch: 1273, train loss: 0.01637, val loss: 0.01679\n",
      "Training epoch: 1274, train loss: 0.01668, val loss: 0.01697\n",
      "Training epoch: 1275, train loss: 0.01863, val loss: 0.01878\n",
      "Training epoch: 1276, train loss: 0.01861, val loss: 0.01877\n",
      "Training epoch: 1277, train loss: 0.01627, val loss: 0.01653\n",
      "Training epoch: 1278, train loss: 0.01645, val loss: 0.01660\n",
      "Training epoch: 1279, train loss: 0.01668, val loss: 0.01694\n",
      "Training epoch: 1280, train loss: 0.01606, val loss: 0.01623\n",
      "Training epoch: 1281, train loss: 0.02406, val loss: 0.02368\n",
      "Training epoch: 1282, train loss: 0.01813, val loss: 0.01810\n",
      "Training epoch: 1283, train loss: 0.01580, val loss: 0.01601\n",
      "Training epoch: 1284, train loss: 0.01983, val loss: 0.02045\n",
      "Training epoch: 1285, train loss: 0.01881, val loss: 0.01967\n",
      "Training epoch: 1286, train loss: 0.01702, val loss: 0.01701\n",
      "Training epoch: 1287, train loss: 0.01823, val loss: 0.01839\n",
      "Training epoch: 1288, train loss: 0.01837, val loss: 0.01874\n",
      "Training epoch: 1289, train loss: 0.01844, val loss: 0.01864\n",
      "Training epoch: 1290, train loss: 0.01783, val loss: 0.01794\n",
      "Training epoch: 1291, train loss: 0.01610, val loss: 0.01637\n",
      "Training epoch: 1292, train loss: 0.01649, val loss: 0.01693\n",
      "Training epoch: 1293, train loss: 0.01715, val loss: 0.01750\n",
      "Training epoch: 1294, train loss: 0.02044, val loss: 0.02074\n",
      "Training epoch: 1295, train loss: 0.02016, val loss: 0.02074\n",
      "Training epoch: 1296, train loss: 0.01945, val loss: 0.01959\n",
      "Training epoch: 1297, train loss: 0.01806, val loss: 0.01839\n",
      "Training epoch: 1298, train loss: 0.02599, val loss: 0.02627\n",
      "Training epoch: 1299, train loss: 0.02079, val loss: 0.02064\n",
      "Training epoch: 1300, train loss: 0.01714, val loss: 0.01737\n",
      "Training epoch: 1301, train loss: 0.02382, val loss: 0.02425\n",
      "Training epoch: 1302, train loss: 0.01667, val loss: 0.01689\n",
      "Training epoch: 1303, train loss: 0.01959, val loss: 0.01979\n",
      "Training epoch: 1304, train loss: 0.01655, val loss: 0.01693\n",
      "Training epoch: 1305, train loss: 0.01658, val loss: 0.01732\n",
      "Training epoch: 1306, train loss: 0.01765, val loss: 0.01757\n",
      "Training epoch: 1307, train loss: 0.01742, val loss: 0.01786\n",
      "Training epoch: 1308, train loss: 0.01921, val loss: 0.01946\n",
      "Training epoch: 1309, train loss: 0.01634, val loss: 0.01647\n",
      "Training epoch: 1310, train loss: 0.01757, val loss: 0.01765\n",
      "Training epoch: 1311, train loss: 0.01763, val loss: 0.01781\n",
      "Training epoch: 1312, train loss: 0.01573, val loss: 0.01603\n",
      "Training epoch: 1313, train loss: 0.01713, val loss: 0.01722\n",
      "Training epoch: 1314, train loss: 0.01707, val loss: 0.01771\n",
      "Training epoch: 1315, train loss: 0.02106, val loss: 0.02107\n",
      "Training epoch: 1316, train loss: 0.02363, val loss: 0.02358\n",
      "Training epoch: 1317, train loss: 0.01831, val loss: 0.01893\n",
      "Training epoch: 1318, train loss: 0.01741, val loss: 0.01843\n",
      "Training epoch: 1319, train loss: 0.02183, val loss: 0.02181\n",
      "Training epoch: 1320, train loss: 0.01745, val loss: 0.01759\n",
      "Training epoch: 1321, train loss: 0.02196, val loss: 0.02202\n",
      "Training epoch: 1322, train loss: 0.02112, val loss: 0.02126\n",
      "Training epoch: 1323, train loss: 0.01827, val loss: 0.01829\n",
      "Training epoch: 1324, train loss: 0.01647, val loss: 0.01672\n",
      "Training epoch: 1325, train loss: 0.02055, val loss: 0.02076\n",
      "Training epoch: 1326, train loss: 0.01682, val loss: 0.01706\n",
      "Training epoch: 1327, train loss: 0.01646, val loss: 0.01683\n",
      "Training epoch: 1328, train loss: 0.01746, val loss: 0.01730\n",
      "Training epoch: 1329, train loss: 0.01690, val loss: 0.01781\n",
      "Training epoch: 1330, train loss: 0.01733, val loss: 0.01760\n",
      "Training epoch: 1331, train loss: 0.01822, val loss: 0.01860\n",
      "Training epoch: 1332, train loss: 0.01799, val loss: 0.01807\n",
      "Training epoch: 1333, train loss: 0.01592, val loss: 0.01614\n",
      "Training epoch: 1334, train loss: 0.01884, val loss: 0.01926\n",
      "Training epoch: 1335, train loss: 0.01578, val loss: 0.01584\n",
      "Training epoch: 1336, train loss: 0.01633, val loss: 0.01667\n",
      "Training epoch: 1337, train loss: 0.01937, val loss: 0.01954\n",
      "Training epoch: 1338, train loss: 0.01807, val loss: 0.01852\n",
      "Training epoch: 1339, train loss: 0.05201, val loss: 0.05286\n",
      "Training epoch: 1340, train loss: 0.01783, val loss: 0.01801\n",
      "Training epoch: 1341, train loss: 0.01950, val loss: 0.01974\n",
      "Training epoch: 1342, train loss: 0.02338, val loss: 0.02355\n",
      "Training epoch: 1343, train loss: 0.02097, val loss: 0.02192\n",
      "Training epoch: 1344, train loss: 0.01811, val loss: 0.01829\n",
      "Training epoch: 1345, train loss: 0.01709, val loss: 0.01753\n",
      "Training epoch: 1346, train loss: 0.02446, val loss: 0.02433\n",
      "Training epoch: 1347, train loss: 0.01884, val loss: 0.01896\n",
      "Training epoch: 1348, train loss: 0.02124, val loss: 0.02201\n",
      "Training epoch: 1349, train loss: 0.01742, val loss: 0.01808\n",
      "Training epoch: 1350, train loss: 0.01787, val loss: 0.01853\n",
      "Training epoch: 1351, train loss: 0.02009, val loss: 0.02096\n",
      "Training epoch: 1352, train loss: 0.01839, val loss: 0.01868\n",
      "Training epoch: 1353, train loss: 0.01843, val loss: 0.01875\n",
      "Training epoch: 1354, train loss: 0.01550, val loss: 0.01577\n",
      "Training epoch: 1355, train loss: 0.01785, val loss: 0.01812\n",
      "Training epoch: 1356, train loss: 0.01658, val loss: 0.01673\n",
      "Training epoch: 1357, train loss: 0.01845, val loss: 0.01843\n",
      "Training epoch: 1358, train loss: 0.01758, val loss: 0.01822\n",
      "Training epoch: 1359, train loss: 0.08791, val loss: 0.08894\n",
      "Training epoch: 1360, train loss: 0.02645, val loss: 0.02691\n",
      "Training epoch: 1361, train loss: 0.02179, val loss: 0.02255\n",
      "Training epoch: 1362, train loss: 0.02087, val loss: 0.02159\n",
      "Training epoch: 1363, train loss: 0.01932, val loss: 0.02013\n",
      "Training epoch: 1364, train loss: 0.01776, val loss: 0.01802\n",
      "Training epoch: 1365, train loss: 0.01722, val loss: 0.01745\n",
      "Training epoch: 1366, train loss: 0.01886, val loss: 0.01885\n",
      "Training epoch: 1367, train loss: 0.01763, val loss: 0.01774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1368, train loss: 0.01775, val loss: 0.01793\n",
      "Training epoch: 1369, train loss: 0.01880, val loss: 0.01898\n",
      "Training epoch: 1370, train loss: 0.01851, val loss: 0.01879\n",
      "Training epoch: 1371, train loss: 0.01928, val loss: 0.01973\n",
      "Training epoch: 1372, train loss: 0.04686, val loss: 0.04760\n",
      "Training epoch: 1373, train loss: 0.01660, val loss: 0.01741\n",
      "Training epoch: 1374, train loss: 0.01671, val loss: 0.01695\n",
      "Training epoch: 1375, train loss: 0.01643, val loss: 0.01697\n",
      "Training epoch: 1376, train loss: 0.01638, val loss: 0.01650\n",
      "Training epoch: 1377, train loss: 0.01581, val loss: 0.01615\n",
      "Training epoch: 1378, train loss: 0.01847, val loss: 0.01873\n",
      "Training epoch: 1379, train loss: 0.01922, val loss: 0.01934\n",
      "Training epoch: 1380, train loss: 0.01657, val loss: 0.01688\n",
      "Training epoch: 1381, train loss: 0.01597, val loss: 0.01638\n",
      "Training epoch: 1382, train loss: 0.01660, val loss: 0.01692\n",
      "Training epoch: 1383, train loss: 0.01612, val loss: 0.01654\n",
      "Training epoch: 1384, train loss: 0.01716, val loss: 0.01744\n",
      "Training epoch: 1385, train loss: 0.01629, val loss: 0.01660\n",
      "Training epoch: 1386, train loss: 0.01647, val loss: 0.01710\n",
      "Training epoch: 1387, train loss: 0.01598, val loss: 0.01628\n",
      "Training epoch: 1388, train loss: 0.01852, val loss: 0.01869\n",
      "Training epoch: 1389, train loss: 0.01591, val loss: 0.01615\n",
      "Training epoch: 1390, train loss: 0.01822, val loss: 0.01881\n",
      "Training epoch: 1391, train loss: 0.01743, val loss: 0.01753\n",
      "Training epoch: 1392, train loss: 0.01876, val loss: 0.01876\n",
      "Training epoch: 1393, train loss: 0.01791, val loss: 0.01851\n",
      "Training epoch: 1394, train loss: 0.01982, val loss: 0.02001\n",
      "Training epoch: 1395, train loss: 0.01790, val loss: 0.01849\n",
      "Training epoch: 1396, train loss: 0.01698, val loss: 0.01718\n",
      "Training epoch: 1397, train loss: 0.01645, val loss: 0.01685\n",
      "Training epoch: 1398, train loss: 0.02094, val loss: 0.02116\n",
      "Training epoch: 1399, train loss: 0.01793, val loss: 0.01871\n",
      "Training epoch: 1400, train loss: 0.01988, val loss: 0.02002\n",
      "Training epoch: 1401, train loss: 0.01834, val loss: 0.01865\n",
      "Training epoch: 1402, train loss: 0.01792, val loss: 0.01858\n",
      "Training epoch: 1403, train loss: 0.03687, val loss: 0.03862\n",
      "Training epoch: 1404, train loss: 0.01966, val loss: 0.02045\n",
      "Training epoch: 1405, train loss: 0.01706, val loss: 0.01707\n",
      "Training epoch: 1406, train loss: 0.01685, val loss: 0.01700\n",
      "Training epoch: 1407, train loss: 0.01760, val loss: 0.01834\n",
      "Training epoch: 1408, train loss: 0.01772, val loss: 0.01791\n",
      "Training epoch: 1409, train loss: 0.01620, val loss: 0.01685\n",
      "Training epoch: 1410, train loss: 0.01873, val loss: 0.01876\n",
      "Training epoch: 1411, train loss: 0.01921, val loss: 0.01982\n",
      "Training epoch: 1412, train loss: 0.01601, val loss: 0.01639\n",
      "Training epoch: 1413, train loss: 0.01647, val loss: 0.01673\n",
      "Training epoch: 1414, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 1415, train loss: 0.01556, val loss: 0.01583\n",
      "Training epoch: 1416, train loss: 0.01817, val loss: 0.01839\n",
      "Training epoch: 1417, train loss: 0.03335, val loss: 0.03300\n",
      "Training epoch: 1418, train loss: 0.02394, val loss: 0.02501\n",
      "Training epoch: 1419, train loss: 0.01943, val loss: 0.02038\n",
      "Training epoch: 1420, train loss: 0.01633, val loss: 0.01689\n",
      "Training epoch: 1421, train loss: 0.01785, val loss: 0.01788\n",
      "Training epoch: 1422, train loss: 0.01809, val loss: 0.01849\n",
      "Training epoch: 1423, train loss: 0.01610, val loss: 0.01627\n",
      "Training epoch: 1424, train loss: 0.01656, val loss: 0.01683\n",
      "Training epoch: 1425, train loss: 0.02365, val loss: 0.02448\n",
      "Training epoch: 1426, train loss: 0.02221, val loss: 0.02380\n",
      "Training epoch: 1427, train loss: 0.02161, val loss: 0.02130\n",
      "Training epoch: 1428, train loss: 0.02403, val loss: 0.02408\n",
      "Training epoch: 1429, train loss: 0.01730, val loss: 0.01757\n",
      "Training epoch: 1430, train loss: 0.01634, val loss: 0.01655\n",
      "Training epoch: 1431, train loss: 0.01893, val loss: 0.01916\n",
      "Training epoch: 1432, train loss: 0.01724, val loss: 0.01748\n",
      "Training epoch: 1433, train loss: 0.01645, val loss: 0.01675\n",
      "Training epoch: 1434, train loss: 0.01619, val loss: 0.01656\n",
      "Training epoch: 1435, train loss: 0.01676, val loss: 0.01731\n",
      "Training epoch: 1436, train loss: 0.01929, val loss: 0.01932\n",
      "Training epoch: 1437, train loss: 0.01689, val loss: 0.01692\n",
      "Training epoch: 1438, train loss: 0.01641, val loss: 0.01669\n",
      "Training epoch: 1439, train loss: 0.02205, val loss: 0.02204\n",
      "Training epoch: 1440, train loss: 0.02202, val loss: 0.02213\n",
      "Training epoch: 1441, train loss: 0.03663, val loss: 0.03711\n",
      "Training epoch: 1442, train loss: 0.02552, val loss: 0.02565\n",
      "Training epoch: 1443, train loss: 0.01961, val loss: 0.02013\n",
      "Training epoch: 1444, train loss: 0.02010, val loss: 0.02025\n",
      "Training epoch: 1445, train loss: 0.02684, val loss: 0.02723\n",
      "Training epoch: 1446, train loss: 0.02663, val loss: 0.02681\n",
      "Training epoch: 1447, train loss: 0.01968, val loss: 0.01983\n",
      "Training epoch: 1448, train loss: 0.01685, val loss: 0.01725\n",
      "Training epoch: 1449, train loss: 0.02380, val loss: 0.02377\n",
      "Training epoch: 1450, train loss: 0.01986, val loss: 0.01994\n",
      "Training epoch: 1451, train loss: 0.02197, val loss: 0.02200\n",
      "Training epoch: 1452, train loss: 0.01665, val loss: 0.01689\n",
      "Training epoch: 1453, train loss: 0.01616, val loss: 0.01643\n",
      "Training epoch: 1454, train loss: 0.01735, val loss: 0.01773\n",
      "Training epoch: 1455, train loss: 0.01731, val loss: 0.01762\n",
      "Training epoch: 1456, train loss: 0.02047, val loss: 0.02018\n",
      "Training epoch: 1457, train loss: 0.01642, val loss: 0.01641\n",
      "Training epoch: 1458, train loss: 0.02176, val loss: 0.02230\n",
      "Training epoch: 1459, train loss: 0.01880, val loss: 0.01945\n",
      "Training epoch: 1460, train loss: 0.01674, val loss: 0.01720\n",
      "Training epoch: 1461, train loss: 0.01682, val loss: 0.01697\n",
      "Training epoch: 1462, train loss: 0.01580, val loss: 0.01610\n",
      "Training epoch: 1463, train loss: 0.01603, val loss: 0.01627\n",
      "Training epoch: 1464, train loss: 0.01613, val loss: 0.01630\n",
      "Training epoch: 1465, train loss: 0.01896, val loss: 0.01918\n",
      "Training epoch: 1466, train loss: 0.01636, val loss: 0.01708\n",
      "Training epoch: 1467, train loss: 0.02079, val loss: 0.02227\n",
      "Training epoch: 1468, train loss: 0.01811, val loss: 0.01863\n",
      "Training epoch: 1469, train loss: 0.01729, val loss: 0.01736\n",
      "Training epoch: 1470, train loss: 0.02510, val loss: 0.02585\n",
      "Training epoch: 1471, train loss: 0.01570, val loss: 0.01595\n",
      "Training epoch: 1472, train loss: 0.02047, val loss: 0.02063\n",
      "Training epoch: 1473, train loss: 0.01659, val loss: 0.01670\n",
      "Training epoch: 1474, train loss: 0.02834, val loss: 0.02938\n",
      "Training epoch: 1475, train loss: 0.01738, val loss: 0.01799\n",
      "Training epoch: 1476, train loss: 0.01751, val loss: 0.01786\n",
      "Training epoch: 1477, train loss: 0.02029, val loss: 0.02048\n",
      "Training epoch: 1478, train loss: 0.01770, val loss: 0.01817\n",
      "Training epoch: 1479, train loss: 0.01695, val loss: 0.01726\n",
      "Training epoch: 1480, train loss: 0.02148, val loss: 0.02204\n",
      "Training epoch: 1481, train loss: 0.02551, val loss: 0.02594\n",
      "Training epoch: 1482, train loss: 0.02290, val loss: 0.02299\n",
      "Training epoch: 1483, train loss: 0.02035, val loss: 0.02050\n",
      "Training epoch: 1484, train loss: 0.01954, val loss: 0.02027\n",
      "Training epoch: 1485, train loss: 0.01839, val loss: 0.01859\n",
      "Training epoch: 1486, train loss: 0.01885, val loss: 0.01898\n",
      "Training epoch: 1487, train loss: 0.01886, val loss: 0.01904\n",
      "Training epoch: 1488, train loss: 0.01640, val loss: 0.01669\n",
      "Training epoch: 1489, train loss: 0.01583, val loss: 0.01604\n",
      "Training epoch: 1490, train loss: 0.01840, val loss: 0.01895\n",
      "Training epoch: 1491, train loss: 0.01672, val loss: 0.01716\n",
      "Training epoch: 1492, train loss: 0.01689, val loss: 0.01739\n",
      "Training epoch: 1493, train loss: 0.01560, val loss: 0.01582\n",
      "Training epoch: 1494, train loss: 0.01707, val loss: 0.01702\n",
      "Training epoch: 1495, train loss: 0.01706, val loss: 0.01730\n",
      "Training epoch: 1496, train loss: 0.01651, val loss: 0.01715\n",
      "Training epoch: 1497, train loss: 0.01662, val loss: 0.01676\n",
      "Training epoch: 1498, train loss: 0.01668, val loss: 0.01677\n",
      "Training epoch: 1499, train loss: 0.01583, val loss: 0.01605\n",
      "Training epoch: 1500, train loss: 0.01608, val loss: 0.01649\n",
      "Training epoch: 1501, train loss: 0.04077, val loss: 0.04127\n",
      "Training epoch: 1502, train loss: 0.02410, val loss: 0.02422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1503, train loss: 0.01700, val loss: 0.01725\n",
      "Training epoch: 1504, train loss: 0.01686, val loss: 0.01716\n",
      "Training epoch: 1505, train loss: 0.01725, val loss: 0.01748\n",
      "Training epoch: 1506, train loss: 0.01741, val loss: 0.01850\n",
      "Training epoch: 1507, train loss: 0.01686, val loss: 0.01729\n",
      "Training epoch: 1508, train loss: 0.01793, val loss: 0.01810\n",
      "Training epoch: 1509, train loss: 0.01607, val loss: 0.01633\n",
      "Training epoch: 1510, train loss: 0.01760, val loss: 0.01798\n",
      "Training epoch: 1511, train loss: 0.02843, val loss: 0.02910\n",
      "Training epoch: 1512, train loss: 0.01697, val loss: 0.01723\n",
      "Training epoch: 1513, train loss: 0.01627, val loss: 0.01637\n",
      "Training epoch: 1514, train loss: 0.01870, val loss: 0.01879\n",
      "Training epoch: 1515, train loss: 0.01618, val loss: 0.01675\n",
      "Training epoch: 1516, train loss: 0.01949, val loss: 0.01981\n",
      "Training epoch: 1517, train loss: 0.01945, val loss: 0.01998\n",
      "Training epoch: 1518, train loss: 0.01863, val loss: 0.01879\n",
      "Training epoch: 1519, train loss: 0.01657, val loss: 0.01696\n",
      "Training epoch: 1520, train loss: 0.02223, val loss: 0.02228\n",
      "Training epoch: 1521, train loss: 0.01633, val loss: 0.01659\n",
      "Training epoch: 1522, train loss: 0.01651, val loss: 0.01660\n",
      "Training epoch: 1523, train loss: 0.02131, val loss: 0.02171\n",
      "Training epoch: 1524, train loss: 0.02338, val loss: 0.02370\n",
      "Training epoch: 1525, train loss: 0.01710, val loss: 0.01706\n",
      "Training epoch: 1526, train loss: 0.01659, val loss: 0.01690\n",
      "Training epoch: 1527, train loss: 0.01590, val loss: 0.01650\n",
      "Training epoch: 1528, train loss: 0.01613, val loss: 0.01624\n",
      "Training epoch: 1529, train loss: 0.01604, val loss: 0.01641\n",
      "Training epoch: 1530, train loss: 0.02069, val loss: 0.02082\n",
      "Training epoch: 1531, train loss: 0.01923, val loss: 0.01924\n",
      "Training epoch: 1532, train loss: 0.02872, val loss: 0.02880\n",
      "Training epoch: 1533, train loss: 0.01716, val loss: 0.01730\n",
      "Training epoch: 1534, train loss: 0.01912, val loss: 0.01918\n",
      "Training epoch: 1535, train loss: 0.01823, val loss: 0.01837\n",
      "Training epoch: 1536, train loss: 0.02376, val loss: 0.02375\n",
      "Training epoch: 1537, train loss: 0.01563, val loss: 0.01584\n",
      "Training epoch: 1538, train loss: 0.02230, val loss: 0.02254\n",
      "Training epoch: 1539, train loss: 0.02209, val loss: 0.02291\n",
      "Training epoch: 1540, train loss: 0.02432, val loss: 0.02621\n",
      "Training epoch: 1541, train loss: 0.01796, val loss: 0.01893\n",
      "Training epoch: 1542, train loss: 0.02099, val loss: 0.02172\n",
      "Training epoch: 1543, train loss: 0.02923, val loss: 0.02970\n",
      "Training epoch: 1544, train loss: 0.01758, val loss: 0.01824\n",
      "Training epoch: 1545, train loss: 0.01783, val loss: 0.01854\n",
      "Training epoch: 1546, train loss: 0.01668, val loss: 0.01728\n",
      "Training epoch: 1547, train loss: 0.01596, val loss: 0.01640\n",
      "Training epoch: 1548, train loss: 0.01708, val loss: 0.01710\n",
      "Training epoch: 1549, train loss: 0.01563, val loss: 0.01580\n",
      "Training epoch: 1550, train loss: 0.01612, val loss: 0.01641\n",
      "Training epoch: 1551, train loss: 0.01918, val loss: 0.01923\n",
      "Training epoch: 1552, train loss: 0.01646, val loss: 0.01678\n",
      "Training epoch: 1553, train loss: 0.01684, val loss: 0.01707\n",
      "Training epoch: 1554, train loss: 0.01749, val loss: 0.01788\n",
      "Training epoch: 1555, train loss: 0.02183, val loss: 0.02186\n",
      "Training epoch: 1556, train loss: 0.01974, val loss: 0.02025\n",
      "Training epoch: 1557, train loss: 0.01610, val loss: 0.01638\n",
      "Training epoch: 1558, train loss: 0.01617, val loss: 0.01670\n",
      "Training epoch: 1559, train loss: 0.01595, val loss: 0.01622\n",
      "Training epoch: 1560, train loss: 0.02190, val loss: 0.02180\n",
      "Training epoch: 1561, train loss: 0.02012, val loss: 0.02011\n",
      "Training epoch: 1562, train loss: 0.01693, val loss: 0.01704\n",
      "Training epoch: 1563, train loss: 0.01824, val loss: 0.01808\n",
      "Training epoch: 1564, train loss: 0.01745, val loss: 0.01821\n",
      "Training epoch: 1565, train loss: 0.01673, val loss: 0.01723\n",
      "Training epoch: 1566, train loss: 0.01700, val loss: 0.01748\n",
      "Training epoch: 1567, train loss: 0.01760, val loss: 0.01778\n",
      "Training epoch: 1568, train loss: 0.01571, val loss: 0.01611\n",
      "Training epoch: 1569, train loss: 0.02015, val loss: 0.02035\n",
      "Training epoch: 1570, train loss: 0.01789, val loss: 0.01791\n",
      "Training epoch: 1571, train loss: 0.02367, val loss: 0.02380\n",
      "Training epoch: 1572, train loss: 0.01791, val loss: 0.01798\n",
      "Training epoch: 1573, train loss: 0.01745, val loss: 0.01787\n",
      "Training epoch: 1574, train loss: 0.01862, val loss: 0.01907\n",
      "Training epoch: 1575, train loss: 0.01764, val loss: 0.01831\n",
      "Training epoch: 1576, train loss: 0.01853, val loss: 0.01921\n",
      "Training epoch: 1577, train loss: 0.01687, val loss: 0.01703\n",
      "Training epoch: 1578, train loss: 0.01755, val loss: 0.01754\n",
      "Training epoch: 1579, train loss: 0.01624, val loss: 0.01657\n",
      "Training epoch: 1580, train loss: 0.01562, val loss: 0.01596\n",
      "Training epoch: 1581, train loss: 0.01806, val loss: 0.01833\n",
      "Training epoch: 1582, train loss: 0.01595, val loss: 0.01654\n",
      "Training epoch: 1583, train loss: 0.01671, val loss: 0.01753\n",
      "Training epoch: 1584, train loss: 0.04722, val loss: 0.04734\n",
      "Training epoch: 1585, train loss: 0.01705, val loss: 0.01774\n",
      "Training epoch: 1586, train loss: 0.01648, val loss: 0.01652\n",
      "Training epoch: 1587, train loss: 0.01707, val loss: 0.01739\n",
      "Training epoch: 1588, train loss: 0.01884, val loss: 0.01882\n",
      "Training epoch: 1589, train loss: 0.01857, val loss: 0.01883\n",
      "Training epoch: 1590, train loss: 0.01604, val loss: 0.01619\n",
      "Training epoch: 1591, train loss: 0.01602, val loss: 0.01620\n",
      "Training epoch: 1592, train loss: 0.01702, val loss: 0.01749\n",
      "Training epoch: 1593, train loss: 0.02860, val loss: 0.02869\n",
      "Training epoch: 1594, train loss: 0.01744, val loss: 0.01794\n",
      "Training epoch: 1595, train loss: 0.01608, val loss: 0.01665\n",
      "Training epoch: 1596, train loss: 0.01770, val loss: 0.01773\n",
      "Training epoch: 1597, train loss: 0.01628, val loss: 0.01667\n",
      "Training epoch: 1598, train loss: 0.01724, val loss: 0.01726\n",
      "Training epoch: 1599, train loss: 0.01746, val loss: 0.01756\n",
      "Training epoch: 1600, train loss: 0.02799, val loss: 0.02769\n",
      "Training epoch: 1601, train loss: 0.01939, val loss: 0.01937\n",
      "Training epoch: 1602, train loss: 0.01896, val loss: 0.01929\n",
      "Training epoch: 1603, train loss: 0.01769, val loss: 0.01822\n",
      "Training epoch: 1604, train loss: 0.02967, val loss: 0.03089\n",
      "Training epoch: 1605, train loss: 0.01645, val loss: 0.01644\n",
      "Training epoch: 1606, train loss: 0.01719, val loss: 0.01754\n",
      "Training epoch: 1607, train loss: 0.02656, val loss: 0.02662\n",
      "Training epoch: 1608, train loss: 0.01746, val loss: 0.01782\n",
      "Training epoch: 1609, train loss: 0.02255, val loss: 0.02405\n",
      "Training epoch: 1610, train loss: 0.02203, val loss: 0.02211\n",
      "Training epoch: 1611, train loss: 0.01670, val loss: 0.01721\n",
      "Training epoch: 1612, train loss: 0.01841, val loss: 0.01869\n",
      "Training epoch: 1613, train loss: 0.01607, val loss: 0.01614\n",
      "Training epoch: 1614, train loss: 0.01733, val loss: 0.01764\n",
      "Training epoch: 1615, train loss: 0.01635, val loss: 0.01697\n",
      "Training epoch: 1616, train loss: 0.02225, val loss: 0.02405\n",
      "Training epoch: 1617, train loss: 0.01786, val loss: 0.01904\n",
      "Training epoch: 1618, train loss: 0.01679, val loss: 0.01768\n",
      "Training epoch: 1619, train loss: 0.02116, val loss: 0.02224\n",
      "Training epoch: 1620, train loss: 0.01725, val loss: 0.01779\n",
      "Training epoch: 1621, train loss: 0.01912, val loss: 0.01922\n",
      "Training epoch: 1622, train loss: 0.01742, val loss: 0.01798\n",
      "Training epoch: 1623, train loss: 0.01640, val loss: 0.01682\n",
      "Training epoch: 1624, train loss: 0.02068, val loss: 0.02084\n",
      "Training epoch: 1625, train loss: 0.01776, val loss: 0.01787\n",
      "Training epoch: 1626, train loss: 0.01844, val loss: 0.01861\n",
      "Training epoch: 1627, train loss: 0.02441, val loss: 0.02495\n",
      "Training epoch: 1628, train loss: 0.01926, val loss: 0.01958\n",
      "Training epoch: 1629, train loss: 0.01592, val loss: 0.01636\n",
      "Training epoch: 1630, train loss: 0.01711, val loss: 0.01741\n",
      "Training epoch: 1631, train loss: 0.01860, val loss: 0.01911\n",
      "Training epoch: 1632, train loss: 0.01748, val loss: 0.01760\n",
      "Training epoch: 1633, train loss: 0.01682, val loss: 0.01763\n",
      "Training epoch: 1634, train loss: 0.02073, val loss: 0.02154\n",
      "Training epoch: 1635, train loss: 0.01752, val loss: 0.01843\n",
      "Training epoch: 1636, train loss: 0.01771, val loss: 0.01860\n",
      "Training epoch: 1637, train loss: 0.02362, val loss: 0.02380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1638, train loss: 0.01708, val loss: 0.01737\n",
      "Training epoch: 1639, train loss: 0.02030, val loss: 0.02054\n",
      "Training epoch: 1640, train loss: 0.01945, val loss: 0.01965\n",
      "Training epoch: 1641, train loss: 0.01703, val loss: 0.01726\n",
      "Training epoch: 1642, train loss: 0.01637, val loss: 0.01668\n",
      "Training epoch: 1643, train loss: 0.01571, val loss: 0.01596\n",
      "Training epoch: 1644, train loss: 0.01662, val loss: 0.01673\n",
      "Training epoch: 1645, train loss: 0.01735, val loss: 0.01735\n",
      "Training epoch: 1646, train loss: 0.01848, val loss: 0.01860\n",
      "Training epoch: 1647, train loss: 0.03226, val loss: 0.03278\n",
      "Training epoch: 1648, train loss: 0.02068, val loss: 0.02074\n",
      "Training epoch: 1649, train loss: 0.02125, val loss: 0.02166\n",
      "Training epoch: 1650, train loss: 0.01748, val loss: 0.01765\n",
      "Training epoch: 1651, train loss: 0.02248, val loss: 0.02266\n",
      "Training epoch: 1652, train loss: 0.01769, val loss: 0.01781\n",
      "Training epoch: 1653, train loss: 0.01710, val loss: 0.01724\n",
      "Training epoch: 1654, train loss: 0.01952, val loss: 0.01986\n",
      "Training epoch: 1655, train loss: 0.03169, val loss: 0.03226\n",
      "Training epoch: 1656, train loss: 0.10389, val loss: 0.10922\n",
      "Training epoch: 1657, train loss: 0.01747, val loss: 0.01823\n",
      "Training epoch: 1658, train loss: 0.02132, val loss: 0.02165\n",
      "Training epoch: 1659, train loss: 0.02200, val loss: 0.02200\n",
      "Training epoch: 1660, train loss: 0.01649, val loss: 0.01672\n",
      "Training epoch: 1661, train loss: 0.01615, val loss: 0.01647\n",
      "Training epoch: 1662, train loss: 0.01599, val loss: 0.01619\n",
      "Training epoch: 1663, train loss: 0.01596, val loss: 0.01619\n",
      "Training epoch: 1664, train loss: 0.01665, val loss: 0.01677\n",
      "Training epoch: 1665, train loss: 0.01563, val loss: 0.01588\n",
      "Training epoch: 1666, train loss: 0.01635, val loss: 0.01648\n",
      "Training epoch: 1667, train loss: 0.14496, val loss: 0.14970\n",
      "Training epoch: 1668, train loss: 0.01747, val loss: 0.01860\n",
      "Training epoch: 1669, train loss: 0.01645, val loss: 0.01687\n",
      "Training epoch: 1670, train loss: 0.01674, val loss: 0.01728\n",
      "Training epoch: 1671, train loss: 0.01963, val loss: 0.02001\n",
      "Training epoch: 1672, train loss: 0.01593, val loss: 0.01622\n",
      "Training epoch: 1673, train loss: 0.01616, val loss: 0.01631\n",
      "Training epoch: 1674, train loss: 0.01615, val loss: 0.01647\n",
      "Training epoch: 1675, train loss: 0.01832, val loss: 0.01839\n",
      "Training epoch: 1676, train loss: 0.01956, val loss: 0.02029\n",
      "Training epoch: 1677, train loss: 0.01809, val loss: 0.01873\n",
      "Training epoch: 1678, train loss: 0.01648, val loss: 0.01688\n",
      "Training epoch: 1679, train loss: 0.01849, val loss: 0.01853\n",
      "Training epoch: 1680, train loss: 0.02171, val loss: 0.02182\n",
      "Training epoch: 1681, train loss: 0.01787, val loss: 0.01848\n",
      "Training epoch: 1682, train loss: 0.02375, val loss: 0.02377\n",
      "Training epoch: 1683, train loss: 0.01852, val loss: 0.01869\n",
      "Training epoch: 1684, train loss: 0.01605, val loss: 0.01635\n",
      "Training epoch: 1685, train loss: 0.01770, val loss: 0.01786\n",
      "Training epoch: 1686, train loss: 0.01585, val loss: 0.01620\n",
      "Training epoch: 1687, train loss: 0.01712, val loss: 0.01744\n",
      "Training epoch: 1688, train loss: 0.02036, val loss: 0.02042\n",
      "Training epoch: 1689, train loss: 0.01915, val loss: 0.01925\n",
      "Training epoch: 1690, train loss: 0.01769, val loss: 0.01859\n",
      "Training epoch: 1691, train loss: 0.01890, val loss: 0.01969\n",
      "Training epoch: 1692, train loss: 0.01618, val loss: 0.01689\n",
      "Training epoch: 1693, train loss: 0.01682, val loss: 0.01734\n",
      "Training epoch: 1694, train loss: 0.01673, val loss: 0.01689\n",
      "Training epoch: 1695, train loss: 0.01623, val loss: 0.01670\n",
      "Training epoch: 1696, train loss: 0.01582, val loss: 0.01601\n",
      "Training epoch: 1697, train loss: 0.01566, val loss: 0.01580\n",
      "Training epoch: 1698, train loss: 0.02296, val loss: 0.02351\n",
      "Training epoch: 1699, train loss: 0.02996, val loss: 0.03073\n",
      "Training epoch: 1700, train loss: 0.01722, val loss: 0.01739\n",
      "Training epoch: 1701, train loss: 0.01755, val loss: 0.01761\n",
      "Training epoch: 1702, train loss: 0.01830, val loss: 0.01847\n",
      "Training epoch: 1703, train loss: 0.01530, val loss: 0.01558\n",
      "Training epoch: 1704, train loss: 0.01592, val loss: 0.01630\n",
      "Training epoch: 1705, train loss: 0.02070, val loss: 0.02075\n",
      "Training epoch: 1706, train loss: 0.02416, val loss: 0.02436\n",
      "Training epoch: 1707, train loss: 0.01810, val loss: 0.01927\n",
      "Training epoch: 1708, train loss: 0.02286, val loss: 0.02314\n",
      "Training epoch: 1709, train loss: 0.01982, val loss: 0.02004\n",
      "Training epoch: 1710, train loss: 0.01685, val loss: 0.01717\n",
      "Training epoch: 1711, train loss: 0.01843, val loss: 0.01860\n",
      "Training epoch: 1712, train loss: 0.01609, val loss: 0.01627\n",
      "Training epoch: 1713, train loss: 0.02445, val loss: 0.02494\n",
      "Training epoch: 1714, train loss: 0.01804, val loss: 0.01813\n",
      "Training epoch: 1715, train loss: 0.01603, val loss: 0.01643\n",
      "Training epoch: 1716, train loss: 0.02178, val loss: 0.02200\n",
      "Training epoch: 1717, train loss: 0.01680, val loss: 0.01745\n",
      "Training epoch: 1718, train loss: 0.01704, val loss: 0.01804\n",
      "Training epoch: 1719, train loss: 0.01756, val loss: 0.01795\n",
      "Training epoch: 1720, train loss: 0.01811, val loss: 0.01857\n",
      "Training epoch: 1721, train loss: 0.01772, val loss: 0.01823\n",
      "Training epoch: 1722, train loss: 0.01676, val loss: 0.01724\n",
      "Training epoch: 1723, train loss: 0.01568, val loss: 0.01583\n",
      "Training epoch: 1724, train loss: 0.01660, val loss: 0.01691\n",
      "Training epoch: 1725, train loss: 0.02000, val loss: 0.02033\n",
      "Training epoch: 1726, train loss: 0.02398, val loss: 0.02523\n",
      "Training epoch: 1727, train loss: 0.02269, val loss: 0.02352\n",
      "Training epoch: 1728, train loss: 0.01686, val loss: 0.01710\n",
      "Training epoch: 1729, train loss: 0.01749, val loss: 0.01771\n",
      "Training epoch: 1730, train loss: 0.01600, val loss: 0.01605\n",
      "Training epoch: 1731, train loss: 0.02351, val loss: 0.02356\n",
      "Training epoch: 1732, train loss: 0.01718, val loss: 0.01735\n",
      "Training epoch: 1733, train loss: 0.02016, val loss: 0.02048\n",
      "Training epoch: 1734, train loss: 0.01807, val loss: 0.01825\n",
      "Training epoch: 1735, train loss: 0.02220, val loss: 0.02247\n",
      "Training epoch: 1736, train loss: 0.01906, val loss: 0.01993\n",
      "Training epoch: 1737, train loss: 0.01683, val loss: 0.01692\n",
      "Training epoch: 1738, train loss: 0.02846, val loss: 0.02886\n",
      "Training epoch: 1739, train loss: 0.02410, val loss: 0.02465\n",
      "Training epoch: 1740, train loss: 0.01894, val loss: 0.01940\n",
      "Training epoch: 1741, train loss: 0.01876, val loss: 0.01908\n",
      "Training epoch: 1742, train loss: 0.01768, val loss: 0.01757\n",
      "Training epoch: 1743, train loss: 0.01853, val loss: 0.01878\n",
      "Training epoch: 1744, train loss: 0.01920, val loss: 0.01936\n",
      "Training epoch: 1745, train loss: 0.01603, val loss: 0.01634\n",
      "Training epoch: 1746, train loss: 0.01722, val loss: 0.01727\n",
      "Training epoch: 1747, train loss: 0.02477, val loss: 0.02550\n",
      "Training epoch: 1748, train loss: 0.01670, val loss: 0.01668\n",
      "Training epoch: 1749, train loss: 0.01871, val loss: 0.01857\n",
      "Training epoch: 1750, train loss: 0.01821, val loss: 0.01839\n",
      "Training epoch: 1751, train loss: 0.01660, val loss: 0.01709\n",
      "Training epoch: 1752, train loss: 0.01832, val loss: 0.01848\n",
      "Training epoch: 1753, train loss: 0.01822, val loss: 0.01859\n",
      "Training epoch: 1754, train loss: 0.02230, val loss: 0.02251\n",
      "Training epoch: 1755, train loss: 0.01678, val loss: 0.01701\n",
      "Training epoch: 1756, train loss: 0.01600, val loss: 0.01630\n",
      "Training epoch: 1757, train loss: 0.01828, val loss: 0.01840\n",
      "Training epoch: 1758, train loss: 0.01838, val loss: 0.01956\n",
      "Training epoch: 1759, train loss: 0.01699, val loss: 0.01811\n",
      "Training epoch: 1760, train loss: 0.01839, val loss: 0.01928\n",
      "Training epoch: 1761, train loss: 0.01788, val loss: 0.01895\n",
      "Training epoch: 1762, train loss: 0.01777, val loss: 0.01862\n",
      "Training epoch: 1763, train loss: 0.01816, val loss: 0.01895\n",
      "Training epoch: 1764, train loss: 0.01851, val loss: 0.01926\n",
      "Training epoch: 1765, train loss: 0.01651, val loss: 0.01729\n",
      "Training epoch: 1766, train loss: 0.01642, val loss: 0.01729\n",
      "Training epoch: 1767, train loss: 0.01692, val loss: 0.01776\n",
      "Training epoch: 1768, train loss: 0.01851, val loss: 0.01920\n",
      "Training epoch: 1769, train loss: 0.03614, val loss: 0.03752\n",
      "Training epoch: 1770, train loss: 0.01687, val loss: 0.01739\n",
      "Training epoch: 1771, train loss: 0.01726, val loss: 0.01753\n",
      "Training epoch: 1772, train loss: 0.01754, val loss: 0.01767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1773, train loss: 0.01816, val loss: 0.01842\n",
      "Training epoch: 1774, train loss: 0.01665, val loss: 0.01694\n",
      "Training epoch: 1775, train loss: 0.01615, val loss: 0.01633\n",
      "Training epoch: 1776, train loss: 0.01620, val loss: 0.01643\n",
      "Training epoch: 1777, train loss: 0.01588, val loss: 0.01622\n",
      "Training epoch: 1778, train loss: 0.02513, val loss: 0.02573\n",
      "Training epoch: 1779, train loss: 0.02541, val loss: 0.02576\n",
      "Training epoch: 1780, train loss: 0.01767, val loss: 0.01796\n",
      "Training epoch: 1781, train loss: 0.01633, val loss: 0.01651\n",
      "Training epoch: 1782, train loss: 0.01748, val loss: 0.01770\n",
      "Training epoch: 1783, train loss: 0.02037, val loss: 0.02030\n",
      "Training epoch: 1784, train loss: 0.02100, val loss: 0.02120\n",
      "Training epoch: 1785, train loss: 0.01904, val loss: 0.01900\n",
      "Training epoch: 1786, train loss: 0.01851, val loss: 0.01913\n",
      "Training epoch: 1787, train loss: 0.02242, val loss: 0.02379\n",
      "Training epoch: 1788, train loss: 0.02996, val loss: 0.03055\n",
      "Training epoch: 1789, train loss: 0.01641, val loss: 0.01655\n",
      "Training epoch: 1790, train loss: 0.01703, val loss: 0.01736\n",
      "Training epoch: 1791, train loss: 0.01810, val loss: 0.01850\n",
      "Training epoch: 1792, train loss: 0.02018, val loss: 0.02051\n",
      "Training epoch: 1793, train loss: 0.01767, val loss: 0.01787\n",
      "Training epoch: 1794, train loss: 0.01585, val loss: 0.01597\n",
      "Training epoch: 1795, train loss: 0.02219, val loss: 0.02324\n",
      "Training epoch: 1796, train loss: 0.02153, val loss: 0.02132\n",
      "Training epoch: 1797, train loss: 0.01981, val loss: 0.02125\n",
      "Training epoch: 1798, train loss: 0.02688, val loss: 0.02767\n",
      "Training epoch: 1799, train loss: 0.01829, val loss: 0.01940\n",
      "Training epoch: 1800, train loss: 0.01724, val loss: 0.01791\n",
      "Training epoch: 1801, train loss: 0.01742, val loss: 0.01803\n",
      "Training epoch: 1802, train loss: 0.01761, val loss: 0.01804\n",
      "Training epoch: 1803, train loss: 0.01640, val loss: 0.01671\n",
      "Training epoch: 1804, train loss: 0.01587, val loss: 0.01598\n",
      "Training epoch: 1805, train loss: 0.01634, val loss: 0.01657\n",
      "Training epoch: 1806, train loss: 0.01884, val loss: 0.01935\n",
      "Training epoch: 1807, train loss: 0.01655, val loss: 0.01680\n",
      "Training epoch: 1808, train loss: 0.02248, val loss: 0.02309\n",
      "Training epoch: 1809, train loss: 0.02028, val loss: 0.02031\n",
      "Training epoch: 1810, train loss: 0.02018, val loss: 0.02038\n",
      "Training epoch: 1811, train loss: 0.01744, val loss: 0.01753\n",
      "Training epoch: 1812, train loss: 0.01747, val loss: 0.01784\n",
      "Training epoch: 1813, train loss: 0.01641, val loss: 0.01684\n",
      "Training epoch: 1814, train loss: 0.02025, val loss: 0.02099\n",
      "Training epoch: 1815, train loss: 0.01668, val loss: 0.01693\n",
      "Training epoch: 1816, train loss: 0.02039, val loss: 0.02076\n",
      "Training epoch: 1817, train loss: 0.02927, val loss: 0.03098\n",
      "Training epoch: 1818, train loss: 0.01735, val loss: 0.01765\n",
      "Training epoch: 1819, train loss: 0.01668, val loss: 0.01719\n",
      "Training epoch: 1820, train loss: 0.04264, val loss: 0.04320\n",
      "Training epoch: 1821, train loss: 0.03056, val loss: 0.03124\n",
      "Training epoch: 1822, train loss: 0.01817, val loss: 0.01829\n",
      "Training epoch: 1823, train loss: 0.01718, val loss: 0.01736\n",
      "Training epoch: 1824, train loss: 0.02234, val loss: 0.02232\n",
      "Training epoch: 1825, train loss: 0.02378, val loss: 0.02384\n",
      "Training epoch: 1826, train loss: 0.02040, val loss: 0.02046\n",
      "Training epoch: 1827, train loss: 0.01667, val loss: 0.01728\n",
      "Training epoch: 1828, train loss: 0.01931, val loss: 0.02033\n",
      "Training epoch: 1829, train loss: 0.09210, val loss: 0.09373\n",
      "Training epoch: 1830, train loss: 0.01734, val loss: 0.01805\n",
      "Training epoch: 1831, train loss: 0.01774, val loss: 0.01823\n",
      "Training epoch: 1832, train loss: 0.01646, val loss: 0.01645\n",
      "Training epoch: 1833, train loss: 0.01595, val loss: 0.01633\n",
      "Training epoch: 1834, train loss: 0.01645, val loss: 0.01655\n",
      "Training epoch: 1835, train loss: 0.01589, val loss: 0.01610\n",
      "Training epoch: 1836, train loss: 0.01641, val loss: 0.01645\n",
      "Training epoch: 1837, train loss: 0.01551, val loss: 0.01589\n",
      "Training epoch: 1838, train loss: 0.01945, val loss: 0.02010\n",
      "Training epoch: 1839, train loss: 0.01622, val loss: 0.01644\n",
      "Training epoch: 1840, train loss: 0.02450, val loss: 0.02435\n",
      "Training epoch: 1841, train loss: 0.01836, val loss: 0.01911\n",
      "Training epoch: 1842, train loss: 0.01691, val loss: 0.01698\n",
      "Training epoch: 1843, train loss: 0.05318, val loss: 0.05197\n",
      "Training epoch: 1844, train loss: 0.01572, val loss: 0.01596\n",
      "Training epoch: 1845, train loss: 0.01619, val loss: 0.01642\n",
      "Training epoch: 1846, train loss: 0.02058, val loss: 0.02049\n",
      "Training epoch: 1847, train loss: 0.01855, val loss: 0.01873\n",
      "Training epoch: 1848, train loss: 0.01724, val loss: 0.01790\n",
      "Training epoch: 1849, train loss: 0.02681, val loss: 0.02705\n",
      "Training epoch: 1850, train loss: 0.01631, val loss: 0.01661\n",
      "Training epoch: 1851, train loss: 0.01707, val loss: 0.01730\n",
      "Training epoch: 1852, train loss: 0.01791, val loss: 0.01824\n",
      "Training epoch: 1853, train loss: 0.01945, val loss: 0.01958\n",
      "Training epoch: 1854, train loss: 0.01685, val loss: 0.01713\n",
      "Training epoch: 1855, train loss: 0.03234, val loss: 0.03319\n",
      "Training epoch: 1856, train loss: 0.02300, val loss: 0.02267\n",
      "Training epoch: 1857, train loss: 0.01895, val loss: 0.01972\n",
      "Training epoch: 1858, train loss: 0.02234, val loss: 0.02265\n",
      "Training epoch: 1859, train loss: 0.01857, val loss: 0.01907\n",
      "Training epoch: 1860, train loss: 0.01776, val loss: 0.01798\n",
      "Training epoch: 1861, train loss: 0.01618, val loss: 0.01648\n",
      "Training epoch: 1862, train loss: 0.01682, val loss: 0.01688\n",
      "Training epoch: 1863, train loss: 0.01722, val loss: 0.01743\n",
      "Training epoch: 1864, train loss: 0.02366, val loss: 0.02370\n",
      "Training epoch: 1865, train loss: 0.01884, val loss: 0.01923\n",
      "Training epoch: 1866, train loss: 0.03376, val loss: 0.03357\n",
      "Training epoch: 1867, train loss: 0.02041, val loss: 0.02058\n",
      "Training epoch: 1868, train loss: 0.01843, val loss: 0.01881\n",
      "Training epoch: 1869, train loss: 0.02832, val loss: 0.02834\n",
      "Training epoch: 1870, train loss: 0.02138, val loss: 0.02157\n",
      "Training epoch: 1871, train loss: 0.01835, val loss: 0.01866\n",
      "Training epoch: 1872, train loss: 0.01663, val loss: 0.01689\n",
      "Training epoch: 1873, train loss: 0.01611, val loss: 0.01644\n",
      "Training epoch: 1874, train loss: 0.01946, val loss: 0.01958\n",
      "Training epoch: 1875, train loss: 0.02172, val loss: 0.02181\n",
      "Training epoch: 1876, train loss: 0.02428, val loss: 0.02526\n",
      "Training epoch: 1877, train loss: 0.01824, val loss: 0.01901\n",
      "Training epoch: 1878, train loss: 0.01689, val loss: 0.01757\n",
      "Training epoch: 1879, train loss: 0.02704, val loss: 0.02721\n",
      "Training epoch: 1880, train loss: 0.01826, val loss: 0.01845\n",
      "Training epoch: 1881, train loss: 0.01983, val loss: 0.02004\n",
      "Training epoch: 1882, train loss: 0.01809, val loss: 0.01839\n",
      "Training epoch: 1883, train loss: 0.01871, val loss: 0.01901\n",
      "Training epoch: 1884, train loss: 0.01606, val loss: 0.01618\n",
      "Training epoch: 1885, train loss: 0.01803, val loss: 0.01818\n",
      "Training epoch: 1886, train loss: 0.02299, val loss: 0.02318\n",
      "Training epoch: 1887, train loss: 0.01663, val loss: 0.01686\n",
      "Training epoch: 1888, train loss: 0.01644, val loss: 0.01687\n",
      "Training epoch: 1889, train loss: 0.01641, val loss: 0.01709\n",
      "Training epoch: 1890, train loss: 0.01730, val loss: 0.01797\n",
      "Training epoch: 1891, train loss: 0.03085, val loss: 0.03066\n",
      "Training epoch: 1892, train loss: 0.02363, val loss: 0.02388\n",
      "Training epoch: 1893, train loss: 0.02069, val loss: 0.02076\n",
      "Training epoch: 1894, train loss: 0.01881, val loss: 0.01927\n",
      "Training epoch: 1895, train loss: 0.01637, val loss: 0.01652\n",
      "Training epoch: 1896, train loss: 0.02048, val loss: 0.02074\n",
      "Training epoch: 1897, train loss: 0.01654, val loss: 0.01697\n",
      "Training epoch: 1898, train loss: 0.02324, val loss: 0.02318\n",
      "Training epoch: 1899, train loss: 0.01933, val loss: 0.01985\n",
      "Training epoch: 1900, train loss: 0.01718, val loss: 0.01724\n",
      "Training epoch: 1901, train loss: 0.01908, val loss: 0.01908\n",
      "Training epoch: 1902, train loss: 0.01789, val loss: 0.01780\n",
      "Training epoch: 1903, train loss: 0.04158, val loss: 0.04159\n",
      "Training epoch: 1904, train loss: 0.01779, val loss: 0.01808\n",
      "Training epoch: 1905, train loss: 0.02373, val loss: 0.02409\n",
      "Training epoch: 1906, train loss: 0.01697, val loss: 0.01711\n",
      "Training epoch: 1907, train loss: 0.01714, val loss: 0.01721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1908, train loss: 0.02399, val loss: 0.02419\n",
      "Training epoch: 1909, train loss: 0.01632, val loss: 0.01643\n",
      "Training epoch: 1910, train loss: 0.01839, val loss: 0.01892\n",
      "Training epoch: 1911, train loss: 0.02130, val loss: 0.02243\n",
      "Training epoch: 1912, train loss: 0.03089, val loss: 0.03065\n",
      "Training epoch: 1913, train loss: 0.01572, val loss: 0.01614\n",
      "Training epoch: 1914, train loss: 0.02398, val loss: 0.02390\n",
      "Training epoch: 1915, train loss: 0.01717, val loss: 0.01722\n",
      "Training epoch: 1916, train loss: 0.04225, val loss: 0.04167\n",
      "Training epoch: 1917, train loss: 0.02429, val loss: 0.02453\n",
      "Training epoch: 1918, train loss: 0.01715, val loss: 0.01740\n",
      "Training epoch: 1919, train loss: 0.02095, val loss: 0.02173\n",
      "Training epoch: 1920, train loss: 0.01628, val loss: 0.01676\n",
      "Training epoch: 1921, train loss: 0.01685, val loss: 0.01691\n",
      "Training epoch: 1922, train loss: 0.01874, val loss: 0.01899\n",
      "Training epoch: 1923, train loss: 0.01896, val loss: 0.01952\n",
      "Training epoch: 1924, train loss: 0.01771, val loss: 0.01784\n",
      "Training epoch: 1925, train loss: 0.01597, val loss: 0.01625\n",
      "Training epoch: 1926, train loss: 0.01936, val loss: 0.01952\n",
      "Training epoch: 1927, train loss: 0.01786, val loss: 0.01780\n",
      "Training epoch: 1928, train loss: 0.01804, val loss: 0.01793\n",
      "Training epoch: 1929, train loss: 0.01952, val loss: 0.01959\n",
      "Training epoch: 1930, train loss: 0.01980, val loss: 0.01996\n",
      "Training epoch: 1931, train loss: 0.01743, val loss: 0.01767\n",
      "Training epoch: 1932, train loss: 0.01903, val loss: 0.01941\n",
      "Training epoch: 1933, train loss: 0.01837, val loss: 0.01896\n",
      "Training epoch: 1934, train loss: 0.01810, val loss: 0.01875\n",
      "Training epoch: 1935, train loss: 0.01941, val loss: 0.01970\n",
      "Training epoch: 1936, train loss: 0.01793, val loss: 0.01849\n",
      "Training epoch: 1937, train loss: 0.01599, val loss: 0.01640\n",
      "Training epoch: 1938, train loss: 0.01889, val loss: 0.01895\n",
      "Training epoch: 1939, train loss: 0.02590, val loss: 0.02562\n",
      "Training epoch: 1940, train loss: 0.01853, val loss: 0.01843\n",
      "Training epoch: 1941, train loss: 0.01808, val loss: 0.01833\n",
      "Training epoch: 1942, train loss: 0.02067, val loss: 0.02183\n",
      "Training epoch: 1943, train loss: 0.01724, val loss: 0.01744\n",
      "Training epoch: 1944, train loss: 0.01867, val loss: 0.01921\n",
      "Training epoch: 1945, train loss: 0.02562, val loss: 0.02645\n",
      "Training epoch: 1946, train loss: 0.01717, val loss: 0.01744\n",
      "Training epoch: 1947, train loss: 0.01776, val loss: 0.01805\n",
      "Training epoch: 1948, train loss: 0.02743, val loss: 0.02736\n",
      "Training epoch: 1949, train loss: 0.01987, val loss: 0.02014\n",
      "Training epoch: 1950, train loss: 0.01604, val loss: 0.01632\n",
      "Training epoch: 1951, train loss: 0.01582, val loss: 0.01621\n",
      "Training epoch: 1952, train loss: 0.01990, val loss: 0.01993\n",
      "Training epoch: 1953, train loss: 0.02049, val loss: 0.02135\n",
      "Training epoch: 1954, train loss: 0.02333, val loss: 0.02486\n",
      "Training epoch: 1955, train loss: 0.01627, val loss: 0.01683\n",
      "Training epoch: 1956, train loss: 0.01706, val loss: 0.01715\n",
      "Training epoch: 1957, train loss: 0.02950, val loss: 0.03047\n",
      "Training epoch: 1958, train loss: 0.01863, val loss: 0.01876\n",
      "Training epoch: 1959, train loss: 0.01699, val loss: 0.01722\n",
      "Training epoch: 1960, train loss: 0.01767, val loss: 0.01767\n",
      "Training epoch: 1961, train loss: 0.01647, val loss: 0.01661\n",
      "Training epoch: 1962, train loss: 0.01718, val loss: 0.01755\n",
      "Training epoch: 1963, train loss: 0.01570, val loss: 0.01608\n",
      "Training epoch: 1964, train loss: 0.01644, val loss: 0.01672\n",
      "Training epoch: 1965, train loss: 0.01694, val loss: 0.01734\n",
      "Training epoch: 1966, train loss: 0.04219, val loss: 0.04289\n",
      "Training epoch: 1967, train loss: 0.01881, val loss: 0.01917\n",
      "Training epoch: 1968, train loss: 0.01632, val loss: 0.01637\n",
      "Training epoch: 1969, train loss: 0.02315, val loss: 0.02325\n",
      "Training epoch: 1970, train loss: 0.02357, val loss: 0.02372\n",
      "Training epoch: 1971, train loss: 0.01639, val loss: 0.01705\n",
      "Training epoch: 1972, train loss: 0.02151, val loss: 0.02234\n",
      "Training epoch: 1973, train loss: 0.01895, val loss: 0.01920\n",
      "Training epoch: 1974, train loss: 0.01873, val loss: 0.01959\n",
      "Training epoch: 1975, train loss: 0.01747, val loss: 0.01755\n",
      "Training epoch: 1976, train loss: 0.01652, val loss: 0.01699\n",
      "Training epoch: 1977, train loss: 0.01592, val loss: 0.01593\n",
      "Training epoch: 1978, train loss: 0.01738, val loss: 0.01772\n",
      "Training epoch: 1979, train loss: 0.01608, val loss: 0.01605\n",
      "Training epoch: 1980, train loss: 0.01698, val loss: 0.01738\n",
      "Training epoch: 1981, train loss: 0.01745, val loss: 0.01768\n",
      "Training epoch: 1982, train loss: 0.03077, val loss: 0.03139\n",
      "Training epoch: 1983, train loss: 0.01724, val loss: 0.01751\n",
      "Training epoch: 1984, train loss: 0.01662, val loss: 0.01668\n",
      "Training epoch: 1985, train loss: 0.02026, val loss: 0.02030\n",
      "Training epoch: 1986, train loss: 0.02163, val loss: 0.02167\n",
      "Training epoch: 1987, train loss: 0.01995, val loss: 0.02013\n",
      "Training epoch: 1988, train loss: 0.01621, val loss: 0.01628\n",
      "Training epoch: 1989, train loss: 0.01619, val loss: 0.01675\n",
      "Training epoch: 1990, train loss: 0.01597, val loss: 0.01616\n",
      "Training epoch: 1991, train loss: 0.01794, val loss: 0.01843\n",
      "Training epoch: 1992, train loss: 0.01609, val loss: 0.01661\n",
      "Training epoch: 1993, train loss: 0.01581, val loss: 0.01594\n",
      "Training epoch: 1994, train loss: 0.01716, val loss: 0.01745\n",
      "Training epoch: 1995, train loss: 0.01710, val loss: 0.01747\n",
      "Training epoch: 1996, train loss: 0.03359, val loss: 0.03475\n",
      "Training epoch: 1997, train loss: 0.02253, val loss: 0.02348\n",
      "Training epoch: 1998, train loss: 0.03349, val loss: 0.03476\n",
      "Training epoch: 1999, train loss: 0.02856, val loss: 0.02881\n",
      "Training epoch: 2000, train loss: 0.04291, val loss: 0.04352\n",
      "Training epoch: 2001, train loss: 0.02006, val loss: 0.02094\n",
      "Training epoch: 2002, train loss: 0.01640, val loss: 0.01690\n",
      "Training epoch: 2003, train loss: 0.01781, val loss: 0.01775\n",
      "Training epoch: 2004, train loss: 0.02138, val loss: 0.02149\n",
      "Training epoch: 2005, train loss: 0.02586, val loss: 0.02585\n",
      "Training epoch: 2006, train loss: 0.01592, val loss: 0.01621\n",
      "Training epoch: 2007, train loss: 0.01673, val loss: 0.01691\n",
      "Training epoch: 2008, train loss: 0.01668, val loss: 0.01692\n",
      "Training epoch: 2009, train loss: 0.01711, val loss: 0.01735\n",
      "Training epoch: 2010, train loss: 0.01608, val loss: 0.01636\n",
      "Training epoch: 2011, train loss: 0.01704, val loss: 0.01765\n",
      "Training epoch: 2012, train loss: 0.01791, val loss: 0.01833\n",
      "Training epoch: 2013, train loss: 0.01672, val loss: 0.01695\n",
      "Training epoch: 2014, train loss: 0.01591, val loss: 0.01634\n",
      "Training epoch: 2015, train loss: 0.01541, val loss: 0.01568\n",
      "Training epoch: 2016, train loss: 0.01555, val loss: 0.01562\n",
      "Training epoch: 2017, train loss: 0.02114, val loss: 0.02159\n",
      "Training epoch: 2018, train loss: 0.01756, val loss: 0.01763\n",
      "Training epoch: 2019, train loss: 0.01618, val loss: 0.01634\n",
      "Training epoch: 2020, train loss: 0.01688, val loss: 0.01691\n",
      "Training epoch: 2021, train loss: 0.01966, val loss: 0.01999\n",
      "Training epoch: 2022, train loss: 0.02170, val loss: 0.02258\n",
      "Training epoch: 2023, train loss: 0.01962, val loss: 0.02089\n",
      "Training epoch: 2024, train loss: 0.02177, val loss: 0.02321\n",
      "Training epoch: 2025, train loss: 0.01803, val loss: 0.01906\n",
      "Training epoch: 2026, train loss: 0.01785, val loss: 0.01888\n",
      "Training epoch: 2027, train loss: 0.01945, val loss: 0.02030\n",
      "Training epoch: 2028, train loss: 0.02008, val loss: 0.02083\n",
      "Training epoch: 2029, train loss: 0.01686, val loss: 0.01766\n",
      "Training epoch: 2030, train loss: 0.01687, val loss: 0.01760\n",
      "Training epoch: 2031, train loss: 0.01622, val loss: 0.01695\n",
      "Training epoch: 2032, train loss: 0.01655, val loss: 0.01716\n",
      "Training epoch: 2033, train loss: 0.01613, val loss: 0.01660\n",
      "Training epoch: 2034, train loss: 0.01602, val loss: 0.01597\n",
      "Training epoch: 2035, train loss: 0.01647, val loss: 0.01678\n",
      "Training epoch: 2036, train loss: 0.01906, val loss: 0.01919\n",
      "Training epoch: 2037, train loss: 0.02078, val loss: 0.02132\n",
      "Training epoch: 2038, train loss: 0.01914, val loss: 0.01958\n",
      "Training epoch: 2039, train loss: 0.02047, val loss: 0.02068\n",
      "Training epoch: 2040, train loss: 0.01919, val loss: 0.01949\n",
      "Training epoch: 2041, train loss: 0.01855, val loss: 0.01860\n",
      "Training epoch: 2042, train loss: 0.01626, val loss: 0.01660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2043, train loss: 0.01632, val loss: 0.01641\n",
      "Training epoch: 2044, train loss: 0.01702, val loss: 0.01716\n",
      "Training epoch: 2045, train loss: 0.02457, val loss: 0.02485\n",
      "Training epoch: 2046, train loss: 0.01662, val loss: 0.01688\n",
      "Training epoch: 2047, train loss: 0.01790, val loss: 0.01834\n",
      "Training epoch: 2048, train loss: 0.01836, val loss: 0.01847\n",
      "Training epoch: 2049, train loss: 0.01620, val loss: 0.01672\n",
      "Training epoch: 2050, train loss: 0.02074, val loss: 0.02176\n",
      "Training epoch: 2051, train loss: 0.02940, val loss: 0.03101\n",
      "Training epoch: 2052, train loss: 0.01640, val loss: 0.01651\n",
      "Training epoch: 2053, train loss: 0.01615, val loss: 0.01659\n",
      "Training epoch: 2054, train loss: 0.01781, val loss: 0.01813\n",
      "Training epoch: 2055, train loss: 0.01664, val loss: 0.01683\n",
      "Training epoch: 2056, train loss: 0.01554, val loss: 0.01597\n",
      "Training epoch: 2057, train loss: 0.01605, val loss: 0.01604\n",
      "Training epoch: 2058, train loss: 0.02088, val loss: 0.02104\n",
      "Training epoch: 2059, train loss: 0.01968, val loss: 0.02055\n",
      "Training epoch: 2060, train loss: 0.01947, val loss: 0.01926\n",
      "Training epoch: 2061, train loss: 0.01709, val loss: 0.01756\n",
      "Training epoch: 2062, train loss: 0.01714, val loss: 0.01742\n",
      "Training epoch: 2063, train loss: 0.01605, val loss: 0.01636\n",
      "Training epoch: 2064, train loss: 0.02008, val loss: 0.02011\n",
      "Training epoch: 2065, train loss: 0.01623, val loss: 0.01666\n",
      "Training epoch: 2066, train loss: 0.01579, val loss: 0.01623\n",
      "Training epoch: 2067, train loss: 0.02164, val loss: 0.02204\n",
      "Training epoch: 2068, train loss: 0.01671, val loss: 0.01674\n",
      "Training epoch: 2069, train loss: 0.01784, val loss: 0.01783\n",
      "Training epoch: 2070, train loss: 0.02116, val loss: 0.02133\n",
      "Training epoch: 2071, train loss: 0.01637, val loss: 0.01669\n",
      "Training epoch: 2072, train loss: 0.01967, val loss: 0.02017\n",
      "Training epoch: 2073, train loss: 0.02868, val loss: 0.03069\n",
      "Training epoch: 2074, train loss: 0.03161, val loss: 0.03171\n",
      "Training epoch: 2075, train loss: 0.01653, val loss: 0.01729\n",
      "Training epoch: 2076, train loss: 0.01731, val loss: 0.01723\n",
      "Training epoch: 2077, train loss: 0.02090, val loss: 0.02115\n",
      "Training epoch: 2078, train loss: 0.01841, val loss: 0.01888\n",
      "Training epoch: 2079, train loss: 0.01589, val loss: 0.01613\n",
      "Training epoch: 2080, train loss: 0.01568, val loss: 0.01576\n",
      "Training epoch: 2081, train loss: 0.01661, val loss: 0.01673\n",
      "Training epoch: 2082, train loss: 0.01683, val loss: 0.01689\n",
      "Training epoch: 2083, train loss: 0.01723, val loss: 0.01720\n",
      "Training epoch: 2084, train loss: 0.02521, val loss: 0.02502\n",
      "Training epoch: 2085, train loss: 0.02008, val loss: 0.02054\n",
      "Training epoch: 2086, train loss: 0.01857, val loss: 0.01926\n",
      "Training epoch: 2087, train loss: 0.01683, val loss: 0.01709\n",
      "Training epoch: 2088, train loss: 0.02387, val loss: 0.02486\n",
      "Training epoch: 2089, train loss: 0.02367, val loss: 0.02484\n",
      "Training epoch: 2090, train loss: 0.02329, val loss: 0.02355\n",
      "Training epoch: 2091, train loss: 0.02010, val loss: 0.02022\n",
      "Training epoch: 2092, train loss: 0.01684, val loss: 0.01728\n",
      "Training epoch: 2093, train loss: 0.01916, val loss: 0.01929\n",
      "Training epoch: 2094, train loss: 0.01728, val loss: 0.01739\n",
      "Training epoch: 2095, train loss: 0.01570, val loss: 0.01610\n",
      "Training epoch: 2096, train loss: 0.01590, val loss: 0.01637\n",
      "Training epoch: 2097, train loss: 0.01753, val loss: 0.01759\n",
      "Training epoch: 2098, train loss: 0.01579, val loss: 0.01610\n",
      "Training epoch: 2099, train loss: 0.02431, val loss: 0.02426\n",
      "Training epoch: 2100, train loss: 0.02896, val loss: 0.03084\n",
      "Training epoch: 2101, train loss: 0.01849, val loss: 0.01901\n",
      "Training epoch: 2102, train loss: 0.01978, val loss: 0.02054\n",
      "Training epoch: 2103, train loss: 0.03833, val loss: 0.03854\n",
      "Training epoch: 2104, train loss: 0.02117, val loss: 0.02165\n",
      "Training epoch: 2105, train loss: 0.01796, val loss: 0.01823\n",
      "Training epoch: 2106, train loss: 0.01617, val loss: 0.01651\n",
      "Training epoch: 2107, train loss: 0.01734, val loss: 0.01753\n",
      "Training epoch: 2108, train loss: 0.01776, val loss: 0.01780\n",
      "Training epoch: 2109, train loss: 0.02093, val loss: 0.02175\n",
      "Training epoch: 2110, train loss: 0.02322, val loss: 0.02492\n",
      "Training epoch: 2111, train loss: 0.02109, val loss: 0.02182\n",
      "Training epoch: 2112, train loss: 0.01698, val loss: 0.01710\n",
      "Training epoch: 2113, train loss: 0.01698, val loss: 0.01716\n",
      "Training epoch: 2114, train loss: 0.01728, val loss: 0.01736\n",
      "Training epoch: 2115, train loss: 0.01609, val loss: 0.01634\n",
      "Training epoch: 2116, train loss: 0.01588, val loss: 0.01601\n",
      "Training epoch: 2117, train loss: 0.01636, val loss: 0.01663\n",
      "Training epoch: 2118, train loss: 0.01587, val loss: 0.01642\n",
      "Training epoch: 2119, train loss: 0.01646, val loss: 0.01704\n",
      "Training epoch: 2120, train loss: 0.01639, val loss: 0.01649\n",
      "Training epoch: 2121, train loss: 0.02398, val loss: 0.02439\n",
      "Training epoch: 2122, train loss: 0.01726, val loss: 0.01809\n",
      "Training epoch: 2123, train loss: 0.01682, val loss: 0.01727\n",
      "Training epoch: 2124, train loss: 0.01743, val loss: 0.01761\n",
      "Training epoch: 2125, train loss: 0.01764, val loss: 0.01796\n",
      "Training epoch: 2126, train loss: 0.01627, val loss: 0.01637\n",
      "Training epoch: 2127, train loss: 0.01666, val loss: 0.01678\n",
      "Training epoch: 2128, train loss: 0.01648, val loss: 0.01660\n",
      "Training epoch: 2129, train loss: 0.01600, val loss: 0.01650\n",
      "Training epoch: 2130, train loss: 0.01821, val loss: 0.01863\n",
      "Training epoch: 2131, train loss: 0.02091, val loss: 0.02101\n",
      "Training epoch: 2132, train loss: 0.01784, val loss: 0.01814\n",
      "Training epoch: 2133, train loss: 0.01902, val loss: 0.01954\n",
      "Training epoch: 2134, train loss: 0.01637, val loss: 0.01655\n",
      "Training epoch: 2135, train loss: 0.01822, val loss: 0.01826\n",
      "Training epoch: 2136, train loss: 0.01573, val loss: 0.01615\n",
      "Training epoch: 2137, train loss: 0.01686, val loss: 0.01739\n",
      "Training epoch: 2138, train loss: 0.02044, val loss: 0.02095\n",
      "Training epoch: 2139, train loss: 0.01713, val loss: 0.01700\n",
      "Training epoch: 2140, train loss: 0.01801, val loss: 0.01830\n",
      "Training epoch: 2141, train loss: 0.01828, val loss: 0.01843\n",
      "Training epoch: 2142, train loss: 0.01642, val loss: 0.01663\n",
      "Training epoch: 2143, train loss: 0.01704, val loss: 0.01737\n",
      "Training epoch: 2144, train loss: 0.02099, val loss: 0.02121\n",
      "Training epoch: 2145, train loss: 0.02134, val loss: 0.02231\n",
      "Training epoch: 2146, train loss: 0.02044, val loss: 0.02047\n",
      "Training epoch: 2147, train loss: 0.02117, val loss: 0.02216\n",
      "Training epoch: 2148, train loss: 0.01734, val loss: 0.01731\n",
      "Training epoch: 2149, train loss: 0.01791, val loss: 0.01824\n",
      "Training epoch: 2150, train loss: 0.01686, val loss: 0.01713\n",
      "Training epoch: 2151, train loss: 0.01628, val loss: 0.01645\n",
      "Training epoch: 2152, train loss: 0.01576, val loss: 0.01607\n",
      "Training epoch: 2153, train loss: 0.01613, val loss: 0.01637\n",
      "Training epoch: 2154, train loss: 0.01751, val loss: 0.01773\n",
      "Training epoch: 2155, train loss: 0.02068, val loss: 0.02130\n",
      "Training epoch: 2156, train loss: 0.01853, val loss: 0.01918\n",
      "Training epoch: 2157, train loss: 0.01817, val loss: 0.01819\n",
      "Training epoch: 2158, train loss: 0.01702, val loss: 0.01736\n",
      "Training epoch: 2159, train loss: 0.01603, val loss: 0.01600\n",
      "Training epoch: 2160, train loss: 0.01622, val loss: 0.01643\n",
      "Training epoch: 2161, train loss: 0.01621, val loss: 0.01644\n",
      "Training epoch: 2162, train loss: 0.01628, val loss: 0.01670\n",
      "Training epoch: 2163, train loss: 0.01641, val loss: 0.01662\n",
      "Training epoch: 2164, train loss: 0.01709, val loss: 0.01770\n",
      "Training epoch: 2165, train loss: 0.01656, val loss: 0.01710\n",
      "Training epoch: 2166, train loss: 0.03321, val loss: 0.03423\n",
      "Training epoch: 2167, train loss: 0.01624, val loss: 0.01667\n",
      "Training epoch: 2168, train loss: 0.01914, val loss: 0.01970\n",
      "Training epoch: 2169, train loss: 0.02318, val loss: 0.02319\n",
      "Training epoch: 2170, train loss: 0.01770, val loss: 0.01800\n",
      "Training epoch: 2171, train loss: 0.01612, val loss: 0.01614\n",
      "Training epoch: 2172, train loss: 0.01755, val loss: 0.01775\n",
      "Training epoch: 2173, train loss: 0.01994, val loss: 0.02073\n",
      "Training epoch: 2174, train loss: 0.01684, val loss: 0.01724\n",
      "Training epoch: 2175, train loss: 0.01693, val loss: 0.01736\n",
      "Training epoch: 2176, train loss: 0.02387, val loss: 0.02397\n",
      "Training epoch: 2177, train loss: 0.01770, val loss: 0.01787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2178, train loss: 0.01550, val loss: 0.01573\n",
      "Training epoch: 2179, train loss: 0.01866, val loss: 0.01850\n",
      "Training epoch: 2180, train loss: 0.01725, val loss: 0.01757\n",
      "Training epoch: 2181, train loss: 0.02279, val loss: 0.02374\n",
      "Training epoch: 2182, train loss: 0.01898, val loss: 0.01913\n",
      "Training epoch: 2183, train loss: 0.01588, val loss: 0.01610\n",
      "Training epoch: 2184, train loss: 0.01899, val loss: 0.01919\n",
      "Training epoch: 2185, train loss: 0.01597, val loss: 0.01621\n",
      "Training epoch: 2186, train loss: 0.01710, val loss: 0.01724\n",
      "Training epoch: 2187, train loss: 0.02246, val loss: 0.02255\n",
      "Training epoch: 2188, train loss: 0.02112, val loss: 0.02094\n",
      "Training epoch: 2189, train loss: 0.01729, val loss: 0.01725\n",
      "Training epoch: 2190, train loss: 0.02210, val loss: 0.02176\n",
      "Training epoch: 2191, train loss: 0.01716, val loss: 0.01814\n",
      "Training epoch: 2192, train loss: 0.01727, val loss: 0.01776\n",
      "Training epoch: 2193, train loss: 0.01617, val loss: 0.01659\n",
      "Training epoch: 2194, train loss: 0.01598, val loss: 0.01621\n",
      "Training epoch: 2195, train loss: 0.01829, val loss: 0.01858\n",
      "Training epoch: 2196, train loss: 0.01629, val loss: 0.01663\n",
      "Training epoch: 2197, train loss: 0.01783, val loss: 0.01798\n",
      "Training epoch: 2198, train loss: 0.01604, val loss: 0.01625\n",
      "Training epoch: 2199, train loss: 0.02081, val loss: 0.02099\n",
      "Training epoch: 2200, train loss: 0.01756, val loss: 0.01770\n",
      "Training epoch: 2201, train loss: 0.01984, val loss: 0.01960\n",
      "Training epoch: 2202, train loss: 0.02205, val loss: 0.02224\n",
      "Training epoch: 2203, train loss: 0.01802, val loss: 0.01892\n",
      "Training epoch: 2204, train loss: 0.01708, val loss: 0.01726\n",
      "Early stop at epoch 2204, With Testing Error: 0.01726\n",
      "Subnetwork pruning.\n",
      "Fine tuning.\n",
      "Tuning epoch: 1, train loss: 0.01600, val loss: 0.01655\n",
      "Tuning epoch: 2, train loss: 0.02127, val loss: 0.02152\n",
      "Tuning epoch: 3, train loss: 0.01591, val loss: 0.01642\n",
      "Tuning epoch: 4, train loss: 0.01578, val loss: 0.01627\n",
      "Tuning epoch: 5, train loss: 0.01864, val loss: 0.01889\n",
      "Tuning epoch: 6, train loss: 0.01584, val loss: 0.01606\n",
      "Tuning epoch: 7, train loss: 0.02530, val loss: 0.02687\n",
      "Tuning epoch: 8, train loss: 0.01984, val loss: 0.01985\n",
      "Tuning epoch: 9, train loss: 0.01618, val loss: 0.01688\n",
      "Tuning epoch: 10, train loss: 0.01727, val loss: 0.01793\n",
      "Tuning epoch: 11, train loss: 0.01641, val loss: 0.01690\n",
      "Tuning epoch: 12, train loss: 0.01886, val loss: 0.01975\n",
      "Tuning epoch: 13, train loss: 0.01634, val loss: 0.01665\n",
      "Tuning epoch: 14, train loss: 0.01597, val loss: 0.01636\n",
      "Tuning epoch: 15, train loss: 0.01652, val loss: 0.01700\n",
      "Tuning epoch: 16, train loss: 0.02167, val loss: 0.02254\n",
      "Tuning epoch: 17, train loss: 0.02165, val loss: 0.02189\n",
      "Tuning epoch: 18, train loss: 0.02664, val loss: 0.02689\n",
      "Tuning epoch: 19, train loss: 0.01625, val loss: 0.01719\n",
      "Tuning epoch: 20, train loss: 0.01689, val loss: 0.01741\n",
      "Tuning epoch: 21, train loss: 0.01845, val loss: 0.01867\n",
      "Tuning epoch: 22, train loss: 0.01840, val loss: 0.01849\n",
      "Tuning epoch: 23, train loss: 0.01885, val loss: 0.01933\n",
      "Tuning epoch: 24, train loss: 0.01718, val loss: 0.01773\n",
      "Tuning epoch: 25, train loss: 0.01736, val loss: 0.01756\n",
      "Tuning epoch: 26, train loss: 0.01596, val loss: 0.01646\n",
      "Tuning epoch: 27, train loss: 0.01602, val loss: 0.01654\n",
      "Tuning epoch: 28, train loss: 0.01557, val loss: 0.01581\n",
      "Tuning epoch: 29, train loss: 0.04677, val loss: 0.04899\n",
      "Tuning epoch: 30, train loss: 0.01629, val loss: 0.01693\n",
      "Tuning epoch: 31, train loss: 0.02247, val loss: 0.02320\n",
      "Tuning epoch: 32, train loss: 0.01622, val loss: 0.01677\n",
      "Tuning epoch: 33, train loss: 0.01712, val loss: 0.01781\n",
      "Tuning epoch: 34, train loss: 0.01736, val loss: 0.01765\n",
      "Tuning epoch: 35, train loss: 0.01756, val loss: 0.01801\n",
      "Tuning epoch: 36, train loss: 0.01810, val loss: 0.01844\n",
      "Tuning epoch: 37, train loss: 0.02038, val loss: 0.02049\n",
      "Tuning epoch: 38, train loss: 0.02460, val loss: 0.02447\n",
      "Tuning epoch: 39, train loss: 0.02090, val loss: 0.02141\n",
      "Tuning epoch: 40, train loss: 0.01713, val loss: 0.01736\n",
      "Tuning epoch: 41, train loss: 0.01608, val loss: 0.01648\n",
      "Tuning epoch: 42, train loss: 0.01676, val loss: 0.01744\n",
      "Tuning epoch: 43, train loss: 0.02591, val loss: 0.02601\n",
      "Tuning epoch: 44, train loss: 0.02600, val loss: 0.02613\n",
      "Tuning epoch: 45, train loss: 0.01556, val loss: 0.01624\n",
      "Tuning epoch: 46, train loss: 0.01800, val loss: 0.01861\n",
      "Tuning epoch: 47, train loss: 0.01862, val loss: 0.01889\n",
      "Tuning epoch: 48, train loss: 0.01630, val loss: 0.01669\n",
      "Tuning epoch: 49, train loss: 0.01774, val loss: 0.01802\n",
      "Tuning epoch: 50, train loss: 0.01554, val loss: 0.01588\n",
      "Tuning epoch: 51, train loss: 0.01684, val loss: 0.01766\n",
      "Tuning epoch: 52, train loss: 0.01608, val loss: 0.01633\n",
      "Tuning epoch: 53, train loss: 0.01774, val loss: 0.01834\n",
      "Tuning epoch: 54, train loss: 0.02352, val loss: 0.02407\n",
      "Tuning epoch: 55, train loss: 0.02190, val loss: 0.02402\n",
      "Tuning epoch: 56, train loss: 0.01983, val loss: 0.02130\n",
      "Tuning epoch: 57, train loss: 0.02228, val loss: 0.02313\n",
      "Tuning epoch: 58, train loss: 0.01743, val loss: 0.01886\n",
      "Tuning epoch: 59, train loss: 0.01875, val loss: 0.01970\n",
      "Tuning epoch: 60, train loss: 0.01714, val loss: 0.01835\n",
      "Tuning epoch: 61, train loss: 0.01850, val loss: 0.01949\n",
      "Tuning epoch: 62, train loss: 0.01679, val loss: 0.01793\n",
      "Tuning epoch: 63, train loss: 0.01660, val loss: 0.01769\n",
      "Tuning epoch: 64, train loss: 0.01700, val loss: 0.01806\n",
      "Tuning epoch: 65, train loss: 0.02290, val loss: 0.02362\n",
      "Tuning epoch: 66, train loss: 0.01693, val loss: 0.01817\n",
      "Tuning epoch: 67, train loss: 0.02132, val loss: 0.02205\n",
      "Tuning epoch: 68, train loss: 0.01691, val loss: 0.01786\n",
      "Tuning epoch: 69, train loss: 0.01765, val loss: 0.01850\n",
      "Tuning epoch: 70, train loss: 0.01874, val loss: 0.01947\n",
      "Tuning epoch: 71, train loss: 0.01797, val loss: 0.01886\n",
      "Tuning epoch: 72, train loss: 0.01991, val loss: 0.02058\n",
      "Tuning epoch: 73, train loss: 0.02427, val loss: 0.02487\n",
      "Tuning epoch: 74, train loss: 0.01593, val loss: 0.01661\n",
      "Tuning epoch: 75, train loss: 0.01958, val loss: 0.01997\n",
      "Tuning epoch: 76, train loss: 0.02366, val loss: 0.02397\n",
      "Tuning epoch: 77, train loss: 0.02000, val loss: 0.02007\n",
      "Tuning epoch: 78, train loss: 0.01892, val loss: 0.01932\n",
      "Tuning epoch: 79, train loss: 0.01652, val loss: 0.01694\n",
      "Tuning epoch: 80, train loss: 0.01692, val loss: 0.01727\n",
      "Tuning epoch: 81, train loss: 0.02239, val loss: 0.02280\n",
      "Tuning epoch: 82, train loss: 0.01965, val loss: 0.02037\n",
      "Tuning epoch: 83, train loss: 0.01906, val loss: 0.01945\n",
      "Tuning epoch: 84, train loss: 0.01659, val loss: 0.01718\n",
      "Tuning epoch: 85, train loss: 0.01663, val loss: 0.01696\n",
      "Tuning epoch: 86, train loss: 0.01658, val loss: 0.01694\n",
      "Tuning epoch: 87, train loss: 0.01923, val loss: 0.01947\n",
      "Tuning epoch: 88, train loss: 0.01879, val loss: 0.01893\n",
      "Tuning epoch: 89, train loss: 0.02165, val loss: 0.02179\n",
      "Tuning epoch: 90, train loss: 0.01641, val loss: 0.01677\n",
      "Tuning epoch: 91, train loss: 0.01569, val loss: 0.01605\n",
      "Tuning epoch: 92, train loss: 0.01592, val loss: 0.01643\n",
      "Tuning epoch: 93, train loss: 0.01672, val loss: 0.01716\n",
      "Tuning epoch: 94, train loss: 0.01771, val loss: 0.01814\n",
      "Tuning epoch: 95, train loss: 0.01719, val loss: 0.01743\n",
      "Tuning epoch: 96, train loss: 0.02131, val loss: 0.02142\n",
      "Tuning epoch: 97, train loss: 0.01670, val loss: 0.01687\n",
      "Tuning epoch: 98, train loss: 0.01729, val loss: 0.01761\n",
      "Tuning epoch: 99, train loss: 0.01592, val loss: 0.01632\n",
      "Tuning epoch: 100, train loss: 0.01631, val loss: 0.01666\n",
      "[1.08095 1.10454 1.09521]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1296 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = SOSxNN(input_num=10, input_dummy_num=0, subnet_num=10, subnet_arch=[10, 6], task=\"Regression\",\n",
    "               activation_func=tf.tanh, batch_size=1000, training_epochs=5000, lr_bp=0.001, lr_cl=0.1,\n",
    "               beta_threshold=0.01, tuning_epochs=100, l1_proj=0.001, l1_subnet = 0.01, smooth_lambda=10**(-3),\n",
    "               verbose=True, val_ratio=0.2, early_stop_thres=500)\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"test\", train_x)\n",
    "\n",
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "\n",
    "sosxnn_mse_stat = np.hstack([np.round(np.mean((scaler_y.inverse_transform(tr_pred) - scaler_y.inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(val_pred) - scaler_y.inverse_transform(model.val_y))**2),5),\\\n",
    "               np.round(np.mean((scaler_y.inverse_transform(pred_test) - scaler_y.inverse_transform(test_y))**2),5)])\n",
    "print(sosxnn_mse_stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
