{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.lines as mlines\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.stats import ortho_group\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from xnn.sosxnn import SOSxNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "corr = 0.5\n",
    "noise_sigma = 1\n",
    "dummy_num = 0\n",
    "feature_num = 10\n",
    "test_num = 10000\n",
    "data_num = 10000\n",
    "\n",
    "proj_matrix = np.zeros((feature_num,4))\n",
    "proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "\n",
    "def data_generator1(data_num, feature_num, corr, proj_matrix, noise_sigma):\n",
    "    u = np.random.uniform(-1,1, [data_num, 1])\n",
    "    t= np.sqrt(corr/(1-corr))\n",
    "    X = np.zeros((data_num, feature_num))\n",
    "    for i in range(feature_num):\n",
    "        X[:, i:i+1] = (np.random.uniform(-1,1,[data_num,1])+t*u)/(1+t)\n",
    "    Y = np.reshape(2*np.dot(X, proj_matrix[:,0])+0.2*np.exp(-4*np.dot(X, proj_matrix[:,1])) + \\\n",
    "              3*(np.dot(X, proj_matrix[:,2]))**2+2.5*np.sin(np.pi*np.dot(X, proj_matrix[:,3])), [-1,1]) + \\\n",
    "              noise_sigma*np.random.normal(0,1, [data_num,1])\n",
    "    return X, Y\n",
    "\n",
    "np.random.seed(0)\n",
    "X, Y = data_generator1(data_num+test_num, feature_num+dummy_num, corr, proj_matrix, noise_sigma)\n",
    "scaler_x = MinMaxScaler((-1, 1)); scaler_y = MinMaxScaler((-1, 1))\n",
    "sX = scaler_x.fit_transform(X); sY = scaler_y.fit_transform(Y)\n",
    "train_x, test_x, train_y, test_y = train_test_split(sX, sY, test_size = test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n",
      "Training epoch: 1, train loss: 0.05350, val loss: 0.05221\n",
      "Training epoch: 2, train loss: 0.04682, val loss: 0.04430\n",
      "Training epoch: 3, train loss: 0.04163, val loss: 0.04032\n",
      "Training epoch: 4, train loss: 0.03716, val loss: 0.03612\n",
      "Training epoch: 5, train loss: 0.03404, val loss: 0.03336\n",
      "Training epoch: 6, train loss: 0.03166, val loss: 0.03108\n",
      "Training epoch: 7, train loss: 0.02991, val loss: 0.02982\n",
      "Training epoch: 8, train loss: 0.02873, val loss: 0.02871\n",
      "Training epoch: 9, train loss: 0.02788, val loss: 0.02783\n",
      "Training epoch: 10, train loss: 0.02725, val loss: 0.02716\n",
      "Training epoch: 11, train loss: 0.02653, val loss: 0.02666\n",
      "Training epoch: 12, train loss: 0.02607, val loss: 0.02589\n",
      "Training epoch: 13, train loss: 0.02575, val loss: 0.02559\n",
      "Training epoch: 14, train loss: 0.02553, val loss: 0.02541\n",
      "Training epoch: 15, train loss: 0.02539, val loss: 0.02531\n",
      "Training epoch: 16, train loss: 0.02524, val loss: 0.02533\n",
      "Training epoch: 17, train loss: 0.02527, val loss: 0.02556\n",
      "Training epoch: 18, train loss: 0.02508, val loss: 0.02527\n",
      "Training epoch: 19, train loss: 0.02503, val loss: 0.02516\n",
      "Training epoch: 20, train loss: 0.02500, val loss: 0.02494\n",
      "Training epoch: 21, train loss: 0.02481, val loss: 0.02508\n",
      "Training epoch: 22, train loss: 0.02474, val loss: 0.02471\n",
      "Training epoch: 23, train loss: 0.02469, val loss: 0.02494\n",
      "Training epoch: 24, train loss: 0.02460, val loss: 0.02488\n",
      "Training epoch: 25, train loss: 0.02457, val loss: 0.02485\n",
      "Training epoch: 26, train loss: 0.02446, val loss: 0.02461\n",
      "Training epoch: 27, train loss: 0.02440, val loss: 0.02449\n",
      "Training epoch: 28, train loss: 0.02430, val loss: 0.02457\n",
      "Training epoch: 29, train loss: 0.02425, val loss: 0.02437\n",
      "Training epoch: 30, train loss: 0.02427, val loss: 0.02462\n",
      "Training epoch: 31, train loss: 0.02413, val loss: 0.02427\n",
      "Training epoch: 32, train loss: 0.02411, val loss: 0.02420\n",
      "Training epoch: 33, train loss: 0.02405, val loss: 0.02436\n",
      "Training epoch: 34, train loss: 0.02398, val loss: 0.02426\n",
      "Training epoch: 35, train loss: 0.02398, val loss: 0.02405\n",
      "Training epoch: 36, train loss: 0.02385, val loss: 0.02406\n",
      "Training epoch: 37, train loss: 0.02383, val loss: 0.02419\n",
      "Training epoch: 38, train loss: 0.02386, val loss: 0.02395\n",
      "Training epoch: 39, train loss: 0.02375, val loss: 0.02389\n",
      "Training epoch: 40, train loss: 0.02362, val loss: 0.02399\n",
      "Training epoch: 41, train loss: 0.02379, val loss: 0.02429\n",
      "Training epoch: 42, train loss: 0.02352, val loss: 0.02385\n",
      "Training epoch: 43, train loss: 0.02358, val loss: 0.02373\n",
      "Training epoch: 44, train loss: 0.02360, val loss: 0.02400\n",
      "Training epoch: 45, train loss: 0.02339, val loss: 0.02360\n",
      "Training epoch: 46, train loss: 0.02335, val loss: 0.02367\n",
      "Training epoch: 47, train loss: 0.02327, val loss: 0.02354\n",
      "Training epoch: 48, train loss: 0.02337, val loss: 0.02369\n",
      "Training epoch: 49, train loss: 0.02344, val loss: 0.02389\n",
      "Training epoch: 50, train loss: 0.02314, val loss: 0.02352\n",
      "Training epoch: 51, train loss: 0.02304, val loss: 0.02331\n",
      "Training epoch: 52, train loss: 0.02306, val loss: 0.02342\n",
      "Training epoch: 53, train loss: 0.02299, val loss: 0.02324\n",
      "Training epoch: 54, train loss: 0.02290, val loss: 0.02317\n",
      "Training epoch: 55, train loss: 0.02287, val loss: 0.02311\n",
      "Training epoch: 56, train loss: 0.02283, val loss: 0.02306\n",
      "Training epoch: 57, train loss: 0.02276, val loss: 0.02309\n",
      "Training epoch: 58, train loss: 0.02277, val loss: 0.02315\n",
      "Training epoch: 59, train loss: 0.02283, val loss: 0.02333\n",
      "Training epoch: 60, train loss: 0.02269, val loss: 0.02304\n",
      "Training epoch: 61, train loss: 0.02262, val loss: 0.02304\n",
      "Training epoch: 62, train loss: 0.02252, val loss: 0.02281\n",
      "Training epoch: 63, train loss: 0.02252, val loss: 0.02280\n",
      "Training epoch: 64, train loss: 0.02246, val loss: 0.02271\n",
      "Training epoch: 65, train loss: 0.02250, val loss: 0.02276\n",
      "Training epoch: 66, train loss: 0.02235, val loss: 0.02274\n",
      "Training epoch: 67, train loss: 0.02241, val loss: 0.02288\n",
      "Training epoch: 68, train loss: 0.02229, val loss: 0.02263\n",
      "Training epoch: 69, train loss: 0.02246, val loss: 0.02299\n",
      "Training epoch: 70, train loss: 0.02229, val loss: 0.02282\n",
      "Training epoch: 71, train loss: 0.02224, val loss: 0.02259\n",
      "Training epoch: 72, train loss: 0.02219, val loss: 0.02253\n",
      "Training epoch: 73, train loss: 0.02210, val loss: 0.02257\n",
      "Training epoch: 74, train loss: 0.02220, val loss: 0.02272\n",
      "Training epoch: 75, train loss: 0.02201, val loss: 0.02238\n",
      "Training epoch: 76, train loss: 0.02208, val loss: 0.02232\n",
      "Training epoch: 77, train loss: 0.02194, val loss: 0.02229\n",
      "Training epoch: 78, train loss: 0.02188, val loss: 0.02224\n",
      "Training epoch: 79, train loss: 0.02184, val loss: 0.02228\n",
      "Training epoch: 80, train loss: 0.02233, val loss: 0.02304\n",
      "Training epoch: 81, train loss: 0.02197, val loss: 0.02254\n",
      "Training epoch: 82, train loss: 0.02177, val loss: 0.02227\n",
      "Training epoch: 83, train loss: 0.02173, val loss: 0.02225\n",
      "Training epoch: 84, train loss: 0.02165, val loss: 0.02213\n",
      "Training epoch: 85, train loss: 0.02167, val loss: 0.02220\n",
      "Training epoch: 86, train loss: 0.02178, val loss: 0.02236\n",
      "Training epoch: 87, train loss: 0.02168, val loss: 0.02225\n",
      "Training epoch: 88, train loss: 0.02164, val loss: 0.02219\n",
      "Training epoch: 89, train loss: 0.02155, val loss: 0.02208\n",
      "Training epoch: 90, train loss: 0.02151, val loss: 0.02202\n",
      "Training epoch: 91, train loss: 0.02167, val loss: 0.02194\n",
      "Training epoch: 92, train loss: 0.02142, val loss: 0.02181\n",
      "Training epoch: 93, train loss: 0.02134, val loss: 0.02177\n",
      "Training epoch: 94, train loss: 0.02142, val loss: 0.02175\n",
      "Training epoch: 95, train loss: 0.02136, val loss: 0.02191\n",
      "Training epoch: 96, train loss: 0.02139, val loss: 0.02195\n",
      "Training epoch: 97, train loss: 0.02122, val loss: 0.02169\n",
      "Training epoch: 98, train loss: 0.02144, val loss: 0.02207\n",
      "Training epoch: 99, train loss: 0.02124, val loss: 0.02178\n",
      "Training epoch: 100, train loss: 0.02118, val loss: 0.02170\n",
      "Training epoch: 101, train loss: 0.02118, val loss: 0.02168\n",
      "Training epoch: 102, train loss: 0.02136, val loss: 0.02198\n",
      "Training epoch: 103, train loss: 0.02150, val loss: 0.02211\n",
      "Training epoch: 104, train loss: 0.02120, val loss: 0.02177\n",
      "Training epoch: 105, train loss: 0.02109, val loss: 0.02160\n",
      "Training epoch: 106, train loss: 0.02109, val loss: 0.02167\n",
      "Training epoch: 107, train loss: 0.02113, val loss: 0.02171\n",
      "Training epoch: 108, train loss: 0.02099, val loss: 0.02154\n",
      "Training epoch: 109, train loss: 0.02094, val loss: 0.02145\n",
      "Training epoch: 110, train loss: 0.02087, val loss: 0.02134\n",
      "Training epoch: 111, train loss: 0.02095, val loss: 0.02127\n",
      "Training epoch: 112, train loss: 0.02086, val loss: 0.02124\n",
      "Training epoch: 113, train loss: 0.02097, val loss: 0.02133\n",
      "Training epoch: 114, train loss: 0.02076, val loss: 0.02121\n",
      "Training epoch: 115, train loss: 0.02077, val loss: 0.02125\n",
      "Training epoch: 116, train loss: 0.02072, val loss: 0.02115\n",
      "Training epoch: 117, train loss: 0.02079, val loss: 0.02133\n",
      "Training epoch: 118, train loss: 0.02070, val loss: 0.02104\n",
      "Training epoch: 119, train loss: 0.02066, val loss: 0.02115\n",
      "Training epoch: 120, train loss: 0.02058, val loss: 0.02101\n",
      "Training epoch: 121, train loss: 0.02056, val loss: 0.02100\n",
      "Training epoch: 122, train loss: 0.02058, val loss: 0.02106\n",
      "Training epoch: 123, train loss: 0.02061, val loss: 0.02099\n",
      "Training epoch: 124, train loss: 0.02059, val loss: 0.02102\n",
      "Training epoch: 125, train loss: 0.02059, val loss: 0.02096\n",
      "Training epoch: 126, train loss: 0.02049, val loss: 0.02094\n",
      "Training epoch: 127, train loss: 0.02043, val loss: 0.02094\n",
      "Training epoch: 128, train loss: 0.02049, val loss: 0.02097\n",
      "Training epoch: 129, train loss: 0.02057, val loss: 0.02115\n",
      "Training epoch: 130, train loss: 0.02052, val loss: 0.02103\n",
      "Training epoch: 131, train loss: 0.02061, val loss: 0.02118\n",
      "Training epoch: 132, train loss: 0.02034, val loss: 0.02081\n",
      "Training epoch: 133, train loss: 0.02027, val loss: 0.02067\n",
      "Training epoch: 134, train loss: 0.02029, val loss: 0.02071\n",
      "Training epoch: 135, train loss: 0.02023, val loss: 0.02066\n",
      "Training epoch: 136, train loss: 0.02025, val loss: 0.02062\n",
      "Training epoch: 137, train loss: 0.02022, val loss: 0.02059\n",
      "Training epoch: 138, train loss: 0.02028, val loss: 0.02064\n",
      "Training epoch: 139, train loss: 0.02025, val loss: 0.02061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 140, train loss: 0.02039, val loss: 0.02071\n",
      "Training epoch: 141, train loss: 0.02023, val loss: 0.02060\n",
      "Training epoch: 142, train loss: 0.02015, val loss: 0.02058\n",
      "Training epoch: 143, train loss: 0.02014, val loss: 0.02060\n",
      "Training epoch: 144, train loss: 0.02008, val loss: 0.02052\n",
      "Training epoch: 145, train loss: 0.02002, val loss: 0.02046\n",
      "Training epoch: 146, train loss: 0.02029, val loss: 0.02056\n",
      "Training epoch: 147, train loss: 0.02013, val loss: 0.02046\n",
      "Training epoch: 148, train loss: 0.02023, val loss: 0.02056\n",
      "Training epoch: 149, train loss: 0.02006, val loss: 0.02052\n",
      "Training epoch: 150, train loss: 0.02007, val loss: 0.02053\n",
      "Training epoch: 151, train loss: 0.02052, val loss: 0.02112\n",
      "Training epoch: 152, train loss: 0.02010, val loss: 0.02061\n",
      "Training epoch: 153, train loss: 0.01993, val loss: 0.02034\n",
      "Training epoch: 154, train loss: 0.02000, val loss: 0.02032\n",
      "Training epoch: 155, train loss: 0.02000, val loss: 0.02033\n",
      "Training epoch: 156, train loss: 0.01996, val loss: 0.02027\n",
      "Training epoch: 157, train loss: 0.02002, val loss: 0.02037\n",
      "Training epoch: 158, train loss: 0.01987, val loss: 0.02023\n",
      "Training epoch: 159, train loss: 0.01982, val loss: 0.02016\n",
      "Training epoch: 160, train loss: 0.01980, val loss: 0.02018\n",
      "Training epoch: 161, train loss: 0.01989, val loss: 0.02017\n",
      "Training epoch: 162, train loss: 0.01978, val loss: 0.02012\n",
      "Training epoch: 163, train loss: 0.01987, val loss: 0.02018\n",
      "Training epoch: 164, train loss: 0.01981, val loss: 0.02013\n",
      "Training epoch: 165, train loss: 0.01977, val loss: 0.02020\n",
      "Training epoch: 166, train loss: 0.01970, val loss: 0.02012\n",
      "Training epoch: 167, train loss: 0.01971, val loss: 0.02012\n",
      "Training epoch: 168, train loss: 0.01977, val loss: 0.02020\n",
      "Training epoch: 169, train loss: 0.01964, val loss: 0.01997\n",
      "Training epoch: 170, train loss: 0.01958, val loss: 0.02004\n",
      "Training epoch: 171, train loss: 0.01959, val loss: 0.02003\n",
      "Training epoch: 172, train loss: 0.01975, val loss: 0.02021\n",
      "Training epoch: 173, train loss: 0.01954, val loss: 0.01991\n",
      "Training epoch: 174, train loss: 0.01957, val loss: 0.01987\n",
      "Training epoch: 175, train loss: 0.01954, val loss: 0.01996\n",
      "Training epoch: 176, train loss: 0.01974, val loss: 0.02018\n",
      "Training epoch: 177, train loss: 0.01952, val loss: 0.01988\n",
      "Training epoch: 178, train loss: 0.01959, val loss: 0.02001\n",
      "Training epoch: 179, train loss: 0.01946, val loss: 0.01983\n",
      "Training epoch: 180, train loss: 0.01962, val loss: 0.02004\n",
      "Training epoch: 181, train loss: 0.01950, val loss: 0.01992\n",
      "Training epoch: 182, train loss: 0.01941, val loss: 0.01979\n",
      "Training epoch: 183, train loss: 0.01937, val loss: 0.01976\n",
      "Training epoch: 184, train loss: 0.01938, val loss: 0.01967\n",
      "Training epoch: 185, train loss: 0.01954, val loss: 0.01995\n",
      "Training epoch: 186, train loss: 0.01935, val loss: 0.01976\n",
      "Training epoch: 187, train loss: 0.01952, val loss: 0.01994\n",
      "Training epoch: 188, train loss: 0.01937, val loss: 0.01975\n",
      "Training epoch: 189, train loss: 0.01991, val loss: 0.02039\n",
      "Training epoch: 190, train loss: 0.01928, val loss: 0.01967\n",
      "Training epoch: 191, train loss: 0.01929, val loss: 0.01959\n",
      "Training epoch: 192, train loss: 0.01933, val loss: 0.01968\n",
      "Training epoch: 193, train loss: 0.01935, val loss: 0.01963\n",
      "Training epoch: 194, train loss: 0.01962, val loss: 0.01992\n",
      "Training epoch: 195, train loss: 0.01923, val loss: 0.01957\n",
      "Training epoch: 196, train loss: 0.01929, val loss: 0.01959\n",
      "Training epoch: 197, train loss: 0.01915, val loss: 0.01943\n",
      "Training epoch: 198, train loss: 0.01922, val loss: 0.01954\n",
      "Training epoch: 199, train loss: 0.01913, val loss: 0.01946\n",
      "Training epoch: 200, train loss: 0.01925, val loss: 0.01970\n",
      "Training epoch: 201, train loss: 0.01951, val loss: 0.02005\n",
      "Training epoch: 202, train loss: 0.01922, val loss: 0.01963\n",
      "Training epoch: 203, train loss: 0.01912, val loss: 0.01943\n",
      "Training epoch: 204, train loss: 0.01906, val loss: 0.01940\n",
      "Training epoch: 205, train loss: 0.01930, val loss: 0.01962\n",
      "Training epoch: 206, train loss: 0.01946, val loss: 0.01969\n",
      "Training epoch: 207, train loss: 0.01919, val loss: 0.01947\n",
      "Training epoch: 208, train loss: 0.01902, val loss: 0.01941\n",
      "Training epoch: 209, train loss: 0.01901, val loss: 0.01934\n",
      "Training epoch: 210, train loss: 0.01896, val loss: 0.01931\n",
      "Training epoch: 211, train loss: 0.01898, val loss: 0.01940\n",
      "Training epoch: 212, train loss: 0.01905, val loss: 0.01943\n",
      "Training epoch: 213, train loss: 0.01894, val loss: 0.01933\n",
      "Training epoch: 214, train loss: 0.01895, val loss: 0.01926\n",
      "Training epoch: 215, train loss: 0.01888, val loss: 0.01920\n",
      "Training epoch: 216, train loss: 0.01889, val loss: 0.01926\n",
      "Training epoch: 217, train loss: 0.01887, val loss: 0.01923\n",
      "Training epoch: 218, train loss: 0.01883, val loss: 0.01917\n",
      "Training epoch: 219, train loss: 0.01883, val loss: 0.01917\n",
      "Training epoch: 220, train loss: 0.01889, val loss: 0.01921\n",
      "Training epoch: 221, train loss: 0.01888, val loss: 0.01917\n",
      "Training epoch: 222, train loss: 0.01881, val loss: 0.01916\n",
      "Training epoch: 223, train loss: 0.01878, val loss: 0.01916\n",
      "Training epoch: 224, train loss: 0.01885, val loss: 0.01914\n",
      "Training epoch: 225, train loss: 0.01894, val loss: 0.01925\n",
      "Training epoch: 226, train loss: 0.01895, val loss: 0.01918\n",
      "Training epoch: 227, train loss: 0.01885, val loss: 0.01932\n",
      "Training epoch: 228, train loss: 0.01870, val loss: 0.01914\n",
      "Training epoch: 229, train loss: 0.01874, val loss: 0.01907\n",
      "Training epoch: 230, train loss: 0.01885, val loss: 0.01916\n",
      "Training epoch: 231, train loss: 0.01869, val loss: 0.01901\n",
      "Training epoch: 232, train loss: 0.01861, val loss: 0.01894\n",
      "Training epoch: 233, train loss: 0.01862, val loss: 0.01900\n",
      "Training epoch: 234, train loss: 0.01861, val loss: 0.01896\n",
      "Training epoch: 235, train loss: 0.01859, val loss: 0.01897\n",
      "Training epoch: 236, train loss: 0.01855, val loss: 0.01892\n",
      "Training epoch: 237, train loss: 0.01858, val loss: 0.01896\n",
      "Training epoch: 238, train loss: 0.01864, val loss: 0.01909\n",
      "Training epoch: 239, train loss: 0.01856, val loss: 0.01890\n",
      "Training epoch: 240, train loss: 0.01854, val loss: 0.01887\n",
      "Training epoch: 241, train loss: 0.01852, val loss: 0.01886\n",
      "Training epoch: 242, train loss: 0.01860, val loss: 0.01890\n",
      "Training epoch: 243, train loss: 0.01847, val loss: 0.01884\n",
      "Training epoch: 244, train loss: 0.01861, val loss: 0.01900\n",
      "Training epoch: 245, train loss: 0.01874, val loss: 0.01896\n",
      "Training epoch: 246, train loss: 0.01864, val loss: 0.01892\n",
      "Training epoch: 247, train loss: 0.01846, val loss: 0.01885\n",
      "Training epoch: 248, train loss: 0.01852, val loss: 0.01884\n",
      "Training epoch: 249, train loss: 0.01840, val loss: 0.01877\n",
      "Training epoch: 250, train loss: 0.01836, val loss: 0.01870\n",
      "Training epoch: 251, train loss: 0.01833, val loss: 0.01866\n",
      "Training epoch: 252, train loss: 0.01832, val loss: 0.01874\n",
      "Training epoch: 253, train loss: 0.01833, val loss: 0.01872\n",
      "Training epoch: 254, train loss: 0.01835, val loss: 0.01864\n",
      "Training epoch: 255, train loss: 0.01830, val loss: 0.01866\n",
      "Training epoch: 256, train loss: 0.01830, val loss: 0.01866\n",
      "Training epoch: 257, train loss: 0.01825, val loss: 0.01859\n",
      "Training epoch: 258, train loss: 0.01827, val loss: 0.01870\n",
      "Training epoch: 259, train loss: 0.01828, val loss: 0.01864\n",
      "Training epoch: 260, train loss: 0.01829, val loss: 0.01880\n",
      "Training epoch: 261, train loss: 0.01819, val loss: 0.01854\n",
      "Training epoch: 262, train loss: 0.01820, val loss: 0.01858\n",
      "Training epoch: 263, train loss: 0.01818, val loss: 0.01867\n",
      "Training epoch: 264, train loss: 0.01816, val loss: 0.01857\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = SOSxNN(input_num=10, input_dummy_num=0, subnet_num=10, subnet_arch=[10, 6], task=\"Regression\",\n",
    "               activation_func=tf.tanh, batch_size=1000, training_epochs=5000, lr_bp=0.001, lr_cl=0.1,\n",
    "               beta_threshold=0.01, tuning_epochs=0, l1_proj=0.001, l1_subnet = 0.01, smooth_lambda=10**(-5),\n",
    "               verbose=True, val_ratio=0.2, early_stop_thres=500)\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"test\", train_x)\n",
    "\n",
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "sosxnn_mse_stat = np.hstack([np.round(np.mean((scaler_y.inverse_transform(tr_pred) - scaler_y.inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(val_pred) - scaler_y.inverse_transform(model.val_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(pred_test) - scaler_y.inverse_transform(test_y))**2),5)])\n",
    "print(sosxnn_mse_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = SOSxNN(input_num=10, input_dummy_num=0, subnet_num=10, subnet_arch=[10, 6], task=\"Regression\",\n",
    "               activation_func=tf.tanh, batch_size=1000, training_epochs=5000, lr_bp=0.001, lr_cl=0.1,\n",
    "               beta_threshold=0.01, tuning_epochs=0, l1_proj=0.001, l1_subnet = 0.01, smooth_lambda=10**(-4),\n",
    "               verbose=True, val_ratio=0.2, early_stop_thres=500)\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"test\", train_x)\n",
    "\n",
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "sosxnn_mse_stat = np.hstack([np.round(np.mean((scaler_y.inverse_transform(tr_pred) - scaler_y.inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(val_pred) - scaler_y.inverse_transform(model.val_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(pred_test) - scaler_y.inverse_transform(test_y))**2),5)])\n",
    "print(sosxnn_mse_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model = SOSxNN(input_num=10, input_dummy_num=0, subnet_num=10, subnet_arch=[10, 6], task=\"Regression\",\n",
    "               activation_func=tf.tanh, batch_size=1000, training_epochs=5000, lr_bp=0.001, lr_cl=0.1,\n",
    "               beta_threshold=0.01, tuning_epochs=100, l1_proj=0.001, l1_subnet = 0.01, smooth_lambda=10**(-3),\n",
    "               verbose=True, val_ratio=0.2, early_stop_thres=500)\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"test\", train_x)\n",
    "\n",
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "\n",
    "sosxnn_mse_stat = np.hstack([np.round(np.mean((scaler_y.inverse_transform(tr_pred) - scaler_y.inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((scaler_y.inverse_transform(val_pred) - scaler_y.inverse_transform(model.val_y))**2),5),\\\n",
    "               np.round(np.mean((scaler_y.inverse_transform(pred_test) - scaler_y.inverse_transform(test_y))**2),5)])\n",
    "print(sosxnn_mse_stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
