{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from exnn import GAMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator1(datanum, testnum=10000, noise_sigma=1, rand_seed=0):\n",
    "    \n",
    "    corr = 0.5\n",
    "    np.random.seed(rand_seed)\n",
    "    proj_matrix = np.zeros((10, 4))\n",
    "    proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "    proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "    proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "    proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "    u = np.random.uniform(-1, 1, [datanum + testnum, 1])\n",
    "    t = np.sqrt(corr / (1 - corr))\n",
    "    x = np.zeros((datanum + testnum, 10))\n",
    "    for i in range(10):\n",
    "        x[:, i:i + 1] = (np.random.uniform(-1, 1, [datanum + testnum, 1]) + t * u) / (1 + t)\n",
    "\n",
    "    y = np.reshape(2 * np.dot(x, proj_matrix[:, 0]) + 0.2 * np.exp(-4 * np.dot(x, proj_matrix[:, 1])) + \\\n",
    "                   3 * (np.dot(x, proj_matrix[:, 2]))**2 + 2.5 * np.sin(np.pi * np.dot(x, proj_matrix[:, 3])), [-1, 1]) + \\\n",
    "              noise_sigma * np.random.normal(0, 1, [datanum + testnum, 1])\n",
    "    \n",
    "    task_type = \"Regression\"\n",
    "    meta_info = {\"X1\":{\"type\":\"continuous\"},\n",
    "             \"X2\":{\"type\":\"continuous\"},\n",
    "             \"X3\":{\"type\":\"continuous\"},\n",
    "             \"X4\":{\"type\":\"continuous\"},\n",
    "             \"X5\":{\"type\":\"continuous\"},\n",
    "             \"X6\":{\"type\":\"continuous\"},\n",
    "             \"X7\":{\"type\":\"continuous\"},\n",
    "             \"X8\":{\"type\":\"continuous\"},\n",
    "             \"X9\":{\"type\":\"continuous\"},\n",
    "             \"X10\":{\"type\":\"continuous\"},\n",
    "             \"Y\":{\"type\":\"target\"}}\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == \"target\":\n",
    "            sy = MinMaxScaler((-1, 1))\n",
    "            y = sy.fit_transform(y)\n",
    "            meta_info[key][\"scaler\"] = sy\n",
    "        elif item['type'] == \"categorical\":\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(x[:,[i]])\n",
    "            ordinal_feature = enc.transform(x[:,[i]])\n",
    "            x[:,[i]] = ordinal_feature\n",
    "            meta_info[key][\"values\"] = enc.categories_[0].tolist()\n",
    "        else:\n",
    "            sx = MinMaxScaler((-1, 1))\n",
    "            x[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "            meta_info[key][\"scaler\"] = sx\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=testnum, random_state=rand_seed)\n",
    "    return train_x, test_x, train_y, test_y, task_type, meta_info\n",
    "\n",
    "train_x, test_x, train_y, test_y, task_type, meta_info = data_generator1(datanum=10000, testnum=10000, noise_sigma=1, rand_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1209 18:35:29.541705 140304144590656 deprecation.py:323] From /home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py:330: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1, train loss: 2.64600, val loss: 2.55468\n",
      "Training epoch: 2, train loss: 1.56205, val loss: 1.51959\n",
      "Training epoch: 3, train loss: 0.90727, val loss: 0.89689\n",
      "Training epoch: 4, train loss: 0.52213, val loss: 0.51681\n",
      "Training epoch: 5, train loss: 0.50752, val loss: 0.50266\n",
      "Training epoch: 6, train loss: 0.49964, val loss: 0.49563\n",
      "Training epoch: 7, train loss: 0.49207, val loss: 0.48888\n",
      "Training epoch: 8, train loss: 0.48383, val loss: 0.48039\n",
      "Training epoch: 9, train loss: 0.47556, val loss: 0.47327\n",
      "Training epoch: 10, train loss: 0.46734, val loss: 0.46487\n",
      "Training epoch: 11, train loss: 0.45869, val loss: 0.45635\n",
      "Training epoch: 12, train loss: 0.45070, val loss: 0.44833\n",
      "Training epoch: 13, train loss: 0.44287, val loss: 0.44098\n",
      "Training epoch: 14, train loss: 0.43490, val loss: 0.43333\n",
      "Training epoch: 15, train loss: 0.42679, val loss: 0.42516\n",
      "Training epoch: 16, train loss: 0.41891, val loss: 0.41811\n",
      "Training epoch: 17, train loss: 0.41078, val loss: 0.40979\n",
      "Training epoch: 18, train loss: 0.40399, val loss: 0.40437\n",
      "Training epoch: 19, train loss: 0.39785, val loss: 0.39787\n",
      "Training epoch: 20, train loss: 0.39035, val loss: 0.39118\n",
      "Training epoch: 21, train loss: 0.38266, val loss: 0.38398\n",
      "Training epoch: 22, train loss: 0.37452, val loss: 0.37515\n",
      "Training epoch: 23, train loss: 0.36647, val loss: 0.36827\n",
      "Training epoch: 24, train loss: 0.35740, val loss: 0.35810\n",
      "Training epoch: 25, train loss: 0.34896, val loss: 0.34981\n",
      "Training epoch: 26, train loss: 0.34053, val loss: 0.34218\n",
      "Training epoch: 27, train loss: 0.33275, val loss: 0.33438\n",
      "Training epoch: 28, train loss: 0.32567, val loss: 0.32752\n",
      "Training epoch: 29, train loss: 0.31785, val loss: 0.31911\n",
      "Training epoch: 30, train loss: 0.31106, val loss: 0.31322\n",
      "Training epoch: 31, train loss: 0.30376, val loss: 0.30578\n",
      "Training epoch: 32, train loss: 0.29643, val loss: 0.29852\n",
      "Training epoch: 33, train loss: 0.28952, val loss: 0.29155\n",
      "Training epoch: 34, train loss: 0.28257, val loss: 0.28193\n",
      "Training epoch: 35, train loss: 0.27636, val loss: 0.27189\n",
      "Training epoch: 36, train loss: 0.26887, val loss: 0.26617\n",
      "Training epoch: 37, train loss: 0.26168, val loss: 0.25839\n",
      "Training epoch: 38, train loss: 0.25410, val loss: 0.25168\n",
      "Training epoch: 39, train loss: 0.24622, val loss: 0.24497\n",
      "Training epoch: 40, train loss: 0.23895, val loss: 0.23780\n",
      "Training epoch: 41, train loss: 0.23230, val loss: 0.23099\n",
      "Training epoch: 42, train loss: 0.22590, val loss: 0.22478\n",
      "Training epoch: 43, train loss: 0.21972, val loss: 0.21851\n",
      "Training epoch: 44, train loss: 0.21349, val loss: 0.21262\n",
      "Training epoch: 45, train loss: 0.20778, val loss: 0.20767\n",
      "Training epoch: 46, train loss: 0.20195, val loss: 0.20197\n",
      "Training epoch: 47, train loss: 0.19668, val loss: 0.19721\n",
      "Training epoch: 48, train loss: 0.19121, val loss: 0.19090\n",
      "Training epoch: 49, train loss: 0.18576, val loss: 0.18635\n",
      "Training epoch: 50, train loss: 0.18146, val loss: 0.18271\n",
      "Training epoch: 51, train loss: 0.17529, val loss: 0.17566\n",
      "Training epoch: 52, train loss: 0.17186, val loss: 0.17340\n",
      "Training epoch: 53, train loss: 0.16599, val loss: 0.16569\n",
      "Training epoch: 54, train loss: 0.16160, val loss: 0.16393\n",
      "Training epoch: 55, train loss: 0.15602, val loss: 0.15767\n",
      "Training epoch: 56, train loss: 0.15135, val loss: 0.15005\n",
      "Training epoch: 57, train loss: 0.14724, val loss: 0.14786\n",
      "Training epoch: 58, train loss: 0.14286, val loss: 0.14428\n",
      "Training epoch: 59, train loss: 0.13835, val loss: 0.13734\n",
      "Training epoch: 60, train loss: 0.13255, val loss: 0.13245\n",
      "Training epoch: 61, train loss: 0.10370, val loss: 0.10247\n",
      "Training epoch: 62, train loss: 0.08592, val loss: 0.08712\n",
      "Training epoch: 63, train loss: 0.08294, val loss: 0.08357\n",
      "Training epoch: 64, train loss: 0.08000, val loss: 0.08158\n",
      "Training epoch: 65, train loss: 0.07733, val loss: 0.07892\n",
      "Training epoch: 66, train loss: 0.07498, val loss: 0.07656\n",
      "Training epoch: 67, train loss: 0.07292, val loss: 0.07468\n",
      "Training epoch: 68, train loss: 0.07082, val loss: 0.07243\n",
      "Training epoch: 69, train loss: 0.06879, val loss: 0.07084\n",
      "Training epoch: 70, train loss: 0.06674, val loss: 0.06847\n",
      "Training epoch: 71, train loss: 0.06482, val loss: 0.06679\n",
      "Training epoch: 72, train loss: 0.06302, val loss: 0.06507\n",
      "Training epoch: 73, train loss: 0.06131, val loss: 0.06315\n",
      "Training epoch: 74, train loss: 0.05964, val loss: 0.06157\n",
      "Training epoch: 75, train loss: 0.05799, val loss: 0.05979\n",
      "Training epoch: 76, train loss: 0.05644, val loss: 0.05819\n",
      "Training epoch: 77, train loss: 0.05488, val loss: 0.05658\n",
      "Training epoch: 78, train loss: 0.05343, val loss: 0.05525\n",
      "Training epoch: 79, train loss: 0.05196, val loss: 0.05359\n",
      "Training epoch: 80, train loss: 0.05059, val loss: 0.05218\n",
      "Training epoch: 81, train loss: 0.04923, val loss: 0.05073\n",
      "Training epoch: 82, train loss: 0.04795, val loss: 0.04954\n",
      "Training epoch: 83, train loss: 0.04667, val loss: 0.04812\n",
      "Training epoch: 84, train loss: 0.04550, val loss: 0.04688\n",
      "Training epoch: 85, train loss: 0.04429, val loss: 0.04564\n",
      "Training epoch: 86, train loss: 0.04316, val loss: 0.04458\n",
      "Training epoch: 87, train loss: 0.04205, val loss: 0.04335\n",
      "Training epoch: 88, train loss: 0.04100, val loss: 0.04222\n",
      "Training epoch: 89, train loss: 0.03998, val loss: 0.04127\n",
      "Training epoch: 90, train loss: 0.03901, val loss: 0.04011\n",
      "Training epoch: 91, train loss: 0.03807, val loss: 0.03934\n",
      "Training epoch: 92, train loss: 0.03713, val loss: 0.03817\n",
      "Training epoch: 93, train loss: 0.03625, val loss: 0.03732\n",
      "Training epoch: 94, train loss: 0.03539, val loss: 0.03645\n",
      "Training epoch: 95, train loss: 0.03460, val loss: 0.03573\n",
      "Training epoch: 96, train loss: 0.03383, val loss: 0.03474\n",
      "Training epoch: 97, train loss: 0.03302, val loss: 0.03394\n",
      "Training epoch: 98, train loss: 0.03255, val loss: 0.03361\n",
      "Training epoch: 99, train loss: 0.03179, val loss: 0.03255\n",
      "Training epoch: 100, train loss: 0.03099, val loss: 0.03201\n",
      "Training epoch: 101, train loss: 0.03032, val loss: 0.03123\n",
      "Training epoch: 102, train loss: 0.02969, val loss: 0.03052\n",
      "Training epoch: 103, train loss: 0.02907, val loss: 0.02992\n",
      "Training epoch: 104, train loss: 0.02850, val loss: 0.02939\n",
      "Training epoch: 105, train loss: 0.02794, val loss: 0.02877\n",
      "Training epoch: 106, train loss: 0.02741, val loss: 0.02828\n",
      "Training epoch: 107, train loss: 0.02690, val loss: 0.02775\n",
      "Training epoch: 108, train loss: 0.02644, val loss: 0.02724\n",
      "Training epoch: 109, train loss: 0.02596, val loss: 0.02676\n",
      "Training epoch: 110, train loss: 0.02559, val loss: 0.02633\n",
      "Training epoch: 111, train loss: 0.02524, val loss: 0.02615\n",
      "Training epoch: 112, train loss: 0.02474, val loss: 0.02550\n",
      "Training epoch: 113, train loss: 0.02433, val loss: 0.02510\n",
      "Training epoch: 114, train loss: 0.02402, val loss: 0.02487\n",
      "Training epoch: 115, train loss: 0.02371, val loss: 0.02457\n",
      "Training epoch: 116, train loss: 0.02329, val loss: 0.02405\n",
      "Training epoch: 117, train loss: 0.02328, val loss: 0.02385\n",
      "Training epoch: 118, train loss: 0.02271, val loss: 0.02351\n",
      "Training epoch: 119, train loss: 0.02295, val loss: 0.02388\n",
      "Training epoch: 120, train loss: 0.02236, val loss: 0.02303\n",
      "Training epoch: 121, train loss: 0.02187, val loss: 0.02262\n",
      "Training epoch: 122, train loss: 0.02166, val loss: 0.02244\n",
      "Training epoch: 123, train loss: 0.02141, val loss: 0.02219\n",
      "Training epoch: 124, train loss: 0.02121, val loss: 0.02197\n",
      "Training epoch: 125, train loss: 0.02098, val loss: 0.02171\n",
      "Training epoch: 126, train loss: 0.02080, val loss: 0.02160\n",
      "Training epoch: 127, train loss: 0.02062, val loss: 0.02139\n",
      "Training epoch: 128, train loss: 0.02046, val loss: 0.02121\n",
      "Training epoch: 129, train loss: 0.02031, val loss: 0.02102\n",
      "Training epoch: 130, train loss: 0.02011, val loss: 0.02084\n",
      "Training epoch: 131, train loss: 0.02000, val loss: 0.02079\n",
      "Training epoch: 132, train loss: 0.01990, val loss: 0.02069\n",
      "Training epoch: 133, train loss: 0.01974, val loss: 0.02045\n",
      "Training epoch: 134, train loss: 0.01959, val loss: 0.02035\n",
      "Training epoch: 135, train loss: 0.01948, val loss: 0.02022\n",
      "Training epoch: 136, train loss: 0.01937, val loss: 0.02013\n",
      "Training epoch: 137, train loss: 0.01928, val loss: 0.02004\n",
      "Training epoch: 138, train loss: 0.01917, val loss: 0.01994\n",
      "Training epoch: 139, train loss: 0.01907, val loss: 0.01984\n",
      "Training epoch: 140, train loss: 0.01900, val loss: 0.01974\n",
      "Training epoch: 141, train loss: 0.01891, val loss: 0.01964\n",
      "Training epoch: 142, train loss: 0.01893, val loss: 0.01964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 143, train loss: 0.01877, val loss: 0.01952\n",
      "Training epoch: 144, train loss: 0.01871, val loss: 0.01946\n",
      "Training epoch: 145, train loss: 0.01863, val loss: 0.01940\n",
      "Training epoch: 146, train loss: 0.01875, val loss: 0.01945\n",
      "Training epoch: 147, train loss: 0.01853, val loss: 0.01930\n",
      "Training epoch: 148, train loss: 0.01847, val loss: 0.01923\n",
      "Training epoch: 149, train loss: 0.01841, val loss: 0.01917\n",
      "Training epoch: 150, train loss: 0.01840, val loss: 0.01917\n",
      "Training epoch: 151, train loss: 0.01834, val loss: 0.01913\n",
      "Training epoch: 152, train loss: 0.01829, val loss: 0.01903\n",
      "Training epoch: 153, train loss: 0.01825, val loss: 0.01901\n",
      "Training epoch: 154, train loss: 0.01822, val loss: 0.01901\n",
      "Training epoch: 155, train loss: 0.01834, val loss: 0.01916\n",
      "Training epoch: 156, train loss: 0.01816, val loss: 0.01893\n",
      "Training epoch: 157, train loss: 0.01813, val loss: 0.01888\n",
      "Training epoch: 158, train loss: 0.01821, val loss: 0.01901\n",
      "Training epoch: 159, train loss: 0.01814, val loss: 0.01892\n",
      "Training epoch: 160, train loss: 0.01803, val loss: 0.01879\n",
      "Training epoch: 161, train loss: 0.01801, val loss: 0.01877\n",
      "Training epoch: 162, train loss: 0.01813, val loss: 0.01893\n",
      "Training epoch: 163, train loss: 0.01795, val loss: 0.01872\n",
      "Training epoch: 164, train loss: 0.01819, val loss: 0.01893\n",
      "Training epoch: 165, train loss: 0.01795, val loss: 0.01871\n",
      "Training epoch: 166, train loss: 0.01800, val loss: 0.01879\n"
     ]
    }
   ],
   "source": [
    "model = GAMNet(meta_info=meta_info,\n",
    "               subnet_arch=[10, 6],\n",
    "               task_type=task_type,\n",
    "               activation_func=tf.tanh,\n",
    "               batch_size=min(1000, int(train_x.shape[0] * 0.2)),\n",
    "               training_epochs=10000,\n",
    "               lr_bp=0.001,\n",
    "               beta_threshold=0.05,\n",
    "               tuning_epochs=100,\n",
    "               l1_subnet=0.001,\n",
    "               verbose=True,\n",
    "               val_ratio=0.2,\n",
    "               early_stop_thres=500)\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"gamnet_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "\n",
    "mse_stat = np.hstack([np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(tr_pred) - meta_info[\"Y\"][\"scaler\"].inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(val_pred) - meta_info[\"Y\"][\"scaler\"].inverse_transform(model.val_y))**2),5),\\\n",
    "               np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(pred_test) - meta_info[\"Y\"][\"scaler\"].inverse_transform(test_y))**2),5)])\n",
    "print(mse_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
