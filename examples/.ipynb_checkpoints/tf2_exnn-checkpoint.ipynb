{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "from exnn import ExNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(label, pred, scaler):\n",
    "    pred = scaler.inverse_transform(pred.reshape([-1, 1]))\n",
    "    label = scaler.inverse_transform(label.reshape([-1, 1]))\n",
    "    return np.mean((pred - label)**2)\n",
    "\n",
    "def simu_loader(generator, datanum, testnum, noise_sigma):\n",
    "    def wrapper(rand_seed=0):\n",
    "        return generator(datanum, testnum=testnum, noise_sigma=noise_sigma, rand_seed=rand_seed)\n",
    "    return wrapper\n",
    "\n",
    "def data_generator1(datanum, testnum=10000, noise_sigma=1, rand_seed=0):\n",
    "    \n",
    "    corr = 0.5\n",
    "    np.random.seed(rand_seed)\n",
    "    proj_matrix = np.zeros((10, 4))\n",
    "    proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "    proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "    proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "    proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "    u = np.random.uniform(-1, 1, [datanum + testnum, 1])\n",
    "    t = np.sqrt(corr / (1 - corr))\n",
    "    x = np.zeros((datanum + testnum, 10))\n",
    "    for i in range(10):\n",
    "        x[:, i:i + 1] = (np.random.uniform(-1, 1, [datanum + testnum, 1]) + t * u) / (1 + t)\n",
    "\n",
    "    y = np.reshape(2 * np.dot(x, proj_matrix[:, 0]) + 0.2 * np.exp(-4 * np.dot(x, proj_matrix[:, 1])) + \\\n",
    "                   3 * (np.dot(x, proj_matrix[:, 2]))**2 + 2.5 * np.sin(np.pi * np.dot(x, proj_matrix[:, 3])), [-1, 1]) + \\\n",
    "              noise_sigma * np.random.normal(0, 1, [datanum + testnum, 1])\n",
    "    \n",
    "    task_type = \"Regression\"\n",
    "    meta_info = {\"X1\":{\"type\":\"continuous\"},\n",
    "             \"X2\":{\"type\":\"continuous\"},\n",
    "             \"X3\":{\"type\":\"continuous\"},\n",
    "             \"X4\":{\"type\":\"continuous\"},\n",
    "             \"X5\":{\"type\":\"continuous\"},\n",
    "             \"X6\":{\"type\":\"continuous\"},\n",
    "             \"X7\":{\"type\":\"continuous\"},\n",
    "             \"X8\":{\"type\":\"continuous\"},\n",
    "             \"X9\":{\"type\":\"continuous\"},\n",
    "             \"X10\":{\"type\":\"continuous\"},\n",
    "             \"Y\":{\"type\":\"target\"}}\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == \"target\":\n",
    "            sy = MinMaxScaler((-1, 1))\n",
    "            y = sy.fit_transform(y)\n",
    "            meta_info[key][\"scaler\"] = sy\n",
    "        elif item['type'] == \"categorical\":\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(x[:,[i]])\n",
    "            ordinal_feature = enc.transform(x[:,[i]])\n",
    "            x[:,[i]] = ordinal_feature\n",
    "            meta_info[key][\"values\"] = enc.categories_[0].tolist()\n",
    "        else:\n",
    "            sx = MinMaxScaler((-1, 1))\n",
    "            x[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "            meta_info[key][\"scaler\"] = sx\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=testnum, random_state=rand_seed)\n",
    "    return train_x, test_x, train_y, test_y, task_type, meta_info\n",
    "\n",
    "train_x, test_x, train_y, test_y, task_type, meta_info = data_generator1(datanum=10000, testnum=10000, noise_sigma=1, rand_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exnn_repeat(folder, name, data_generator,\n",
    "                  subnet_num=10,\n",
    "                  subnet_arch=[10, 6],\n",
    "                  task=\"Regression\",\n",
    "                  activation_func=tf.tanh,\n",
    "                  lr_bp=0.001,\n",
    "                  lr_cl=0.1,\n",
    "                  l1_proj=0.001,\n",
    "                  l1_subnet=0.001,\n",
    "                  smooth_lambda=0.00001,\n",
    "                  batch_size=1000,\n",
    "                  training_epochs=5000,\n",
    "                  tuning_epochs=500,\n",
    "                  beta_threshold=0.05,\n",
    "                  verbose=False,\n",
    "                  val_ratio=0.2,\n",
    "                  early_stop_thres=1000,\n",
    "                  rand_seed=0):\n",
    "\n",
    "    train_x, test_x, train_y, test_y, task_type, meta_info = data_generator(rand_seed)\n",
    "\n",
    "    input_num = train_x.shape[1]\n",
    "    model = ExNN(meta_info=meta_info,\n",
    "                   subnet_num=10,\n",
    "                   subnet_arch=subnet_arch,\n",
    "                   task_type=task_type,\n",
    "                   activation_func=tf.tanh,\n",
    "                   batch_size=min(batch_size, int(train_x.shape[0] * 0.2)),\n",
    "                   training_epochs=training_epochs,\n",
    "                   lr_bp=lr_bp,\n",
    "                   lr_cl=lr_cl,\n",
    "                   beta_threshold=beta_threshold,\n",
    "                   tuning_epochs=tuning_epochs,\n",
    "                   l1_proj=l1_proj,\n",
    "                   l1_subnet=l1_subnet,\n",
    "                   smooth_lambda=smooth_lambda,\n",
    "                   verbose=verbose,\n",
    "                   val_ratio=val_ratio,\n",
    "                   early_stop_thres=early_stop_thres)\n",
    "    model.fit(train_x, train_y)\n",
    "    model.visualize(folder=folder,\n",
    "                    name=name,\n",
    "                    save_eps=True)\n",
    "\n",
    "    tr_pred = model.predict(model.tr_x)\n",
    "    val_pred = model.predict(model.val_x)\n",
    "    pred_test = model.predict(test_x)\n",
    "\n",
    "    if task_type == \"Regression\":\n",
    "        stat = np.hstack([np.round(mse(model.tr_y, tr_pred, meta_info[\"Y\"][\"scaler\"]), 5),\\\n",
    "                              np.round(mse(model.val_y, val_pred, meta_info[\"Y\"][\"scaler\"]), 5),\\\n",
    "                              np.round(mse(test_y, pred_test, meta_info[\"Y\"][\"scaler\"]), 5)])\n",
    "    elif task_type == \"Classification\":\n",
    "        stat = np.hstack([np.round(auc(model.tr_y, tr_pred), 5),\\\n",
    "                          np.round(auc(model.val_y, val_pred), 5),\\\n",
    "                          np.round(auc(test_y, pred_test), 5)])\n",
    "\n",
    "    res_stat = pd.DataFrame(np.vstack([stat[0], stat[1], stat[2]]).T, columns=['train_metric', \"val_metric\", \"test_metric\"])\n",
    "    res_stat[\"Subnet_Number\"] = min(input_num, 10)\n",
    "    res_stat[\"lr_BP\"] = lr_bp\n",
    "    res_stat[\"lr_CL\"] = lr_cl\n",
    "    res_stat[\"L1_Penalty_Proj\"] = l1_proj\n",
    "    res_stat[\"L1_Penalty_Subnet\"] = l1_subnet\n",
    "    res_stat[\"Smooth_labmda\"] = smooth_lambda\n",
    "    res_stat[\"Training_Epochs\"] = training_epochs\n",
    "    return res_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_metric</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>Subnet_Number</th>\n",
       "      <th>lr_BP</th>\n",
       "      <th>lr_CL</th>\n",
       "      <th>L1_Penalty_Proj</th>\n",
       "      <th>L1_Penalty_Subnet</th>\n",
       "      <th>Smooth_labmda</th>\n",
       "      <th>Training_Epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.01115</td>\n",
       "      <td>1.03267</td>\n",
       "      <td>0.99258</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00916</td>\n",
       "      <td>1.03276</td>\n",
       "      <td>0.99327</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00782</td>\n",
       "      <td>1.03336</td>\n",
       "      <td>0.98987</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.01251</td>\n",
       "      <td>1.03609</td>\n",
       "      <td>0.99334</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.01786</td>\n",
       "      <td>1.05360</td>\n",
       "      <td>1.00659</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.03170</td>\n",
       "      <td>1.06093</td>\n",
       "      <td>1.01467</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.03731</td>\n",
       "      <td>1.06140</td>\n",
       "      <td>1.01464</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.02535</td>\n",
       "      <td>1.06308</td>\n",
       "      <td>1.01606</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.03461</td>\n",
       "      <td>1.06404</td>\n",
       "      <td>1.01676</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.02708</td>\n",
       "      <td>1.06512</td>\n",
       "      <td>1.01828</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.03715</td>\n",
       "      <td>1.06828</td>\n",
       "      <td>1.01898</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.05116</td>\n",
       "      <td>1.07347</td>\n",
       "      <td>1.02679</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.04661</td>\n",
       "      <td>1.08390</td>\n",
       "      <td>1.03113</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.07626</td>\n",
       "      <td>1.10814</td>\n",
       "      <td>1.05510</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08089</td>\n",
       "      <td>1.11493</td>\n",
       "      <td>1.05860</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13172</td>\n",
       "      <td>1.16976</td>\n",
       "      <td>1.10615</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13382</td>\n",
       "      <td>1.17234</td>\n",
       "      <td>1.11041</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13263</td>\n",
       "      <td>1.17311</td>\n",
       "      <td>1.11077</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13210</td>\n",
       "      <td>1.17343</td>\n",
       "      <td>1.11078</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13641</td>\n",
       "      <td>1.17789</td>\n",
       "      <td>1.11313</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.13648</td>\n",
       "      <td>1.17792</td>\n",
       "      <td>1.11329</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.14031</td>\n",
       "      <td>1.18092</td>\n",
       "      <td>1.11876</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.14741</td>\n",
       "      <td>1.18782</td>\n",
       "      <td>1.12032</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.14826</td>\n",
       "      <td>1.18807</td>\n",
       "      <td>1.12112</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.15019</td>\n",
       "      <td>1.18807</td>\n",
       "      <td>1.12244</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_metric  val_metric  test_metric  Subnet_Number  lr_BP  lr_CL  \\\n",
       "0       1.01115     1.03267      0.99258             10  0.001    0.1   \n",
       "0       1.00916     1.03276      0.99327             10  0.001    0.1   \n",
       "0       1.00782     1.03336      0.98987             10  0.001    0.1   \n",
       "0       1.01251     1.03609      0.99334             10  0.001    0.1   \n",
       "0       1.01786     1.05360      1.00659             10  0.001    0.1   \n",
       "0       1.03170     1.06093      1.01467             10  0.001    0.1   \n",
       "0       1.03731     1.06140      1.01464             10  0.001    0.1   \n",
       "0       1.02535     1.06308      1.01606             10  0.001    0.1   \n",
       "0       1.03461     1.06404      1.01676             10  0.001    0.1   \n",
       "0       1.02708     1.06512      1.01828             10  0.001    0.1   \n",
       "0       1.03715     1.06828      1.01898             10  0.001    0.1   \n",
       "0       1.05116     1.07347      1.02679             10  0.001    0.1   \n",
       "0       1.04661     1.08390      1.03113             10  0.001    0.1   \n",
       "0       1.07626     1.10814      1.05510             10  0.001    0.1   \n",
       "0       1.08089     1.11493      1.05860             10  0.001    0.1   \n",
       "0       1.13172     1.16976      1.10615             10  0.001    0.1   \n",
       "0       1.13382     1.17234      1.11041             10  0.001    0.1   \n",
       "0       1.13263     1.17311      1.11077             10  0.001    0.1   \n",
       "0       1.13210     1.17343      1.11078             10  0.001    0.1   \n",
       "0       1.13641     1.17789      1.11313             10  0.001    0.1   \n",
       "0       1.13648     1.17792      1.11329             10  0.001    0.1   \n",
       "0       1.14031     1.18092      1.11876             10  0.001    0.1   \n",
       "0       1.14741     1.18782      1.12032             10  0.001    0.1   \n",
       "0       1.14826     1.18807      1.12112             10  0.001    0.1   \n",
       "0       1.15019     1.18807      1.12244             10  0.001    0.1   \n",
       "\n",
       "   L1_Penalty_Proj  L1_Penalty_Subnet  Smooth_labmda  Training_Epochs  \n",
       "0         0.000316           0.003162       0.000001            10000  \n",
       "0         0.000100           0.003162       0.000001            10000  \n",
       "0         0.000100           0.010000       0.000001            10000  \n",
       "0         0.000316           0.010000       0.000001            10000  \n",
       "0         0.000100           0.001000       0.000001            10000  \n",
       "0         0.000316           0.001000       0.000001            10000  \n",
       "0         0.001000           0.010000       0.000001            10000  \n",
       "0         0.000100           0.000316       0.000001            10000  \n",
       "0         0.000316           0.000316       0.000001            10000  \n",
       "0         0.000100           0.000100       0.000001            10000  \n",
       "0         0.000316           0.000100       0.000001            10000  \n",
       "0         0.001000           0.003162       0.000001            10000  \n",
       "0         0.001000           0.000100       0.000001            10000  \n",
       "0         0.001000           0.001000       0.000001            10000  \n",
       "0         0.001000           0.000316       0.000001            10000  \n",
       "0         0.010000           0.000316       0.000001            10000  \n",
       "0         0.003162           0.010000       0.000001            10000  \n",
       "0         0.003162           0.003162       0.000001            10000  \n",
       "0         0.003162           0.000316       0.000001            10000  \n",
       "0         0.003162           0.000100       0.000001            10000  \n",
       "0         0.003162           0.001000       0.000001            10000  \n",
       "0         0.010000           0.000100       0.000001            10000  \n",
       "0         0.010000           0.003162       0.000001            10000  \n",
       "0         0.010000           0.001000       0.000001            10000  \n",
       "0         0.010000           0.010000       0.000001            10000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = Parallel(n_jobs=10)(delayed(exnn_repeat)(folder=\"./results/S1_exnn/\",\n",
    "                      name=str(i + 1).zfill(2) + \"_\" + str(j + 1).zfill(2),\n",
    "                      data_generator=simu_loader(data_generator1, 10000, 10000, 1),\n",
    "                      task=task_type,\n",
    "                      subnet_arch=[10, 6],\n",
    "                      beta_threshold=0.05,\n",
    "                      l1_proj=10**(-2 - i*0.5),\n",
    "                      l1_subnet=10**(-2 - j*0.5),\n",
    "                      smooth_lambda=10**(-5 - k),\n",
    "                      training_epochs=10000,\n",
    "                      lr_bp=0.001,\n",
    "                      lr_cl=0.1,\n",
    "                      batch_size=1000,\n",
    "                      early_stop_thres=500,\n",
    "                      tuning_epochs=100,\n",
    "                      rand_seed=0) for i in range(5) for j in range(5) for k in [1])\n",
    "exnn_stat_all = pd.concat(cv_results)\n",
    "exnn_stat_all.sort_values(\"val_metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_l1_prob = exnn_stat_all.sort_values(\"val_metric\").loc[:,\"L1_Penalty_Proj\"].iloc[0]\n",
    "best_l1_subnet = exnn_stat_all.sort_values(\"val_metric\").loc[:,\"L1_Penalty_Subnet\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1209 18:57:40.577913 139977503205184 deprecation.py:323] From /home/r7user1/anaconda2_local/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py:330: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1, train loss: 0.16716, val loss: 0.16760\n",
      "Training epoch: 2, train loss: 0.13424, val loss: 0.13548\n",
      "Training epoch: 3, train loss: 0.11456, val loss: 0.11571\n",
      "Training epoch: 4, train loss: 0.09981, val loss: 0.10082\n",
      "Training epoch: 5, train loss: 0.08709, val loss: 0.08770\n",
      "Training epoch: 6, train loss: 0.07618, val loss: 0.07620\n",
      "Training epoch: 7, train loss: 0.06944, val loss: 0.06937\n",
      "Training epoch: 8, train loss: 0.06408, val loss: 0.06399\n",
      "Training epoch: 9, train loss: 0.05923, val loss: 0.05918\n",
      "Training epoch: 10, train loss: 0.05566, val loss: 0.05602\n",
      "Training epoch: 11, train loss: 0.05226, val loss: 0.05260\n",
      "Training epoch: 12, train loss: 0.04936, val loss: 0.04981\n",
      "Training epoch: 13, train loss: 0.04689, val loss: 0.04731\n",
      "Training epoch: 14, train loss: 0.04469, val loss: 0.04491\n",
      "Training epoch: 15, train loss: 0.04270, val loss: 0.04296\n",
      "Training epoch: 16, train loss: 0.04093, val loss: 0.04126\n",
      "Training epoch: 17, train loss: 0.03939, val loss: 0.03971\n",
      "Training epoch: 18, train loss: 0.03799, val loss: 0.03813\n",
      "Training epoch: 19, train loss: 0.03673, val loss: 0.03686\n",
      "Training epoch: 20, train loss: 0.03559, val loss: 0.03576\n",
      "Training epoch: 21, train loss: 0.03454, val loss: 0.03476\n",
      "Training epoch: 22, train loss: 0.03358, val loss: 0.03374\n",
      "Training epoch: 23, train loss: 0.03270, val loss: 0.03288\n",
      "Training epoch: 24, train loss: 0.03189, val loss: 0.03203\n",
      "Training epoch: 25, train loss: 0.03116, val loss: 0.03125\n",
      "Training epoch: 26, train loss: 0.03045, val loss: 0.03058\n",
      "Training epoch: 27, train loss: 0.02983, val loss: 0.03000\n",
      "Training epoch: 28, train loss: 0.02925, val loss: 0.02937\n",
      "Training epoch: 29, train loss: 0.02868, val loss: 0.02886\n",
      "Training epoch: 30, train loss: 0.02815, val loss: 0.02833\n",
      "Training epoch: 31, train loss: 0.02773, val loss: 0.02790\n",
      "Training epoch: 32, train loss: 0.02723, val loss: 0.02747\n",
      "Training epoch: 33, train loss: 0.02687, val loss: 0.02708\n",
      "Training epoch: 34, train loss: 0.02646, val loss: 0.02669\n",
      "Training epoch: 35, train loss: 0.02611, val loss: 0.02631\n",
      "Training epoch: 36, train loss: 0.02581, val loss: 0.02611\n",
      "Training epoch: 37, train loss: 0.02548, val loss: 0.02567\n",
      "Training epoch: 38, train loss: 0.02520, val loss: 0.02548\n",
      "Training epoch: 39, train loss: 0.02486, val loss: 0.02512\n",
      "Training epoch: 40, train loss: 0.02459, val loss: 0.02488\n",
      "Training epoch: 41, train loss: 0.02438, val loss: 0.02466\n",
      "Training epoch: 42, train loss: 0.02424, val loss: 0.02450\n",
      "Training epoch: 43, train loss: 0.02392, val loss: 0.02424\n",
      "Training epoch: 44, train loss: 0.02377, val loss: 0.02407\n",
      "Training epoch: 45, train loss: 0.02359, val loss: 0.02389\n",
      "Training epoch: 46, train loss: 0.02345, val loss: 0.02377\n",
      "Training epoch: 47, train loss: 0.02321, val loss: 0.02354\n",
      "Training epoch: 48, train loss: 0.02308, val loss: 0.02338\n",
      "Training epoch: 49, train loss: 0.02292, val loss: 0.02328\n",
      "Training epoch: 50, train loss: 0.02279, val loss: 0.02315\n",
      "Training epoch: 51, train loss: 0.02269, val loss: 0.02300\n",
      "Training epoch: 52, train loss: 0.02257, val loss: 0.02296\n",
      "Training epoch: 53, train loss: 0.02262, val loss: 0.02296\n",
      "Training epoch: 54, train loss: 0.02248, val loss: 0.02285\n",
      "Training epoch: 55, train loss: 0.02229, val loss: 0.02265\n",
      "Training epoch: 56, train loss: 0.02231, val loss: 0.02264\n",
      "Training epoch: 57, train loss: 0.02207, val loss: 0.02243\n",
      "Training epoch: 58, train loss: 0.02197, val loss: 0.02236\n",
      "Training epoch: 59, train loss: 0.02192, val loss: 0.02232\n",
      "Training epoch: 60, train loss: 0.02182, val loss: 0.02218\n",
      "Training epoch: 61, train loss: 0.02180, val loss: 0.02218\n",
      "Training epoch: 62, train loss: 0.02167, val loss: 0.02206\n",
      "Training epoch: 63, train loss: 0.02158, val loss: 0.02198\n",
      "Training epoch: 64, train loss: 0.02152, val loss: 0.02192\n",
      "Training epoch: 65, train loss: 0.02152, val loss: 0.02190\n",
      "Training epoch: 66, train loss: 0.02148, val loss: 0.02186\n",
      "Training epoch: 67, train loss: 0.02138, val loss: 0.02178\n",
      "Training epoch: 68, train loss: 0.02143, val loss: 0.02182\n",
      "Training epoch: 69, train loss: 0.02144, val loss: 0.02186\n",
      "Training epoch: 70, train loss: 0.02140, val loss: 0.02176\n",
      "Training epoch: 71, train loss: 0.02131, val loss: 0.02175\n",
      "Training epoch: 72, train loss: 0.02124, val loss: 0.02162\n",
      "Training epoch: 73, train loss: 0.02123, val loss: 0.02163\n",
      "Training epoch: 74, train loss: 0.02116, val loss: 0.02157\n",
      "Training epoch: 75, train loss: 0.02110, val loss: 0.02151\n",
      "Training epoch: 76, train loss: 0.02102, val loss: 0.02145\n",
      "Training epoch: 77, train loss: 0.02106, val loss: 0.02146\n",
      "Training epoch: 78, train loss: 0.02102, val loss: 0.02144\n",
      "Training epoch: 79, train loss: 0.02095, val loss: 0.02135\n",
      "Training epoch: 80, train loss: 0.02092, val loss: 0.02132\n",
      "Training epoch: 81, train loss: 0.02089, val loss: 0.02135\n",
      "Training epoch: 82, train loss: 0.02090, val loss: 0.02134\n",
      "Training epoch: 83, train loss: 0.02083, val loss: 0.02128\n",
      "Training epoch: 84, train loss: 0.02077, val loss: 0.02122\n",
      "Training epoch: 85, train loss: 0.02075, val loss: 0.02118\n",
      "Training epoch: 86, train loss: 0.02078, val loss: 0.02123\n",
      "Training epoch: 87, train loss: 0.02081, val loss: 0.02124\n",
      "Training epoch: 88, train loss: 0.02073, val loss: 0.02117\n",
      "Training epoch: 89, train loss: 0.02078, val loss: 0.02123\n",
      "Training epoch: 90, train loss: 0.02065, val loss: 0.02111\n",
      "Training epoch: 91, train loss: 0.02070, val loss: 0.02113\n",
      "Training epoch: 92, train loss: 0.02062, val loss: 0.02110\n",
      "Training epoch: 93, train loss: 0.02064, val loss: 0.02109\n",
      "Training epoch: 94, train loss: 0.02071, val loss: 0.02117\n",
      "Training epoch: 95, train loss: 0.02053, val loss: 0.02097\n",
      "Training epoch: 96, train loss: 0.02053, val loss: 0.02101\n",
      "Training epoch: 97, train loss: 0.02050, val loss: 0.02095\n",
      "Training epoch: 98, train loss: 0.02049, val loss: 0.02098\n",
      "Training epoch: 99, train loss: 0.02045, val loss: 0.02093\n",
      "Training epoch: 100, train loss: 0.02042, val loss: 0.02089\n",
      "Training epoch: 101, train loss: 0.02038, val loss: 0.02087\n",
      "Training epoch: 102, train loss: 0.02063, val loss: 0.02113\n",
      "Training epoch: 103, train loss: 0.02035, val loss: 0.02079\n",
      "Training epoch: 104, train loss: 0.02034, val loss: 0.02084\n",
      "Training epoch: 105, train loss: 0.02036, val loss: 0.02087\n",
      "Training epoch: 106, train loss: 0.02033, val loss: 0.02079\n",
      "Training epoch: 107, train loss: 0.02033, val loss: 0.02078\n",
      "Training epoch: 108, train loss: 0.02025, val loss: 0.02077\n",
      "Training epoch: 109, train loss: 0.02028, val loss: 0.02070\n",
      "Training epoch: 110, train loss: 0.02026, val loss: 0.02080\n",
      "Training epoch: 111, train loss: 0.02023, val loss: 0.02074\n",
      "Training epoch: 112, train loss: 0.02018, val loss: 0.02067\n",
      "Training epoch: 113, train loss: 0.02016, val loss: 0.02064\n",
      "Training epoch: 114, train loss: 0.02016, val loss: 0.02067\n",
      "Training epoch: 115, train loss: 0.02013, val loss: 0.02063\n",
      "Training epoch: 116, train loss: 0.02014, val loss: 0.02060\n",
      "Training epoch: 117, train loss: 0.02012, val loss: 0.02063\n",
      "Training epoch: 118, train loss: 0.02023, val loss: 0.02073\n",
      "Training epoch: 119, train loss: 0.02005, val loss: 0.02055\n",
      "Training epoch: 120, train loss: 0.02003, val loss: 0.02051\n",
      "Training epoch: 121, train loss: 0.02027, val loss: 0.02074\n",
      "Training epoch: 122, train loss: 0.02000, val loss: 0.02053\n",
      "Training epoch: 123, train loss: 0.01999, val loss: 0.02050\n",
      "Training epoch: 124, train loss: 0.02008, val loss: 0.02061\n",
      "Training epoch: 125, train loss: 0.02029, val loss: 0.02081\n",
      "Training epoch: 126, train loss: 0.01997, val loss: 0.02048\n",
      "Training epoch: 127, train loss: 0.02015, val loss: 0.02065\n",
      "Training epoch: 128, train loss: 0.01992, val loss: 0.02043\n",
      "Training epoch: 129, train loss: 0.02015, val loss: 0.02067\n",
      "Training epoch: 130, train loss: 0.01987, val loss: 0.02037\n",
      "Training epoch: 131, train loss: 0.01985, val loss: 0.02039\n",
      "Training epoch: 132, train loss: 0.02008, val loss: 0.02056\n",
      "Training epoch: 133, train loss: 0.01984, val loss: 0.02037\n",
      "Training epoch: 134, train loss: 0.01980, val loss: 0.02033\n",
      "Training epoch: 135, train loss: 0.01980, val loss: 0.02031\n",
      "Training epoch: 136, train loss: 0.01984, val loss: 0.02036\n",
      "Training epoch: 137, train loss: 0.01984, val loss: 0.02038\n",
      "Training epoch: 138, train loss: 0.01979, val loss: 0.02031\n",
      "Training epoch: 139, train loss: 0.01975, val loss: 0.02027\n",
      "Training epoch: 140, train loss: 0.01971, val loss: 0.02024\n",
      "Training epoch: 141, train loss: 0.01978, val loss: 0.02029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 142, train loss: 0.01980, val loss: 0.02036\n",
      "Training epoch: 143, train loss: 0.01968, val loss: 0.02021\n",
      "Training epoch: 144, train loss: 0.01974, val loss: 0.02025\n",
      "Training epoch: 145, train loss: 0.01969, val loss: 0.02026\n",
      "Training epoch: 146, train loss: 0.01972, val loss: 0.02025\n",
      "Training epoch: 147, train loss: 0.01968, val loss: 0.02023\n",
      "Training epoch: 148, train loss: 0.01970, val loss: 0.02022\n",
      "Training epoch: 149, train loss: 0.01956, val loss: 0.02009\n",
      "Training epoch: 150, train loss: 0.01967, val loss: 0.02019\n",
      "Training epoch: 151, train loss: 0.01972, val loss: 0.02028\n",
      "Training epoch: 152, train loss: 0.01956, val loss: 0.02009\n",
      "Training epoch: 153, train loss: 0.01953, val loss: 0.02006\n",
      "Training epoch: 154, train loss: 0.01950, val loss: 0.02005\n",
      "Training epoch: 155, train loss: 0.01946, val loss: 0.02000\n",
      "Training epoch: 156, train loss: 0.01956, val loss: 0.02010\n",
      "Training epoch: 157, train loss: 0.01945, val loss: 0.02000\n",
      "Training epoch: 158, train loss: 0.01944, val loss: 0.01998\n",
      "Training epoch: 159, train loss: 0.01945, val loss: 0.01997\n",
      "Training epoch: 160, train loss: 0.01939, val loss: 0.01997\n",
      "Training epoch: 161, train loss: 0.01935, val loss: 0.01991\n",
      "Training epoch: 162, train loss: 0.01937, val loss: 0.01988\n",
      "Training epoch: 163, train loss: 0.01933, val loss: 0.01990\n",
      "Training epoch: 164, train loss: 0.01946, val loss: 0.02002\n",
      "Training epoch: 165, train loss: 0.01940, val loss: 0.01994\n",
      "Training epoch: 166, train loss: 0.01928, val loss: 0.01985\n",
      "Training epoch: 167, train loss: 0.01928, val loss: 0.01983\n",
      "Training epoch: 168, train loss: 0.01927, val loss: 0.01981\n",
      "Training epoch: 169, train loss: 0.01924, val loss: 0.01982\n",
      "Training epoch: 170, train loss: 0.01921, val loss: 0.01976\n",
      "Training epoch: 171, train loss: 0.01920, val loss: 0.01976\n",
      "Training epoch: 172, train loss: 0.01921, val loss: 0.01977\n",
      "Training epoch: 173, train loss: 0.01916, val loss: 0.01974\n",
      "Training epoch: 174, train loss: 0.01920, val loss: 0.01975\n",
      "Training epoch: 175, train loss: 0.01912, val loss: 0.01970\n",
      "Training epoch: 176, train loss: 0.01913, val loss: 0.01970\n",
      "Training epoch: 177, train loss: 0.01910, val loss: 0.01969\n",
      "Training epoch: 178, train loss: 0.01909, val loss: 0.01965\n",
      "Training epoch: 179, train loss: 0.01906, val loss: 0.01964\n",
      "Training epoch: 180, train loss: 0.01903, val loss: 0.01958\n",
      "Training epoch: 181, train loss: 0.01903, val loss: 0.01962\n",
      "Training epoch: 182, train loss: 0.01908, val loss: 0.01965\n",
      "Training epoch: 183, train loss: 0.01922, val loss: 0.01981\n",
      "Training epoch: 184, train loss: 0.01898, val loss: 0.01955\n",
      "Training epoch: 185, train loss: 0.01896, val loss: 0.01957\n",
      "Training epoch: 186, train loss: 0.01894, val loss: 0.01954\n",
      "Training epoch: 187, train loss: 0.01910, val loss: 0.01969\n",
      "Training epoch: 188, train loss: 0.01908, val loss: 0.01970\n",
      "Training epoch: 189, train loss: 0.01916, val loss: 0.01979\n",
      "Training epoch: 190, train loss: 0.01889, val loss: 0.01947\n",
      "Training epoch: 191, train loss: 0.01893, val loss: 0.01954\n",
      "Training epoch: 192, train loss: 0.01891, val loss: 0.01949\n",
      "Training epoch: 193, train loss: 0.01893, val loss: 0.01953\n",
      "Training epoch: 194, train loss: 0.01889, val loss: 0.01947\n",
      "Training epoch: 195, train loss: 0.01880, val loss: 0.01942\n",
      "Training epoch: 196, train loss: 0.01876, val loss: 0.01935\n",
      "Training epoch: 197, train loss: 0.01878, val loss: 0.01936\n",
      "Training epoch: 198, train loss: 0.01877, val loss: 0.01938\n",
      "Training epoch: 199, train loss: 0.01872, val loss: 0.01931\n",
      "Training epoch: 200, train loss: 0.01873, val loss: 0.01936\n",
      "Training epoch: 201, train loss: 0.01867, val loss: 0.01929\n",
      "Training epoch: 202, train loss: 0.01876, val loss: 0.01939\n",
      "Training epoch: 203, train loss: 0.01865, val loss: 0.01928\n",
      "Training epoch: 204, train loss: 0.01862, val loss: 0.01922\n",
      "Training epoch: 205, train loss: 0.01865, val loss: 0.01926\n",
      "Training epoch: 206, train loss: 0.01860, val loss: 0.01923\n",
      "Training epoch: 207, train loss: 0.01856, val loss: 0.01918\n",
      "Training epoch: 208, train loss: 0.01854, val loss: 0.01916\n",
      "Training epoch: 209, train loss: 0.01855, val loss: 0.01918\n",
      "Training epoch: 210, train loss: 0.01865, val loss: 0.01927\n",
      "Training epoch: 211, train loss: 0.01848, val loss: 0.01910\n",
      "Training epoch: 212, train loss: 0.01846, val loss: 0.01908\n",
      "Training epoch: 213, train loss: 0.01846, val loss: 0.01912\n",
      "Training epoch: 214, train loss: 0.01847, val loss: 0.01911\n",
      "Training epoch: 215, train loss: 0.01843, val loss: 0.01907\n",
      "Training epoch: 216, train loss: 0.01840, val loss: 0.01907\n",
      "Training epoch: 217, train loss: 0.01844, val loss: 0.01906\n",
      "Training epoch: 218, train loss: 0.01837, val loss: 0.01901\n",
      "Training epoch: 219, train loss: 0.01834, val loss: 0.01896\n",
      "Training epoch: 220, train loss: 0.01832, val loss: 0.01899\n",
      "Training epoch: 221, train loss: 0.01847, val loss: 0.01916\n",
      "Training epoch: 222, train loss: 0.01835, val loss: 0.01902\n",
      "Training epoch: 223, train loss: 0.01826, val loss: 0.01892\n",
      "Training epoch: 224, train loss: 0.01830, val loss: 0.01899\n",
      "Training epoch: 225, train loss: 0.01824, val loss: 0.01891\n",
      "Training epoch: 226, train loss: 0.01820, val loss: 0.01886\n",
      "Training epoch: 227, train loss: 0.01818, val loss: 0.01885\n",
      "Training epoch: 228, train loss: 0.01826, val loss: 0.01889\n",
      "Training epoch: 229, train loss: 0.01825, val loss: 0.01895\n",
      "Training epoch: 230, train loss: 0.01813, val loss: 0.01880\n",
      "Training epoch: 231, train loss: 0.01810, val loss: 0.01877\n",
      "Training epoch: 232, train loss: 0.01807, val loss: 0.01873\n",
      "Training epoch: 233, train loss: 0.01807, val loss: 0.01876\n",
      "Training epoch: 234, train loss: 0.01803, val loss: 0.01869\n",
      "Training epoch: 235, train loss: 0.01843, val loss: 0.01905\n",
      "Training epoch: 236, train loss: 0.01847, val loss: 0.01911\n",
      "Training epoch: 237, train loss: 0.01830, val loss: 0.01897\n",
      "Training epoch: 238, train loss: 0.01797, val loss: 0.01864\n",
      "Training epoch: 239, train loss: 0.01802, val loss: 0.01872\n",
      "Training epoch: 240, train loss: 0.01793, val loss: 0.01863\n",
      "Training epoch: 241, train loss: 0.01798, val loss: 0.01862\n",
      "Training epoch: 242, train loss: 0.01789, val loss: 0.01860\n",
      "Training epoch: 243, train loss: 0.01786, val loss: 0.01855\n",
      "Training epoch: 244, train loss: 0.01783, val loss: 0.01852\n",
      "Training epoch: 245, train loss: 0.01829, val loss: 0.01907\n",
      "Training epoch: 246, train loss: 0.01784, val loss: 0.01855\n",
      "Training epoch: 247, train loss: 0.01782, val loss: 0.01849\n",
      "Training epoch: 248, train loss: 0.01775, val loss: 0.01846\n",
      "Training epoch: 249, train loss: 0.01776, val loss: 0.01848\n",
      "Training epoch: 250, train loss: 0.01834, val loss: 0.01898\n",
      "Training epoch: 251, train loss: 0.01819, val loss: 0.01888\n",
      "Training epoch: 252, train loss: 0.01770, val loss: 0.01842\n",
      "Training epoch: 253, train loss: 0.01768, val loss: 0.01837\n",
      "Training epoch: 254, train loss: 0.01767, val loss: 0.01841\n",
      "Training epoch: 255, train loss: 0.01763, val loss: 0.01836\n",
      "Training epoch: 256, train loss: 0.01762, val loss: 0.01835\n",
      "Training epoch: 257, train loss: 0.01761, val loss: 0.01834\n",
      "Training epoch: 258, train loss: 0.01757, val loss: 0.01831\n",
      "Training epoch: 259, train loss: 0.01755, val loss: 0.01826\n",
      "Training epoch: 260, train loss: 0.01757, val loss: 0.01827\n",
      "Training epoch: 261, train loss: 0.01755, val loss: 0.01828\n",
      "Training epoch: 262, train loss: 0.01794, val loss: 0.01860\n",
      "Training epoch: 263, train loss: 0.01773, val loss: 0.01850\n",
      "Training epoch: 264, train loss: 0.01750, val loss: 0.01822\n",
      "Training epoch: 265, train loss: 0.01753, val loss: 0.01825\n",
      "Training epoch: 266, train loss: 0.01744, val loss: 0.01817\n",
      "Training epoch: 267, train loss: 0.01743, val loss: 0.01816\n",
      "Training epoch: 268, train loss: 0.01743, val loss: 0.01814\n",
      "Training epoch: 269, train loss: 0.01759, val loss: 0.01836\n",
      "Training epoch: 270, train loss: 0.01766, val loss: 0.01837\n",
      "Training epoch: 271, train loss: 0.01735, val loss: 0.01808\n",
      "Training epoch: 272, train loss: 0.01743, val loss: 0.01815\n",
      "Training epoch: 273, train loss: 0.01732, val loss: 0.01805\n",
      "Training epoch: 274, train loss: 0.01734, val loss: 0.01808\n",
      "Training epoch: 275, train loss: 0.01750, val loss: 0.01826\n",
      "Training epoch: 276, train loss: 0.01755, val loss: 0.01827\n",
      "Training epoch: 277, train loss: 0.01757, val loss: 0.01826\n",
      "Training epoch: 278, train loss: 0.01735, val loss: 0.01811\n",
      "Training epoch: 279, train loss: 0.01731, val loss: 0.01808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 280, train loss: 0.01726, val loss: 0.01800\n",
      "Training epoch: 281, train loss: 0.01760, val loss: 0.01831\n",
      "Training epoch: 282, train loss: 0.01722, val loss: 0.01796\n",
      "Training epoch: 283, train loss: 0.01720, val loss: 0.01795\n",
      "Training epoch: 284, train loss: 0.01719, val loss: 0.01792\n",
      "Training epoch: 285, train loss: 0.01718, val loss: 0.01792\n",
      "Training epoch: 286, train loss: 0.01729, val loss: 0.01807\n",
      "Training epoch: 287, train loss: 0.01743, val loss: 0.01814\n",
      "Training epoch: 288, train loss: 0.01748, val loss: 0.01820\n",
      "Training epoch: 289, train loss: 0.01712, val loss: 0.01788\n",
      "Training epoch: 290, train loss: 0.01712, val loss: 0.01786\n",
      "Training epoch: 291, train loss: 0.01731, val loss: 0.01811\n",
      "Training epoch: 292, train loss: 0.01709, val loss: 0.01785\n",
      "Training epoch: 293, train loss: 0.01711, val loss: 0.01784\n",
      "Training epoch: 294, train loss: 0.01709, val loss: 0.01787\n",
      "Training epoch: 295, train loss: 0.01728, val loss: 0.01809\n",
      "Training epoch: 296, train loss: 0.01705, val loss: 0.01778\n",
      "Training epoch: 297, train loss: 0.01701, val loss: 0.01777\n",
      "Training epoch: 298, train loss: 0.01712, val loss: 0.01783\n",
      "Training epoch: 299, train loss: 0.01715, val loss: 0.01790\n",
      "Training epoch: 300, train loss: 0.01700, val loss: 0.01776\n",
      "Training epoch: 301, train loss: 0.01721, val loss: 0.01791\n",
      "Training epoch: 302, train loss: 0.01705, val loss: 0.01780\n",
      "Training epoch: 303, train loss: 0.01698, val loss: 0.01772\n",
      "Training epoch: 304, train loss: 0.01696, val loss: 0.01773\n",
      "Training epoch: 305, train loss: 0.01695, val loss: 0.01768\n",
      "Training epoch: 306, train loss: 0.01714, val loss: 0.01794\n",
      "Training epoch: 307, train loss: 0.01692, val loss: 0.01767\n",
      "Training epoch: 308, train loss: 0.01707, val loss: 0.01779\n",
      "Training epoch: 309, train loss: 0.01690, val loss: 0.01764\n",
      "Training epoch: 310, train loss: 0.01688, val loss: 0.01764\n",
      "Training epoch: 311, train loss: 0.01696, val loss: 0.01768\n",
      "Training epoch: 312, train loss: 0.01706, val loss: 0.01787\n",
      "Training epoch: 313, train loss: 0.01688, val loss: 0.01759\n",
      "Training epoch: 314, train loss: 0.01684, val loss: 0.01760\n",
      "Training epoch: 315, train loss: 0.01684, val loss: 0.01760\n",
      "Training epoch: 316, train loss: 0.01699, val loss: 0.01769\n",
      "Training epoch: 317, train loss: 0.01689, val loss: 0.01767\n",
      "Training epoch: 318, train loss: 0.01685, val loss: 0.01759\n",
      "Training epoch: 319, train loss: 0.01710, val loss: 0.01782\n",
      "Training epoch: 320, train loss: 0.01680, val loss: 0.01757\n",
      "Training epoch: 321, train loss: 0.01688, val loss: 0.01763\n",
      "Training epoch: 322, train loss: 0.01690, val loss: 0.01766\n",
      "Training epoch: 323, train loss: 0.01706, val loss: 0.01780\n",
      "Training epoch: 324, train loss: 0.01686, val loss: 0.01760\n",
      "Training epoch: 325, train loss: 0.01683, val loss: 0.01762\n",
      "Training epoch: 326, train loss: 0.01676, val loss: 0.01753\n",
      "Training epoch: 327, train loss: 0.01676, val loss: 0.01751\n",
      "Training epoch: 328, train loss: 0.01674, val loss: 0.01750\n",
      "Training epoch: 329, train loss: 0.01672, val loss: 0.01748\n",
      "Training epoch: 330, train loss: 0.01675, val loss: 0.01751\n",
      "Training epoch: 331, train loss: 0.01674, val loss: 0.01746\n",
      "Training epoch: 332, train loss: 0.01674, val loss: 0.01755\n",
      "Training epoch: 333, train loss: 0.01784, val loss: 0.01865\n",
      "Training epoch: 334, train loss: 0.01682, val loss: 0.01759\n",
      "Training epoch: 335, train loss: 0.01672, val loss: 0.01750\n",
      "Training epoch: 336, train loss: 0.01673, val loss: 0.01745\n",
      "Training epoch: 337, train loss: 0.01668, val loss: 0.01744\n",
      "Training epoch: 338, train loss: 0.01671, val loss: 0.01752\n",
      "Training epoch: 339, train loss: 0.01666, val loss: 0.01741\n",
      "Training epoch: 340, train loss: 0.01698, val loss: 0.01772\n",
      "Training epoch: 341, train loss: 0.01666, val loss: 0.01742\n",
      "Training epoch: 342, train loss: 0.01678, val loss: 0.01754\n",
      "Training epoch: 343, train loss: 0.01693, val loss: 0.01770\n",
      "Training epoch: 344, train loss: 0.01684, val loss: 0.01757\n",
      "Training epoch: 345, train loss: 0.01669, val loss: 0.01745\n",
      "Training epoch: 346, train loss: 0.01668, val loss: 0.01746\n",
      "Training epoch: 347, train loss: 0.01669, val loss: 0.01745\n",
      "Training epoch: 348, train loss: 0.01665, val loss: 0.01743\n",
      "Training epoch: 349, train loss: 0.01666, val loss: 0.01745\n",
      "Training epoch: 350, train loss: 0.01662, val loss: 0.01737\n",
      "Training epoch: 351, train loss: 0.01673, val loss: 0.01754\n",
      "Training epoch: 352, train loss: 0.01677, val loss: 0.01755\n",
      "Training epoch: 353, train loss: 0.01687, val loss: 0.01767\n",
      "Training epoch: 354, train loss: 0.01659, val loss: 0.01737\n",
      "Training epoch: 355, train loss: 0.01661, val loss: 0.01736\n",
      "Training epoch: 356, train loss: 0.01680, val loss: 0.01753\n",
      "Training epoch: 357, train loss: 0.01673, val loss: 0.01743\n",
      "Training epoch: 358, train loss: 0.01675, val loss: 0.01752\n",
      "Training epoch: 359, train loss: 0.01655, val loss: 0.01728\n",
      "Training epoch: 360, train loss: 0.01654, val loss: 0.01727\n",
      "Training epoch: 361, train loss: 0.01655, val loss: 0.01729\n",
      "Training epoch: 362, train loss: 0.01663, val loss: 0.01741\n",
      "Training epoch: 363, train loss: 0.01665, val loss: 0.01740\n",
      "Training epoch: 364, train loss: 0.01654, val loss: 0.01729\n",
      "Training epoch: 365, train loss: 0.01658, val loss: 0.01731\n",
      "Training epoch: 366, train loss: 0.01649, val loss: 0.01723\n",
      "Training epoch: 367, train loss: 0.01661, val loss: 0.01739\n",
      "Training epoch: 368, train loss: 0.01651, val loss: 0.01725\n",
      "Training epoch: 369, train loss: 0.01661, val loss: 0.01734\n",
      "Training epoch: 370, train loss: 0.01657, val loss: 0.01730\n",
      "Training epoch: 371, train loss: 0.01658, val loss: 0.01729\n",
      "Training epoch: 372, train loss: 0.01649, val loss: 0.01725\n",
      "Training epoch: 373, train loss: 0.01647, val loss: 0.01718\n",
      "Training epoch: 374, train loss: 0.01655, val loss: 0.01729\n",
      "Training epoch: 375, train loss: 0.01648, val loss: 0.01718\n",
      "Training epoch: 376, train loss: 0.01644, val loss: 0.01717\n",
      "Training epoch: 377, train loss: 0.01642, val loss: 0.01714\n",
      "Training epoch: 378, train loss: 0.01675, val loss: 0.01747\n",
      "Training epoch: 379, train loss: 0.01659, val loss: 0.01735\n",
      "Training epoch: 380, train loss: 0.01642, val loss: 0.01715\n",
      "Training epoch: 381, train loss: 0.01652, val loss: 0.01723\n",
      "Training epoch: 382, train loss: 0.01639, val loss: 0.01710\n",
      "Training epoch: 383, train loss: 0.01688, val loss: 0.01765\n",
      "Training epoch: 384, train loss: 0.01669, val loss: 0.01741\n",
      "Training epoch: 385, train loss: 0.01663, val loss: 0.01733\n",
      "Training epoch: 386, train loss: 0.01717, val loss: 0.01787\n",
      "Training epoch: 387, train loss: 0.01641, val loss: 0.01715\n",
      "Training epoch: 388, train loss: 0.01657, val loss: 0.01732\n",
      "Training epoch: 389, train loss: 0.01644, val loss: 0.01718\n",
      "Training epoch: 390, train loss: 0.01637, val loss: 0.01708\n",
      "Training epoch: 391, train loss: 0.01675, val loss: 0.01746\n",
      "Training epoch: 392, train loss: 0.01635, val loss: 0.01710\n",
      "Training epoch: 393, train loss: 0.01636, val loss: 0.01708\n",
      "Training epoch: 394, train loss: 0.01647, val loss: 0.01721\n",
      "Training epoch: 395, train loss: 0.01636, val loss: 0.01710\n",
      "Training epoch: 396, train loss: 0.01632, val loss: 0.01704\n",
      "Training epoch: 397, train loss: 0.01668, val loss: 0.01739\n",
      "Training epoch: 398, train loss: 0.01631, val loss: 0.01703\n",
      "Training epoch: 399, train loss: 0.01630, val loss: 0.01703\n",
      "Training epoch: 400, train loss: 0.01630, val loss: 0.01702\n",
      "Training epoch: 401, train loss: 0.01637, val loss: 0.01706\n",
      "Training epoch: 402, train loss: 0.01629, val loss: 0.01702\n",
      "Training epoch: 403, train loss: 0.01635, val loss: 0.01708\n",
      "Training epoch: 404, train loss: 0.01628, val loss: 0.01698\n",
      "Training epoch: 405, train loss: 0.01629, val loss: 0.01699\n",
      "Training epoch: 406, train loss: 0.01630, val loss: 0.01699\n",
      "Training epoch: 407, train loss: 0.01626, val loss: 0.01698\n",
      "Training epoch: 408, train loss: 0.01629, val loss: 0.01698\n",
      "Training epoch: 409, train loss: 0.01638, val loss: 0.01711\n",
      "Training epoch: 410, train loss: 0.01634, val loss: 0.01701\n",
      "Training epoch: 411, train loss: 0.01677, val loss: 0.01748\n",
      "Training epoch: 412, train loss: 0.01626, val loss: 0.01698\n",
      "Training epoch: 413, train loss: 0.01665, val loss: 0.01738\n",
      "Training epoch: 414, train loss: 0.01624, val loss: 0.01695\n",
      "Training epoch: 415, train loss: 0.01625, val loss: 0.01698\n",
      "Training epoch: 416, train loss: 0.01623, val loss: 0.01693\n",
      "Training epoch: 417, train loss: 0.01658, val loss: 0.01734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 418, train loss: 0.01626, val loss: 0.01695\n",
      "Training epoch: 419, train loss: 0.01622, val loss: 0.01694\n",
      "Training epoch: 420, train loss: 0.01644, val loss: 0.01712\n",
      "Training epoch: 421, train loss: 0.01625, val loss: 0.01695\n",
      "Training epoch: 422, train loss: 0.01619, val loss: 0.01690\n",
      "Training epoch: 423, train loss: 0.01683, val loss: 0.01757\n",
      "Training epoch: 424, train loss: 0.01619, val loss: 0.01692\n",
      "Training epoch: 425, train loss: 0.01635, val loss: 0.01705\n",
      "Training epoch: 426, train loss: 0.01630, val loss: 0.01699\n",
      "Training epoch: 427, train loss: 0.01618, val loss: 0.01691\n",
      "Training epoch: 428, train loss: 0.01618, val loss: 0.01691\n",
      "Training epoch: 429, train loss: 0.01623, val loss: 0.01694\n",
      "Training epoch: 430, train loss: 0.01654, val loss: 0.01728\n",
      "Training epoch: 431, train loss: 0.01615, val loss: 0.01689\n",
      "Training epoch: 432, train loss: 0.01623, val loss: 0.01694\n",
      "Training epoch: 433, train loss: 0.01615, val loss: 0.01684\n",
      "Training epoch: 434, train loss: 0.01613, val loss: 0.01683\n",
      "Training epoch: 435, train loss: 0.01614, val loss: 0.01685\n",
      "Training epoch: 436, train loss: 0.01618, val loss: 0.01688\n",
      "Training epoch: 437, train loss: 0.01611, val loss: 0.01682\n",
      "Training epoch: 438, train loss: 0.01611, val loss: 0.01679\n",
      "Training epoch: 439, train loss: 0.01629, val loss: 0.01699\n",
      "Training epoch: 440, train loss: 0.01624, val loss: 0.01694\n",
      "Training epoch: 441, train loss: 0.01611, val loss: 0.01681\n",
      "Training epoch: 442, train loss: 0.01609, val loss: 0.01681\n",
      "Training epoch: 443, train loss: 0.01611, val loss: 0.01678\n",
      "Training epoch: 444, train loss: 0.01615, val loss: 0.01683\n",
      "Training epoch: 445, train loss: 0.01614, val loss: 0.01686\n",
      "Training epoch: 446, train loss: 0.01642, val loss: 0.01710\n",
      "Training epoch: 447, train loss: 0.01606, val loss: 0.01679\n",
      "Training epoch: 448, train loss: 0.01616, val loss: 0.01684\n",
      "Training epoch: 449, train loss: 0.01612, val loss: 0.01682\n",
      "Training epoch: 450, train loss: 0.01605, val loss: 0.01674\n",
      "Training epoch: 451, train loss: 0.01612, val loss: 0.01682\n",
      "Training epoch: 452, train loss: 0.01624, val loss: 0.01695\n",
      "Training epoch: 453, train loss: 0.01675, val loss: 0.01744\n",
      "Training epoch: 454, train loss: 0.01645, val loss: 0.01714\n",
      "Training epoch: 455, train loss: 0.01605, val loss: 0.01676\n",
      "Training epoch: 456, train loss: 0.01665, val loss: 0.01737\n",
      "Training epoch: 457, train loss: 0.01606, val loss: 0.01678\n",
      "Training epoch: 458, train loss: 0.01609, val loss: 0.01681\n",
      "Training epoch: 459, train loss: 0.01613, val loss: 0.01683\n",
      "Training epoch: 460, train loss: 0.01635, val loss: 0.01706\n",
      "Training epoch: 461, train loss: 0.01638, val loss: 0.01709\n",
      "Training epoch: 462, train loss: 0.01628, val loss: 0.01700\n",
      "Training epoch: 463, train loss: 0.01602, val loss: 0.01672\n",
      "Training epoch: 464, train loss: 0.01602, val loss: 0.01675\n",
      "Training epoch: 465, train loss: 0.01601, val loss: 0.01672\n",
      "Training epoch: 466, train loss: 0.01602, val loss: 0.01672\n",
      "Training epoch: 467, train loss: 0.01608, val loss: 0.01681\n",
      "Training epoch: 468, train loss: 0.01598, val loss: 0.01667\n",
      "Training epoch: 469, train loss: 0.01633, val loss: 0.01706\n",
      "Training epoch: 470, train loss: 0.01618, val loss: 0.01686\n",
      "Training epoch: 471, train loss: 0.01598, val loss: 0.01671\n",
      "Training epoch: 472, train loss: 0.01625, val loss: 0.01696\n",
      "Training epoch: 473, train loss: 0.01596, val loss: 0.01666\n",
      "Training epoch: 474, train loss: 0.01628, val loss: 0.01701\n",
      "Training epoch: 475, train loss: 0.01607, val loss: 0.01677\n",
      "Training epoch: 476, train loss: 0.01595, val loss: 0.01666\n",
      "Training epoch: 477, train loss: 0.01595, val loss: 0.01663\n",
      "Training epoch: 478, train loss: 0.01610, val loss: 0.01683\n",
      "Training epoch: 479, train loss: 0.01595, val loss: 0.01666\n",
      "Training epoch: 480, train loss: 0.01594, val loss: 0.01664\n",
      "Training epoch: 481, train loss: 0.01593, val loss: 0.01664\n",
      "Training epoch: 482, train loss: 0.01595, val loss: 0.01665\n",
      "Training epoch: 483, train loss: 0.01600, val loss: 0.01675\n",
      "Training epoch: 484, train loss: 0.01603, val loss: 0.01670\n",
      "Training epoch: 485, train loss: 0.01660, val loss: 0.01730\n",
      "Training epoch: 486, train loss: 0.01604, val loss: 0.01673\n",
      "Training epoch: 487, train loss: 0.01594, val loss: 0.01665\n",
      "Training epoch: 488, train loss: 0.01593, val loss: 0.01662\n",
      "Training epoch: 489, train loss: 0.01590, val loss: 0.01662\n",
      "Training epoch: 490, train loss: 0.01590, val loss: 0.01662\n",
      "Training epoch: 491, train loss: 0.01590, val loss: 0.01658\n",
      "Training epoch: 492, train loss: 0.01598, val loss: 0.01668\n",
      "Training epoch: 493, train loss: 0.01592, val loss: 0.01662\n",
      "Training epoch: 494, train loss: 0.01587, val loss: 0.01658\n",
      "Training epoch: 495, train loss: 0.01588, val loss: 0.01656\n",
      "Training epoch: 496, train loss: 0.01602, val loss: 0.01673\n",
      "Training epoch: 497, train loss: 0.01597, val loss: 0.01668\n",
      "Training epoch: 498, train loss: 0.01586, val loss: 0.01656\n",
      "Training epoch: 499, train loss: 0.01585, val loss: 0.01655\n",
      "Training epoch: 500, train loss: 0.01598, val loss: 0.01665\n",
      "Training epoch: 501, train loss: 0.01597, val loss: 0.01666\n",
      "Training epoch: 502, train loss: 0.01588, val loss: 0.01659\n",
      "Training epoch: 503, train loss: 0.01631, val loss: 0.01705\n",
      "Training epoch: 504, train loss: 0.01591, val loss: 0.01659\n",
      "Training epoch: 505, train loss: 0.01640, val loss: 0.01711\n",
      "Training epoch: 506, train loss: 0.01589, val loss: 0.01660\n",
      "Training epoch: 507, train loss: 0.01588, val loss: 0.01656\n",
      "Training epoch: 508, train loss: 0.01598, val loss: 0.01669\n",
      "Training epoch: 509, train loss: 0.01611, val loss: 0.01681\n",
      "Training epoch: 510, train loss: 0.01585, val loss: 0.01654\n",
      "Training epoch: 511, train loss: 0.01601, val loss: 0.01675\n",
      "Training epoch: 512, train loss: 0.01597, val loss: 0.01669\n",
      "Training epoch: 513, train loss: 0.01583, val loss: 0.01654\n",
      "Training epoch: 514, train loss: 0.01588, val loss: 0.01656\n",
      "Training epoch: 515, train loss: 0.01582, val loss: 0.01654\n",
      "Training epoch: 516, train loss: 0.01581, val loss: 0.01652\n",
      "Training epoch: 517, train loss: 0.01589, val loss: 0.01658\n",
      "Training epoch: 518, train loss: 0.01586, val loss: 0.01655\n",
      "Training epoch: 519, train loss: 0.01604, val loss: 0.01676\n",
      "Training epoch: 520, train loss: 0.01581, val loss: 0.01652\n",
      "Training epoch: 521, train loss: 0.01593, val loss: 0.01665\n",
      "Training epoch: 522, train loss: 0.01582, val loss: 0.01654\n",
      "Training epoch: 523, train loss: 0.01579, val loss: 0.01649\n",
      "Training epoch: 524, train loss: 0.01582, val loss: 0.01651\n",
      "Training epoch: 525, train loss: 0.01578, val loss: 0.01650\n",
      "Training epoch: 526, train loss: 0.01577, val loss: 0.01647\n",
      "Training epoch: 527, train loss: 0.01583, val loss: 0.01651\n",
      "Training epoch: 528, train loss: 0.01580, val loss: 0.01651\n",
      "Training epoch: 529, train loss: 0.01579, val loss: 0.01648\n",
      "Training epoch: 530, train loss: 0.01621, val loss: 0.01694\n",
      "Training epoch: 531, train loss: 0.01594, val loss: 0.01663\n",
      "Training epoch: 532, train loss: 0.01577, val loss: 0.01648\n",
      "Training epoch: 533, train loss: 0.01630, val loss: 0.01705\n",
      "Training epoch: 534, train loss: 0.01580, val loss: 0.01650\n",
      "Training epoch: 535, train loss: 0.01580, val loss: 0.01647\n",
      "Training epoch: 536, train loss: 0.01598, val loss: 0.01669\n",
      "Training epoch: 537, train loss: 0.01607, val loss: 0.01678\n",
      "Training epoch: 538, train loss: 0.01596, val loss: 0.01665\n",
      "Training epoch: 539, train loss: 0.01579, val loss: 0.01651\n",
      "Training epoch: 540, train loss: 0.01585, val loss: 0.01656\n",
      "Training epoch: 541, train loss: 0.01625, val loss: 0.01699\n",
      "Training epoch: 542, train loss: 0.01642, val loss: 0.01715\n",
      "Training epoch: 543, train loss: 0.01611, val loss: 0.01684\n",
      "Training epoch: 544, train loss: 0.01579, val loss: 0.01650\n",
      "Training epoch: 545, train loss: 0.01579, val loss: 0.01650\n",
      "Training epoch: 546, train loss: 0.01599, val loss: 0.01669\n",
      "Training epoch: 547, train loss: 0.01588, val loss: 0.01661\n",
      "Training epoch: 548, train loss: 0.01590, val loss: 0.01662\n",
      "Training epoch: 549, train loss: 0.01582, val loss: 0.01652\n",
      "Training epoch: 550, train loss: 0.01579, val loss: 0.01650\n",
      "Training epoch: 551, train loss: 0.01574, val loss: 0.01645\n",
      "Training epoch: 552, train loss: 0.01581, val loss: 0.01653\n",
      "Training epoch: 553, train loss: 0.01572, val loss: 0.01642\n",
      "Training epoch: 554, train loss: 0.01573, val loss: 0.01645\n",
      "Training epoch: 555, train loss: 0.01572, val loss: 0.01643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 556, train loss: 0.01580, val loss: 0.01650\n",
      "Training epoch: 557, train loss: 0.01579, val loss: 0.01650\n",
      "Training epoch: 558, train loss: 0.01576, val loss: 0.01645\n",
      "Training epoch: 559, train loss: 0.01575, val loss: 0.01646\n",
      "Training epoch: 560, train loss: 0.01581, val loss: 0.01652\n",
      "Training epoch: 561, train loss: 0.01571, val loss: 0.01642\n",
      "Training epoch: 562, train loss: 0.01574, val loss: 0.01644\n",
      "Training epoch: 563, train loss: 0.01598, val loss: 0.01670\n",
      "Training epoch: 564, train loss: 0.01571, val loss: 0.01642\n",
      "Training epoch: 565, train loss: 0.01580, val loss: 0.01651\n",
      "Training epoch: 566, train loss: 0.01599, val loss: 0.01670\n",
      "Training epoch: 567, train loss: 0.01581, val loss: 0.01652\n",
      "Training epoch: 568, train loss: 0.01575, val loss: 0.01643\n",
      "Training epoch: 569, train loss: 0.01603, val loss: 0.01675\n",
      "Training epoch: 570, train loss: 0.01574, val loss: 0.01648\n",
      "Training epoch: 571, train loss: 0.01576, val loss: 0.01646\n",
      "Training epoch: 572, train loss: 0.01572, val loss: 0.01641\n",
      "Training epoch: 573, train loss: 0.01574, val loss: 0.01646\n",
      "Training epoch: 574, train loss: 0.01571, val loss: 0.01642\n",
      "Training epoch: 575, train loss: 0.01600, val loss: 0.01673\n",
      "Training epoch: 576, train loss: 0.01571, val loss: 0.01640\n",
      "Training epoch: 577, train loss: 0.01574, val loss: 0.01645\n",
      "Training epoch: 578, train loss: 0.01582, val loss: 0.01653\n",
      "Training epoch: 579, train loss: 0.01570, val loss: 0.01640\n",
      "Training epoch: 580, train loss: 0.01598, val loss: 0.01669\n",
      "Training epoch: 581, train loss: 0.01592, val loss: 0.01661\n",
      "Training epoch: 582, train loss: 0.01572, val loss: 0.01644\n",
      "Training epoch: 583, train loss: 0.01574, val loss: 0.01643\n",
      "Training epoch: 584, train loss: 0.01575, val loss: 0.01646\n",
      "Training epoch: 585, train loss: 0.01572, val loss: 0.01642\n",
      "Training epoch: 586, train loss: 0.01602, val loss: 0.01675\n",
      "Training epoch: 587, train loss: 0.01569, val loss: 0.01642\n",
      "Training epoch: 588, train loss: 0.01566, val loss: 0.01637\n",
      "Training epoch: 589, train loss: 0.01609, val loss: 0.01678\n",
      "Training epoch: 590, train loss: 0.01569, val loss: 0.01638\n",
      "Training epoch: 591, train loss: 0.01579, val loss: 0.01650\n",
      "Training epoch: 592, train loss: 0.01569, val loss: 0.01640\n",
      "Training epoch: 593, train loss: 0.01580, val loss: 0.01649\n",
      "Training epoch: 594, train loss: 0.01570, val loss: 0.01643\n",
      "Training epoch: 595, train loss: 0.01566, val loss: 0.01636\n",
      "Training epoch: 596, train loss: 0.01582, val loss: 0.01654\n",
      "Training epoch: 597, train loss: 0.01573, val loss: 0.01645\n",
      "Training epoch: 598, train loss: 0.01566, val loss: 0.01635\n",
      "Training epoch: 599, train loss: 0.01595, val loss: 0.01665\n",
      "Training epoch: 600, train loss: 0.01568, val loss: 0.01641\n",
      "Training epoch: 601, train loss: 0.01568, val loss: 0.01638\n",
      "Training epoch: 602, train loss: 0.01569, val loss: 0.01638\n",
      "Training epoch: 603, train loss: 0.01566, val loss: 0.01637\n",
      "Training epoch: 604, train loss: 0.01565, val loss: 0.01636\n",
      "Training epoch: 605, train loss: 0.01566, val loss: 0.01636\n",
      "Training epoch: 606, train loss: 0.01568, val loss: 0.01639\n",
      "Training epoch: 607, train loss: 0.01574, val loss: 0.01643\n",
      "Training epoch: 608, train loss: 0.01573, val loss: 0.01641\n",
      "Training epoch: 609, train loss: 0.01575, val loss: 0.01645\n",
      "Training epoch: 610, train loss: 0.01591, val loss: 0.01663\n",
      "Training epoch: 611, train loss: 0.01613, val loss: 0.01685\n",
      "Training epoch: 612, train loss: 0.01566, val loss: 0.01637\n",
      "Training epoch: 613, train loss: 0.01570, val loss: 0.01637\n",
      "Training epoch: 614, train loss: 0.01568, val loss: 0.01640\n",
      "Training epoch: 615, train loss: 0.01574, val loss: 0.01644\n",
      "Training epoch: 616, train loss: 0.01579, val loss: 0.01648\n",
      "Training epoch: 617, train loss: 0.01565, val loss: 0.01635\n",
      "Training epoch: 618, train loss: 0.01592, val loss: 0.01664\n",
      "Training epoch: 619, train loss: 0.01578, val loss: 0.01649\n",
      "Training epoch: 620, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 621, train loss: 0.01565, val loss: 0.01635\n",
      "Training epoch: 622, train loss: 0.01566, val loss: 0.01637\n",
      "Training epoch: 623, train loss: 0.01574, val loss: 0.01645\n",
      "Training epoch: 624, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 625, train loss: 0.01584, val loss: 0.01653\n",
      "Training epoch: 626, train loss: 0.01576, val loss: 0.01646\n",
      "Training epoch: 627, train loss: 0.01564, val loss: 0.01634\n",
      "Training epoch: 628, train loss: 0.01564, val loss: 0.01634\n",
      "Training epoch: 629, train loss: 0.01562, val loss: 0.01632\n",
      "Training epoch: 630, train loss: 0.01562, val loss: 0.01633\n",
      "Training epoch: 631, train loss: 0.01580, val loss: 0.01650\n",
      "Training epoch: 632, train loss: 0.01565, val loss: 0.01635\n",
      "Training epoch: 633, train loss: 0.01563, val loss: 0.01630\n",
      "Training epoch: 634, train loss: 0.01564, val loss: 0.01632\n",
      "Training epoch: 635, train loss: 0.01568, val loss: 0.01638\n",
      "Training epoch: 636, train loss: 0.01571, val loss: 0.01640\n",
      "Training epoch: 637, train loss: 0.01561, val loss: 0.01633\n",
      "Training epoch: 638, train loss: 0.01573, val loss: 0.01643\n",
      "Training epoch: 639, train loss: 0.01563, val loss: 0.01633\n",
      "Training epoch: 640, train loss: 0.01561, val loss: 0.01632\n",
      "Training epoch: 641, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 642, train loss: 0.01564, val loss: 0.01635\n",
      "Training epoch: 643, train loss: 0.01561, val loss: 0.01631\n",
      "Training epoch: 644, train loss: 0.01562, val loss: 0.01631\n",
      "Training epoch: 645, train loss: 0.01561, val loss: 0.01631\n",
      "Training epoch: 646, train loss: 0.01571, val loss: 0.01640\n",
      "Training epoch: 647, train loss: 0.01568, val loss: 0.01637\n",
      "Training epoch: 648, train loss: 0.01563, val loss: 0.01630\n",
      "Training epoch: 649, train loss: 0.01581, val loss: 0.01651\n",
      "Training epoch: 650, train loss: 0.01562, val loss: 0.01633\n",
      "Training epoch: 651, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 652, train loss: 0.01560, val loss: 0.01630\n",
      "Training epoch: 653, train loss: 0.01572, val loss: 0.01642\n",
      "Training epoch: 654, train loss: 0.01564, val loss: 0.01633\n",
      "Training epoch: 655, train loss: 0.01559, val loss: 0.01629\n",
      "Training epoch: 656, train loss: 0.01567, val loss: 0.01633\n",
      "Training epoch: 657, train loss: 0.01572, val loss: 0.01641\n",
      "Training epoch: 658, train loss: 0.01560, val loss: 0.01631\n",
      "Training epoch: 659, train loss: 0.01565, val loss: 0.01634\n",
      "Training epoch: 660, train loss: 0.01560, val loss: 0.01627\n",
      "Training epoch: 661, train loss: 0.01565, val loss: 0.01630\n",
      "Training epoch: 662, train loss: 0.01577, val loss: 0.01647\n",
      "Training epoch: 663, train loss: 0.01559, val loss: 0.01630\n",
      "Training epoch: 664, train loss: 0.01563, val loss: 0.01631\n",
      "Training epoch: 665, train loss: 0.01581, val loss: 0.01646\n",
      "Training epoch: 666, train loss: 0.01562, val loss: 0.01631\n",
      "Training epoch: 667, train loss: 0.01562, val loss: 0.01631\n",
      "Training epoch: 668, train loss: 0.01600, val loss: 0.01669\n",
      "Training epoch: 669, train loss: 0.01572, val loss: 0.01642\n",
      "Training epoch: 670, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 671, train loss: 0.01567, val loss: 0.01636\n",
      "Training epoch: 672, train loss: 0.01568, val loss: 0.01635\n",
      "Training epoch: 673, train loss: 0.01569, val loss: 0.01638\n",
      "Training epoch: 674, train loss: 0.01562, val loss: 0.01632\n",
      "Training epoch: 675, train loss: 0.01559, val loss: 0.01629\n",
      "Training epoch: 676, train loss: 0.01560, val loss: 0.01629\n",
      "Training epoch: 677, train loss: 0.01581, val loss: 0.01649\n",
      "Training epoch: 678, train loss: 0.01559, val loss: 0.01628\n",
      "Training epoch: 679, train loss: 0.01560, val loss: 0.01626\n",
      "Training epoch: 680, train loss: 0.01569, val loss: 0.01638\n",
      "Training epoch: 681, train loss: 0.01573, val loss: 0.01643\n",
      "Training epoch: 682, train loss: 0.01558, val loss: 0.01627\n",
      "Training epoch: 683, train loss: 0.01558, val loss: 0.01625\n",
      "Training epoch: 684, train loss: 0.01572, val loss: 0.01639\n",
      "Training epoch: 685, train loss: 0.01566, val loss: 0.01633\n",
      "Training epoch: 686, train loss: 0.01557, val loss: 0.01625\n",
      "Training epoch: 687, train loss: 0.01589, val loss: 0.01657\n",
      "Training epoch: 688, train loss: 0.01618, val loss: 0.01687\n",
      "Training epoch: 689, train loss: 0.01585, val loss: 0.01654\n",
      "Training epoch: 690, train loss: 0.01558, val loss: 0.01626\n",
      "Training epoch: 691, train loss: 0.01559, val loss: 0.01626\n",
      "Training epoch: 692, train loss: 0.01567, val loss: 0.01635\n",
      "Training epoch: 693, train loss: 0.01579, val loss: 0.01648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 694, train loss: 0.01559, val loss: 0.01627\n",
      "Training epoch: 695, train loss: 0.01581, val loss: 0.01648\n",
      "Training epoch: 696, train loss: 0.01564, val loss: 0.01632\n",
      "Training epoch: 697, train loss: 0.01568, val loss: 0.01636\n",
      "Training epoch: 698, train loss: 0.01560, val loss: 0.01628\n",
      "Training epoch: 699, train loss: 0.01563, val loss: 0.01630\n",
      "Training epoch: 700, train loss: 0.01558, val loss: 0.01629\n",
      "Training epoch: 701, train loss: 0.01556, val loss: 0.01622\n",
      "Training epoch: 702, train loss: 0.01569, val loss: 0.01635\n",
      "Training epoch: 703, train loss: 0.01570, val loss: 0.01638\n",
      "Training epoch: 704, train loss: 0.01571, val loss: 0.01639\n",
      "Training epoch: 705, train loss: 0.01557, val loss: 0.01624\n",
      "Training epoch: 706, train loss: 0.01556, val loss: 0.01624\n",
      "Training epoch: 707, train loss: 0.01603, val loss: 0.01672\n",
      "Training epoch: 708, train loss: 0.01613, val loss: 0.01682\n",
      "Training epoch: 709, train loss: 0.01563, val loss: 0.01632\n",
      "Training epoch: 710, train loss: 0.01559, val loss: 0.01626\n",
      "Training epoch: 711, train loss: 0.01567, val loss: 0.01636\n",
      "Training epoch: 712, train loss: 0.01614, val loss: 0.01683\n",
      "Training epoch: 713, train loss: 0.01560, val loss: 0.01628\n",
      "Training epoch: 714, train loss: 0.01557, val loss: 0.01626\n",
      "Training epoch: 715, train loss: 0.01572, val loss: 0.01640\n",
      "Training epoch: 716, train loss: 0.01566, val loss: 0.01634\n",
      "Training epoch: 717, train loss: 0.01562, val loss: 0.01630\n",
      "Training epoch: 718, train loss: 0.01571, val loss: 0.01638\n",
      "Training epoch: 719, train loss: 0.01603, val loss: 0.01671\n",
      "Training epoch: 720, train loss: 0.01577, val loss: 0.01644\n",
      "Training epoch: 721, train loss: 0.01559, val loss: 0.01626\n",
      "Training epoch: 722, train loss: 0.01564, val loss: 0.01631\n",
      "Training epoch: 723, train loss: 0.01558, val loss: 0.01625\n",
      "Training epoch: 724, train loss: 0.01577, val loss: 0.01642\n",
      "Training epoch: 725, train loss: 0.01578, val loss: 0.01646\n",
      "Training epoch: 726, train loss: 0.01556, val loss: 0.01624\n",
      "Training epoch: 727, train loss: 0.01558, val loss: 0.01625\n",
      "Training epoch: 728, train loss: 0.01569, val loss: 0.01633\n",
      "Training epoch: 729, train loss: 0.01556, val loss: 0.01622\n",
      "Training epoch: 730, train loss: 0.01557, val loss: 0.01625\n",
      "Training epoch: 731, train loss: 0.01565, val loss: 0.01632\n",
      "Training epoch: 732, train loss: 0.01557, val loss: 0.01623\n",
      "Training epoch: 733, train loss: 0.01555, val loss: 0.01618\n",
      "Training epoch: 734, train loss: 0.01554, val loss: 0.01620\n",
      "Training epoch: 735, train loss: 0.01554, val loss: 0.01621\n",
      "Training epoch: 736, train loss: 0.01555, val loss: 0.01621\n",
      "Training epoch: 737, train loss: 0.01562, val loss: 0.01625\n",
      "Training epoch: 738, train loss: 0.01558, val loss: 0.01623\n",
      "Training epoch: 739, train loss: 0.01569, val loss: 0.01636\n",
      "Training epoch: 740, train loss: 0.01561, val loss: 0.01626\n",
      "Training epoch: 741, train loss: 0.01555, val loss: 0.01621\n",
      "Training epoch: 742, train loss: 0.01563, val loss: 0.01628\n",
      "Training epoch: 743, train loss: 0.01561, val loss: 0.01625\n",
      "Training epoch: 744, train loss: 0.01565, val loss: 0.01629\n",
      "Training epoch: 745, train loss: 0.01555, val loss: 0.01619\n",
      "Training epoch: 746, train loss: 0.01590, val loss: 0.01656\n",
      "Training epoch: 747, train loss: 0.01555, val loss: 0.01619\n",
      "Training epoch: 748, train loss: 0.01555, val loss: 0.01620\n",
      "Training epoch: 749, train loss: 0.01561, val loss: 0.01626\n",
      "Training epoch: 750, train loss: 0.01553, val loss: 0.01618\n",
      "Training epoch: 751, train loss: 0.01560, val loss: 0.01625\n",
      "Training epoch: 752, train loss: 0.01562, val loss: 0.01625\n",
      "Training epoch: 753, train loss: 0.01555, val loss: 0.01618\n",
      "Training epoch: 754, train loss: 0.01562, val loss: 0.01625\n",
      "Training epoch: 755, train loss: 0.01554, val loss: 0.01618\n",
      "Training epoch: 756, train loss: 0.01555, val loss: 0.01619\n",
      "Training epoch: 757, train loss: 0.01564, val loss: 0.01629\n",
      "Training epoch: 758, train loss: 0.01561, val loss: 0.01623\n",
      "Training epoch: 759, train loss: 0.01579, val loss: 0.01641\n",
      "Training epoch: 760, train loss: 0.01556, val loss: 0.01622\n",
      "Training epoch: 761, train loss: 0.01557, val loss: 0.01620\n",
      "Training epoch: 762, train loss: 0.01582, val loss: 0.01646\n",
      "Training epoch: 763, train loss: 0.01556, val loss: 0.01620\n",
      "Training epoch: 764, train loss: 0.01570, val loss: 0.01632\n",
      "Training epoch: 765, train loss: 0.01570, val loss: 0.01633\n",
      "Training epoch: 766, train loss: 0.01553, val loss: 0.01618\n",
      "Training epoch: 767, train loss: 0.01591, val loss: 0.01653\n",
      "Training epoch: 768, train loss: 0.01567, val loss: 0.01630\n",
      "Training epoch: 769, train loss: 0.01557, val loss: 0.01618\n",
      "Training epoch: 770, train loss: 0.01581, val loss: 0.01647\n",
      "Training epoch: 771, train loss: 0.01592, val loss: 0.01654\n",
      "Training epoch: 772, train loss: 0.01559, val loss: 0.01620\n",
      "Training epoch: 773, train loss: 0.01563, val loss: 0.01627\n",
      "Training epoch: 774, train loss: 0.01565, val loss: 0.01628\n",
      "Training epoch: 775, train loss: 0.01555, val loss: 0.01621\n",
      "Training epoch: 776, train loss: 0.01569, val loss: 0.01631\n",
      "Training epoch: 777, train loss: 0.01559, val loss: 0.01620\n",
      "Training epoch: 778, train loss: 0.01554, val loss: 0.01616\n",
      "Training epoch: 779, train loss: 0.01554, val loss: 0.01617\n",
      "Training epoch: 780, train loss: 0.01555, val loss: 0.01620\n",
      "Training epoch: 781, train loss: 0.01564, val loss: 0.01624\n",
      "Training epoch: 782, train loss: 0.01554, val loss: 0.01614\n",
      "Training epoch: 783, train loss: 0.01551, val loss: 0.01612\n",
      "Training epoch: 784, train loss: 0.01568, val loss: 0.01628\n",
      "Training epoch: 785, train loss: 0.01552, val loss: 0.01617\n",
      "Training epoch: 786, train loss: 0.01561, val loss: 0.01624\n",
      "Training epoch: 787, train loss: 0.01554, val loss: 0.01615\n",
      "Training epoch: 788, train loss: 0.01562, val loss: 0.01621\n",
      "Training epoch: 789, train loss: 0.01563, val loss: 0.01624\n",
      "Training epoch: 790, train loss: 0.01556, val loss: 0.01618\n",
      "Training epoch: 791, train loss: 0.01552, val loss: 0.01612\n",
      "Training epoch: 792, train loss: 0.01570, val loss: 0.01631\n",
      "Training epoch: 793, train loss: 0.01551, val loss: 0.01611\n",
      "Training epoch: 794, train loss: 0.01551, val loss: 0.01611\n",
      "Training epoch: 795, train loss: 0.01552, val loss: 0.01613\n",
      "Training epoch: 796, train loss: 0.01554, val loss: 0.01614\n",
      "Training epoch: 797, train loss: 0.01564, val loss: 0.01625\n",
      "Training epoch: 798, train loss: 0.01555, val loss: 0.01613\n",
      "Training epoch: 799, train loss: 0.01555, val loss: 0.01616\n",
      "Training epoch: 800, train loss: 0.01573, val loss: 0.01634\n",
      "Training epoch: 801, train loss: 0.01555, val loss: 0.01616\n",
      "Training epoch: 802, train loss: 0.01552, val loss: 0.01611\n",
      "Training epoch: 803, train loss: 0.01559, val loss: 0.01618\n",
      "Training epoch: 804, train loss: 0.01552, val loss: 0.01612\n",
      "Training epoch: 805, train loss: 0.01562, val loss: 0.01624\n",
      "Training epoch: 806, train loss: 0.01582, val loss: 0.01641\n",
      "Training epoch: 807, train loss: 0.01560, val loss: 0.01620\n",
      "Training epoch: 808, train loss: 0.01561, val loss: 0.01620\n",
      "Training epoch: 809, train loss: 0.01561, val loss: 0.01619\n",
      "Training epoch: 810, train loss: 0.01563, val loss: 0.01624\n",
      "Training epoch: 811, train loss: 0.01552, val loss: 0.01612\n",
      "Training epoch: 812, train loss: 0.01551, val loss: 0.01610\n",
      "Training epoch: 813, train loss: 0.01551, val loss: 0.01610\n",
      "Training epoch: 814, train loss: 0.01552, val loss: 0.01610\n",
      "Training epoch: 815, train loss: 0.01574, val loss: 0.01633\n",
      "Training epoch: 816, train loss: 0.01554, val loss: 0.01614\n",
      "Training epoch: 817, train loss: 0.01551, val loss: 0.01611\n",
      "Training epoch: 818, train loss: 0.01554, val loss: 0.01612\n",
      "Training epoch: 819, train loss: 0.01551, val loss: 0.01608\n",
      "Training epoch: 820, train loss: 0.01570, val loss: 0.01627\n",
      "Training epoch: 821, train loss: 0.01571, val loss: 0.01630\n",
      "Training epoch: 822, train loss: 0.01558, val loss: 0.01617\n",
      "Training epoch: 823, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 824, train loss: 0.01551, val loss: 0.01610\n",
      "Training epoch: 825, train loss: 0.01555, val loss: 0.01614\n",
      "Training epoch: 826, train loss: 0.01556, val loss: 0.01613\n",
      "Training epoch: 827, train loss: 0.01613, val loss: 0.01671\n",
      "Training epoch: 828, train loss: 0.01554, val loss: 0.01613\n",
      "Training epoch: 829, train loss: 0.01558, val loss: 0.01615\n",
      "Training epoch: 830, train loss: 0.01568, val loss: 0.01625\n",
      "Training epoch: 831, train loss: 0.01554, val loss: 0.01609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 832, train loss: 0.01553, val loss: 0.01610\n",
      "Training epoch: 833, train loss: 0.01553, val loss: 0.01613\n",
      "Training epoch: 834, train loss: 0.01554, val loss: 0.01612\n",
      "Training epoch: 835, train loss: 0.01554, val loss: 0.01612\n",
      "Training epoch: 836, train loss: 0.01551, val loss: 0.01610\n",
      "Training epoch: 837, train loss: 0.01550, val loss: 0.01607\n",
      "Training epoch: 838, train loss: 0.01551, val loss: 0.01608\n",
      "Training epoch: 839, train loss: 0.01552, val loss: 0.01609\n",
      "Training epoch: 840, train loss: 0.01559, val loss: 0.01615\n",
      "Training epoch: 841, train loss: 0.01551, val loss: 0.01608\n",
      "Training epoch: 842, train loss: 0.01553, val loss: 0.01608\n",
      "Training epoch: 843, train loss: 0.01554, val loss: 0.01612\n",
      "Training epoch: 844, train loss: 0.01553, val loss: 0.01610\n",
      "Training epoch: 845, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 846, train loss: 0.01550, val loss: 0.01607\n",
      "Training epoch: 847, train loss: 0.01559, val loss: 0.01615\n",
      "Training epoch: 848, train loss: 0.01553, val loss: 0.01610\n",
      "Training epoch: 849, train loss: 0.01571, val loss: 0.01630\n",
      "Training epoch: 850, train loss: 0.01553, val loss: 0.01611\n",
      "Training epoch: 851, train loss: 0.01603, val loss: 0.01661\n",
      "Training epoch: 852, train loss: 0.01577, val loss: 0.01633\n",
      "Training epoch: 853, train loss: 0.01572, val loss: 0.01630\n",
      "Training epoch: 854, train loss: 0.01554, val loss: 0.01610\n",
      "Training epoch: 855, train loss: 0.01554, val loss: 0.01612\n",
      "Training epoch: 856, train loss: 0.01552, val loss: 0.01609\n",
      "Training epoch: 857, train loss: 0.01568, val loss: 0.01623\n",
      "Training epoch: 858, train loss: 0.01557, val loss: 0.01613\n",
      "Training epoch: 859, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 860, train loss: 0.01560, val loss: 0.01615\n",
      "Training epoch: 861, train loss: 0.01552, val loss: 0.01608\n",
      "Training epoch: 862, train loss: 0.01553, val loss: 0.01610\n",
      "Training epoch: 863, train loss: 0.01562, val loss: 0.01619\n",
      "Training epoch: 864, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 865, train loss: 0.01556, val loss: 0.01612\n",
      "Training epoch: 866, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 867, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 868, train loss: 0.01551, val loss: 0.01609\n",
      "Training epoch: 869, train loss: 0.01553, val loss: 0.01608\n",
      "Training epoch: 870, train loss: 0.01561, val loss: 0.01615\n",
      "Training epoch: 871, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 872, train loss: 0.01550, val loss: 0.01609\n",
      "Training epoch: 873, train loss: 0.01561, val loss: 0.01617\n",
      "Training epoch: 874, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 875, train loss: 0.01570, val loss: 0.01626\n",
      "Training epoch: 876, train loss: 0.01559, val loss: 0.01613\n",
      "Training epoch: 877, train loss: 0.01550, val loss: 0.01606\n",
      "Training epoch: 878, train loss: 0.01565, val loss: 0.01621\n",
      "Training epoch: 879, train loss: 0.01571, val loss: 0.01629\n",
      "Training epoch: 880, train loss: 0.01552, val loss: 0.01608\n",
      "Training epoch: 881, train loss: 0.01550, val loss: 0.01606\n",
      "Training epoch: 882, train loss: 0.01566, val loss: 0.01621\n",
      "Training epoch: 883, train loss: 0.01557, val loss: 0.01612\n",
      "Training epoch: 884, train loss: 0.01564, val loss: 0.01619\n",
      "Training epoch: 885, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 886, train loss: 0.01550, val loss: 0.01605\n",
      "Training epoch: 887, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 888, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 889, train loss: 0.01554, val loss: 0.01611\n",
      "Training epoch: 890, train loss: 0.01553, val loss: 0.01609\n",
      "Training epoch: 891, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 892, train loss: 0.01552, val loss: 0.01606\n",
      "Training epoch: 893, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 894, train loss: 0.01554, val loss: 0.01610\n",
      "Training epoch: 895, train loss: 0.01565, val loss: 0.01621\n",
      "Training epoch: 896, train loss: 0.01555, val loss: 0.01611\n",
      "Training epoch: 897, train loss: 0.01562, val loss: 0.01618\n",
      "Training epoch: 898, train loss: 0.01550, val loss: 0.01604\n",
      "Training epoch: 899, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 900, train loss: 0.01550, val loss: 0.01607\n",
      "Training epoch: 901, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 902, train loss: 0.01552, val loss: 0.01604\n",
      "Training epoch: 903, train loss: 0.01573, val loss: 0.01628\n",
      "Training epoch: 904, train loss: 0.01578, val loss: 0.01636\n",
      "Training epoch: 905, train loss: 0.01572, val loss: 0.01626\n",
      "Training epoch: 906, train loss: 0.01555, val loss: 0.01608\n",
      "Training epoch: 907, train loss: 0.01555, val loss: 0.01611\n",
      "Training epoch: 908, train loss: 0.01562, val loss: 0.01616\n",
      "Training epoch: 909, train loss: 0.01558, val loss: 0.01611\n",
      "Training epoch: 910, train loss: 0.01563, val loss: 0.01616\n",
      "Training epoch: 911, train loss: 0.01588, val loss: 0.01644\n",
      "Training epoch: 912, train loss: 0.01578, val loss: 0.01631\n",
      "Training epoch: 913, train loss: 0.01561, val loss: 0.01616\n",
      "Training epoch: 914, train loss: 0.01554, val loss: 0.01609\n",
      "Training epoch: 915, train loss: 0.01553, val loss: 0.01607\n",
      "Training epoch: 916, train loss: 0.01558, val loss: 0.01614\n",
      "Training epoch: 917, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 918, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 919, train loss: 0.01550, val loss: 0.01606\n",
      "Training epoch: 920, train loss: 0.01550, val loss: 0.01605\n",
      "Training epoch: 921, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 922, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 923, train loss: 0.01551, val loss: 0.01604\n",
      "Training epoch: 924, train loss: 0.01558, val loss: 0.01612\n",
      "Training epoch: 925, train loss: 0.01595, val loss: 0.01650\n",
      "Training epoch: 926, train loss: 0.01581, val loss: 0.01638\n",
      "Training epoch: 927, train loss: 0.01569, val loss: 0.01622\n",
      "Training epoch: 928, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 929, train loss: 0.01556, val loss: 0.01610\n",
      "Training epoch: 930, train loss: 0.01553, val loss: 0.01607\n",
      "Training epoch: 931, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 932, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 933, train loss: 0.01562, val loss: 0.01616\n",
      "Training epoch: 934, train loss: 0.01552, val loss: 0.01606\n",
      "Training epoch: 935, train loss: 0.01558, val loss: 0.01608\n",
      "Training epoch: 936, train loss: 0.01558, val loss: 0.01612\n",
      "Training epoch: 937, train loss: 0.01559, val loss: 0.01615\n",
      "Training epoch: 938, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 939, train loss: 0.01553, val loss: 0.01604\n",
      "Training epoch: 940, train loss: 0.01579, val loss: 0.01634\n",
      "Training epoch: 941, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 942, train loss: 0.01569, val loss: 0.01622\n",
      "Training epoch: 943, train loss: 0.01557, val loss: 0.01610\n",
      "Training epoch: 944, train loss: 0.01556, val loss: 0.01609\n",
      "Training epoch: 945, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 946, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 947, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 948, train loss: 0.01554, val loss: 0.01609\n",
      "Training epoch: 949, train loss: 0.01550, val loss: 0.01604\n",
      "Training epoch: 950, train loss: 0.01556, val loss: 0.01609\n",
      "Training epoch: 951, train loss: 0.01557, val loss: 0.01609\n",
      "Training epoch: 952, train loss: 0.01554, val loss: 0.01607\n",
      "Training epoch: 953, train loss: 0.01575, val loss: 0.01629\n",
      "Training epoch: 954, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 955, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 956, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 957, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 958, train loss: 0.01557, val loss: 0.01611\n",
      "Training epoch: 959, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 960, train loss: 0.01560, val loss: 0.01613\n",
      "Training epoch: 961, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 962, train loss: 0.01554, val loss: 0.01609\n",
      "Training epoch: 963, train loss: 0.01551, val loss: 0.01604\n",
      "Training epoch: 964, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 965, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 966, train loss: 0.01556, val loss: 0.01609\n",
      "Training epoch: 967, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 968, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 969, train loss: 0.01577, val loss: 0.01631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 970, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 971, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 972, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 973, train loss: 0.01555, val loss: 0.01609\n",
      "Training epoch: 974, train loss: 0.01563, val loss: 0.01617\n",
      "Training epoch: 975, train loss: 0.01552, val loss: 0.01604\n",
      "Training epoch: 976, train loss: 0.01552, val loss: 0.01606\n",
      "Training epoch: 977, train loss: 0.01570, val loss: 0.01623\n",
      "Training epoch: 978, train loss: 0.01561, val loss: 0.01614\n",
      "Training epoch: 979, train loss: 0.01548, val loss: 0.01603\n",
      "Training epoch: 980, train loss: 0.01558, val loss: 0.01611\n",
      "Training epoch: 981, train loss: 0.01593, val loss: 0.01645\n",
      "Training epoch: 982, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 983, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 984, train loss: 0.01551, val loss: 0.01603\n",
      "Training epoch: 985, train loss: 0.01552, val loss: 0.01606\n",
      "Training epoch: 986, train loss: 0.01564, val loss: 0.01618\n",
      "Training epoch: 987, train loss: 0.01551, val loss: 0.01606\n",
      "Training epoch: 988, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 989, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 990, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 991, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 992, train loss: 0.01568, val loss: 0.01620\n",
      "Training epoch: 993, train loss: 0.01551, val loss: 0.01605\n",
      "Training epoch: 994, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 995, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 996, train loss: 0.01550, val loss: 0.01604\n",
      "Training epoch: 997, train loss: 0.01569, val loss: 0.01623\n",
      "Training epoch: 998, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 999, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 1000, train loss: 0.01592, val loss: 0.01647\n",
      "Training epoch: 1001, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1002, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1003, train loss: 0.01565, val loss: 0.01617\n",
      "Training epoch: 1004, train loss: 0.01574, val loss: 0.01629\n",
      "Training epoch: 1005, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 1006, train loss: 0.01571, val loss: 0.01625\n",
      "Training epoch: 1007, train loss: 0.01564, val loss: 0.01618\n",
      "Training epoch: 1008, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 1009, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 1010, train loss: 0.01565, val loss: 0.01617\n",
      "Training epoch: 1011, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1012, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 1013, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 1014, train loss: 0.01598, val loss: 0.01653\n",
      "Training epoch: 1015, train loss: 0.01578, val loss: 0.01631\n",
      "Training epoch: 1016, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 1017, train loss: 0.01594, val loss: 0.01647\n",
      "Training epoch: 1018, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1019, train loss: 0.01554, val loss: 0.01605\n",
      "Training epoch: 1020, train loss: 0.01549, val loss: 0.01604\n",
      "Training epoch: 1021, train loss: 0.01568, val loss: 0.01621\n",
      "Training epoch: 1022, train loss: 0.01561, val loss: 0.01613\n",
      "Training epoch: 1023, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1024, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1025, train loss: 0.01562, val loss: 0.01615\n",
      "Training epoch: 1026, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1027, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1028, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 1029, train loss: 0.01585, val loss: 0.01639\n",
      "Training epoch: 1030, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1031, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1032, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1033, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1034, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1035, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1036, train loss: 0.01562, val loss: 0.01616\n",
      "Training epoch: 1037, train loss: 0.01569, val loss: 0.01622\n",
      "Training epoch: 1038, train loss: 0.01557, val loss: 0.01608\n",
      "Training epoch: 1039, train loss: 0.01551, val loss: 0.01604\n",
      "Training epoch: 1040, train loss: 0.01555, val loss: 0.01608\n",
      "Training epoch: 1041, train loss: 0.01574, val loss: 0.01627\n",
      "Training epoch: 1042, train loss: 0.01560, val loss: 0.01612\n",
      "Training epoch: 1043, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 1044, train loss: 0.01559, val loss: 0.01611\n",
      "Training epoch: 1045, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 1046, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 1047, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1048, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1049, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1050, train loss: 0.01551, val loss: 0.01607\n",
      "Training epoch: 1051, train loss: 0.01558, val loss: 0.01611\n",
      "Training epoch: 1052, train loss: 0.01552, val loss: 0.01604\n",
      "Training epoch: 1053, train loss: 0.01568, val loss: 0.01622\n",
      "Training epoch: 1054, train loss: 0.01577, val loss: 0.01630\n",
      "Training epoch: 1055, train loss: 0.01570, val loss: 0.01623\n",
      "Training epoch: 1056, train loss: 0.01572, val loss: 0.01623\n",
      "Training epoch: 1057, train loss: 0.01562, val loss: 0.01617\n",
      "Training epoch: 1058, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1059, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1060, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1061, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1062, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1063, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 1064, train loss: 0.01548, val loss: 0.01602\n",
      "Training epoch: 1065, train loss: 0.01562, val loss: 0.01615\n",
      "Training epoch: 1066, train loss: 0.01576, val loss: 0.01630\n",
      "Training epoch: 1067, train loss: 0.01570, val loss: 0.01620\n",
      "Training epoch: 1068, train loss: 0.01574, val loss: 0.01628\n",
      "Training epoch: 1069, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1070, train loss: 0.01558, val loss: 0.01610\n",
      "Training epoch: 1071, train loss: 0.01569, val loss: 0.01620\n",
      "Training epoch: 1072, train loss: 0.01554, val loss: 0.01606\n",
      "Training epoch: 1073, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1074, train loss: 0.01577, val loss: 0.01630\n",
      "Training epoch: 1075, train loss: 0.01606, val loss: 0.01658\n",
      "Training epoch: 1076, train loss: 0.01607, val loss: 0.01661\n",
      "Training epoch: 1077, train loss: 0.01564, val loss: 0.01615\n",
      "Training epoch: 1078, train loss: 0.01551, val loss: 0.01603\n",
      "Training epoch: 1079, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1080, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1081, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1082, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1083, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1084, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1085, train loss: 0.01551, val loss: 0.01604\n",
      "Training epoch: 1086, train loss: 0.01560, val loss: 0.01610\n",
      "Training epoch: 1087, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1088, train loss: 0.01566, val loss: 0.01619\n",
      "Training epoch: 1089, train loss: 0.01557, val loss: 0.01608\n",
      "Training epoch: 1090, train loss: 0.01553, val loss: 0.01608\n",
      "Training epoch: 1091, train loss: 0.01549, val loss: 0.01603\n",
      "Training epoch: 1092, train loss: 0.01561, val loss: 0.01614\n",
      "Training epoch: 1093, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1094, train loss: 0.01553, val loss: 0.01604\n",
      "Training epoch: 1095, train loss: 0.01553, val loss: 0.01604\n",
      "Training epoch: 1096, train loss: 0.01558, val loss: 0.01610\n",
      "Training epoch: 1097, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1098, train loss: 0.01562, val loss: 0.01613\n",
      "Training epoch: 1099, train loss: 0.01562, val loss: 0.01615\n",
      "Training epoch: 1100, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1101, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1102, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1103, train loss: 0.01564, val loss: 0.01617\n",
      "Training epoch: 1104, train loss: 0.01592, val loss: 0.01646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1105, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 1106, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1107, train loss: 0.01578, val loss: 0.01632\n",
      "Training epoch: 1108, train loss: 0.01576, val loss: 0.01629\n",
      "Training epoch: 1109, train loss: 0.01556, val loss: 0.01607\n",
      "Training epoch: 1110, train loss: 0.01559, val loss: 0.01610\n",
      "Training epoch: 1111, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 1112, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1113, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1114, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1115, train loss: 0.01557, val loss: 0.01609\n",
      "Training epoch: 1116, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1117, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1118, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1119, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1120, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1121, train loss: 0.01551, val loss: 0.01604\n",
      "Training epoch: 1122, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1123, train loss: 0.01546, val loss: 0.01597\n",
      "Training epoch: 1124, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1125, train loss: 0.01579, val loss: 0.01631\n",
      "Training epoch: 1126, train loss: 0.01564, val loss: 0.01616\n",
      "Training epoch: 1127, train loss: 0.01568, val loss: 0.01619\n",
      "Training epoch: 1128, train loss: 0.01565, val loss: 0.01617\n",
      "Training epoch: 1129, train loss: 0.01565, val loss: 0.01618\n",
      "Training epoch: 1130, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1131, train loss: 0.01550, val loss: 0.01603\n",
      "Training epoch: 1132, train loss: 0.01559, val loss: 0.01611\n",
      "Training epoch: 1133, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1134, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1135, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1136, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1137, train loss: 0.01552, val loss: 0.01605\n",
      "Training epoch: 1138, train loss: 0.01565, val loss: 0.01617\n",
      "Training epoch: 1139, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1140, train loss: 0.01555, val loss: 0.01608\n",
      "Training epoch: 1141, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1142, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1143, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1144, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1145, train loss: 0.01569, val loss: 0.01621\n",
      "Training epoch: 1146, train loss: 0.01559, val loss: 0.01611\n",
      "Training epoch: 1147, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1148, train loss: 0.01569, val loss: 0.01619\n",
      "Training epoch: 1149, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1150, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1151, train loss: 0.01554, val loss: 0.01605\n",
      "Training epoch: 1152, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1153, train loss: 0.01547, val loss: 0.01598\n",
      "Training epoch: 1154, train loss: 0.01553, val loss: 0.01604\n",
      "Training epoch: 1155, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1156, train loss: 0.01552, val loss: 0.01603\n",
      "Training epoch: 1157, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1158, train loss: 0.01552, val loss: 0.01603\n",
      "Training epoch: 1159, train loss: 0.01607, val loss: 0.01660\n",
      "Training epoch: 1160, train loss: 0.01575, val loss: 0.01629\n",
      "Training epoch: 1161, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1162, train loss: 0.01561, val loss: 0.01613\n",
      "Training epoch: 1163, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1164, train loss: 0.01565, val loss: 0.01616\n",
      "Training epoch: 1165, train loss: 0.01598, val loss: 0.01648\n",
      "Training epoch: 1166, train loss: 0.01570, val loss: 0.01622\n",
      "Training epoch: 1167, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1168, train loss: 0.01564, val loss: 0.01616\n",
      "Training epoch: 1169, train loss: 0.01589, val loss: 0.01640\n",
      "Training epoch: 1170, train loss: 0.01583, val loss: 0.01637\n",
      "Training epoch: 1171, train loss: 0.01573, val loss: 0.01627\n",
      "Training epoch: 1172, train loss: 0.01571, val loss: 0.01622\n",
      "Training epoch: 1173, train loss: 0.01574, val loss: 0.01627\n",
      "Training epoch: 1174, train loss: 0.01557, val loss: 0.01609\n",
      "Training epoch: 1175, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1176, train loss: 0.01562, val loss: 0.01615\n",
      "Training epoch: 1177, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 1178, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1179, train loss: 0.01584, val loss: 0.01634\n",
      "Training epoch: 1180, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1181, train loss: 0.01553, val loss: 0.01606\n",
      "Training epoch: 1182, train loss: 0.01560, val loss: 0.01611\n",
      "Training epoch: 1183, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1184, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1185, train loss: 0.01556, val loss: 0.01608\n",
      "Training epoch: 1186, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1187, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1188, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1189, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1190, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1191, train loss: 0.01555, val loss: 0.01606\n",
      "Training epoch: 1192, train loss: 0.01555, val loss: 0.01606\n",
      "Training epoch: 1193, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1194, train loss: 0.01549, val loss: 0.01602\n",
      "Training epoch: 1195, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1196, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1197, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1198, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1199, train loss: 0.01553, val loss: 0.01605\n",
      "Training epoch: 1200, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1201, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1202, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1203, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1204, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1205, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1206, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1207, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1208, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1209, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1210, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1211, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1212, train loss: 0.01549, val loss: 0.01601\n",
      "Training epoch: 1213, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1214, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1215, train loss: 0.01547, val loss: 0.01600\n",
      "Training epoch: 1216, train loss: 0.01548, val loss: 0.01600\n",
      "Training epoch: 1217, train loss: 0.01566, val loss: 0.01616\n",
      "Training epoch: 1218, train loss: 0.01579, val loss: 0.01629\n",
      "Training epoch: 1219, train loss: 0.01571, val loss: 0.01622\n",
      "Training epoch: 1220, train loss: 0.01568, val loss: 0.01617\n",
      "Training epoch: 1221, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 1222, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1223, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1224, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1225, train loss: 0.01556, val loss: 0.01609\n",
      "Training epoch: 1226, train loss: 0.01571, val loss: 0.01623\n",
      "Training epoch: 1227, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1228, train loss: 0.01550, val loss: 0.01602\n",
      "Training epoch: 1229, train loss: 0.01557, val loss: 0.01606\n",
      "Training epoch: 1230, train loss: 0.01555, val loss: 0.01607\n",
      "Training epoch: 1231, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1232, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1233, train loss: 0.01569, val loss: 0.01621\n",
      "Training epoch: 1234, train loss: 0.01578, val loss: 0.01632\n",
      "Training epoch: 1235, train loss: 0.01559, val loss: 0.01609\n",
      "Training epoch: 1236, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 1237, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1238, train loss: 0.01556, val loss: 0.01605\n",
      "Training epoch: 1239, train loss: 0.01554, val loss: 0.01606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1240, train loss: 0.01582, val loss: 0.01633\n",
      "Training epoch: 1241, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1242, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1243, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1244, train loss: 0.01559, val loss: 0.01609\n",
      "Training epoch: 1245, train loss: 0.01547, val loss: 0.01598\n",
      "Training epoch: 1246, train loss: 0.01564, val loss: 0.01613\n",
      "Training epoch: 1247, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1248, train loss: 0.01568, val loss: 0.01619\n",
      "Training epoch: 1249, train loss: 0.01564, val loss: 0.01617\n",
      "Training epoch: 1250, train loss: 0.01559, val loss: 0.01607\n",
      "Training epoch: 1251, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1252, train loss: 0.01559, val loss: 0.01610\n",
      "Training epoch: 1253, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1254, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1255, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1256, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1257, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1258, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1259, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1260, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1261, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1262, train loss: 0.01552, val loss: 0.01604\n",
      "Training epoch: 1263, train loss: 0.01570, val loss: 0.01622\n",
      "Training epoch: 1264, train loss: 0.01588, val loss: 0.01639\n",
      "Training epoch: 1265, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1266, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1267, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1268, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1269, train loss: 0.01548, val loss: 0.01601\n",
      "Training epoch: 1270, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1271, train loss: 0.01568, val loss: 0.01617\n",
      "Training epoch: 1272, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1273, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1274, train loss: 0.01558, val loss: 0.01609\n",
      "Training epoch: 1275, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1276, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1277, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1278, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1279, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1280, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1281, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1282, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1283, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1284, train loss: 0.01558, val loss: 0.01606\n",
      "Training epoch: 1285, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1286, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1287, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1288, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1289, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1290, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1291, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 1292, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1293, train loss: 0.01547, val loss: 0.01598\n",
      "Training epoch: 1294, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1295, train loss: 0.01560, val loss: 0.01611\n",
      "Training epoch: 1296, train loss: 0.01555, val loss: 0.01605\n",
      "Training epoch: 1297, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1298, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1299, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1300, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1301, train loss: 0.01566, val loss: 0.01616\n",
      "Training epoch: 1302, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1303, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1304, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1305, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1306, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1307, train loss: 0.01559, val loss: 0.01609\n",
      "Training epoch: 1308, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1309, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1310, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 1311, train loss: 0.01572, val loss: 0.01624\n",
      "Training epoch: 1312, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 1313, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1314, train loss: 0.01551, val loss: 0.01603\n",
      "Training epoch: 1315, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1316, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1317, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1318, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1319, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1320, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 1321, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1322, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1323, train loss: 0.01558, val loss: 0.01608\n",
      "Training epoch: 1324, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1325, train loss: 0.01564, val loss: 0.01615\n",
      "Training epoch: 1326, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1327, train loss: 0.01569, val loss: 0.01617\n",
      "Training epoch: 1328, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 1329, train loss: 0.01578, val loss: 0.01627\n",
      "Training epoch: 1330, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1331, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1332, train loss: 0.01562, val loss: 0.01611\n",
      "Training epoch: 1333, train loss: 0.01562, val loss: 0.01612\n",
      "Training epoch: 1334, train loss: 0.01556, val loss: 0.01605\n",
      "Training epoch: 1335, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1336, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1337, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1338, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1339, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1340, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1341, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1342, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1343, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1344, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1345, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1346, train loss: 0.01557, val loss: 0.01608\n",
      "Training epoch: 1347, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1348, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1349, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1350, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1351, train loss: 0.01563, val loss: 0.01614\n",
      "Training epoch: 1352, train loss: 0.01552, val loss: 0.01602\n",
      "Training epoch: 1353, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 1354, train loss: 0.01570, val loss: 0.01620\n",
      "Training epoch: 1355, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 1356, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1357, train loss: 0.01588, val loss: 0.01637\n",
      "Training epoch: 1358, train loss: 0.01557, val loss: 0.01607\n",
      "Training epoch: 1359, train loss: 0.01577, val loss: 0.01626\n",
      "Training epoch: 1360, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1361, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1362, train loss: 0.01557, val loss: 0.01609\n",
      "Training epoch: 1363, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1364, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1365, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1366, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1367, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1368, train loss: 0.01572, val loss: 0.01621\n",
      "Training epoch: 1369, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1370, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1371, train loss: 0.01565, val loss: 0.01616\n",
      "Training epoch: 1372, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1373, train loss: 0.01557, val loss: 0.01606\n",
      "Training epoch: 1374, train loss: 0.01554, val loss: 0.01603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1375, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1376, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1377, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1378, train loss: 0.01568, val loss: 0.01617\n",
      "Training epoch: 1379, train loss: 0.01563, val loss: 0.01613\n",
      "Training epoch: 1380, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1381, train loss: 0.01547, val loss: 0.01596\n",
      "Training epoch: 1382, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1383, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1384, train loss: 0.01569, val loss: 0.01618\n",
      "Training epoch: 1385, train loss: 0.01557, val loss: 0.01607\n",
      "Training epoch: 1386, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1387, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1388, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1389, train loss: 0.01556, val loss: 0.01605\n",
      "Training epoch: 1390, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1391, train loss: 0.01549, val loss: 0.01600\n",
      "Training epoch: 1392, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1393, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1394, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1395, train loss: 0.01552, val loss: 0.01603\n",
      "Training epoch: 1396, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1397, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1398, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1399, train loss: 0.01565, val loss: 0.01615\n",
      "Training epoch: 1400, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 1401, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1402, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1403, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1404, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1405, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1406, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1407, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1408, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1409, train loss: 0.01552, val loss: 0.01603\n",
      "Training epoch: 1410, train loss: 0.01565, val loss: 0.01614\n",
      "Training epoch: 1411, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1412, train loss: 0.01548, val loss: 0.01599\n",
      "Training epoch: 1413, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1414, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1415, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1416, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1417, train loss: 0.01570, val loss: 0.01619\n",
      "Training epoch: 1418, train loss: 0.01598, val loss: 0.01649\n",
      "Training epoch: 1419, train loss: 0.01576, val loss: 0.01628\n",
      "Training epoch: 1420, train loss: 0.01560, val loss: 0.01610\n",
      "Training epoch: 1421, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1422, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 1423, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 1424, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1425, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1426, train loss: 0.01595, val loss: 0.01644\n",
      "Training epoch: 1427, train loss: 0.01593, val loss: 0.01641\n",
      "Training epoch: 1428, train loss: 0.01561, val loss: 0.01613\n",
      "Training epoch: 1429, train loss: 0.01561, val loss: 0.01612\n",
      "Training epoch: 1430, train loss: 0.01586, val loss: 0.01637\n",
      "Training epoch: 1431, train loss: 0.01574, val loss: 0.01624\n",
      "Training epoch: 1432, train loss: 0.01554, val loss: 0.01604\n",
      "Training epoch: 1433, train loss: 0.01558, val loss: 0.01608\n",
      "Training epoch: 1434, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1435, train loss: 0.01564, val loss: 0.01610\n",
      "Training epoch: 1436, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1437, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1438, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1439, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1440, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1441, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1442, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1443, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1444, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1445, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 1446, train loss: 0.01553, val loss: 0.01603\n",
      "Training epoch: 1447, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1448, train loss: 0.01560, val loss: 0.01610\n",
      "Training epoch: 1449, train loss: 0.01555, val loss: 0.01604\n",
      "Training epoch: 1450, train loss: 0.01595, val loss: 0.01646\n",
      "Training epoch: 1451, train loss: 0.01590, val loss: 0.01641\n",
      "Training epoch: 1452, train loss: 0.01557, val loss: 0.01606\n",
      "Training epoch: 1453, train loss: 0.01562, val loss: 0.01610\n",
      "Training epoch: 1454, train loss: 0.01566, val loss: 0.01616\n",
      "Training epoch: 1455, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1456, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1457, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1458, train loss: 0.01563, val loss: 0.01613\n",
      "Training epoch: 1459, train loss: 0.01601, val loss: 0.01653\n",
      "Training epoch: 1460, train loss: 0.01592, val loss: 0.01644\n",
      "Training epoch: 1461, train loss: 0.01569, val loss: 0.01619\n",
      "Training epoch: 1462, train loss: 0.01577, val loss: 0.01626\n",
      "Training epoch: 1463, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1464, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1465, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1466, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1467, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1468, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1469, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1470, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1471, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1472, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1473, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1474, train loss: 0.01592, val loss: 0.01641\n",
      "Training epoch: 1475, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1476, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 1477, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1478, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1479, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1480, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1481, train loss: 0.01560, val loss: 0.01610\n",
      "Training epoch: 1482, train loss: 0.01552, val loss: 0.01603\n",
      "Training epoch: 1483, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1484, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1485, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1486, train loss: 0.01584, val loss: 0.01635\n",
      "Training epoch: 1487, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 1488, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 1489, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1490, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1491, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1492, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1493, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1494, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1495, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1496, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1497, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1498, train loss: 0.01574, val loss: 0.01624\n",
      "Training epoch: 1499, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 1500, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1501, train loss: 0.01555, val loss: 0.01605\n",
      "Training epoch: 1502, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1503, train loss: 0.01559, val loss: 0.01608\n",
      "Training epoch: 1504, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1505, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1506, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 1507, train loss: 0.01555, val loss: 0.01604\n",
      "Training epoch: 1508, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1509, train loss: 0.01553, val loss: 0.01603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1510, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1511, train loss: 0.01557, val loss: 0.01607\n",
      "Training epoch: 1512, train loss: 0.01565, val loss: 0.01614\n",
      "Training epoch: 1513, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1514, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1515, train loss: 0.01561, val loss: 0.01611\n",
      "Training epoch: 1516, train loss: 0.01551, val loss: 0.01601\n",
      "Training epoch: 1517, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1518, train loss: 0.01589, val loss: 0.01639\n",
      "Training epoch: 1519, train loss: 0.01559, val loss: 0.01608\n",
      "Training epoch: 1520, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1521, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1522, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1523, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1524, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 1525, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1526, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1527, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1528, train loss: 0.01553, val loss: 0.01604\n",
      "Training epoch: 1529, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1530, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1531, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1532, train loss: 0.01577, val loss: 0.01628\n",
      "Training epoch: 1533, train loss: 0.01587, val loss: 0.01638\n",
      "Training epoch: 1534, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1535, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 1536, train loss: 0.01559, val loss: 0.01609\n",
      "Training epoch: 1537, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1538, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1539, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1540, train loss: 0.01551, val loss: 0.01602\n",
      "Training epoch: 1541, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1542, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1543, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1544, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1545, train loss: 0.01573, val loss: 0.01623\n",
      "Training epoch: 1546, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1547, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1548, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1549, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1550, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1551, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1552, train loss: 0.01578, val loss: 0.01626\n",
      "Training epoch: 1553, train loss: 0.01558, val loss: 0.01606\n",
      "Training epoch: 1554, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1555, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1556, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1557, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1558, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1559, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1560, train loss: 0.01563, val loss: 0.01613\n",
      "Training epoch: 1561, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1562, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 1563, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 1564, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1565, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1566, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1567, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1568, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1569, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1570, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1571, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1572, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1573, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1574, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1575, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1576, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1577, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1578, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1579, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1580, train loss: 0.01562, val loss: 0.01611\n",
      "Training epoch: 1581, train loss: 0.01593, val loss: 0.01642\n",
      "Training epoch: 1582, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1583, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1584, train loss: 0.01580, val loss: 0.01629\n",
      "Training epoch: 1585, train loss: 0.01578, val loss: 0.01627\n",
      "Training epoch: 1586, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 1587, train loss: 0.01561, val loss: 0.01611\n",
      "Training epoch: 1588, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1589, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1590, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1591, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1592, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1593, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1594, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1595, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 1596, train loss: 0.01560, val loss: 0.01610\n",
      "Training epoch: 1597, train loss: 0.01574, val loss: 0.01622\n",
      "Training epoch: 1598, train loss: 0.01557, val loss: 0.01606\n",
      "Training epoch: 1599, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1600, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1601, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1602, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1603, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1604, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1605, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1606, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1607, train loss: 0.01559, val loss: 0.01607\n",
      "Training epoch: 1608, train loss: 0.01577, val loss: 0.01626\n",
      "Training epoch: 1609, train loss: 0.01578, val loss: 0.01627\n",
      "Training epoch: 1610, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1611, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1612, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 1613, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 1614, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1615, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1616, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1617, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1618, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1619, train loss: 0.01582, val loss: 0.01630\n",
      "Training epoch: 1620, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1621, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1622, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1623, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1624, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1625, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1626, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1627, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 1628, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1629, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1630, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1631, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1632, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 1633, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1634, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 1635, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1636, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 1637, train loss: 0.01557, val loss: 0.01603\n",
      "Training epoch: 1638, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1639, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1640, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1641, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1642, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1643, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1644, train loss: 0.01555, val loss: 0.01603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1645, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1646, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1647, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1648, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1649, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1650, train loss: 0.01602, val loss: 0.01651\n",
      "Training epoch: 1651, train loss: 0.01569, val loss: 0.01616\n",
      "Training epoch: 1652, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1653, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1654, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1655, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1656, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1657, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1658, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1659, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1660, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1661, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1662, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1663, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1664, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1665, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1666, train loss: 0.01568, val loss: 0.01614\n",
      "Training epoch: 1667, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 1668, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1669, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1670, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1671, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1672, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1673, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1674, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1675, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 1676, train loss: 0.01556, val loss: 0.01601\n",
      "Training epoch: 1677, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1678, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 1679, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1680, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1681, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1682, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1683, train loss: 0.01562, val loss: 0.01608\n",
      "Training epoch: 1684, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1685, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1686, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1687, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1688, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1689, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1690, train loss: 0.01561, val loss: 0.01605\n",
      "Training epoch: 1691, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1692, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1693, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1694, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1695, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1696, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 1697, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1698, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1699, train loss: 0.01572, val loss: 0.01620\n",
      "Training epoch: 1700, train loss: 0.01564, val loss: 0.01609\n",
      "Training epoch: 1701, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 1702, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1703, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1704, train loss: 0.01571, val loss: 0.01618\n",
      "Training epoch: 1705, train loss: 0.01581, val loss: 0.01627\n",
      "Training epoch: 1706, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 1707, train loss: 0.01576, val loss: 0.01625\n",
      "Training epoch: 1708, train loss: 0.01602, val loss: 0.01651\n",
      "Training epoch: 1709, train loss: 0.01576, val loss: 0.01624\n",
      "Training epoch: 1710, train loss: 0.01612, val loss: 0.01660\n",
      "Training epoch: 1711, train loss: 0.01570, val loss: 0.01618\n",
      "Training epoch: 1712, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 1713, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1714, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 1715, train loss: 0.01579, val loss: 0.01626\n",
      "Training epoch: 1716, train loss: 0.01567, val loss: 0.01613\n",
      "Training epoch: 1717, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1718, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1719, train loss: 0.01552, val loss: 0.01600\n",
      "Training epoch: 1720, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1721, train loss: 0.01554, val loss: 0.01599\n",
      "Training epoch: 1722, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1723, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 1724, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 1725, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1726, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1727, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1728, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1729, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1730, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1731, train loss: 0.01554, val loss: 0.01599\n",
      "Training epoch: 1732, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1733, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1734, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1735, train loss: 0.01562, val loss: 0.01608\n",
      "Training epoch: 1736, train loss: 0.01571, val loss: 0.01618\n",
      "Training epoch: 1737, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 1738, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 1739, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1740, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1741, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 1742, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1743, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1744, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1745, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1746, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1747, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1748, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1749, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1750, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1751, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1752, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1753, train loss: 0.01559, val loss: 0.01607\n",
      "Training epoch: 1754, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 1755, train loss: 0.01555, val loss: 0.01604\n",
      "Training epoch: 1756, train loss: 0.01567, val loss: 0.01614\n",
      "Training epoch: 1757, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1758, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1759, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 1760, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1761, train loss: 0.01575, val loss: 0.01622\n",
      "Training epoch: 1762, train loss: 0.01585, val loss: 0.01635\n",
      "Training epoch: 1763, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1764, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 1765, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1766, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1767, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1768, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 1769, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 1770, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1771, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1772, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 1773, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1774, train loss: 0.01556, val loss: 0.01605\n",
      "Training epoch: 1775, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1776, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1777, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1778, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1779, train loss: 0.01549, val loss: 0.01596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1780, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1781, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1782, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1783, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 1784, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1785, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1786, train loss: 0.01563, val loss: 0.01610\n",
      "Training epoch: 1787, train loss: 0.01572, val loss: 0.01618\n",
      "Training epoch: 1788, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1789, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 1790, train loss: 0.01556, val loss: 0.01606\n",
      "Training epoch: 1791, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1792, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1793, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1794, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1795, train loss: 0.01557, val loss: 0.01603\n",
      "Training epoch: 1796, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1797, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1798, train loss: 0.01570, val loss: 0.01615\n",
      "Training epoch: 1799, train loss: 0.01570, val loss: 0.01618\n",
      "Training epoch: 1800, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1801, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1802, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1803, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1804, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 1805, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1806, train loss: 0.01558, val loss: 0.01606\n",
      "Training epoch: 1807, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1808, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1809, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1810, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 1811, train loss: 0.01562, val loss: 0.01610\n",
      "Training epoch: 1812, train loss: 0.01564, val loss: 0.01612\n",
      "Training epoch: 1813, train loss: 0.01578, val loss: 0.01625\n",
      "Training epoch: 1814, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1815, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1816, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1817, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 1818, train loss: 0.01572, val loss: 0.01618\n",
      "Training epoch: 1819, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1820, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1821, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1822, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 1823, train loss: 0.01564, val loss: 0.01610\n",
      "Training epoch: 1824, train loss: 0.01563, val loss: 0.01610\n",
      "Training epoch: 1825, train loss: 0.01566, val loss: 0.01613\n",
      "Training epoch: 1826, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1827, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 1828, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1829, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1830, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1831, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1832, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1833, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1834, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 1835, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1836, train loss: 0.01555, val loss: 0.01602\n",
      "Training epoch: 1837, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1838, train loss: 0.01566, val loss: 0.01611\n",
      "Training epoch: 1839, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1840, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 1841, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1842, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1843, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1844, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 1845, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1846, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 1847, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1848, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 1849, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1850, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1851, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 1852, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1853, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 1854, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 1855, train loss: 0.01585, val loss: 0.01634\n",
      "Training epoch: 1856, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1857, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1858, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1859, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1860, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1861, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 1862, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1863, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1864, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1865, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1866, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1867, train loss: 0.01569, val loss: 0.01618\n",
      "Training epoch: 1868, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1869, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 1870, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1871, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1872, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1873, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 1874, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1875, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 1876, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1877, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1878, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1879, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1880, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1881, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1882, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1883, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1884, train loss: 0.01594, val loss: 0.01644\n",
      "Training epoch: 1885, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 1886, train loss: 0.01567, val loss: 0.01612\n",
      "Training epoch: 1887, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 1888, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 1889, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1890, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1891, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1892, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1893, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 1894, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1895, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 1896, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1897, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 1898, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 1899, train loss: 0.01557, val loss: 0.01603\n",
      "Training epoch: 1900, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1901, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 1902, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1903, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1904, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1905, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1906, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1907, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1908, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1909, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1910, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1911, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1912, train loss: 0.01549, val loss: 0.01598\n",
      "Training epoch: 1913, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1914, train loss: 0.01566, val loss: 0.01614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1915, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1916, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1917, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1918, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1919, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1920, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1921, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1922, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1923, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1924, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 1925, train loss: 0.01547, val loss: 0.01596\n",
      "Training epoch: 1926, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 1927, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 1928, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1929, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1930, train loss: 0.01556, val loss: 0.01603\n",
      "Training epoch: 1931, train loss: 0.01587, val loss: 0.01636\n",
      "Training epoch: 1932, train loss: 0.01644, val loss: 0.01696\n",
      "Training epoch: 1933, train loss: 0.01581, val loss: 0.01630\n",
      "Training epoch: 1934, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1935, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 1936, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 1937, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 1938, train loss: 0.01567, val loss: 0.01616\n",
      "Training epoch: 1939, train loss: 0.01566, val loss: 0.01612\n",
      "Training epoch: 1940, train loss: 0.01579, val loss: 0.01626\n",
      "Training epoch: 1941, train loss: 0.01559, val loss: 0.01607\n",
      "Training epoch: 1942, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 1943, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 1944, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 1945, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 1946, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1947, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1948, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 1949, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 1950, train loss: 0.01582, val loss: 0.01628\n",
      "Training epoch: 1951, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1952, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1953, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1954, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 1955, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1956, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1957, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 1958, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1959, train loss: 0.01550, val loss: 0.01599\n",
      "Training epoch: 1960, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1961, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1962, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1963, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 1964, train loss: 0.01561, val loss: 0.01610\n",
      "Training epoch: 1965, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 1966, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1967, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 1968, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 1969, train loss: 0.01586, val loss: 0.01635\n",
      "Training epoch: 1970, train loss: 0.01563, val loss: 0.01610\n",
      "Training epoch: 1971, train loss: 0.01569, val loss: 0.01616\n",
      "Training epoch: 1972, train loss: 0.01559, val loss: 0.01607\n",
      "Training epoch: 1973, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1974, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 1975, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 1976, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 1977, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1978, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1979, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1980, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 1981, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 1982, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1983, train loss: 0.01562, val loss: 0.01607\n",
      "Training epoch: 1984, train loss: 0.01560, val loss: 0.01605\n",
      "Training epoch: 1985, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 1986, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 1987, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 1988, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 1989, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1990, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1991, train loss: 0.01557, val loss: 0.01605\n",
      "Training epoch: 1992, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 1993, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 1994, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 1995, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 1996, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 1997, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 1998, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 1999, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 2000, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 2001, train loss: 0.01576, val loss: 0.01623\n",
      "Training epoch: 2002, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 2003, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 2004, train loss: 0.01547, val loss: 0.01596\n",
      "Training epoch: 2005, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 2006, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 2007, train loss: 0.01576, val loss: 0.01623\n",
      "Training epoch: 2008, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2009, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2010, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2011, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2012, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 2013, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 2014, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 2015, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2016, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2017, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 2018, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2019, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2020, train loss: 0.01564, val loss: 0.01610\n",
      "Training epoch: 2021, train loss: 0.01554, val loss: 0.01599\n",
      "Training epoch: 2022, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2023, train loss: 0.01575, val loss: 0.01621\n",
      "Training epoch: 2024, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 2025, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 2026, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 2027, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2028, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2029, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2030, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 2031, train loss: 0.01559, val loss: 0.01604\n",
      "Training epoch: 2032, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 2033, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2034, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 2035, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 2036, train loss: 0.01566, val loss: 0.01613\n",
      "Training epoch: 2037, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 2038, train loss: 0.01567, val loss: 0.01615\n",
      "Training epoch: 2039, train loss: 0.01569, val loss: 0.01615\n",
      "Training epoch: 2040, train loss: 0.01587, val loss: 0.01635\n",
      "Training epoch: 2041, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2042, train loss: 0.01561, val loss: 0.01611\n",
      "Training epoch: 2043, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 2044, train loss: 0.01566, val loss: 0.01614\n",
      "Training epoch: 2045, train loss: 0.01562, val loss: 0.01608\n",
      "Training epoch: 2046, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2047, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2048, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2049, train loss: 0.01547, val loss: 0.01595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2050, train loss: 0.01570, val loss: 0.01617\n",
      "Training epoch: 2051, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2052, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2053, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2054, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2055, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2056, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2057, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2058, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2059, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2060, train loss: 0.01546, val loss: 0.01590\n",
      "Training epoch: 2061, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2062, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2063, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2064, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2065, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2066, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2067, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2068, train loss: 0.01580, val loss: 0.01628\n",
      "Training epoch: 2069, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2070, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2071, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2072, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2073, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 2074, train loss: 0.01563, val loss: 0.01612\n",
      "Training epoch: 2075, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2076, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2077, train loss: 0.01559, val loss: 0.01605\n",
      "Training epoch: 2078, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 2079, train loss: 0.01576, val loss: 0.01626\n",
      "Training epoch: 2080, train loss: 0.01562, val loss: 0.01611\n",
      "Training epoch: 2081, train loss: 0.01588, val loss: 0.01636\n",
      "Training epoch: 2082, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2083, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 2084, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2085, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 2086, train loss: 0.01571, val loss: 0.01618\n",
      "Training epoch: 2087, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 2088, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 2089, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 2090, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2091, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2092, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2093, train loss: 0.01560, val loss: 0.01608\n",
      "Training epoch: 2094, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2095, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2096, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2097, train loss: 0.01545, val loss: 0.01594\n",
      "Training epoch: 2098, train loss: 0.01555, val loss: 0.01603\n",
      "Training epoch: 2099, train loss: 0.01561, val loss: 0.01605\n",
      "Training epoch: 2100, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2101, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 2102, train loss: 0.01573, val loss: 0.01620\n",
      "Training epoch: 2103, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2104, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2105, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2106, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2107, train loss: 0.01569, val loss: 0.01617\n",
      "Training epoch: 2108, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 2109, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 2110, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 2111, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 2112, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2113, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 2114, train loss: 0.01551, val loss: 0.01600\n",
      "Training epoch: 2115, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 2116, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2117, train loss: 0.01562, val loss: 0.01609\n",
      "Training epoch: 2118, train loss: 0.01564, val loss: 0.01611\n",
      "Training epoch: 2119, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2120, train loss: 0.01561, val loss: 0.01607\n",
      "Training epoch: 2121, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2122, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2123, train loss: 0.01545, val loss: 0.01594\n",
      "Training epoch: 2124, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2125, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 2126, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2127, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 2128, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2129, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2130, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2131, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2132, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2133, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 2134, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 2135, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2136, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 2137, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2138, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2139, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 2140, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2141, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2142, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2143, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2144, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 2145, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2146, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 2147, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2148, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2149, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2150, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 2151, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2152, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2153, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 2154, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2155, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 2156, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2157, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 2158, train loss: 0.01597, val loss: 0.01644\n",
      "Training epoch: 2159, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 2160, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2161, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2162, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 2163, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2164, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2165, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 2166, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2167, train loss: 0.01573, val loss: 0.01619\n",
      "Training epoch: 2168, train loss: 0.01557, val loss: 0.01606\n",
      "Training epoch: 2169, train loss: 0.01556, val loss: 0.01605\n",
      "Training epoch: 2170, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2171, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2172, train loss: 0.01567, val loss: 0.01613\n",
      "Training epoch: 2173, train loss: 0.01557, val loss: 0.01604\n",
      "Training epoch: 2174, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2175, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2176, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2177, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2178, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2179, train loss: 0.01554, val loss: 0.01603\n",
      "Training epoch: 2180, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2181, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2182, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2183, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 2184, train loss: 0.01547, val loss: 0.01594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2185, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2186, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2187, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2188, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 2189, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2190, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 2191, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2192, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2193, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2194, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2195, train loss: 0.01545, val loss: 0.01594\n",
      "Training epoch: 2196, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2197, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2198, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 2199, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 2200, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2201, train loss: 0.01556, val loss: 0.01604\n",
      "Training epoch: 2202, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2203, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2204, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2205, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2206, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2207, train loss: 0.01551, val loss: 0.01595\n",
      "Training epoch: 2208, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2209, train loss: 0.01543, val loss: 0.01592\n",
      "Training epoch: 2210, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 2211, train loss: 0.01555, val loss: 0.01600\n",
      "Training epoch: 2212, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2213, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2214, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2215, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2216, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2217, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2218, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 2219, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2220, train loss: 0.01569, val loss: 0.01615\n",
      "Training epoch: 2221, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 2222, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2223, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2224, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2225, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2226, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2227, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2228, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2229, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2230, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2231, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2232, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2233, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2234, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2235, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2236, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2237, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 2238, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2239, train loss: 0.01542, val loss: 0.01590\n",
      "Training epoch: 2240, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2241, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2242, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2243, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2244, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2245, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2246, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 2247, train loss: 0.01543, val loss: 0.01591\n",
      "Training epoch: 2248, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2249, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2250, train loss: 0.01569, val loss: 0.01616\n",
      "Training epoch: 2251, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 2252, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2253, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 2254, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2255, train loss: 0.01567, val loss: 0.01617\n",
      "Training epoch: 2256, train loss: 0.01571, val loss: 0.01619\n",
      "Training epoch: 2257, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2258, train loss: 0.01545, val loss: 0.01592\n",
      "Training epoch: 2259, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 2260, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2261, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2262, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2263, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 2264, train loss: 0.01559, val loss: 0.01606\n",
      "Training epoch: 2265, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2266, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 2267, train loss: 0.01543, val loss: 0.01591\n",
      "Training epoch: 2268, train loss: 0.01561, val loss: 0.01606\n",
      "Training epoch: 2269, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 2270, train loss: 0.01576, val loss: 0.01623\n",
      "Training epoch: 2271, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 2272, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2273, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2274, train loss: 0.01571, val loss: 0.01618\n",
      "Training epoch: 2275, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 2276, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2277, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2278, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 2279, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 2280, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 2281, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2282, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2283, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2284, train loss: 0.01542, val loss: 0.01590\n",
      "Training epoch: 2285, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2286, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 2287, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2288, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 2289, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2290, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2291, train loss: 0.01546, val loss: 0.01590\n",
      "Training epoch: 2292, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2293, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2294, train loss: 0.01543, val loss: 0.01591\n",
      "Training epoch: 2295, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2296, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2297, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2298, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2299, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2300, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 2301, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2302, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2303, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2304, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2305, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2306, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2307, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2308, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2309, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2310, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2311, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2312, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2313, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2314, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2315, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 2316, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2317, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2318, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2319, train loss: 0.01545, val loss: 0.01591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2320, train loss: 0.01560, val loss: 0.01605\n",
      "Training epoch: 2321, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2322, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2323, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2324, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2325, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 2326, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 2327, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 2328, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2329, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2330, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2331, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 2332, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2333, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2334, train loss: 0.01547, val loss: 0.01592\n",
      "Training epoch: 2335, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 2336, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 2337, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 2338, train loss: 0.01563, val loss: 0.01609\n",
      "Training epoch: 2339, train loss: 0.01548, val loss: 0.01592\n",
      "Training epoch: 2340, train loss: 0.01559, val loss: 0.01604\n",
      "Training epoch: 2341, train loss: 0.01558, val loss: 0.01605\n",
      "Training epoch: 2342, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2343, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2344, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2345, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 2346, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2347, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2348, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 2349, train loss: 0.01587, val loss: 0.01634\n",
      "Training epoch: 2350, train loss: 0.01570, val loss: 0.01616\n",
      "Training epoch: 2351, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2352, train loss: 0.01564, val loss: 0.01609\n",
      "Training epoch: 2353, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2354, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 2355, train loss: 0.01572, val loss: 0.01621\n",
      "Training epoch: 2356, train loss: 0.01550, val loss: 0.01598\n",
      "Training epoch: 2357, train loss: 0.01554, val loss: 0.01602\n",
      "Training epoch: 2358, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 2359, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2360, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 2361, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 2362, train loss: 0.01556, val loss: 0.01602\n",
      "Training epoch: 2363, train loss: 0.01566, val loss: 0.01610\n",
      "Training epoch: 2364, train loss: 0.01557, val loss: 0.01603\n",
      "Training epoch: 2365, train loss: 0.01543, val loss: 0.01591\n",
      "Training epoch: 2366, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2367, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2368, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2369, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2370, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2371, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2372, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2373, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2374, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2375, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2376, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2377, train loss: 0.01549, val loss: 0.01596\n",
      "Training epoch: 2378, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2379, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2380, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2381, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2382, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2383, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2384, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2385, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2386, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2387, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 2388, train loss: 0.01555, val loss: 0.01601\n",
      "Training epoch: 2389, train loss: 0.01562, val loss: 0.01608\n",
      "Training epoch: 2390, train loss: 0.01542, val loss: 0.01586\n",
      "Training epoch: 2391, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2392, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2393, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2394, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2395, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 2396, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2397, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2398, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2399, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2400, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2401, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 2402, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 2403, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2404, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2405, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 2406, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 2407, train loss: 0.01547, val loss: 0.01596\n",
      "Training epoch: 2408, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2409, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2410, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2411, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2412, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2413, train loss: 0.01538, val loss: 0.01585\n",
      "Training epoch: 2414, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2415, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2416, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 2417, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2418, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2419, train loss: 0.01548, val loss: 0.01596\n",
      "Training epoch: 2420, train loss: 0.01552, val loss: 0.01598\n",
      "Training epoch: 2421, train loss: 0.01543, val loss: 0.01588\n",
      "Training epoch: 2422, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2423, train loss: 0.01562, val loss: 0.01610\n",
      "Training epoch: 2424, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 2425, train loss: 0.01560, val loss: 0.01607\n",
      "Training epoch: 2426, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2427, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2428, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 2429, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 2430, train loss: 0.01547, val loss: 0.01593\n",
      "Training epoch: 2431, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2432, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 2433, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2434, train loss: 0.01539, val loss: 0.01583\n",
      "Training epoch: 2435, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2436, train loss: 0.01546, val loss: 0.01594\n",
      "Training epoch: 2437, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2438, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 2439, train loss: 0.01550, val loss: 0.01596\n",
      "Training epoch: 2440, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2441, train loss: 0.01550, val loss: 0.01597\n",
      "Training epoch: 2442, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2443, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2444, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2445, train loss: 0.01560, val loss: 0.01609\n",
      "Training epoch: 2446, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2447, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2448, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2449, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2450, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2451, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2452, train loss: 0.01545, val loss: 0.01588\n",
      "Training epoch: 2453, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2454, train loss: 0.01538, val loss: 0.01582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2455, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2456, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2457, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2458, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2459, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2460, train loss: 0.01541, val loss: 0.01585\n",
      "Training epoch: 2461, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2462, train loss: 0.01561, val loss: 0.01606\n",
      "Training epoch: 2463, train loss: 0.01549, val loss: 0.01593\n",
      "Training epoch: 2464, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 2465, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2466, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 2467, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 2468, train loss: 0.01540, val loss: 0.01588\n",
      "Training epoch: 2469, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2470, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2471, train loss: 0.01569, val loss: 0.01614\n",
      "Training epoch: 2472, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 2473, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 2474, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 2475, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2476, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2477, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2478, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2479, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2480, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2481, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2482, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 2483, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2484, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2485, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2486, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2487, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2488, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2489, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2490, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 2491, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2492, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2493, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2494, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 2495, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2496, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2497, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2498, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2499, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2500, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2501, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2502, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2503, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2504, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2505, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2506, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 2507, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2508, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 2509, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2510, train loss: 0.01541, val loss: 0.01585\n",
      "Training epoch: 2511, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 2512, train loss: 0.01544, val loss: 0.01589\n",
      "Training epoch: 2513, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2514, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 2515, train loss: 0.01540, val loss: 0.01587\n",
      "Training epoch: 2516, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 2517, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2518, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2519, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2520, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 2521, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 2522, train loss: 0.01551, val loss: 0.01598\n",
      "Training epoch: 2523, train loss: 0.01582, val loss: 0.01630\n",
      "Training epoch: 2524, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2525, train loss: 0.01554, val loss: 0.01600\n",
      "Training epoch: 2526, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 2527, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2528, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2529, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2530, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2531, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2532, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2533, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 2534, train loss: 0.01548, val loss: 0.01593\n",
      "Training epoch: 2535, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 2536, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 2537, train loss: 0.01565, val loss: 0.01610\n",
      "Training epoch: 2538, train loss: 0.01576, val loss: 0.01622\n",
      "Training epoch: 2539, train loss: 0.01582, val loss: 0.01626\n",
      "Training epoch: 2540, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2541, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2542, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 2543, train loss: 0.01538, val loss: 0.01587\n",
      "Training epoch: 2544, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 2545, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 2546, train loss: 0.01571, val loss: 0.01620\n",
      "Training epoch: 2547, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 2548, train loss: 0.01561, val loss: 0.01609\n",
      "Training epoch: 2549, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2550, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2551, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2552, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2553, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2554, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2555, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2556, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2557, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2558, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2559, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2560, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2561, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2562, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2563, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2564, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2565, train loss: 0.01543, val loss: 0.01590\n",
      "Training epoch: 2566, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2567, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2568, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2569, train loss: 0.01534, val loss: 0.01581\n",
      "Training epoch: 2570, train loss: 0.01547, val loss: 0.01595\n",
      "Training epoch: 2571, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2572, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2573, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2574, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 2575, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2576, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 2577, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2578, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2579, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 2580, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 2581, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2582, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2583, train loss: 0.01555, val loss: 0.01600\n",
      "Training epoch: 2584, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 2585, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 2586, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2587, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2588, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2589, train loss: 0.01534, val loss: 0.01580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2590, train loss: 0.01539, val loss: 0.01583\n",
      "Training epoch: 2591, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2592, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2593, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2594, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2595, train loss: 0.01536, val loss: 0.01579\n",
      "Training epoch: 2596, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2597, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2598, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2599, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 2600, train loss: 0.01544, val loss: 0.01588\n",
      "Training epoch: 2601, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2602, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2603, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2604, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2605, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2606, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2607, train loss: 0.01546, val loss: 0.01590\n",
      "Training epoch: 2608, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2609, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2610, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2611, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 2612, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2613, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2614, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2615, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2616, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2617, train loss: 0.01563, val loss: 0.01607\n",
      "Training epoch: 2618, train loss: 0.01564, val loss: 0.01607\n",
      "Training epoch: 2619, train loss: 0.01553, val loss: 0.01597\n",
      "Training epoch: 2620, train loss: 0.01538, val loss: 0.01581\n",
      "Training epoch: 2621, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 2622, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 2623, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2624, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 2625, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 2626, train loss: 0.01549, val loss: 0.01594\n",
      "Training epoch: 2627, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2628, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2629, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 2630, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2631, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2632, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 2633, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2634, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2635, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 2636, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 2637, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2638, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 2639, train loss: 0.01551, val loss: 0.01595\n",
      "Training epoch: 2640, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2641, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 2642, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2643, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 2644, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2645, train loss: 0.01538, val loss: 0.01581\n",
      "Training epoch: 2646, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2647, train loss: 0.01563, val loss: 0.01610\n",
      "Training epoch: 2648, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 2649, train loss: 0.01565, val loss: 0.01613\n",
      "Training epoch: 2650, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2651, train loss: 0.01555, val loss: 0.01600\n",
      "Training epoch: 2652, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 2653, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2654, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2655, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2656, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 2657, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2658, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 2659, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2660, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2661, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2662, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 2663, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2664, train loss: 0.01546, val loss: 0.01591\n",
      "Training epoch: 2665, train loss: 0.01542, val loss: 0.01587\n",
      "Training epoch: 2666, train loss: 0.01553, val loss: 0.01597\n",
      "Training epoch: 2667, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 2668, train loss: 0.01564, val loss: 0.01607\n",
      "Training epoch: 2669, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2670, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 2671, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 2672, train loss: 0.01537, val loss: 0.01582\n",
      "Training epoch: 2673, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 2674, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2675, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2676, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2677, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2678, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2679, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2680, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2681, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 2682, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 2683, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2684, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2685, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2686, train loss: 0.01544, val loss: 0.01587\n",
      "Training epoch: 2687, train loss: 0.01556, val loss: 0.01599\n",
      "Training epoch: 2688, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 2689, train loss: 0.01547, val loss: 0.01590\n",
      "Training epoch: 2690, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2691, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2692, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2693, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 2694, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 2695, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2696, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2697, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 2698, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 2699, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2700, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 2701, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 2702, train loss: 0.01535, val loss: 0.01578\n",
      "Training epoch: 2703, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2704, train loss: 0.01539, val loss: 0.01582\n",
      "Training epoch: 2705, train loss: 0.01545, val loss: 0.01587\n",
      "Training epoch: 2706, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 2707, train loss: 0.01564, val loss: 0.01606\n",
      "Training epoch: 2708, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 2709, train loss: 0.01553, val loss: 0.01595\n",
      "Training epoch: 2710, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 2711, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2712, train loss: 0.01539, val loss: 0.01582\n",
      "Training epoch: 2713, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 2714, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2715, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2716, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2717, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 2718, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 2719, train loss: 0.01537, val loss: 0.01580\n",
      "Training epoch: 2720, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2721, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2722, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 2723, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2724, train loss: 0.01531, val loss: 0.01573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2725, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2726, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2727, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2728, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 2729, train loss: 0.01538, val loss: 0.01582\n",
      "Training epoch: 2730, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 2731, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2732, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2733, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2734, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2735, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 2736, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2737, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 2738, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2739, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 2740, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2741, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2742, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2743, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2744, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2745, train loss: 0.01534, val loss: 0.01576\n",
      "Training epoch: 2746, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2747, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2748, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2749, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2750, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 2751, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2752, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2753, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2754, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2755, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2756, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 2757, train loss: 0.01534, val loss: 0.01575\n",
      "Training epoch: 2758, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2759, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2760, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2761, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 2762, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2763, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 2764, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2765, train loss: 0.01537, val loss: 0.01580\n",
      "Training epoch: 2766, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 2767, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 2768, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 2769, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2770, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2771, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 2772, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2773, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 2774, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2775, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 2776, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 2777, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2778, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 2779, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2780, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 2781, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2782, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2783, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 2784, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 2785, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 2786, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 2787, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2788, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2789, train loss: 0.01536, val loss: 0.01579\n",
      "Training epoch: 2790, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 2791, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 2792, train loss: 0.01528, val loss: 0.01572\n",
      "Training epoch: 2793, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2794, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2795, train loss: 0.01538, val loss: 0.01581\n",
      "Training epoch: 2796, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2797, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 2798, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 2799, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2800, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2801, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2802, train loss: 0.01540, val loss: 0.01582\n",
      "Training epoch: 2803, train loss: 0.01565, val loss: 0.01605\n",
      "Training epoch: 2804, train loss: 0.01540, val loss: 0.01581\n",
      "Training epoch: 2805, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2806, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 2807, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 2808, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2809, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2810, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 2811, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2812, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2813, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2814, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2815, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 2816, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2817, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2818, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2819, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2820, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2821, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 2822, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2823, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2824, train loss: 0.01539, val loss: 0.01583\n",
      "Training epoch: 2825, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 2826, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 2827, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2828, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2829, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2830, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 2831, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 2832, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2833, train loss: 0.01549, val loss: 0.01593\n",
      "Training epoch: 2834, train loss: 0.01561, val loss: 0.01606\n",
      "Training epoch: 2835, train loss: 0.01550, val loss: 0.01595\n",
      "Training epoch: 2836, train loss: 0.01554, val loss: 0.01598\n",
      "Training epoch: 2837, train loss: 0.01538, val loss: 0.01581\n",
      "Training epoch: 2838, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2839, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2840, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2841, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 2842, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2843, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 2844, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2845, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2846, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2847, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2848, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2849, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2850, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 2851, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2852, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 2853, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2854, train loss: 0.01555, val loss: 0.01598\n",
      "Training epoch: 2855, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2856, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2857, train loss: 0.01533, val loss: 0.01577\n",
      "Training epoch: 2858, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 2859, train loss: 0.01541, val loss: 0.01581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2860, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2861, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 2862, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2863, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 2864, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2865, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 2866, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2867, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 2868, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2869, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2870, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2871, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2872, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 2873, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 2874, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2875, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2876, train loss: 0.01540, val loss: 0.01580\n",
      "Training epoch: 2877, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2878, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2879, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 2880, train loss: 0.01528, val loss: 0.01572\n",
      "Training epoch: 2881, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2882, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2883, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2884, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 2885, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 2886, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2887, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2888, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 2889, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 2890, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2891, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2892, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2893, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2894, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2895, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 2896, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 2897, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 2898, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 2899, train loss: 0.01551, val loss: 0.01592\n",
      "Training epoch: 2900, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2901, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 2902, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 2903, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 2904, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2905, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 2906, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2907, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2908, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 2909, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2910, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2911, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2912, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 2913, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 2914, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 2915, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2916, train loss: 0.01547, val loss: 0.01591\n",
      "Training epoch: 2917, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2918, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2919, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 2920, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2921, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2922, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2923, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2924, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2925, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2926, train loss: 0.01532, val loss: 0.01574\n",
      "Training epoch: 2927, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2928, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 2929, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2930, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 2931, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 2932, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2933, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 2934, train loss: 0.01542, val loss: 0.01585\n",
      "Training epoch: 2935, train loss: 0.01543, val loss: 0.01586\n",
      "Training epoch: 2936, train loss: 0.01549, val loss: 0.01592\n",
      "Training epoch: 2937, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2938, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2939, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2940, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 2941, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2942, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2943, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2944, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2945, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 2946, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 2947, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2948, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2949, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 2950, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 2951, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2952, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2953, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 2954, train loss: 0.01534, val loss: 0.01575\n",
      "Training epoch: 2955, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 2956, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2957, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2958, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2959, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2960, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 2961, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2962, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 2963, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2964, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2965, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2966, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 2967, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 2968, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 2969, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 2970, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 2971, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2972, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 2973, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2974, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2975, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 2976, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 2977, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2978, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2979, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2980, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2981, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 2982, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 2983, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2984, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 2985, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 2986, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2987, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 2988, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 2989, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 2990, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 2991, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 2992, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 2993, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 2994, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 2995, train loss: 0.01535, val loss: 0.01575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2996, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 2997, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 2998, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 2999, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3000, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 3001, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3002, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3003, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3004, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 3005, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3006, train loss: 0.01542, val loss: 0.01579\n",
      "Training epoch: 3007, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 3008, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3009, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3010, train loss: 0.01550, val loss: 0.01588\n",
      "Training epoch: 3011, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 3012, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3013, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3014, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 3015, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3016, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3017, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 3018, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3019, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 3020, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 3021, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3022, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3023, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3024, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3025, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3026, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3027, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3028, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3029, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3030, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3031, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3032, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3033, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3034, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3035, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3036, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3037, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3038, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3039, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3040, train loss: 0.01548, val loss: 0.01591\n",
      "Training epoch: 3041, train loss: 0.01557, val loss: 0.01600\n",
      "Training epoch: 3042, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 3043, train loss: 0.01593, val loss: 0.01637\n",
      "Training epoch: 3044, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 3045, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 3046, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 3047, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3048, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3049, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3050, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3051, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3052, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3053, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3054, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 3055, train loss: 0.01551, val loss: 0.01592\n",
      "Training epoch: 3056, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3057, train loss: 0.01531, val loss: 0.01572\n",
      "Training epoch: 3058, train loss: 0.01561, val loss: 0.01598\n",
      "Training epoch: 3059, train loss: 0.01562, val loss: 0.01599\n",
      "Training epoch: 3060, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3061, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3062, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3063, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 3064, train loss: 0.01576, val loss: 0.01620\n",
      "Training epoch: 3065, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 3066, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 3067, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3068, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 3069, train loss: 0.01544, val loss: 0.01581\n",
      "Training epoch: 3070, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3071, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 3072, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3073, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3074, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3075, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3076, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3077, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3078, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3079, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3080, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3081, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3082, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3083, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3084, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3085, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3086, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3087, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3088, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3089, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3090, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3091, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3092, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3093, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3094, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3095, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3096, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3097, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3098, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3099, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3100, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3101, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3102, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3103, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3104, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3105, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3106, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3107, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3108, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3109, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3110, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 3111, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3112, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 3113, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3114, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3115, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3116, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 3117, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3118, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3119, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3120, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 3121, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 3122, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3123, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3124, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3125, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3126, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3127, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 3128, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 3129, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3130, train loss: 0.01533, val loss: 0.01572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3131, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3132, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3133, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3134, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3135, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3136, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3137, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3138, train loss: 0.01542, val loss: 0.01584\n",
      "Training epoch: 3139, train loss: 0.01544, val loss: 0.01586\n",
      "Training epoch: 3140, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3141, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3142, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3143, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3144, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3145, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3146, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 3147, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3148, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3149, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3150, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3151, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3152, train loss: 0.01543, val loss: 0.01583\n",
      "Training epoch: 3153, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3154, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3155, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3156, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 3157, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 3158, train loss: 0.01549, val loss: 0.01591\n",
      "Training epoch: 3159, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 3160, train loss: 0.01551, val loss: 0.01594\n",
      "Training epoch: 3161, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3162, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3163, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 3164, train loss: 0.01526, val loss: 0.01567\n",
      "Training epoch: 3165, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3166, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3167, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3168, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3169, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3170, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3171, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3172, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3173, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 3174, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3175, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3176, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 3177, train loss: 0.01534, val loss: 0.01570\n",
      "Training epoch: 3178, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3179, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3180, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3181, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3182, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 3183, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3184, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3185, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3186, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3187, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3188, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3189, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3190, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 3191, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3192, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3193, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3194, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3195, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3196, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3197, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3198, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3199, train loss: 0.01540, val loss: 0.01576\n",
      "Training epoch: 3200, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3201, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 3202, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3203, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3204, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3205, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 3206, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3207, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 3208, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3209, train loss: 0.01538, val loss: 0.01575\n",
      "Training epoch: 3210, train loss: 0.01554, val loss: 0.01590\n",
      "Training epoch: 3211, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 3212, train loss: 0.01540, val loss: 0.01578\n",
      "Training epoch: 3213, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 3214, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3215, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3216, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3217, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3218, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3219, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3220, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3221, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3222, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3223, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3224, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3225, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3226, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3227, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3228, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3229, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 3230, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 3231, train loss: 0.01546, val loss: 0.01587\n",
      "Training epoch: 3232, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3233, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3234, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3235, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3236, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3237, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3238, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3239, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3240, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3241, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3242, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3243, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3244, train loss: 0.01545, val loss: 0.01581\n",
      "Training epoch: 3245, train loss: 0.01556, val loss: 0.01592\n",
      "Training epoch: 3246, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 3247, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3248, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3249, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3250, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3251, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3252, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3253, train loss: 0.01541, val loss: 0.01578\n",
      "Training epoch: 3254, train loss: 0.01568, val loss: 0.01602\n",
      "Training epoch: 3255, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 3256, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3257, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3258, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3259, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3260, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3261, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3262, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3263, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3264, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3265, train loss: 0.01534, val loss: 0.01574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3266, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3267, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3268, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3269, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3270, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3271, train loss: 0.01582, val loss: 0.01616\n",
      "Training epoch: 3272, train loss: 0.01545, val loss: 0.01580\n",
      "Training epoch: 3273, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3274, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3275, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3276, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3277, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3278, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3279, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3280, train loss: 0.01549, val loss: 0.01590\n",
      "Training epoch: 3281, train loss: 0.01548, val loss: 0.01591\n",
      "Training epoch: 3282, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 3283, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3284, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3285, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3286, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3287, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3288, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3289, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3290, train loss: 0.01552, val loss: 0.01593\n",
      "Training epoch: 3291, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3292, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3293, train loss: 0.01551, val loss: 0.01587\n",
      "Training epoch: 3294, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 3295, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3296, train loss: 0.01545, val loss: 0.01587\n",
      "Training epoch: 3297, train loss: 0.01574, val loss: 0.01618\n",
      "Training epoch: 3298, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3299, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3300, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3301, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3302, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3303, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3304, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3305, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3306, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3307, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3308, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 3309, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3310, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3311, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3312, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3313, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 3314, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3315, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3316, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3317, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3318, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3319, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3320, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3321, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3322, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3323, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 3324, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3325, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3326, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3327, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3328, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3329, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3330, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3331, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3332, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3333, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3334, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3335, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3336, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3337, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3338, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3339, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3340, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3341, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3342, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3343, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3344, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 3345, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3346, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3347, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3348, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3349, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 3350, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3351, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3352, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3353, train loss: 0.01531, val loss: 0.01571\n",
      "Training epoch: 3354, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3355, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3356, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3357, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3358, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3359, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3360, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 3361, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3362, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3363, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3364, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3365, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3366, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3367, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3368, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3369, train loss: 0.01533, val loss: 0.01569\n",
      "Training epoch: 3370, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3371, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3372, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3373, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3374, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3375, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3376, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 3377, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 3378, train loss: 0.01544, val loss: 0.01585\n",
      "Training epoch: 3379, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3380, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3381, train loss: 0.01565, val loss: 0.01600\n",
      "Training epoch: 3382, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3383, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3384, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3385, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3386, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3387, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3388, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3389, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3390, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3391, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3392, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3393, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3394, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3395, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3396, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3397, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3398, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3399, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3400, train loss: 0.01525, val loss: 0.01562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3401, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3402, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3403, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3404, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3405, train loss: 0.01542, val loss: 0.01582\n",
      "Training epoch: 3406, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3407, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 3408, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3409, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3410, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 3411, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3412, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3413, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3414, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3415, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3416, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3417, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3418, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3419, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3420, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 3421, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3422, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3423, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3424, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3425, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3426, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3427, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3428, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3429, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3430, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3431, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 3432, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3433, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3434, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3435, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3436, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 3437, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3438, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3439, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 3440, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3441, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 3442, train loss: 0.01541, val loss: 0.01582\n",
      "Training epoch: 3443, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 3444, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 3445, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3446, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3447, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 3448, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 3449, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 3450, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3451, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3452, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3453, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3454, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3455, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3456, train loss: 0.01550, val loss: 0.01593\n",
      "Training epoch: 3457, train loss: 0.01549, val loss: 0.01589\n",
      "Training epoch: 3458, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3459, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3460, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3461, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3462, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3463, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3464, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3465, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3466, train loss: 0.01547, val loss: 0.01582\n",
      "Training epoch: 3467, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3468, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3469, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3470, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3471, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3472, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3473, train loss: 0.01551, val loss: 0.01586\n",
      "Training epoch: 3474, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3475, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3476, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 3477, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3478, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3479, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3480, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3481, train loss: 0.01538, val loss: 0.01574\n",
      "Training epoch: 3482, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3483, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3484, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3485, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 3486, train loss: 0.01535, val loss: 0.01571\n",
      "Training epoch: 3487, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 3488, train loss: 0.01533, val loss: 0.01569\n",
      "Training epoch: 3489, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3490, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3491, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3492, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3493, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3494, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3495, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3496, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3497, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3498, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3499, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3500, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3501, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3502, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3503, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3504, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3505, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3506, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3507, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3508, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3509, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3510, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3511, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3512, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 3513, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3514, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3515, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3516, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3517, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3518, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3519, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3520, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3521, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3522, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3523, train loss: 0.01558, val loss: 0.01598\n",
      "Training epoch: 3524, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 3525, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 3526, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 3527, train loss: 0.01533, val loss: 0.01567\n",
      "Training epoch: 3528, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3529, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3530, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3531, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3532, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3533, train loss: 0.01522, val loss: 0.01561\n",
      "Training epoch: 3534, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3535, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3536, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3537, train loss: 0.01525, val loss: 0.01562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3538, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3539, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3540, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3541, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3542, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3543, train loss: 0.01566, val loss: 0.01607\n",
      "Training epoch: 3544, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3545, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3546, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3547, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3548, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3549, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3550, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3551, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3552, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3553, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3554, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3555, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3556, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3557, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3558, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3559, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3560, train loss: 0.01549, val loss: 0.01583\n",
      "Training epoch: 3561, train loss: 0.01534, val loss: 0.01568\n",
      "Training epoch: 3562, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3563, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3564, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3565, train loss: 0.01536, val loss: 0.01570\n",
      "Training epoch: 3566, train loss: 0.01540, val loss: 0.01574\n",
      "Training epoch: 3567, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3568, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3569, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3570, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3571, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3572, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3573, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3574, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3575, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 3576, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3577, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3578, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 3579, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3580, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3581, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3582, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3583, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 3584, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 3585, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3586, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3587, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3588, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3589, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 3590, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3591, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3592, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3593, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3594, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3595, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3596, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3597, train loss: 0.01530, val loss: 0.01570\n",
      "Training epoch: 3598, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3599, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3600, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3601, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3602, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3603, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3604, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3605, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3606, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3607, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3608, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3609, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 3610, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3611, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3612, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 3613, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3614, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3615, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3616, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3617, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3618, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3619, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3620, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3621, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3622, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 3623, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3624, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3625, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3626, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3627, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3628, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3629, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3630, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3631, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3632, train loss: 0.01554, val loss: 0.01594\n",
      "Training epoch: 3633, train loss: 0.01560, val loss: 0.01600\n",
      "Training epoch: 3634, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3635, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 3636, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3637, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3638, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3639, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 3640, train loss: 0.01548, val loss: 0.01583\n",
      "Training epoch: 3641, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 3642, train loss: 0.01533, val loss: 0.01567\n",
      "Training epoch: 3643, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 3644, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 3645, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3646, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3647, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3648, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3649, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3650, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3651, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3652, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 3653, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3654, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3655, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3656, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3657, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3658, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 3659, train loss: 0.01539, val loss: 0.01572\n",
      "Training epoch: 3660, train loss: 0.01544, val loss: 0.01578\n",
      "Training epoch: 3661, train loss: 0.01531, val loss: 0.01567\n",
      "Training epoch: 3662, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3663, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3664, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3665, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3666, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3667, train loss: 0.01544, val loss: 0.01584\n",
      "Training epoch: 3668, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3669, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3670, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3671, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3672, train loss: 0.01524, val loss: 0.01562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3673, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3674, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3675, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3676, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3677, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3678, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3679, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 3680, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3681, train loss: 0.01535, val loss: 0.01576\n",
      "Training epoch: 3682, train loss: 0.01543, val loss: 0.01581\n",
      "Training epoch: 3683, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3684, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3685, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3686, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3687, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3688, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3689, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 3690, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3691, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 3692, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3693, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3694, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3695, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 3696, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3697, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3698, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3699, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 3700, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 3701, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3702, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 3703, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 3704, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 3705, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3706, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3707, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 3708, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3709, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 3710, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3711, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3712, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3713, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3714, train loss: 0.01536, val loss: 0.01574\n",
      "Training epoch: 3715, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3716, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3717, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3718, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 3719, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 3720, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3721, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3722, train loss: 0.01537, val loss: 0.01573\n",
      "Training epoch: 3723, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 3724, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3725, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 3726, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 3727, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3728, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3729, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3730, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3731, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3732, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3733, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3734, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 3735, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3736, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3737, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3738, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3739, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3740, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3741, train loss: 0.01551, val loss: 0.01586\n",
      "Training epoch: 3742, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3743, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3744, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3745, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3746, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3747, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3748, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3749, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3750, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3751, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3752, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3753, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3754, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3755, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 3756, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3757, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3758, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3759, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3760, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3761, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 3762, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3763, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 3764, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3765, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 3766, train loss: 0.01537, val loss: 0.01575\n",
      "Training epoch: 3767, train loss: 0.01554, val loss: 0.01593\n",
      "Training epoch: 3768, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3769, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3770, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3771, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3772, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3773, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 3774, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3775, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3776, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3777, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3778, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3779, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3780, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3781, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3782, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3783, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 3784, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 3785, train loss: 0.01533, val loss: 0.01568\n",
      "Training epoch: 3786, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 3787, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3788, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3789, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3790, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3791, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3792, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 3793, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3794, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3795, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3796, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3797, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3798, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3799, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3800, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3801, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3802, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3803, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3804, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 3805, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3806, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3807, train loss: 0.01522, val loss: 0.01559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3808, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3809, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3810, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3811, train loss: 0.01569, val loss: 0.01611\n",
      "Training epoch: 3812, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3813, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3814, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3815, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3816, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3817, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3818, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3819, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3820, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3821, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3822, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3823, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3824, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3825, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3826, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3827, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3828, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3829, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3830, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 3831, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3832, train loss: 0.01536, val loss: 0.01573\n",
      "Training epoch: 3833, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 3834, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 3835, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3836, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 3837, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 3838, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3839, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 3840, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3841, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3842, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 3843, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3844, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 3845, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3846, train loss: 0.01541, val loss: 0.01581\n",
      "Training epoch: 3847, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3848, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3849, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3850, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 3851, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3852, train loss: 0.01547, val loss: 0.01580\n",
      "Training epoch: 3853, train loss: 0.01535, val loss: 0.01569\n",
      "Training epoch: 3854, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3855, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3856, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3857, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3858, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3859, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3860, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3861, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3862, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3863, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 3864, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3865, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3866, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3867, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3868, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3869, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3870, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3871, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 3872, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3873, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3874, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3875, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 3876, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3877, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3878, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3879, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 3880, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3881, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 3882, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3883, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3884, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3885, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3886, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3887, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3888, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 3889, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3890, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3891, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3892, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3893, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3894, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3895, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3896, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3897, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3898, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3899, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3900, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3901, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3902, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3903, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3904, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3905, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3906, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 3907, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3908, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3909, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3910, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 3911, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3912, train loss: 0.01540, val loss: 0.01579\n",
      "Training epoch: 3913, train loss: 0.01540, val loss: 0.01577\n",
      "Training epoch: 3914, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3915, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3916, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3917, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3918, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3919, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3920, train loss: 0.01534, val loss: 0.01572\n",
      "Training epoch: 3921, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 3922, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 3923, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 3924, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3925, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 3926, train loss: 0.01539, val loss: 0.01573\n",
      "Training epoch: 3927, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3928, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3929, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3930, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 3931, train loss: 0.01534, val loss: 0.01568\n",
      "Training epoch: 3932, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3933, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3934, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3935, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 3936, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 3937, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 3938, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3939, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3940, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 3941, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 3942, train loss: 0.01529, val loss: 0.01567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3943, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3944, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3945, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3946, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3947, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3948, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3949, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3950, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 3951, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3952, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 3953, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 3954, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3955, train loss: 0.01538, val loss: 0.01577\n",
      "Training epoch: 3956, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 3957, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 3958, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 3959, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3960, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 3961, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3962, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3963, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 3964, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 3965, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3966, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3967, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3968, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 3969, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 3970, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3971, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 3972, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 3973, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 3974, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 3975, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 3976, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 3977, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 3978, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 3979, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 3980, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 3981, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3982, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3983, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 3984, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3985, train loss: 0.01539, val loss: 0.01580\n",
      "Training epoch: 3986, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3987, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 3988, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 3989, train loss: 0.01526, val loss: 0.01565\n",
      "Training epoch: 3990, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3991, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 3992, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 3993, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3994, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3995, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 3996, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 3997, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 3998, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 3999, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 4000, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 4001, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 4002, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4003, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 4004, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4005, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4006, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4007, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4008, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4009, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4010, train loss: 0.01537, val loss: 0.01577\n",
      "Training epoch: 4011, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4012, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4013, train loss: 0.01554, val loss: 0.01588\n",
      "Training epoch: 4014, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4015, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4016, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 4017, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4018, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4019, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 4020, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4021, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4022, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 4023, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 4024, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4025, train loss: 0.01542, val loss: 0.01576\n",
      "Training epoch: 4026, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4027, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4028, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4029, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4030, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4031, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4032, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4033, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4034, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4035, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4036, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4037, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4038, train loss: 0.01534, val loss: 0.01574\n",
      "Training epoch: 4039, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4040, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 4041, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 4042, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4043, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4044, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4045, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4046, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4047, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4048, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4049, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4050, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 4051, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4052, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4053, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4054, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4055, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4056, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4057, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4058, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4059, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4060, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 4061, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4062, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4063, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4064, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4065, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4066, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4067, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4068, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4069, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4070, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4071, train loss: 0.01536, val loss: 0.01570\n",
      "Training epoch: 4072, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4073, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4074, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4075, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4076, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4077, train loss: 0.01528, val loss: 0.01566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4078, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4079, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4080, train loss: 0.01535, val loss: 0.01575\n",
      "Training epoch: 4081, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4082, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4083, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4084, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 4085, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4086, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4087, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4088, train loss: 0.01555, val loss: 0.01589\n",
      "Training epoch: 4089, train loss: 0.01547, val loss: 0.01581\n",
      "Training epoch: 4090, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 4091, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4092, train loss: 0.01547, val loss: 0.01586\n",
      "Training epoch: 4093, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 4094, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 4095, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4096, train loss: 0.01539, val loss: 0.01574\n",
      "Training epoch: 4097, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4098, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4099, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4100, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4101, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4102, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 4103, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4104, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 4105, train loss: 0.01573, val loss: 0.01613\n",
      "Training epoch: 4106, train loss: 0.01533, val loss: 0.01570\n",
      "Training epoch: 4107, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 4108, train loss: 0.01553, val loss: 0.01588\n",
      "Training epoch: 4109, train loss: 0.01538, val loss: 0.01572\n",
      "Training epoch: 4110, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4111, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4112, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4113, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4114, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4115, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4116, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4117, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4118, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4119, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4120, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4121, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4122, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 4123, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 4124, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4125, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4126, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4127, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4128, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4129, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4130, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4131, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4132, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4133, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4134, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4135, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4136, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4137, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4138, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4139, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4140, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4141, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4142, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4143, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4144, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4145, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4146, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4147, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4148, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4149, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4150, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 4151, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4152, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4153, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 4154, train loss: 0.01546, val loss: 0.01580\n",
      "Training epoch: 4155, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4156, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 4157, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4158, train loss: 0.01523, val loss: 0.01562\n",
      "Training epoch: 4159, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4160, train loss: 0.01537, val loss: 0.01576\n",
      "Training epoch: 4161, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4162, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4163, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 4164, train loss: 0.01541, val loss: 0.01579\n",
      "Training epoch: 4165, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4166, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4167, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4168, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4169, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4170, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4171, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4172, train loss: 0.01530, val loss: 0.01564\n",
      "Training epoch: 4173, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4174, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4175, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4176, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4177, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4178, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4179, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4180, train loss: 0.01543, val loss: 0.01576\n",
      "Training epoch: 4181, train loss: 0.01542, val loss: 0.01576\n",
      "Training epoch: 4182, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4183, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4184, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4185, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4186, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4187, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 4188, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4189, train loss: 0.01541, val loss: 0.01576\n",
      "Training epoch: 4190, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4191, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4192, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4193, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4194, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4195, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4196, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4197, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4198, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4199, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4200, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4201, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4202, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4203, train loss: 0.01550, val loss: 0.01590\n",
      "Training epoch: 4204, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4205, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4206, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4207, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4208, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4209, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4210, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4211, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4212, train loss: 0.01537, val loss: 0.01575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4213, train loss: 0.01536, val loss: 0.01576\n",
      "Training epoch: 4214, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4215, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4216, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4217, train loss: 0.01538, val loss: 0.01572\n",
      "Training epoch: 4218, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4219, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4220, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4221, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4222, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4223, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4224, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4225, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4226, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4227, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4228, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4229, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4230, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4231, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4232, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4233, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4234, train loss: 0.01523, val loss: 0.01556\n",
      "Training epoch: 4235, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4236, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4237, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4238, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4239, train loss: 0.01527, val loss: 0.01560\n",
      "Training epoch: 4240, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4241, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4242, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4243, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4244, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4245, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4246, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4247, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4248, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 4249, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4250, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4251, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4252, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4253, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4254, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4255, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4256, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4257, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4258, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4259, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4260, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4261, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4262, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4263, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4264, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4265, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4266, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4267, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4268, train loss: 0.01539, val loss: 0.01577\n",
      "Training epoch: 4269, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4270, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4271, train loss: 0.01532, val loss: 0.01568\n",
      "Training epoch: 4272, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 4273, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4274, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4275, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4276, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4277, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4278, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4279, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4280, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4281, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 4282, train loss: 0.01550, val loss: 0.01591\n",
      "Training epoch: 4283, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4284, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4285, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4286, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4287, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4288, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4289, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4290, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4291, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4292, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4293, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4294, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4295, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4296, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4297, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4298, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4299, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4300, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4301, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4302, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4303, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4304, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 4305, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4306, train loss: 0.01538, val loss: 0.01573\n",
      "Training epoch: 4307, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4308, train loss: 0.01526, val loss: 0.01560\n",
      "Training epoch: 4309, train loss: 0.01546, val loss: 0.01581\n",
      "Training epoch: 4310, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4311, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4312, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4313, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 4314, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4315, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4316, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4317, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4318, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4319, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4320, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4321, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4322, train loss: 0.01543, val loss: 0.01582\n",
      "Training epoch: 4323, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 4324, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4325, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4326, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4327, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4328, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 4329, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4330, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4331, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4332, train loss: 0.01532, val loss: 0.01571\n",
      "Training epoch: 4333, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4334, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4335, train loss: 0.01541, val loss: 0.01575\n",
      "Training epoch: 4336, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4337, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4338, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4339, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4340, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4341, train loss: 0.01527, val loss: 0.01566\n",
      "Training epoch: 4342, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 4343, train loss: 0.01548, val loss: 0.01581\n",
      "Training epoch: 4344, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4345, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4346, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 4347, train loss: 0.01542, val loss: 0.01582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4348, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4349, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4350, train loss: 0.01532, val loss: 0.01566\n",
      "Training epoch: 4351, train loss: 0.01538, val loss: 0.01571\n",
      "Training epoch: 4352, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4353, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4354, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 4355, train loss: 0.01533, val loss: 0.01571\n",
      "Training epoch: 4356, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4357, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4358, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4359, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4360, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4361, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4362, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 4363, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4364, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4365, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4366, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4367, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4368, train loss: 0.01540, val loss: 0.01574\n",
      "Training epoch: 4369, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4370, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4371, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4372, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4373, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4374, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4375, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4376, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4377, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4378, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4379, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4380, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4381, train loss: 0.01557, val loss: 0.01596\n",
      "Training epoch: 4382, train loss: 0.01538, val loss: 0.01576\n",
      "Training epoch: 4383, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4384, train loss: 0.01559, val loss: 0.01592\n",
      "Training epoch: 4385, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4386, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4387, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 4388, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 4389, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4390, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4391, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4392, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4393, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4394, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4395, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4396, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4397, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4398, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4399, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4400, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4401, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4402, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4403, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4404, train loss: 0.01534, val loss: 0.01569\n",
      "Training epoch: 4405, train loss: 0.01528, val loss: 0.01564\n",
      "Training epoch: 4406, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4407, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4408, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4409, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4410, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4411, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4412, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4413, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4414, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4415, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4416, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4417, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4418, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4419, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4420, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 4421, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4422, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4423, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4424, train loss: 0.01533, val loss: 0.01567\n",
      "Training epoch: 4425, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 4426, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4427, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4428, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4429, train loss: 0.01528, val loss: 0.01565\n",
      "Training epoch: 4430, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4431, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4432, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4433, train loss: 0.01551, val loss: 0.01591\n",
      "Training epoch: 4434, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4435, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4436, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4437, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 4438, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4439, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4440, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4441, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4442, train loss: 0.01528, val loss: 0.01562\n",
      "Training epoch: 4443, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4444, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4445, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4446, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4447, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4448, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4449, train loss: 0.01525, val loss: 0.01559\n",
      "Training epoch: 4450, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4451, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4452, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4453, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4454, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4455, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4456, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4457, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4458, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4459, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4460, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4461, train loss: 0.01520, val loss: 0.01558\n",
      "Training epoch: 4462, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4463, train loss: 0.01542, val loss: 0.01581\n",
      "Training epoch: 4464, train loss: 0.01532, val loss: 0.01569\n",
      "Training epoch: 4465, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 4466, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4467, train loss: 0.01549, val loss: 0.01583\n",
      "Training epoch: 4468, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4469, train loss: 0.01533, val loss: 0.01573\n",
      "Training epoch: 4470, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4471, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4472, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4473, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4474, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4475, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4476, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4477, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4478, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4479, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4480, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4481, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4482, train loss: 0.01523, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4483, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4484, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4485, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4486, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4487, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4488, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4489, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4490, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4491, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4492, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4493, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4494, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4495, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4496, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 4497, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4498, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4499, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4500, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4501, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4502, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4503, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4504, train loss: 0.01541, val loss: 0.01574\n",
      "Training epoch: 4505, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4506, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4507, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4508, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4509, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4510, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4511, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4512, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4513, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4514, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4515, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4516, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4517, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4518, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4519, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 4520, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 4521, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4522, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4523, train loss: 0.01543, val loss: 0.01578\n",
      "Training epoch: 4524, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4525, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4526, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4527, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4528, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4529, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4530, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4531, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4532, train loss: 0.01544, val loss: 0.01583\n",
      "Training epoch: 4533, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4534, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4535, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4536, train loss: 0.01536, val loss: 0.01575\n",
      "Training epoch: 4537, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4538, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4539, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4540, train loss: 0.01531, val loss: 0.01566\n",
      "Training epoch: 4541, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4542, train loss: 0.01535, val loss: 0.01570\n",
      "Training epoch: 4543, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4544, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4545, train loss: 0.01541, val loss: 0.01580\n",
      "Training epoch: 4546, train loss: 0.01546, val loss: 0.01586\n",
      "Training epoch: 4547, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4548, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4549, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4550, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4551, train loss: 0.01545, val loss: 0.01579\n",
      "Training epoch: 4552, train loss: 0.01530, val loss: 0.01566\n",
      "Training epoch: 4553, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4554, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4555, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 4556, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4557, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4558, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4559, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4560, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4561, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4562, train loss: 0.01532, val loss: 0.01570\n",
      "Training epoch: 4563, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4564, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4565, train loss: 0.01529, val loss: 0.01566\n",
      "Training epoch: 4566, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4567, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4568, train loss: 0.01535, val loss: 0.01573\n",
      "Training epoch: 4569, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4570, train loss: 0.01523, val loss: 0.01561\n",
      "Training epoch: 4571, train loss: 0.01531, val loss: 0.01570\n",
      "Training epoch: 4572, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4573, train loss: 0.01540, val loss: 0.01575\n",
      "Training epoch: 4574, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4575, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4576, train loss: 0.01531, val loss: 0.01568\n",
      "Training epoch: 4577, train loss: 0.01535, val loss: 0.01574\n",
      "Training epoch: 4578, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4579, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4580, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4581, train loss: 0.01533, val loss: 0.01567\n",
      "Training epoch: 4582, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4583, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 4584, train loss: 0.01544, val loss: 0.01582\n",
      "Training epoch: 4585, train loss: 0.01548, val loss: 0.01588\n",
      "Training epoch: 4586, train loss: 0.01530, val loss: 0.01565\n",
      "Training epoch: 4587, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4588, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4589, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4590, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4591, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4592, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4593, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4594, train loss: 0.01526, val loss: 0.01564\n",
      "Training epoch: 4595, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4596, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4597, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4598, train loss: 0.01532, val loss: 0.01567\n",
      "Training epoch: 4599, train loss: 0.01537, val loss: 0.01572\n",
      "Training epoch: 4600, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4601, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4602, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4603, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4604, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4605, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4606, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4607, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4608, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4609, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4610, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4611, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4612, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4613, train loss: 0.01524, val loss: 0.01558\n",
      "Training epoch: 4614, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4615, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4616, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4617, train loss: 0.01525, val loss: 0.01562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4618, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4619, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4620, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4621, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4622, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4623, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4624, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4625, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4626, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4627, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4628, train loss: 0.01531, val loss: 0.01565\n",
      "Training epoch: 4629, train loss: 0.01524, val loss: 0.01559\n",
      "Training epoch: 4630, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4631, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4632, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 4633, train loss: 0.01529, val loss: 0.01565\n",
      "Training epoch: 4634, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4635, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4636, train loss: 0.01544, val loss: 0.01578\n",
      "Training epoch: 4637, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4638, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4639, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4640, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4641, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4642, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4643, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 4644, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4645, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4646, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4647, train loss: 0.01527, val loss: 0.01561\n",
      "Training epoch: 4648, train loss: 0.01525, val loss: 0.01561\n",
      "Training epoch: 4649, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4650, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4651, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4652, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4653, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4654, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4655, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4656, train loss: 0.01535, val loss: 0.01572\n",
      "Training epoch: 4657, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4658, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 4659, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4660, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4661, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4662, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4663, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4664, train loss: 0.01538, val loss: 0.01571\n",
      "Training epoch: 4665, train loss: 0.01525, val loss: 0.01560\n",
      "Training epoch: 4666, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4667, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4668, train loss: 0.01536, val loss: 0.01571\n",
      "Training epoch: 4669, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4670, train loss: 0.01527, val loss: 0.01564\n",
      "Training epoch: 4671, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4672, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4673, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4674, train loss: 0.01523, val loss: 0.01557\n",
      "Training epoch: 4675, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4676, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4677, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4678, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4679, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4680, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4681, train loss: 0.01529, val loss: 0.01564\n",
      "Training epoch: 4682, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4683, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4684, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4685, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4686, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4687, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4688, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4689, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4690, train loss: 0.01522, val loss: 0.01557\n",
      "Training epoch: 4691, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4692, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4693, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4694, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4695, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4696, train loss: 0.01525, val loss: 0.01563\n",
      "Training epoch: 4697, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4698, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4699, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4700, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4701, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4702, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4703, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4704, train loss: 0.01527, val loss: 0.01562\n",
      "Training epoch: 4705, train loss: 0.01530, val loss: 0.01564\n",
      "Training epoch: 4706, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4707, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4708, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4709, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4710, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4711, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4712, train loss: 0.01523, val loss: 0.01560\n",
      "Training epoch: 4713, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4714, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4715, train loss: 0.01521, val loss: 0.01558\n",
      "Training epoch: 4716, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4717, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4718, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4719, train loss: 0.01526, val loss: 0.01562\n",
      "Training epoch: 4720, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4721, train loss: 0.01522, val loss: 0.01558\n",
      "Training epoch: 4722, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4723, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4724, train loss: 0.01531, val loss: 0.01569\n",
      "Training epoch: 4725, train loss: 0.01538, val loss: 0.01578\n",
      "Training epoch: 4726, train loss: 0.01530, val loss: 0.01568\n",
      "Training epoch: 4727, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4728, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4729, train loss: 0.01529, val loss: 0.01562\n",
      "Training epoch: 4730, train loss: 0.01535, val loss: 0.01569\n",
      "Training epoch: 4731, train loss: 0.01527, val loss: 0.01563\n",
      "Training epoch: 4732, train loss: 0.01529, val loss: 0.01568\n",
      "Training epoch: 4733, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4734, train loss: 0.01530, val loss: 0.01569\n",
      "Training epoch: 4735, train loss: 0.01530, val loss: 0.01567\n",
      "Training epoch: 4736, train loss: 0.01521, val loss: 0.01559\n",
      "Training epoch: 4737, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4738, train loss: 0.01521, val loss: 0.01556\n",
      "Training epoch: 4739, train loss: 0.01534, val loss: 0.01571\n",
      "Training epoch: 4740, train loss: 0.01555, val loss: 0.01596\n",
      "Training epoch: 4741, train loss: 0.01522, val loss: 0.01560\n",
      "Training epoch: 4742, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4743, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4744, train loss: 0.01528, val loss: 0.01563\n",
      "Training epoch: 4745, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4746, train loss: 0.01527, val loss: 0.01565\n",
      "Training epoch: 4747, train loss: 0.01521, val loss: 0.01557\n",
      "Training epoch: 4748, train loss: 0.01529, val loss: 0.01563\n",
      "Training epoch: 4749, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4750, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4751, train loss: 0.01522, val loss: 0.01559\n",
      "Training epoch: 4752, train loss: 0.01549, val loss: 0.01591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 4753, train loss: 0.01524, val loss: 0.01561\n",
      "Training epoch: 4754, train loss: 0.01542, val loss: 0.01576\n",
      "Training epoch: 4755, train loss: 0.01526, val loss: 0.01561\n",
      "Training epoch: 4756, train loss: 0.01528, val loss: 0.01566\n",
      "Training epoch: 4757, train loss: 0.01526, val loss: 0.01563\n",
      "Training epoch: 4758, train loss: 0.01523, val loss: 0.01558\n",
      "Training epoch: 4759, train loss: 0.01536, val loss: 0.01572\n",
      "Training epoch: 4760, train loss: 0.01523, val loss: 0.01559\n",
      "Training epoch: 4761, train loss: 0.01529, val loss: 0.01567\n",
      "Training epoch: 4762, train loss: 0.01524, val loss: 0.01562\n",
      "Training epoch: 4763, train loss: 0.01524, val loss: 0.01560\n",
      "Training epoch: 4764, train loss: 0.01525, val loss: 0.01562\n",
      "Training epoch: 4765, train loss: 0.01525, val loss: 0.01563\n",
      "Early stop at epoch 4765, With Testing Error: 0.01563\n",
      "Subnetwork pruning.\n",
      "Fine tuning.\n",
      "Tuning epoch: 1, train loss: 0.01521, val loss: 0.01556\n",
      "Tuning epoch: 2, train loss: 0.01525, val loss: 0.01560\n",
      "Tuning epoch: 3, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 4, train loss: 0.01519, val loss: 0.01553\n",
      "Tuning epoch: 5, train loss: 0.01530, val loss: 0.01565\n",
      "Tuning epoch: 6, train loss: 0.01527, val loss: 0.01563\n",
      "Tuning epoch: 7, train loss: 0.01520, val loss: 0.01556\n",
      "Tuning epoch: 8, train loss: 0.01520, val loss: 0.01554\n",
      "Tuning epoch: 9, train loss: 0.01526, val loss: 0.01558\n",
      "Tuning epoch: 10, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 11, train loss: 0.01522, val loss: 0.01558\n",
      "Tuning epoch: 12, train loss: 0.01523, val loss: 0.01557\n",
      "Tuning epoch: 13, train loss: 0.01524, val loss: 0.01560\n",
      "Tuning epoch: 14, train loss: 0.01523, val loss: 0.01555\n",
      "Tuning epoch: 15, train loss: 0.01524, val loss: 0.01558\n",
      "Tuning epoch: 16, train loss: 0.01520, val loss: 0.01554\n",
      "Tuning epoch: 17, train loss: 0.01522, val loss: 0.01557\n",
      "Tuning epoch: 18, train loss: 0.01523, val loss: 0.01558\n",
      "Tuning epoch: 19, train loss: 0.01537, val loss: 0.01573\n",
      "Tuning epoch: 20, train loss: 0.01521, val loss: 0.01557\n",
      "Tuning epoch: 21, train loss: 0.01525, val loss: 0.01557\n",
      "Tuning epoch: 22, train loss: 0.01523, val loss: 0.01557\n",
      "Tuning epoch: 23, train loss: 0.01520, val loss: 0.01553\n",
      "Tuning epoch: 24, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 25, train loss: 0.01523, val loss: 0.01557\n",
      "Tuning epoch: 26, train loss: 0.01520, val loss: 0.01554\n",
      "Tuning epoch: 27, train loss: 0.01524, val loss: 0.01560\n",
      "Tuning epoch: 28, train loss: 0.01519, val loss: 0.01553\n",
      "Tuning epoch: 29, train loss: 0.01522, val loss: 0.01555\n",
      "Tuning epoch: 30, train loss: 0.01534, val loss: 0.01573\n",
      "Tuning epoch: 31, train loss: 0.01525, val loss: 0.01558\n",
      "Tuning epoch: 32, train loss: 0.01521, val loss: 0.01555\n",
      "Tuning epoch: 33, train loss: 0.01520, val loss: 0.01554\n",
      "Tuning epoch: 34, train loss: 0.01519, val loss: 0.01554\n",
      "Tuning epoch: 35, train loss: 0.01527, val loss: 0.01560\n",
      "Tuning epoch: 36, train loss: 0.01523, val loss: 0.01559\n",
      "Tuning epoch: 37, train loss: 0.01525, val loss: 0.01559\n",
      "Tuning epoch: 38, train loss: 0.01522, val loss: 0.01556\n",
      "Tuning epoch: 39, train loss: 0.01519, val loss: 0.01555\n",
      "Tuning epoch: 40, train loss: 0.01544, val loss: 0.01581\n",
      "Tuning epoch: 41, train loss: 0.01530, val loss: 0.01567\n",
      "Tuning epoch: 42, train loss: 0.01523, val loss: 0.01556\n",
      "Tuning epoch: 43, train loss: 0.01520, val loss: 0.01556\n",
      "Tuning epoch: 44, train loss: 0.01530, val loss: 0.01561\n",
      "Tuning epoch: 45, train loss: 0.01519, val loss: 0.01553\n",
      "Tuning epoch: 46, train loss: 0.01522, val loss: 0.01559\n",
      "Tuning epoch: 47, train loss: 0.01519, val loss: 0.01553\n",
      "Tuning epoch: 48, train loss: 0.01521, val loss: 0.01556\n",
      "Tuning epoch: 49, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 50, train loss: 0.01520, val loss: 0.01553\n",
      "Tuning epoch: 51, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 52, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 53, train loss: 0.01525, val loss: 0.01561\n",
      "Tuning epoch: 54, train loss: 0.01521, val loss: 0.01554\n",
      "Tuning epoch: 55, train loss: 0.01522, val loss: 0.01555\n",
      "Tuning epoch: 56, train loss: 0.01520, val loss: 0.01553\n",
      "Tuning epoch: 57, train loss: 0.01525, val loss: 0.01561\n",
      "Tuning epoch: 58, train loss: 0.01527, val loss: 0.01563\n",
      "Tuning epoch: 59, train loss: 0.01521, val loss: 0.01556\n",
      "Tuning epoch: 60, train loss: 0.01520, val loss: 0.01553\n",
      "Tuning epoch: 61, train loss: 0.01520, val loss: 0.01554\n",
      "Tuning epoch: 62, train loss: 0.01527, val loss: 0.01559\n",
      "Tuning epoch: 63, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 64, train loss: 0.01521, val loss: 0.01556\n",
      "Tuning epoch: 65, train loss: 0.01519, val loss: 0.01554\n",
      "Tuning epoch: 66, train loss: 0.01529, val loss: 0.01565\n",
      "Tuning epoch: 67, train loss: 0.01519, val loss: 0.01554\n",
      "Tuning epoch: 68, train loss: 0.01521, val loss: 0.01555\n",
      "Tuning epoch: 69, train loss: 0.01519, val loss: 0.01553\n",
      "Tuning epoch: 70, train loss: 0.01539, val loss: 0.01576\n",
      "Tuning epoch: 71, train loss: 0.01520, val loss: 0.01553\n",
      "Tuning epoch: 72, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 73, train loss: 0.01520, val loss: 0.01554\n",
      "Tuning epoch: 74, train loss: 0.01522, val loss: 0.01555\n",
      "Tuning epoch: 75, train loss: 0.01522, val loss: 0.01556\n",
      "Tuning epoch: 76, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 77, train loss: 0.01519, val loss: 0.01553\n",
      "Tuning epoch: 78, train loss: 0.01519, val loss: 0.01553\n",
      "Tuning epoch: 79, train loss: 0.01520, val loss: 0.01554\n",
      "Tuning epoch: 80, train loss: 0.01519, val loss: 0.01554\n",
      "Tuning epoch: 81, train loss: 0.01520, val loss: 0.01553\n",
      "Tuning epoch: 82, train loss: 0.01527, val loss: 0.01559\n",
      "Tuning epoch: 83, train loss: 0.01519, val loss: 0.01554\n",
      "Tuning epoch: 84, train loss: 0.01526, val loss: 0.01563\n",
      "Tuning epoch: 85, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 86, train loss: 0.01521, val loss: 0.01553\n",
      "Tuning epoch: 87, train loss: 0.01525, val loss: 0.01558\n",
      "Tuning epoch: 88, train loss: 0.01520, val loss: 0.01554\n",
      "Tuning epoch: 89, train loss: 0.01524, val loss: 0.01561\n",
      "Tuning epoch: 90, train loss: 0.01526, val loss: 0.01561\n",
      "Tuning epoch: 91, train loss: 0.01521, val loss: 0.01554\n",
      "Tuning epoch: 92, train loss: 0.01522, val loss: 0.01557\n",
      "Tuning epoch: 93, train loss: 0.01522, val loss: 0.01557\n",
      "Tuning epoch: 94, train loss: 0.01522, val loss: 0.01555\n",
      "Tuning epoch: 95, train loss: 0.01522, val loss: 0.01555\n",
      "Tuning epoch: 96, train loss: 0.01520, val loss: 0.01556\n",
      "Tuning epoch: 97, train loss: 0.01520, val loss: 0.01555\n",
      "Tuning epoch: 98, train loss: 0.01520, val loss: 0.01554\n",
      "Tuning epoch: 99, train loss: 0.01525, val loss: 0.01558\n",
      "Tuning epoch: 100, train loss: 0.01526, val loss: 0.01558\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAQOCAYAAAAQbxSAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdd5xU1fnH8c+zu3RYEOkdpBdBAVssGBtRFMWoP40ixmAhtsQe0WCJokYTuyKJCMYeG1GsiMYWWBSkKisivUtnKbvP7497hx2GmWV32N3Z8n2/Xvc1O/fcc+8zMztnnrlz7jnm7oiIiIiIyL5LS3UAIiIiIiIVhZJrEREREZFiouRaRERERKSYKLkWERERESkmSq5FRERERIqJkmsRERERkWKi5LqSMbM2ZuZmltQYjGY2Kaw/pJhDkxJmZp+Fr935qY5FRIqPmY0J39sjUh1LYZjZiDDeMamOpawzsyHhczUp1bFI4Sm5LmeiGtHYZaOZzTKzx82sS6rjLCuiGvG9LX9PdazJMrN24eO8KtWxiFRUBbS9G8xsmpndb2YtUh1nqoVt0Qgzq5fqWIqLmfWLer3bpDoeKfsyUh2AJG0HsDb824AGQNdwudjMznf3VxLU+650QixT8oBVBZRvKK1ASkA74M/AD8DDBWz3E8H/yfrSCEqkgoptexsCPcPld2Z2qrt/VsoxLSNo11eX8nHj+XN4OwZYl2Cb1QTxLiuNgERKm5Lr8usLd+8XuWNmVYDjgCeANsAzZjbJ3XdLKN19CdC5FOMsKxa5e5tUB5FK7v6bVMcgUgHEtr01gTMJvtjWA14xs3buvrW0AnL3m4GbS+t4+8rdHwUeTXUcIiVF3UIqCHff4e7vApEEqhZBgy8iIiXE3be4+zgg0i2rCXB6CkMSkRRTcl3xfAlsCv/uGltYmAsazay/mU00s/Vhf8KvzOyCwhzczLqa2UtmttLMtprZXDO73cyqF+YiFjM71czeNLPlZrY93M94MzupMMcvDmaWEdW/Lm4fSjNrH5bvjFO268JBM6tpZneY2fdmlmNmK8zseTM7YC8xNDCzO83s6/B12Bzu4wUzOy1qu8XAB+HdA+L0Bz0/atsCL2g0s7phrN+a2aZwmR6+bpkJ6twV7nN0eP8iM5sc1l1vZh+Z2XEFPM6DzGycmS0ws23htQPzzWyCmV1tZjUKep5EypCXCbqfAfSOrLSYC9LM7Ddm9omZrQnX75aIm9kBZvZU+D7IMbOfzexTM/udmaXHO7AV4oLGZNtWM6tiZpeE7+VV4fv0JzN7P1xfKzqGqKo/xrRFY6L2WeBngZmlmdnF4fO0NnwefjSzUWbWPkGdSL/oBeH9X5jZf8xstQWfRdPN7Aozs4Ieb1HFeX1PNbOPzWxd2A5+ZWbn7mUfzcLHtiR8rPPN7EErZL91MzvSzF40s8Xh67PGzD40s3NjH6+Z1TKzeWHM/0qwv/Zh7G5mf9zXY0bVaWtmT1jwWbbVzLaE/0uTzOxmM2tQmMdbLri7lnK0EPRjc2BSgnIjSK4deCxOeZuwzBPUvz5STvBB8TOQG95/AJgU/j0kTt3jga1R9dcD28K/vwTuCf8eE6duFeC5qLqR+tH3703i+RoR1l1QhDoZUcdskWCb9mH5zjhln4Vlvwemh39vBbZE7XcV0DbBvvsR9OmMbLsNWBP1OuyM2vbrqG13AstjljPjxHV+nGN2JOiTHTnm5nCJ3P8ROCBOvbvC8tFR/5s7CPqwR+rmAgPj1D013Day3daYeg60T/V7TosW9723veE2K8JtRkWtGxKpR9B1JPKeWBvenh617QB2b0PXAduj7n8A1CogthFxypJuW4HmwDcx7+U15LfrDvQLt30obHOi27jotuihqP2OIPFnQU3gvaj9bA+fh+h2Il570i8sXxA+5zsJPsOi6zrw9yRe+35R9dvElEW/vrdGPU+xx70mwb67ACujtttE/mfFPOCPBf3fAffGeW3zou6/AKTF1DmU/Lb3nJiydOCrsGwiYMV0zIPZvX3fTpBfRO+nf6rf58XWXqQ6AC1FfMH2nlz/Iuof9do45W0i5XHKjox6g4wDmoTr60W9mSINxpCYug0ILlJx4H9A93B9FeA8YGPUG2lMnGP/LaoxOYvwAwSoA1we9aY8t4jP1whSl1z/THCR4QkEvxKlAccAS8Ly5+PU7Rj1WKcSNOppYVkN4CTglZg6x4fbZ+/lccVNroFqwMzI8wT8kuBLmoWxLwrLpgNVY+pGkuufCT4QLgFqhmXtgP+G5YuA9Kh6Rn4y/wbQIaosM3yeRgMtU/2e06LFvVBtb42o9vO+qPVDwnUbw/LbgHphWSbQKPz7APJPjEwCOoXrq4Xvq5ywbHQBsY2IU5ZU2xoe92vyE+XBUXXTCZKlvwGHxtSLm4TGbDOCxJ8FT4ZlOcClQLVwfUfgY/K//HeMqdcvqmwb8AjQOCyrR/4XmzygWxFf+8i+C0qu1xEk9MOjXt/GwCvkfymoH1O3CjArLP8BODpcn0Zw8mEl+Z+5e/zfAVeHZcuBoUDdqP/FcwguGHXg5gJeg7VA86j1t5Hfpu/R/iZ7TIJE3QkS94Oi1tcE+oT/S4en+n1eXEvKA9BSxBcsQQMfvklPIjjDGPlWuEdiSMHJ9UcU/G11dFQDMySm7PZw/YpIwxJTfnZU3TExZR3CBm9lvDdzuM3/hXVnFvH5ijQguex5VjeyfBhTp7iS681Auzjl54TlW4CMmLLXwrLZQO1CPsZ9Ta4vCtdvA7rEqXcg+Wc5BseURZJrJ+YMSFjegvwzb0dErW8WVa9Bqt9XWrTsbUnU9kaVXxH1Px39i9GQqPV3F7D/f0Tex4RfUGPKLyE/OWyfILYRMeuTbluBYeQnuQcW4XlKOrkm+HyK/EJ3aZx6NcPnx4GxMWX9oo79dILjfhuW31bE175foscV8/reEqduDfLPTMe2nxdEtb2d4tQ9Kmrfk2LK6hF8YdsK9EwQ9+Hh67+WPU+MZJB/hvoDghMefchv638TZ39JH5P8s/GHxqtX0Rb1uS6/jgj7zi03sxUEDeC7BI1THkHDtLiwOzOz+sCx4d17PXw3xLi7gF0MCm9Hufsewy+5+8vA/AR1BxO8sV9y90UJtnmVoAHqZmZNC4gjkTSCswjxlpLq5/WSu8d7zG+GtzUIzu4CQZ9nYGB491Z33xRbsYT8Orx9zd3nxBa6+7fA6+HdsxPsY767vxSn7mKCM/AA3aOKNkb93aRo4YqUDRZoY2bXAfeFq38CxsfZPBd4MNF+yL8A/W/uviXOZqMJfvUy8t+ze7Mvbevg8PaZsA0oDWcQtNXLCR7vbsLnJfI8D0rUB52gC2I8kba3e4LyfZED7DFfggejxryX4LjRbe8eQ+S6+3+BTxMc70ygNsHJoenxNnD3LwlOuO1H1HUAYdlOguR+M8EJmpsJug9lEPy/xOuPvS/HjAx3m8znd7mj5Lr8qkJ+ctiI/NdyLcE3w2eKuL+DCBrhPIIznHsIE8U9Gmgzq0b+xZMFje+aqOyI8PbCqC8Muy3AYoLHDNCy4IcS10/ubgmWXknsrzCmxFvp7jkE/RYhaIAi+hK8jnnkN8al4eDw9uMCtpkYs22srALqLglvdz1Wd99I0GUE4AMzu8XMepqZ2iQp646x/IvC8wgSifsJviwvI+hDvT1OvWx3TzQOdTugbvh33Pehu+cRdBeBxO/DWEm1rRYM7RpJjN4p5LGKQ+Rx/dfdcxNsE2mLagGd4pSvTXBSA+K0RcVotrtvLuJxI4/3kwL2m6gs8tr+MtFrG76+kc/LPT433T3SpxvgLwTP5xKC7kLFfczI/9FYMxtpZoeF/2cVksa5Lr8+8XCs1TC57UzQ1+vXwD/MrJ+7/1yE/TUMb9cX0EBA8MaLfZPuR35yX9CkAEsTrI98k60TLntTsxDblAUbCyjLCW+jG5fG4e3aUjxrDfln7pcUsE3kV5CGCcqL+lgBfgv8h6BBvytcNprZJwQXxLxUwAesSKpETyIT6f41n+Cn9dEFtLsFTWIV/b7al/dhrGTb1vrk5wcLC3ms4hB5XIV5DqK3j5ZMW1QckjluJP5En42Q+LmIvLY1KdxnYtxt3H2UmV1IfuJ8SQH/w/tyzOsJ2vojgBvDJcfMviTolz7GS3Fs+JKms0QVgLtvC3+iOZvgjOeBwFOpjapIIv+Hfyjg7HL0MimVwVZg1UvzYO6eTfAz6SDgaWAuQQIwAPgX8KWFw3yJlCFfuHuTcGnq7u3d/UR3v38vJzQK+0WxON+H5bVtLdW2qJyKvLYPFfK1HRNvJ2Z2IEFf64gjS+KY7r4m3PcJBBeXfgNUJeiO+jgw0xIMfVseKbmuQMJ+0lcRNOJnmdkxRageOatS14IZxxJpFmfdz+SP71pQf6pEZSvC21YF1C1NkQtqIHEjXzfB+n0ReR7qm1ntEth/IpGfqgt6/iONXkFn34rM3Xe6++vufom7dyH4/7qRoA9oX4JfY0Qquuj3VXG+D5NtW9cSjHwB0LqIdfdF5HEV5jmI3r68isQf73OVvZTt8+dm+Kv3vwiS3Jnh6hvM7IgEVfbpmB740N2vdveDCX41vZTg/60dwYghFYKS6wrG3b8HIheW/aUIVSNjmaaR4JurmbUlzpvK3bcRjG5BorqhoxKs/zK87V+oSEtY+CUlcvFFom/SfUvg0FMIEvs0ivZcRL7YJDs5wtfh7bEFbPPLmG1LhLsvc/f7CIbRgmBIPpGKbj7BkGuQ4H0YXo/QL7xb2PdhUm2ru+8g/0Lkk4tSl/wTE8m0R5HHdWgBJ3kibdFmYI+LAMuZyOM9uoBtErWBkde2nyU/2dbdBL8eriD4vxtDMMziuAQneIrjmLu4+8/uPgr4U7iqwrT3Sq4rpr+Gt78ws36FqeDua8m/UOSGBDMs3VTALiKjSQwNR73YjZmdSdTIGDHGEjTIXczs0oLiNLOSuBAlnhnh7cDYAjOrTjDWZ7Fy9/XAW+HdO4pw9jryRSDZs+mvhrcDzKxHbGH4s+EZ4d2XkzxG7D731ucx0veuWnEcT6QsC7/QvxbevTpBYvk7gkldnKCPamHsS9s6NrwdErYBhRVpjwo1u2CM1whOFuxPMPTgbsLn5frIthXgmozI6zjIzDrEFoZnkBMl3q8QfMHYj2Bs6oTifW6a2bHAH8K7F4cX215FMNdBO+KMfJLsMS2YcbOga/wqXHuv5LoCcvdvgA/Du0X5WX0EQUN8HDDGzBrDrmmx7yZo7NYnqPsIQfeQxsAEM+sW1s0ws/8DniH/zExsvLPJ/znocTO7J7rvlZnVMbMTzew5Cv+hsq8iSeRlZnZh+PMZZtYdmED+xYfF7WaCxqsL8ImZHRMZQcPMalgwte5/Yup8T/AT7v5mtseXgUJ4nmAiAwPeChvdyDBjJwBvE1zc9C3wYjIPKo6eZjbDzK4ysw6RL3NmVtXMzgKuCbcrzVFTRFLpboL3fjPgbTPrBMFP92Y2lKCfKsA/3P2HwuxwH9vWfwDTCBKej8zsgkjSb2bpZtbHzJ42s0Nj6s0KbwcXMFReonh/AkaFd0daML16pO3tSNAWtScYM/muouy7jHqJ4FffasA7ZnYk7EpGTyH4srEhXsWwD/PN4d2bwteiY6Q8/Lw4ysyeAL6IrmvBtOrPErT5o9z97XCfG4ELCb7gXGxmpxXTMTOBbAtGheoR+b8IH+dx5P/KXnHaey8Dg21rKfxCIabgDbc7gfzB5w+LWt8msj5BvdjpzyN975y9T39+EvmziDlBMh25/xn5058/FaduOsFFDR61rA/3ET2t6sdFfL5GhPUWFLFeVYJuGpHjRk/pvZrgTK5T8CQye0wzHrXN4nCbI+OUHcfuU+dGhu7bY/rzqDr/inneF4TL6YWJi2D2s4VR+9jEntOf7zEVOVHTnxfwWCNTLw+PWtcn5rWOfYxOMMFBoSbS0aKlpBcK2fbGqTeksPUIZuWLnv78Z3af/vxDij79edJtK8HIUDOittkZtn97TH8eVeeiqLKtBON+LwD+GrXNCOJMIhOW1QTej9pH7DTZOexl+vPieC0S7NspYPrzAuoX9Hi7svv05xsp2vTnw2Nex00En9vRbemPMXX+FbX/eP9P94XlKwhnEN2XYxL8ihH9/7edoL3fGbXuBxJM2lYeF525rqDc/QOCftQAtxah3v3ArwjGWt1EcMYyi2BmqWv3Uvc9gqTpVYI3TjWCpOzPBAljpI9WvElmct19GEGf7ecIGuRqBBcULiToLnEFhZ88YZ94ME7tcQRfKH4iaEg2EpyBP5j8biMlceyPCIZWvI/gLNBOgucim6BRjHd2eijBFPXfETxnrcOlUF1LPOirfyBBsjyT/P6SMwlm3+zpwegexWUmwVTMTxGcHVtPcHZjPcH4178HjvLSHZJQJKXcfTzQg2D0nAUEieYWgi/GlwAnecFDpcbbZ9JtqwcTz/Qh6C7wGUEbWJtgyNX3CLqqTI6p8wxBezSZoO1qSdAWFWqyLg8mivlVuO//Ejz+mmHco4Ee7v5m4j2ULx78utCL4LEtIxiubznBLw59yR/2MVH9u4CeBGf85xH0SKhF/mt0A1HXO5nZ2cB5BInwBQn+n24l+IxrRPzJfIp0TIITUwMIuppMJriQsw7BCZwpwC1ALy/CxHdlnYXfKkRKnJn9l6CBv8gTDAskIiJFE3br+A3wJ3dPNDuhiJQSnbmWUmFmhxMk1nnARykOR0SkIokM17YypVGICKAZGqUYmdklBD/9vUTQ7y03HPFiEPkX1bwc/tQoIiL7KBxRIjIu8eSCthWR0qFuIVJszOwugr5TEPTnWk9wIUPkF5JpwAkeDPkjIiJJMrP+BCcyMsNVH7n78SkMSURCOnMtxelFgosWjyGYfKU+wYUMswkucnzS3bcmri4iIoVUneDiwuUEFyXemNpwRCSiwpy5btCggbdp0ybVYYiIJGXq1Kmr3b1hquMoLWqzRaQ8K6jNrjBnrtu0aUNWVlaqwxARSYqZ/ZTqGEqT2mwRKc8KarM1WoiIiIiISDFRci0iIiIiUkyUXIuIiIiIFBMl1yIiIiIixUTJtYiIiIhIMVFyLSIimFlvM5thZtlm9rCZWZxt6prZeDObbmazzOyicH0vM/syXPetmZ1T+o9ARKRsUHItIiIATwBDgQ7h0j/ONr8HZrt7T6Af8ICZVQW2AIPdvVtY7+9mVq9UohYRKWOUXIuIVHJm1hTIdPevPJhZbCxwepxNHagTntWuDawFdrr79+4+D8DdlwIrgUozIY6ISLQKM4mMiIgkrTmwOOr+4nBdrEcJptpeCtQBznH3vOgNzOwQoCrwQ2xlM7sEuASgVatWxRK4SFnU5qa3S2S/C0aeUiL7leKlM9cJDBkyBDOjX79+e5SNGDECM9tjqVWrFh06dODCCy9k8uTJpR7zNddcsyuWeHFHzJs3j3/+858MGzaMvn37Uq1aNcyMww47rFji2Lx5MyNHjqRPnz5kZmZSq1YtunXrxvDhw1m/fn3Cerm5udx99920b9+eatWq0bp1a2666Sa2bduWsM6sWbOoWrUqp512WrHELiIFOgmYBjQDegGPmllmpDA8Az4OuCg26QZw91Hu3sfd+zRsqBPbIlIx6cz1PkhLSyP6A2LNmjVkZ2eTnZ3Nc889xwMPPMA111xTKrFMnTqVRx99tFDbXn/99bz55pslEsfChQs56aSTmDt3LgA1atQgIyOD2bNnM3v2bMaOHcukSZNo167dHnWHDRvGqFGjAKhVqxYLFy7k3nvvZcaMGbz9dvyzAMOGDSMjI4OHH364RB6PSCWxBGgRdb9FuC7WRcDIsOtItpn9CHQGJodJ9tvALe7+VUkHLCJSVunM9T5o2bIly5cv37Xk5OTw+eef06tXL/Ly8rj22muZOXNmiceRl5fHpZdeipnRu3fvvW6fnp5Oly5dGDx4MA8//DAXXHBBscUxaNAg5s6dS5MmTZgwYQKbNm1iw4YNTJ48me7du7No0SJOPfVUdu7cuVvd7777jqeffpp69erxxRdfsGnTJmbOnEmLFi145513+PDDD/c43tixY/n000+55ZZbaNOmTbE8BpHKyN2XARvM7LCwP/VgIN438IXAcQBm1hjoBMwPL2p8HRjr7q+WUtgiImWSkutilJ6ezhFHHMEbb7xBlSpVyMvL47nnnivx4z7yyCNMnTqVK6+8ku7du+91+5dffpnZs2fz7LPPcuWVV8Y9i5yM8ePHM3XqVACeffZZ+vfvT1pa8C/Wt2/fXc/L7NmzeeaZZ3arO3HiRNydoUOHcvjhhwPQrVs3brjhBgA++uij3bZft24d119/PR07duT6668vlvhFKrlhwGggm6C/9AQAM7vMzC4Lt7kTOMLMZgAfATe6+2rgbOBoYIiZTQuXXqX+CEREygAl1yWgdevWdOzYEYDZs2eX6LEWL17MrbfeSrNmzbj99tsLVSc9Pb1EYpkwYQIAXbp04cQTT9yj/IADDtjVN3rs2LG7la1ZswZgj0S/ffv2AKxevXq39X/6059YuXIljz76KFWrVi2eByBSibl7lrt3d/cD3P2KsOsH7v6kuz8Z/r3U3U909x7hts+F659z9yru3itqmZbKxyMikipKrktI+LlEbm5u3PLoiyL3xVVXXcXGjRt58MEHqVOnzj7ta1/99NNPAHTq1CnhNp07dwbgiy++YMuWLbvW77///gDMnz9/t+1/+OGH3coBsrKyeOqppzj77LM54YQTiid4ERERkWKg5LoELFiwgHnz5gF7noktTuPHj+f111/n+OOP55xzUj8hWuSLQqIvFMCuvtZ5eXnMmTNn1/pjjz0WgKeffpqvvgquhZozZw733XcfAMcdd9yuepdffjm1atXiwQcfLP4HISIiIrIPlFwXo9zcXL788kvOOOMMduzYAcD5559fIsfavHkzV1xxBVWrVuWxxx4rkWMUVevWrQF2S5pjRXeTWbZs2a6/O3fuzMUXX8y6des4/PDDqV27Nl27dmXRokX079+f448/HoAnn3ySrKwsRowYQfPm8YbhFREREUkdJdf7YNGiRTRp0mTXUqNGDY444gimTQu6Go4YMYJDDz00bt0RI0bg7ru6jxTVbbfdxsKFC3dd1FcWRPpZZ2dn8/rrr+9RPnPmTN55551d9zdu3Lhb+VNPPcWdd95J27Zt2b59Oy1atOC6667jtddew8xYuXIlt9xyC927d+eqq64C4MUXX+TAAw+kevXqtGrVittuu22PkUhERERESovGud4HeXl5rFixYo/11atX59///jcnn3xyiRx32rRpPPTQQ7Rp04ZbbrmlRI6RjNNOO42ePXsyffp0fvvb37JhwwYGDhxItWrVmDhxIldccQVpaWm7uo1ERhKJSE9PZ/jw4QwfPjzu/q+77jrWr1/PW2+9RUZGBuPGjWPw4ME0btyYc845h6ysLO68806WLl3K6NGjS/zxioiIiMTSmet90Lp1611nn7dv387cuXO5/PLLycnJ4dJLL2XBggXFfsy8vDwuueQScnNzefjhh6lRo0axHyNZ6enpvPbaaxxwwAGsW7eOIUOGsN9++1GzZk0GDBjAypUrd/WhBqhXr16h9/3pp5/uSqaPOuooduzYwfXXX0+NGjX46quvePbZZ8nKyqJHjx784x//YMaMGSXxEEVEREQKpOS6mFSpUoVOnTrx+OOPM3ToUBYvXsy5555LXt4eMwDvk2effZYpU6Zw4okncuyxx7Jp06bdlkiXiNzc3F3rCrrAsLi1a9eOadOmcd9993H00UfTunVrunTpwsUXX8zUqVPp1St/6NsOHToUap87duxg2LBh1KtXb1dynpWVxYoVKxgwYMCuCWRq1KjB0KFDARLO6CgiIiJSktQtpATce++9vPzyy3z11VeMGzeOCy+8sNj2HRnu7v333y9w6L3PPvtsV/nHH39Mv379ii2GvalduzbXX3993MldIn2uGzVqVOiRVP72t78xa9YsHnvsMRo1agTkPw9t27bdbdvIuNiRchEREZHSpDPXJWC//fbj97//PRBcuKgL7PK9+OKLAJx33nmF2n7RokXccccd9O7dm8suu2yP8pycnN3ub926dd+DFBEREUmSkusScuWVV1KtWjUWLFhQrFOgR48yEm+JnCU/5phjdq0rzbPWBRk1ahRTpkyhZs2aXH311YWqc/XVV7N161aeeOKJ3S6AjAz7F5luPWLKlCkAu7qKiIiIiJQmJdclpEmTJlxwwQUA3HPPPXv0vS6uGRqTsW3bNlavXr1ricyUuHPnzt3Wr1+/fo+6Y8aM2RV3vAs2R40axbhx43YbRWXhwoXceOONXH755QD89a9/LVTyO2HCBF5//XWGDh1K3759dyvr06cPjRo14vPPP2fMmDG4O1lZWTz55JMAJTZSi4iIiEhBlFyXoOuuu460tDS+//57XnrppVSHs8sLL7xAw4YNdy33338/EJwFjl4/cODAIu/7iy++YPDgwTRp0oSaNWuSmZlJ69atue+++0hPT+fBBx/clWQXJCcnhyuuuIKGDRtyzz337FFepUoVRo4cCcBFF11ErVq16Nu3L+vWrePiiy+mR48eRY5dREREZF8puS5BnTp14rTTTgPg7rvvTnrCmPLkwgsv5MILL6Rz585kZGSQm5tLhw4dGDZsGNOnT+cPf/hDofZz9913M3/+fO69917222+/uNtcdNFFPPfcc3Tv3p3c3FxatGjBrbfeuuvstYiIiEhps4qS8PXp08ezsrJSHYaISFLMbKq790l1HKVFbbZUZG1uKpnhYBeMPKVE9itFV1CbrTPXIiIiIiLFRMm1iIiIiEgxUXItIiIiIlJMlFyLiIiIiBQTJdciIiIiIsVEybWIiIiISDFJOrk2s4fMLMvMcsxsQRL1nzIzN7PrYtZfYmYfm9m6sLxNsjGKiIiIiJSmfTlznQY8C4wtakUz+zVwCLA0TnFN4H1gxD7EJiIiIiJS6jKSrejuVwKEZ55PLGw9M2sNPAQcD0yIs9+/h9tVmskURCqKHbl55LlTNT0NM0t1OCIiIqUu6eQ6GWaWAbwA3OXuc/ThK1L+rNm0jbnI85QAACAASURBVOmL1zFt4TrmrdzE4p+3snTdVjZu28n2nXkAmEHNKuk0rlud5vVq0Kp+TXo0r0vPlvXo0Kg2Gem63ENERCqmUk2ugduB1e7+RHHszMwuAS4BaNWqVXHsUkRiuDszlqzng9kr+HDOSuYs2wBAmkGb/WvRsn5NujevS2aNDGpXzSAtzcjZkcumbTtZsSGHJetyeGv6Uv71v4UAZFbP4JhOjTi+SyOO7dyIzOpVUvnwREREilWpJddm1g8YAvQqrn26+yhgFECfPn28uPYrIsEZ6te/WcKLUxaRvXITaQZ92tTnxv6dObhVPXq0qEvNqoVrQtydBWu2MH3ROj7PXs3H361k/PSlVK+Sxsndm3J235Yc2ra+upKIiEi5V5pnrvsBTYFlUR+g6cC9ZnaNu7coxVhEJIHFP2/hqU/m81LWIrbvzOPgVvUYOagHJ3Vrwn61qia1TzOjbYNatG1Qi9MPak5envPNonW89vVi3pq2lNe+WUK3Zplc3u8AftW9KelpSrJFRKR8Ks3k+nHg1Zh17xH0wX66FOMQkThWbszhbx98zytZizGDMw9uwUW/aEunJnWK/VhpaUbv1vvRu/V+DD+lK+OnL+XJT3/giue/oW2D77nhpE70795EZ7JFRKTcSTq5NrP2QG2gGVDVzCLdPWa7+3Yzaw58BNzs7q+7+0pgZcw+dgDL3f27qHVNgCZAx3BVVzOrByx097XJxisi8eXsyOUfn/3I4x9nsz03j98c2opLjzmAZvVqlMrxa1RN5+y+LTmzdwvem7Wcv3/4PZf/62v6tgkS754t65VKHJWdmfUGxgA1gHeAq919j+52YRe/vwNVCK6hOSZcXw8YDXQHHPitu39ZKsGLiJQh+3LmejRwTNT9b8LbtsACgoa3E1C3iPu9DPhz1P23w9uLCBp+ESkmWQvWcsOr3zJ/9WZO7NqYm0/uQtsGtVISS3qacXKPppzYtTEvZy3mwQ++4/THP+eiI9py3UkdC92/W5L2BDAU+B9Bct2fmOFSwwT6caC/uy80s0ZRxQ8B77r7r82sKsGcBSIilc6+jHPdby/lC4ACf9N19zZx1o1AE8iIlKgt23dy/3vfMeaLBTSvV4NxFx/CUR0apjosADLS0zjv0Fac2rMp97/3Hf/8/Efen72ce888kF+0b5Dq8CokM2sKZLr7V+H9scDp7DkXwXnAa+6+ECD8RRIzqwscTXDROu6+HdheKsGLiJQxGmxWpJL5bvlGTn3kM575fAGDD2vNe9ccXWYS62h1qlfhjoHdefnSw6mansZvRv+PeybMYUduXqpDq4iaA4uj7i8O18XqCOxnZpPMbKqZDQ7XtwVWAc+Y2TdmNtrMUvMTiIhIiim5Fqkk3J2Xpixk4GOfsSFnJ8//7lBuH9idWtXKdneLQ9rW552rj+K8Q1vx1CfzOfupL1m0dkuqw6qsMoDewCnAScCtZtYxXH8w8IS7HwRsBm6KrWxml5hZlpllrVq1qhTDFhEpPUquRSqBbTtzueHVb7nx3zPo07o+71x1FEeUoy4W1aukc/cZPXj0vIPIXrGJAY98xufZq1MdVkWyBIgeDrVFuC7WYuA9d9/s7quBT4Ge4frF7v6/cLtXCZLt3bj7KHfv4+59GjYse7+WiIgUByXXIhXc2s3buWD0ZF6ZupirftmeZ397CA3rVEt1WEkZcGAz/nPVkTSqU43B/5zMuK9+SnVIFYK7LwM2mNlhFox/OBh4M86mbwJHmlmGmdUEDgXmuPtyYJGZdQq3Ow6YXRqxi4iUNWX792AR2SfZKzdy0ZgprNywjUfOPYhTezZLdUj7rPX+tXht2BFc9cI33PrGTOat2MhtA7qSka5zBftoGPlD8U0IF8zsMgB3f9Ld55jZu8C3QB4w2t1nhvWvBP4VjhQyn2CEJxGRSkfJtUgF9c3CnxnyzBSqZqTx0qWH06sCjRddp3oVRl/Yl5ET5vD0f39k2focHjn3IKpXSU91aOWWu2cRjFEdu/7JmPv3A/fH2W4a0KfEAhQRKSd0qkekAvps3mp+M/p/1KtZhdcuP6JCJdYR6WnGLad0ZcSpXflg9gouemYKm7btTHVYIiJSySm5Fqlg3p25nN+OmUKr+jV55dLDaVm/Ys/lMeQXbfnbOT2ZvGAt5z39FWs3a3hlERFJHSXXIhXI298u4/fPf0235pm8eMlhNMqsnuqQSsUZB7XgqfN7893yjUqwRUQkpZRci1QQ781aztUvfkOvlvUYd/Gh1KtZNdUhlarjuzbmn0P68uPqzZw/+n+s26IEW0RESp+Sa5EKYOLcFVzx/Nd0b16XMRf1pXYZnximpPyifQOeuqA32Ss3Mfifk9mQsyPVIYmISCWj5FqknPs8ezWXjfuazk0yefa3h1CnepVUh5RS/To14onzD2bOsg0M+edktmzXRY4iIlJ6lFyLlGMzl6zn0nFTadugFuMuPoS6NSp3Yh1xXJfGPHLuQUxbtI4rn/+Gnbl5qQ5JREQqCSXXIuXUorVbGPLMFDKrZzDmt30rXR/rvenfvSl3DOzOR3NXcsvrM3H3VIckIiKVQOXsmClSzq3ZtI3B/5zMjtw8Xhh6OE3r1kh1SGXS+Ye1ZsWGHB6ZmE3jzGr88cROe68kIiKyD5Rci5QzOTtyufjZLJau28q/fncoHRrXSXVIZdofT+jIig05PDwxm6b1anDuIa1SHZKIiFRgSq5FyhF354ZXv2XaonU8eX5v+rSpn+qQyjwz4+4zerBiwzZufWMmbRvU4rB2+6c6LBERqaDU51qkHHl80g+8NX0p15/Uif7dm6Q6nHIjIz2NR847iNb71+Ty56aycM2WVIckIiIVlJJrkXLi3ZnLuf+97xjYqxnD+h2Q6nDKnczqVRh9YV/yHIaOzWLTNg3RJyIixU/JtUg5MHvpBv748jR6tqzHvWceiJmlOqRyqW2DWjx23sFkr9rENS9OIy9PI4iIiEjxUnItUsat27KdS8ZlkVm9Ck9f0JvqVdJTHVK5dmSHBtw2oCsfzlnBYx9npzocERGpYJJOrs2slZmNN7PNZrbazB42swIH2jWzJmY2zsyWm9kWM5tuZr+J2WaBmXnMMjLZOEXKs7w85w8vTWPFhhyevKA3jTKrpzqkCmHw4a05vVcz/vbh93yevTrV4YiISAWSVHJtZunA20Ad4CjgXODXwAN7qToW6AIMBLqH98eZ2dEx290BNI1a7komTpHy7vFJ2Xz83SpuG9CVXi3rpTqcCsPM+MsZPTigYW2ufvEblq/PSXVIIiJSQSR75vpEoBtwgbt/7e4fADcAQ80ss4B6RwCPufv/3H2+uz8ALAIOidluo7svj1o2JRmnSLn12bzVPPDB95zeqxnnH9Y61eFUOLWqZfDE+QezZXsuV77wNTs0RbqIiBSDZJPrw4E57r4oat17QDWgdwH1PgPONrP9zSzNzAYCDYEPY7a7zszWmNk0M7slUXcTM7vEzLLMLGvVqlVJPhSRsmfZ+q1c9eI3dGhUm7sH9dAFjCWkfaM63DOoB1MW/Mz9732X6nBERKQCSDa5bgKsiFm3GsgNyxI5G/Bw223Av4Bz3X1a1DYPE3QzORZ4FPgD8Hi8nbn7KHfv4+59GjZsmMzjEClzdubmceXz37BtRy5PnN+bmlU111NJGtirOecf1opRn85n0ncrUx2OiIiUc6U9WshdQAPgeKAPcD8w1sx6RjZw9wfd/WN3/9bdRwPDgIvNTFOqSaXw8MRssn76mbsHBX2CpeQNP6UrnRrX4bpXvmX1pm2pDkdERMqxZJPr5UDjmHUNgPSwbA9mdgBwJTDU3T9y9+nufjswJVyfyP/C2/ZJxipSbkz+cS2PTpzHmQe3YGCv5qkOp9KoXiWdh87txYacHdz46re4a/xrERFJTrLJ9ZdAFzNrEbXuBIKuHlMT1KkZ3ubGrM/dSxy9wttlRQ1SpDxZv2UH17z4Da3q1+T2gd1SHU6l07lJJjf/qjMfzV3Jc/9bmOpwRESknEo2uX4fmEXQpeMgMzueoIvH0+6+AcDMDjGzuWYWGQlkLpANPB6WHWBm1xIk5a+HdQ43sz+YWS8za2tmZxP0t37L3fVpJxWWu3PTa9+yatM2Hj73IGpXUz/rVBhyRBuO6diQu/4zm+yVG1MdjoiIlENJJdfungucAmwBPgdeAv4NXBe1WU2gU3iLu+8ATgZWAeOBb4HBwEXuPj6ssw04B5gEzCYY7/ppggscRSqsF6csYsLM5Vx3YicObKHxrFPFzLj/rAOpVS2DK1+YxradsT+0iYiIFCzp02PhmeQBBZRPAixm3TzgzALqfA0clmxMIuXR/FWbuH38LI7q0IChR7VLdTiVXqM61bnvzAP53dgsHp2YzbUndkp1SCIiUo6U9mghIhJlZ24e174ynWoZ6fz1rJ6kpWk867Lg+K6NOfPgFjw+6QdmLlmf6nBERKQcUXItkkJPfTqfbxau487Tu9M4s3qqw5Eotw3oyv61qnLdK9PZvlOzN4qISOEouRZJkTnLNvD3D7/nlAObclrPZqkOR2LUrVmFewb1YO7yjTw6cV6qwxERkXJCybVICmzfmccfX55O3RpVuXNg91SHIwkc16Uxgw5uzmPqHiIiIoWk5FokBR7+aB5zlm1g5KAe1K9VNdXhSAH+PKCbuoeIiEihKbkWKWXfLPyZxydlc1bvFhzfNXaiUylr6taswt1nBN1Dnvzkh1SHIyIiZZySa5FSlLMjl2tfmU7TujW47dSuqQ5HCun4ro055cCmPPpxNvNXbUp1OCXCzHqb2Qwzyzazh80s4dA1ZtbXzHaa2a+j1t1rZjPD5ZzSiVpEpOxRci1Sih6ZOI/5qzYz8swe1KleJdXhSBH8eUBXqmWkMfyNmbh7qsMpCU8AQ4EO4dI/3kZmlg7cSzBTb2TdKcDBQC/gUOA6M8ss6YBFRMoiJdcipWTW0vU8+cl8zurdgqM6NEx1OFJEjTKrc2P/znzxwxpe/2ZJqsMpVmbWFMh09688+OYwFjg9weZXEszIuzJqXVfgU3ff6e6bCWbgjZuci4hUdEquRUrBztw8bvz3t9SvVZXhp6g7SHl13iGtOKhVPe56ew4/b96e6nCKU3NgcdT9xeG63ZhZc+AMgrPc0aYD/c2sppk1AI4FWsapf4mZZZlZ1qpVq4oteBGRskTJtUgpGP3Zj8xcsoE7TutG3ZrqDlJepaUZ9wzqwYatO7hnwpxUh5MKfwdudPfdhk1x9/eBd4AvgBeAL4Hc2MruPsrd+7h7n4YN9euNiFRMGakOQKSim79qE3/74Hv6d2vCr3o0TXU4so86N8nkd0e148lPfmDQwS04rN3+qQ6pOCwBWkTdbxGui9UHeDG81rEBcLKZ7XT3N9z9L8BfAMzseeD7kg1ZpHDa3PR2ie17wchTSmzfUn7pzLVICcrLc256bQbVMtK4Y2C3VIcjxeTq4zrQsn4Nhr8xkx255X/sa3dfBmwws8PCUUIGA2/G2a6tu7dx9zbAq8Awd3/DzNLNbH8AMzsQOJCoCx5FRCoTJdciJej5yQuZ/ONahg/oSqPM6qkOR4pJjarp/HlAN7JXbuLZLxakOpziMgwYDWQDPwATAMzsMjO7bC91qwD/NbPZwCjgfHffWZLBioiUVeoWIlJClq/PYeSEuRzZvgFn9W6x9wpSrhzXpRHHdmrI3z+cx2m9mtGoTvn+8uTuWUD3OOufTLD9kKi/cwhGDBERqfR05lqkhNw+fhY7cvO4+4weFDAfh5RTZsZtp3Zj+848Rk6Ym+pwRESkjFByLVICJs5dwYSZy7nquA602r9mqsOREtK2QS0uPqotr329hKk/rU11OCIiUgYouRYpZlu27+TWN2bRoVFthh7VLtXhSAm74tj2NMmszp/fmkVuXoWcuVFERIpAybVIMXvoo3ksWbeVuwf1oGqG3mIVXa1qGdx8cmdmLtnAS1MWpTocERFJMX3yixSjucs38I///sg5fVrSt039VIcjpeS0ns04pG197n9vLuu2VKiZG0VEpIiSTq7NrJWZjTezzWa22sweNrOqBWxf38weMbO5ZrbVzBaZ2RORsVGjtjvYzD4ws3VmtsbMRplZ7WTjFCkteXnOn16bQWaNKtz0q86pDkdKkZkx4tRurN+6gyc++SHV4YiISAollVybWTrwNlAHOAo4F/g18EAB1ZoBzYEbgB7A+cDRBFPlRvbbDPgQmA8cCvQHugFjkolTpDS9lLWIrxeu408nd2G/Wgm/Z0oF1bVZJk+c35srjm2f6lBERCSFkh3n+kSCpLe1uy8CMLMbgNFmdou7b4it4O4zgUFRq7LN7HrgP2aWGdYZAOQRzPqVG+73MuBbM2vv7tlJxitSolZv2sbICXM5rF19zjy4earDkRQ5qVuTVIcgIiIplmy3kMOBOZHEOvQeUA3oXYT9ZALbgC3h/WrAjkhiHdoa3h6ZZKwiJe4vb89hy/ad3HW6xrQWERGpzJJNrpsAK2LWrQZyw7K9MrN6wJ3A01HT5E4EGpjZTWZW1cz2A0aGZU3j7OMSM8sys6xVq1Yl8zhE9tkX2at5/ZslXH7MAbRvpMsDREREKrOUjBYSXqA4HlhC0AcbAHefBVwIXENwxno58CNBIp8Xux93H+Xufdy9T8OGDUsjdJHdbN+Zx61vzqT1/jUZpr62IiIilV6yyfVyoHHMugZAeliWUJhYvxPeHeDuOdHl7v68uzchuAByf2AE0JDgIkeRMuWfn//ID6s2M+K0blSvkp7qcERERCTFkk2uvwS6mFmLqHUnEPSfnpqokpnVAd4lSMJPdvdNibZ19xVh+TlADvBBkrGKlIil67by0IfzOLFrY47t1CjV4YiIiEgZkGxy/T4wCxhrZgeZ2fHA/QT9pzcAmNkh4ZjWh4T364T19gOGALXMrEm47Bq3zMyuMLPeZtbRzH4PPArc7O7rkn2QIiXhrrdn4zi3Duia6lBERESkjEhqKD53zzWzU4DHgc8J+kf/C7g+arOaQKfwFoJRRA4L//4+ZpfHApPCvw8BbgdqA3OBS919XDJxipSUT79fxTszlnPdiR1pWb/m3iuIiIhIpZDsONe4+0KCcakTlU8CLNH9AuoNTjYmkdKwbWcuI96aRZv9azL06HapDkdERETKkKSTa5HKavR/f2T+6s2Muagv1TJ0EaOIiIjkS8lQfCLl1ZJ1W3lk4jz6d2tCP13EKCIiIjGUXIsUwZ3jZwNw66m6iFFERET2pORapJAmfbeSd2ct58pfdqB5vRqpDkdERETKICXXIoUQuYixXYNa/O6otqkOR0RERMooXdAoUghPfzqfBWu2MPa3h+giRhEREUlIZ65F9mLR2i08+nE2J/dowtEdG6Y6HBERESnDlFyL7MWd/5mNYQw/RRcxioiISMGUXIsU4OO5K3l/9gquOq4DzXQRo4iIiOyFkmuRBHJ25DJi/CzaNazFxUfqIkYRERHZO13QKJLAU5/M56c1W3ju4kOpmqHvoSIiIrJ3yhhE4li4ZguPT8rmlAObcmSHBqkOR0RERMoJJdcicdw+fhbpacatuohRKgkz621mM8ws28weNjOLs81AM/vWzKaZWZaZHRmub21mX4frZ5nZZaX/CEREygYl1yIxPpi9go/mruSa4zvQpG71VIcjUlqeAIYCHcKlf5xtPgJ6unsv4LfA6HD9MuDwcP2hwE1m1qzkQxYRKXuUXItE2bo9mImxY+PaXPQLXcQolYOZNQUy3f0rd3dgLHB67HbuviksB6gFeLh+u7tvC9dXQ58tIlKJqQEUifL4pGyWrNvKHQO7UyVdbw+pNJoDi6PuLw7X7cHMzjCzucDbBGevI+tbmtm3wCLgXndfGqfuJWF3kqxVq1YV6wMQESkrlD2IhH5cvZmnPpnP6b2acVi7/VMdjkiZ5O6vu3tngjPbd0atX+TuBwLtgQvNrHGcuqPcvY+792nYULOdikjFpORaBHB3bntzJtUy0vjTKV1SHY5IaVsCtIi63yJcl5C7fwq0M7MGMeuXAjOBo4o7SBGR8kDjXIsA785czn/nrea2AV1pVEcXMUrl4u7LzGyDmR0G/A8YDDwSu52ZtQd+cHc3s4MJ+levMbMWwBp332pm+wFHAn8rxYcg5Uibm94usX0vGHlKie1bpLCUXEult3nbTu74z2y6NM1k8OGtUx2OSKoMA8YANYAJ4UJkWD13fxI4ExhsZjuArcA5YaLdBXjAzBww4K/uPqP0H4KISOopuZZK75GJ2Sxbn8Mj5x5Ehi5ilErK3bOA7nHWPxn1973AvXG2+QA4sEQDFBEpJ5LKJCwwwsyWmtlWM5tkZt32Umeomf3XzH42s3Vm9nFkAoKobRaYmcdZSu43JKnUslduZPR/5/Pr3i3o06Z+qsMRERGRci7Z03Q3ANcCVwJ9gZXAB2ZWp4A6/YCXgF8STDLwHfCemXWI2qYv0DRqOZhgHNWXk4xTJCF359Y3ZlGzajo3/apzqsMRERGRCqDI3ULCKXGvAUa6+7/DdRcSJNjnAU/Fq+fuv4nZz+UEQzn1B+aF26yK2eZiYANKrqUEjP92GV/OX8Odp3enQe1qqQ5HREREKoBkzly3BZoA70dWuPtW4FPgiCLspypQHfg5XmGYxF8MPBfuP942mpBAkrIxZwd3/Wc2PZrX5bxDWqU6HBEREakgkkmum4S3K2LWr4gqK4y7gE3AWwnKTyBI5J9OtANNSCDJ+vuH81i1aRt3nt6d9DRLdTgiIiJSQew1uTaz35jZpsgCVNnXg5rZ1cClwCB335Bgs6HAFHefvq/HS+S75Rv55Hud8a5s5i7fwJgvFvB/fVvRq2W9VIcjIiIiFUhh+ly/RTCpQESkc2pjYGHU+sbA8r3tzMyuIZgy91fuPjnBNo2AgcDvCxFf0oa/MYMfV2/mo2v7UbfGPn9nkHLA3bntjVlkVs/ghpM6pTocERERqWD2euba3Te6e3ZkAWYTJNEnRLYxs+oEU91+UdC+zOyPBIn1Ke7+WQGbDgG2AS/s9RHsgz+f2o21m7fzwPvfleRhpAx57eslTF6wlhv7d2a/WlVTHY6IiIhUMEXuc+3uDvwduNHMBplZd4JZvTYBz0e2M7OPzOyeqPvXAyMJLlL83syahEvd6P2HFzL+DnjR3Tcl8ZgKrXvzugw+vA3jvvqJGYvXl+ShpAxYu3k7f3lnDge1qsfZfVqmOhwRERGpgJId5/o+4G/AY0AWwZjUJ7r7xqhtDgjXR/yeoL/2S8CyqOWhmH33AzpQwIWMxemPJ3Zk/1rVGP7GDHLzvDQOKSnyl7fnsGHrDu4Z1IM0XcQoIiIiJSCp5NoDI9y9qbtXd/dj3H1mzDZt3H1IzH2LswyJqfdxuD5uf+zillm9CrcO6ML0xet5YfLCvVeQcumL7NX8++vFXHJ0Ozo3yUx1OCIiIlJBJXvmukI5rWczDm+3P/e9O5fVm7alOhwpZjk7cvnT6zNovX9Nrjquw94riIiIiCRJyTVgZtx5eje2bM9l5IS5qQ5HitmjE7NZsGYLd5/Rg+pV0lMdjoiIiFRgSq5D7RvVYejR7Xh16mIm/7g21eFIMflu+Uae/OQHBh3cnF+0b5DqcERERKSCU3Id5cpftqd5vRrc8voMtu/MS3U4so/y8pw/vT6DOtUzGH5K11SHIyIiIpWAkusoNatmcMfAbsxbuYknJv2Q6nBkHz0/eSFTf/qZ4ad0pb7GtBYREZFSoOQ6xnFdGnNqz2Y8+vE85q3YuPcKUiat2JDDvRPm8ov2+zPo4OapDkdEREQqCSXXcfz51K7UqpbBTa/NIE9jX5dLt4+fxfbcPP5yeg+CeYlERERESp6S6zga1K7G8FO6MvWnn3nufz+lOhwpondmLOOdGcu56rgOtGlQK9XhiIiISCWi5DqBMw9uzlEdGnDvhLksXbc11eFIIa3dvJ1b35hJj+Z1ufTodqkOR0RERCoZJdcJmBl3n9GDPIdb35iJu7qHlAd/fmsWG3J2cP9ZB5KRrn9vERERKV3KPgrQsn5Nrj2xIx/NXclb05emOhzZi3dnLmP89KVc9csOmuJcREREUkLJ9V4MOaINPVvW489vzWLlhpxUhyMJ/Lx5O8PfmEm3Zplc1u+AVIcjIiIilZSS673ISE/jgbN6snV7Lje/NkPdQ8qoEeNnsW7LDv56Vk+qqDuIiIiIpIiykEJo36g2N/TvzEdzV/LK1MWpDkdivDdrOW9OW8qVv+xAl6bqDiIiIiKpo+S6kC46og2Htq3PHeNns/jnLakOR0I/b97OLa/PpGvTTIYdq+4gIiIiklpKrgspLc3461k9cXduePVbTS5TBrg7N782g/Vbt6s7iIiIiJQJykaKoGX9mgwf0JUvflijyWXKgFeyFvPurOVcd2InujZTdxARERFJPSXXRfR/fVtyTMeG3P3OHLJXbkx1OJXWT2s2M2L8LA5vtz9Dj9JkMSIiIlI2KLkuIjPj/l8fSK2qGVzx/Dfk7MhNdUiVzs7cPK55aRrpacYDZ/ckLc1SHZKIiIgIoOQ6KY0yq/PXs3oyd/lGRk6Ym+pwKp1HP87mm4Xr+MsZPWhWr0aqwxERERHZJank2gIjzGypmW01s0lm1q0Q9a42s7lhncVm9piZ1Y4q/72ZfWtmG8LlSzM7JZkYS9qxnRtx0S/aMOaLBXw0Z0Wqw6k0vl74M49MzOaMg5pzWs9mqQ5HpMIws95mNsPMss3sYTPb4ychM+sctsvbzOy6mLJ6ZvZq2MbPMbPDSy96EZGyI9kz1zcA1wJXAn2BlcAHZlYnUQUzOw+4D/gL0AUYDJwMPBS12WLgRuBgoA8wEXjDzA5MMs4SddOvOtOlaSbXv/qtZm8sBRtydnDNi9Noklmd2wfu9buciBTNE8BQoEO49I+zzVrgKuCvccoeAt51985AT2BOCcUpGdwhDgAAIABJREFUIlKmFTm5Ds9mXAOMdPd/u/tM4EKgDnBeAVWPAL5y93HuvsDdJwJjgUMjG7j7m+4+wd2z3f17d78F2AiUyTMg1TLSeeTcg9i6PZc/vDyNXA3PV2LcnZv+/S1L1m3lof/rRWb1KqkOSaTCMLOmQKa7f+XBNLRjgdNjt3P3le4+BdgRU78ucDTwj3C77e6+ruQjFxEpe5I5c90WaAK8H1nh7luBTwkS6EQ+A3qZ2WEAZtYKOA14J97GZpZuZv8H1Aa+SCLOUtG+UW1GnNaVz7PX8PBH81IdToU19sufeGfGcm44qRN92tRPdTgiFU1zgl8OIxaH6wqrLbAKeMbMvjGz0WZWqzgDFBEpL5JJrpuEt7EdjVdEle3B3V8E/gR8amY7gJ+AGQTdQHYxsx5mtgnYBjwJnOHuM+Lt08wuMbMsM8tatWpVEg+leJzdpyW/7t2ChyfO4+PvVqYsjorq28XruOvt2RzXuZGG3RMpmzIIuvM94e4HAZuBm2I3KitttohISdprcm1mvzGzTZEFSOr3eDM7BrgVGEbQCA8C+gG3x2z6HdCLoLvIE8CzZtY93j7dfdT/s3ff8VVU6R/HP09CEnqT3juCKCio2LGAomJ37ej+FAuKHdu6imt3XRc7K7oidneVFWyIrrg20KgoVYyAhg5KhxACz++PmeAlJCS5uSXl+3695pXMmXPmPHcSLk/OPXPG3fu4e5/GjRtHE1ZMmBl3ntiD3ZvV5epXppH9mx6PHitrNm3h8pe+oXHtDB48XcvuicTJIqBVxH6rsKykFgIL3X1quP9vgvf5HZSX92wRkXgqycj1eIJkN39bGZY3LVCvKbB0F+e5C3jZ3Z929+nuPo5gJPsGM6uWXymcq5fl7l+7+83ANOCakr2c5KmRnsqoc/dhmztDX/xG61/HQPCo+e9YsjqHx87Zhwa10pMdkkil5O5LgLVm1je8r2Yw8GYp2i8Fss2sa1h0JDAr9pGKiJR/xSbX7r4uTHaz3D2L4A1zKdA/v46ZVQcOYddzo2sCBTPOrUBxQ5EpQEZxcZYHbXerxd//0Ivpi9Zw+5szCe4Lkmg9MfknJs5cxk0Dd2efNg2SHY5IZTcUeBrIAn4C3gUws0vN7NLw+2ZmthC4Frg1XFK1bth+GPCimX1PMBBzT6JfgIhIeVCt+Co7cnc3s5HALWY2B5gL3AqsB17Kr2dmHwJfhqPPABOAa80sE5gKdALuBN5y97ywzX3A20A2v68+0g8ol2tdF+ao7k254vBOPPZRFt2a1+GCg9onO6QK6cPZy3jw/R84qVcLLjxY11Ak3tw9E9hpCp67j4r4fik7Th+JrDeNYAlVEZEqrdTJdegBoAbwONCAIFke4O7rIup0JEiS890FOEFC3YpgeskE4E8RdZoBL4Rf1wDfAwPdfWKUcSbFtf27MGfpOv7y1iw6NK7NoV00t7A0spav46pXprFHi7rcd+peFPIsCxEREZFyKarkOlwHdUS4FVWnXYH9PIKbFwvewBhZ54Jo4ilvUlKMkWf24rQnP+fyl75h3NCD6NSkdvENhTWbtjBk7NdUT0vhqfP6UD0tNdkhiYiIiJRYtE9olGLUzqjG0+f3IaNaChc99xWrN+YmO6RyL2/rNq58+VsWrtrIk+f2pkX9GskOSURERKRUlFzHUasGNfnHeb1ZvDqHi8d+rRVEdsHd+fObM/h47gr+cmIP9tWDYkRERKQCUnIdZ73bNuRvf+jJlwt+4+pX9Ij0ojwx+Sde/jKbof06ctZ+bZIdjoiIiEhUlFwnwKCeLbjt+O68N3Mpt705Q0v0FTDu24X8deIPnNirBcOP7lp8AxEREZFyKtrVQqSU/u/g9ixft5lRH/9EkzrVueqozskOqVz4LGslN/z7e/p2aMgDp2llEBEREanYlFwn0I3HdGX5uhz+/sFcalevVuXXb/76598YMjaTDo1q84/z+pBRTSuDiIiISMWm5DqBzIz7T92LTblbufOtWVRLMc4/sF2yw0qK6QvXcME/v6JZ3eo8f9F+1KuRluyQRERERMpMc64TLC01hYfP3Jv+3Zty+/iZvDj152SHlHA/LF3Hef+cSt0aabxw0f40qVM92SGJiIiIxISS6yRIr5bC42fvw5G7N+FP42bw/JSqk2DPWryWs0dPIaNaCi8P6au1rEVERKRSUXKdJOnVUnji3H04qlsT/vyfGTz23x8r/Soi3/yyijOf+oL0MLFus1vNZIckIiIiElNKrpMoo1oqT57bm5P3bsmD78/lrrdns62SroP9xU+/cu7TU2lQK53XLjmADo31OHgRERGpfHRDY5Klpabwt9N7Ur9mGs98Op/fNuRy36l7VqqVM96ZvoRrXp1Gm4Y1efGi/WlSV3OsRUREpHJScl0OpKQYtx3fnd1qpfPg+3P55beN/OO83jSqnZHs0MrE3Rn18Tzuf28O+7Spz9Pn70vDWunJDktEREQkbjQtpJwwM644ojOPn70PMxev4cTHPmP2krXJDitquXnbuPmN6dz/3hwG9WzBS0P6KrEWERGRSk/JdTlz3F7N+dclB5K3bRsnP/EZr32VXeFudFy4aiOn/+MLXvkqm2FHdOLhM3pRPa3yTHMRERERKYqS63Joz1b1mDDsYPZp04AbXv+ea16dxvrNeckOq0T+O2cZxz3yKfOWr2fUuftw3YCupKTokeYiIiJSNSi5Lqea1KnO8xfuz3X9uzD+u8Uc98gnTJn3a7LDKtKGzXn8+T8z+L8xmbSsX4MJww7mmB7Nkx2WiIiISEIpuS7HUlOMYUd25pWLDwDgzKemcOt/prMuZ0uSI9vRZ1krOXrk/3hh6s9ceHB73hh6IO0a1Up2WCIiIiIJp9VCKoD92jfkvasO5W/v/8Azn81n4sxlXD+gC6f1bk1qEqdcLFq9ifvfncP47xbToVEt/n3pAfRu2zBp8YiIiIgkm5LrCqJGeiq3Ht+dQT1bcMeEmdz4+nSe+/xnrhvQhSN2b4JZ4pLs1RtzGf3JPJ7+ZD4Aw47oxOWHd9JNiyIiIlLlKbmuYHq2rs/rlx3IW98v4f735nDhc5ns3qwOQw/vxDF7NCO9Wvxm+ixbm8Mzn87nhSk/szF3K4N6tuCmgbvTsn6NuPUpIiIiUpFElVyb2SnAJcA+QCPgcHefXEybw4B7ga5ATeBn4Gl3fzCiThpwM3A+0BL4AbjR3d+LJs7KyswY1LMFx/Roxvhpi3lichZXvvwtu9VK55R9WnJa79Z0aVo7JqPZuXnb+OiH5fwrM5uPfliBu3NCzxZc1q8TXZvVicGrEREREak8oh25rgV8DrwAjC1hm/XAI8B0YCNwEPAPM9vo7k+Ede4CBgMXAbOBo4FxZnagu38bZayVVlpqCqf2bsVJe7fk47nLefWrbJ79bAGjP5lPm4Y1ObJbE/p22I29W9cv8SPHt25zspav59tfVjH5hxV8mrWS9ZvzaFIng4sP7cBZ+7ahzW414/zKRERERCqmqJJrd38ewMwalaLN18DXEUXzwxHwQ4D85Po84D53fzvcf9LMjgKuA86NJtaqIDXFOGL3phyxe1NWrNvMxJlL+XD2Ml6c+gvPfrYAgCZ1Mmi7W01aN6xJ/Rrp1M5IpVpqCpu2bGVT7laWrslh4eqNzF+xgQ25WwFoVrc6g3q2YED3phzSuRHVUrW4jIiIiMiuJG3OtZntDRwIjIgozgByClTdBBxcxDkuBi4GaNOmTeyDrIAa18ng3L5tObdvW3K2bGXm4rV8+8sqZi9ZR/aqjUz56VfW5uSxITcPd0hLNaqnpdK0bnVa1q9Bn7YN2atVPfZqVZ+OjWsl9EZJERERkYou4cm1mS0EGod93+HuoyIOTwSuNrPJwI/AkcApQKHLULj7U8BTAH369KlYzwhPgOppqfRu24DebRvsdMzd2brNNRotIiIiEkPFZlZmdo6ZrY/YDiljn4cAfYBLCRLp8yKOXUVwE+MsIBd4DHgW2FbGPqUAM1NiLSIiIhJjJcmuxgO9IrbMsnTo7vPdfbq7jwYeImJaiLuvcPeTCG6YbAvsTnAj5Lyy9CkiIkWzwCNmlmVm35vZPkXU621m08N6j1g4b8zMGprZJDP7Mfy688dlIiJVRLHJtbuvc/esiG1TjPvPKKTPHHdfRDB15FTgzRj2KSIiOxoIdA63i4Eni6j3JDAkou4xYflNwIfu3hn4MNwXEamSol3nuiHQBqgfFnUys9XAUndfGtYZC+Dug8P9YcB8gmkfAIcC1/P7SiGY2f4E61tPC7+OIEjAH4gmThERKZETgbHu7sAUM6tvZs3dfUl+BTNrDtR19ynh/ljgJODdsH2/sOpzwGTgxsSFLyJSfkR7Q+MJBHOh840Ov97B79M8Ci7fkQrcD7QD8oCfCEY3Im9orE6w1nUHgukg7wDnufvqKOMUEZHitQSyI/YXhmVLCtRZWEgdgKYRifhSoGlhnWiFJwFYcN9xlbq/ZPUp5Ue061yPAcYUU6dfgf2RwMhi2nwMdI8mJhERST53dzMrdPUmrfAkIlWBlosQEamCzOxyM5tmZtMIRqhbRxxuBSwq0GRRWF5YnWXhtJH86SPL4xO1iEj5p+RaRKQKcvfH3b2Xu/cC/gMMDlcN6QusiZxvHdZfAqw1s77hKiGD+f1m8/HA+eH356Ob0EWkClNyLSIi7xAseZpFcA/N0PwD4ch2vqHA02G9nwhuZgS4D+hvZj8CR4X7IiJVUtIefy4iIuVDuErI5UUc6xXxfSbQo5A6vxI8UVdEpMrTyLWIiIiISIwouRYRERERiREl1yIiIiIiMaLkWkREREQkRiy4j6XiM7MVwM9RNG0ErIxxOBWdrsnOdE0Kp+uyo7Jcj7bu3jiWwZRnZXjPLq1E/46qv4rfp/qr2P0lqs8i37MrTXIdLTPLdPc+yY6jPNE12ZmuSeF0XXak61H+JPpnov4qfp/qr2L3l6w+I2laiIiIiIhIjCi5FhERERGJESXX8FSyAyiHdE12pmtSOF2XHel6lD+J/pmov4rfp/qr2P0lq8/tqvycaxERERGRWNHItYiIiIhIjCi5FhERERGJESXXIiJSoZlZazObb2YNw/0G4X47M3vPzFab2VsJ6K+XmX1hZjPN7HszOyMBfR5mZt+Y2bSw30vj3F+7cL+umS00s8fi3Z+ZbQ1f3zQzG5+A/tqY2ftmNtvMZuW/5jj2+ceI1zfNzHLM7KQ49tfOzB4If19mm9kjZmZx7u9+M5sRblH/u4jm37qZtTezqWaWZWavmll62V5pCbh7pdqAU4CJwArAgX4laNMvrFtw271AvVOBWcDm8OvJyX69JbwmBowAFgObgMnAHsW0mVzENZlZRP2zwuNvJfv1lvCatAEmABsIFpp/BEgvpk0G8GhYfwMwHmhVoM7DQCaQAyxI9uuM93UB2hXxe+LA8JLWKc9bND9TYEwhr3VKxPGG4e/SnPDfZDbwJLBbsl9vRd2AG4Cnwu//Adwcfn8kMCjW702F9Qd0ATqHZS2AJUD9OPeZDmSEZbWBBUCLeF7TcP9h4CXgsQT8DNcn+HdmMtA/4prWjHefEccbAr/Fqs8ifmcOBD4DUsPtC0qQK5Whv+OASUA1oBbwFVA3Dj+3Qv+tA68BZ4bfjwIui8fv0w59xruDRG/AecDt4dfSJtfdgWYRW2pEnQOAPOBPQLfwax6wf7Jfcwle343AOoI/DnqEv2iLgTq7aNOwwLVoC6wFbi+kbgdgIfC/gr/U5XEL30ymh2+g+wD9w+vxaDHtngzr9Q/bTQamFfg9eRQYRnCn8oJkv9Z4X5ewTbMC22XANqB9SeuU5y2anylBcj2pwGtuGHG8B/AGcALQCTgMmAm8n+zXW1E3IA34Hrg6vJZpEcf6xfq9aVf9RdT5jjDZTkSfwG7AL8QuuS60P6A38ApwAbFNrovqL17J9U79EeQBnybj9zQ8fjHwYpxf4wHA10ANoCbB4EG3OPY3HPhzRJ1ngD/E4xoW/LdOMLi4EqgW7h8ATIzXz3d7v/HuIFkbwaMvS5tcN9pFnVeBSQXKPgBeTvZrLea1GcHoyZ8iymoQJNuXlOI85xD8MdG6QHkaMBU4nyChqAjJ9UCCxK51RNm5BCOThf41DdQDcoFzIspah+c5upD611PxkutSX5cizjOJYpLEktQpb1tpfqbR/FsAjg2vf9QjOlV9A44O38v7FyjvV9qfR1n6C4/tB8wGUuLdZ/he9D2wEbg8nv0RTCedDLQixsn1Ll5fHkECOAU4Kc6v7yTgLYI/fr8F/krEAEoCfm/+CxyfgGv6ILAaWAPcHedrOoBgpLwmQW42D7guHtew4L/1sL+siP3WwIxYvt7CNs253lGmmS0xsw/N7PACxw4A3i9QNpHg45XyrD3BqNn22N19E8Eoc2liHwK85+7ZBcrvJkg4nitroAl0ADC7wGuZSDDto3cRbXoT/CEReR2zCf7zLO+/AyUVzXXZgZl1IPhorsg1RktSp5I42MyWm9lcMxttZk2KqV+XYMrZxgTEVlkNJBhM6JHM/sysOfA88Ed33xbvPt092933IvgU5HwzaxrH/oYC77j7whj2sav+ANp68Cjrs4GRZtYxjv1VAw4h+GN6X4JPZi+IYX+F9Qls/73Zk+B9N279mVkngk/gWwEtgSPM7JB49efu7wPvAJ8DLxNMQ9kayz7KGyXXgSUEH1OfSjBn+wfgwwK/bM2AZQXaLQvLy7P8+KKO3cy6EHxsPbpA+QDgD8AlZYwx0Qr7Wa4k+Mde1DVpFh5fWaC8IvwOlFQ016Wgiwjud3izjHUquveAwQR/RFxHMIr5XzPLKKyymdUH7gRGu3tewqKsRMysF8FUpr7ANWGikvD+zKwu8DbBp4VTEtFnPndfDMwgSA7j1d8BwBVmtoBg9HOwmd0Xx/5w90Xh13kEo+Z7x7G/hcA0d58X/lv8D8E0uZgo5mf4B2Ccu2+Jc38nE9wDst7d1wPvEvxc49Uf7n63u/dy9/4En6jPjXUfRfgVqG9m1cL9VsCiaPsuqQqdXJvZOWa2PmKL6g3F3X9w91Hu/rW7f+HuQwn+cxwe24jjr+A1IRhtLashBH+AvB3RT2OCj77Pd/fVMehDKrjwzeuPwHNF/edQkjqVgbu/4u7j3X26u08gGGXpSnBjzw7MrDbBTaSLCG7UkVIKVzp4Erja3X8h+Cj/wUT3F65CMA4Y6+7/TlCfrcysRlinAXAwwQBRXPpz93PcvY27tyMY3R3r7jfFq79wNYiMsE4j4CCCBQXi0h/BzXb1w//jAI6IRX/F9JnvLIKR3ZjYRX+/AIeZWTUzSyMYPJsdr/7MLNXMdgvr7AXsxc4zAcr6mgrlwVyQj4DTwqLzScDAToVOrglWa+gVsWXG8NxTgc4R+0uBgh+1NQ3Ly5OC1yR/pDWq2MP/LM4Hni0worYH0JxghD/PzPIIRuqODfe7lu1lxFVhP8tGBDfeFXVNlobHGxUoL4+/A9GK5rpEGkQwwv10GetUOuGI4kJ2fE/JT6zfCXePd/ecRMdWSQwBfnH3SeH+E0A3C5ap+wT4F3CkBUvHHR2v/ghWRjgUuMB+X1atVwz621WfFwJTzew74GOCBHh6vPozs8NicO4S90eQiGWGr+8j4D53j0WyW1R/BxP80fChmU0nGGUdXfgpYtNn+HvajmA+8Mcx6qvI/gjez38iuIH9O+C7cBAgXv0dDHxiZrMIpgOeW4ZP6KL5t34jcK2ZZRHc9PtMlH2XWKV9/Hn4F+4K4HB3nxxF+3FAPXc/Itx/FWjg7gMi6rwP/OruZ8Um6tgL/8rLX/HhnrCsOrCcYBm0fxTT/g8Ed4V3Cj+Syy+vRTCfO9JdQAPgcmCuu+fG7IXEkJkNJBiFb5M/b9DMzgb+CTRx97WFtKlH8Pt0gbu/FJa1IhgBGOjuEwvUvx64IhzdqRCiuS4F2r9DsHxUv7LUKa/K8jMN348WAxe5+9iwrA7Bx7EGHOPu62IYroiIJEm14qtULBYsLN4GqB8WdTKz1cBSd18a1hkL4O6Dw/2rCdYGnUmwZui5BHcMnxpx6oeB/5nZTQRzsE4GDif4i6zccnc3s5HALWY2h2Ce063AeoI1SgEwsw+BL9395gKnuBj4MDKxDs+7gWBu33bhda7m7juUl0PvE/ysx5rZdQR/yf6VYL7rWgAz2w8YCwx29y/dfY2ZPQM8YGbLCeZxPURwh/4H+Se24EaR2gRr3KZHjFrNKq9/bEQo9XXJb2hmbQju3h5c1MlLUqc8Ku5namYtgQ8J1lodF45GjwBeJ5hO1Q64l+AP2nHhOesQXO+6BO81tcI/WAF+qwC/KyIiUoRKl1wTrBv7bMR+/sc5dxD8hwdB8h0pnSCJaEXwQIeZwHHunv9xLe7+uZmdSTA6+xeCj1TOcPepsX4BcfAAwfJ7jxOMLE8FBhQYKetI8CCL7SxY1eEI4MwExZkQ7r7VzI4j+DjpM4Kf+YvsOMe+JsEc2ZoRZVcTLAn1KsH1/JAgyYy86/lpgvlr+b4Nv7Yn+AOu3CrDdYHgo+k1BAllUUpSpzwq7meaRnBN6oXlWwnu+B9M8Ef+EoKPs/8Q8W+uN8HNOLDzjT2HE9y0JSIiFVClnRYiIiIiIpJoFf2GRhERERGRckPJtYiIiIhIjCi5FhERERGJESXXIiIiIiIxouRaRERERCRGlFyLiIiIiMSIkmsRERERkRhRci0iIiIiEiNKrkVEREREYkTJtYiIiIhIjCi5FhERERGJESXXIiIiIiIxouRaRERERCRGlFyLiIiIiMSIkmsRERERkRhRci0iIiIiEiNKrkVEREREYkTJtYiIiIhIjCi5FhERERGJESXXIiIiIiIxouRaRERERCRGlFyLiIiIiMSIkmsRERERkRipluwAYqVRo0berl27ZIchIhKVr7/+eqW7N052HImi92wRqch29Z5daZLrdu3akZmZmewwRESiYmY/JzuGRNJ7tohUZLt6z9a0EBERERGRGFFyLSIiIiISI0quRURERERiRMm1iIiIiEiMKLkWEREREYkRJdciIoKZ3W1m2Wa2vph6N5tZlpn9YGZHR5QfE5ZlmdlN8Y9YRKR8UnItIiIAE4D9dlXBzLoDZwJ7AMcAT5hZqpmlAo8DA4HuwFlhXRGRKqfSrHMtIiLRc/cpAGa2q2onAq+4+2Zgvpll8XtCnuXu88JzvBLWnRW/iBOn3U1vx+W8C+47Li7nFZHk0si1iIiUVEsgO2J/YVhWVPkOzOxiM8s0s8wVK1bENVARkWRRcl2ECy64ADOjX79+Ox0bMWIEZrbTVqtWLTp37sz555/Pl19+GbfYcnJyeP3117nooovYa6+9qF27NhkZGbRp04YzzjiDyZMnF3uOJUuWcMMNN2xvn56eTosWLTjhhBMYP358mWP84IMPGDRoEE2aNKF69ep07NiRq666imXLlhXZZtOmTQwfPpzWrVuTkZFBly5deOCBB9i2bVuRbSZNmoSZceWVV5Y5ZhGJL3d/yt37uHufxo2rzJPeRaSK0bSQMkhJSSHyP4hff/2VrKwssrKyeOGFF/jb3/7G1VdfHfN+Bw0axAcffLB9PyMjg7S0NLKzs8nOzua1117jqquuYuTIkYW2nzJlCsceeyyrVq0CIDU1lZo1a7JkyRImTJjAhAkTGDx4MGPGjCnuI+JC3X333dx6661AcI1q167NvHnzeOSRR3j55Zf573//S48ePXZo4+6cfPLJTJw4EYBatWrx448/cuONN7JgwQKeeOKJnfrZvHkzl19+Oc2aNePOO+8sdZwiUmqLgNYR+63CMnZRLiJSpWjkugxat27N0qVLt285OTl89tln9OrVi23btnHdddcxY8aMmPe7ZcsWOnfuzAMPPMDs2bPJyclh/fr1ZGVlcfrppwPw8MMPF5qQbtmyhTPOOINVq1bRoUMHJk2aRE5ODmvXrmXJkiUMHToUgLFjx/L888+XOrZ33nlne2J93XXXsXr1atasWcOMGTPo1asXK1as4MQTT2Tz5s07tJs0aRITJ06kbdu2zJo1i/Xr1/PJJ59Qp04dRo0axdy5c3fq6/777+fHH3/kwQcfpF69eqWOVURKbTxwppllmFl7oDPwJfAV0NnM2ptZOsFNj2X/CExEpAJSch1DqampHHjggfznP/8hLS2Nbdu28cILL8S8n3vuuYfZs2czfPhwdt999+3lHTt25NVXX+WII44A4MEHH9yp7aeffsovv/wCwJgxYzjqqKOoVi34AKNZs2Y8/vjjHHbYYQC88cYbpY7tlltuAeDkk0/mwQcfpE6dOgDsscceTJgwYfso9lNPPbVDuw8//BCAG264gW7dugFw8MEHM2TIENydjz76aIf68+bN495776Vfv36cc845pY5TRHZkZg+Y2UKgppktNLMRYfkJZvYXAHefCbxGcKPie8Dl7r7V3fOAK4CJwGzgtbCuiEiVo+Q6Dtq2bUuXLl0AmDUr9jfLH3jggaSmphZ6zMwYPHgwAPPnz+e3337b4XjknOe999670HP07t0bgA0bNpQqrpkzZ/Ldd98BMHz48J2Ot2rVirPOOguAF198cYdjv/76KwAdOnTYobxTp04ArFy5cofyYcOGsXXrVh5//PFSxSgihXP3G9y9lbunhF9HhOXj3f22iHp3u3tHd+/q7u9GlL/j7l3CY3cn4SWIiJQLSq7jxN0B2Lp1a6HHI2+KjLXddttt+/cF+2/Xrt3277/99ttC23/99dcA7LPPPqXqN390uV69euy///6F1jn66OCZE19++SXr1//+rIr8mOfNm7dD/Z9++mmH4xCMqL/zzjtcc801dO+upXRFRESk/FByHQcLFizgxx9/BHYeiU2Ejz/+GICmTZvSqFGjHY7tt99+9OzZEwhWRPnggw/Iy8sDYOnSpVxxxRV8/PHHtGjRguusymY2AAAgAElEQVSvv75U/eaP0nfr1o2UlMJ/tfKTYXdnzpw528vzp7I88MAD28s///xzRo8ejZltP75hwwauvvpqWrduzW233YaIiIhIeaLVQmJo69atfPnllwwdOpQtW7YAcO655yY0hkWLFjFq1Cjg9+UEI6WkpPDGG29wwgknMHPmTPr37799tZB169ZRo0YNzjvvPO69915Ku1TWkiVLAGjRokWRdSKP5dcHGDBgAEcddRQffPAB3bp1o3bt2ttHti+55JLt02zuuOMOsrOzef3116lVq1ap4hMRERGJN41cl0F2djbNmjXbvtWoUYMDDzyQadOmAcHUj6KmR4wYMQJ33z59JBby8vI455xzWL9+PW3atOHmm28utF6HDh344IMPGDBgABD8UbBu3TogWE1k/fr125fpK438Odo1atQosk7NmjW3fx85LcTMePPNN7nmmmto2bIlmzdvpmPHjtxzzz3bVz2ZOXMmI0eO5JhjjuGUU04B4NFHH6VLly5kZGTQuXNnHn744VLHLSIiIhIrSq7LYNu2bSxbtmz7lj9aXb16dd5++21uv/32hMYzbNgwPv74Y9LT03nppZeKXJ5uwoQJdO7cmczMTEaNGsWCBQtYu3YtU6dO5ZhjjmHcuHEcdNBBfPXVVwmNv2bNmjz00EMsXLiQ3NxcsrKyuPnmm7dPMRk6dCipqak8+uijANx5551ceeWVbNmyhbPOOou8vDyuvvpq7rrrroTGLSIiIpJPyXUZtG3bdvvoc25uLnPmzOGyyy4jJyeHSy65hAULFiQslltuuYVRo0aRmprKiy++yEEHHVRovfnz53PaaaexYcMGxo0bxyWXXELbtm2pU6cO++23HxMmTODII49k7dq1DBs2rFQx5E/T2LRpU5F1Nm7cuP372rVrl/jcY8eO5X//+x833ngjnTp1YuXKldx11120bNmSb775hjFjxvDVV1/RtGlT7rrrrp1WFxERERFJBCXXMZKWlkbXrl154oknGDJkCAsXLuSss87a5aO7Y+Xuu+/m3nvvxcwYPXo0p512WpF1n3zySXJzc+nduzeHHnpooXXynyo5depUli5dWuI48udTL168uMg6kceaN29eovOuXr2a4cOH06FDB2666SYgeOhMbm4uZ599Ng0aNACgUaNGnHPOOWzevHmHJ1iKiIiIJIqS6zi4//77qVevHlOmTInqKYel8fe//337ExEffvhh/vjHP+6y/uzZswFo3759kXUiVzgpzeh7/kogs2fPLvKPivwVRcxs+8NiinPLLbewfPlyHn30UapXrw7Azz//DOz8OvLXxc4/LiIiIpJISq7joEGDBlx++eVAcONi/lJ3sfbkk09y7bXXAnDfffeVaBpH/vzl/Kc0FiYyMc1/wmJJHH744QCsWbOmyPna77//PgD7779/iVb7yMzM5B//+Acnn3wyxx577E7Hc3Jydtjf1ZQUERERkXhTch0nw4YNIyMjgwULFsTlEejPPffc9gT+tttu48YbbyxRu/w1rr/++usiHyIzevRoIHgYTOTj1YvTvXv37ef/61//utPxxYsX8/LLLwOU6JHl27Zt47LLLqN69eqMHDlyh2Nt27bd/joi5Sf1kQ/LEREREUkUJddx0qxZM8477zwA7r333p2mSZTlCY2vv/46F154Ie7O8OHDueOOO0rc9v/+7//IyMggLy+PE088kTfffHP76G92djYXXXQR48aNA35fnaM0cd9zzz3bY7zhhhu2L/E3a9YsBg0axLp16+jQoQNDhgwpNtZRo0aRmZnJn//8Z9q0abPDsaOOOor09HT+/e9/89577wHw7rvv8sYbb5CRkcGRRx5Z4msiIiIiEitKruPo+uuvJyUlhblz5/Lqq6/G7LzDhw/f/ljzsWPH7rDWdsHt888/36Ftu3bteO6558jIyCA7O5uTTjqJWrVqUbt2bdq0acMzzzwDwPHHH8+IESNKHduxxx7LnXfeCQSj1w0aNKBevXrssccefPPNNzRq1Ig333yTjIyMXZ5n+fLl/OlPf6Jbt25cd911Ox1v3LgxN998M5s3b2bgwIHUrFmTY489ltzcXG699dadnkwpIiIikghKruOoa9eunHDCCUAwohurB8ZEjoJHrrNd2Jabm7tT+zPOOIPvv/+eyy+/nO7du1O9enU2b95M06ZNGThwIC+99BLjx48nPT09qvhuvfVWJk2axHHHHUeDBg3YvHkzHTp04Morr2TGjBn06NGj2HNcf/31rF69mscee4y0tLRC64wYMYKHHnqIjh07kpeXR8eOHRk5cuT2GzxFREREEs1i+YTAZOrTp49nZmYmOwwRkaiY2dfu3ifZcSRKRXrPbnfT23E574L7jovLeUUk/nb1nq2RaxERERGRGFFyLSIiIiISI0quRURERERiRMm1iIiIiEiMKLkWEREREYmRUifXZnaBmXkR2767aHezmX1lZmvNbIWZTTCzHgXq1DazR81soZltMrMfzOyaaF6YiIiIiEiiRTNy/SrQvMD2AjAP2NW6Sv2AJ4ADgSOAPOADM2sYUech4DjgPKAbcDdwn5mdF0WcIiIiIiIJVa20Ddx9E7Apf9/MagKDgAd8F4tmu/vRkfthwrwGOAiYEBYfCDzv7h+F+wvM7EJgf+D50sYqIiIiIpJIsZhz/QegFvDPUrarE/a/KqLsU2CQmbUGMLMDgV7AezGIcyd3TJjJ8H99F49Ti4iIiEgVFIvk+mLgLXdfWsp2DwPTgC8iyq4EvgN+MbMtwMfAje7+VmEnMLOLzSzTzDJXrFhR6sC3bN3G29OXkLNla6nbioiIiIgUVKbk2sz2AA4ARpey3UPAwcCp7h6Z2Q4jmBpyAtAbuAZ40MyOKew87v6Uu/dx9z6NGzcudfz9uzdjY+5WPv9pZanbioiIiIgUVNaR64uBbEoxbcPM/g6cBRzh7vMiymsA9wI3uPsEd//e3R8DXgGuL2OcherboSG1M6oxadayeJxeRERERKqYqJNrM6tOsKrHP919WwnbPMzvifWcAofTwq3gHI2tZYlzVzKqpdKva2MmzVrOtm1F3ospIiIiIlIiZUlaTwPqUcSNjGY2x8yuiNh/HPgjcDawysyahVttAHdfSzDH+j4z62dm7c3sAmAwMK4Mce5S/+5NWbl+M99mr45XFyIiIiJSRZQluR4CTHT3X4o43hVoFLE/lGCFkA+BJRFb5JSPM4GvgBeBWcBNwJ+Bx8oQ5y7169qEaimmqSEiIiIiUmalXuc6n7sfVsxx29V+EW2WEoxuJ0y9Gmn07bAb789ayk0Dd09k1yIi5YaZ9QbGADWAd4CrCj67wMzqETw0rA3B/x8Puvuz4bHzgVvDqne5+3MJCl1EpFyJy1zmimbAHk2Zt2IDWcvXJzsUEZFkeZLgE8nO4VbYKk2XA7PcvSfBU3f/Zmbp4ZN2byd44Nd+wO1m1iAhUYuIlDNKroGjujUF0NQQEamSzKw5UNfdp4Sj1WOBkwqp6kAdMzOgNvAbkAccDUxy99/cfRUwicKTcxGRSk/JNdCifg16tKzLpFmlfQ6OiEil0BJYGLG/MCwr6DGgG7AYmE4wdWRbWDe7uPZlffCXiEhFoOQ6NKB7M77NXs3ydTnJDkVEpLw6muDJui2AXsBjZla3pI3L+uAvEZGKQMl1qH/3prjDh7OXJzsUEZFEWwS0ithvFZYV9EfgDQ9kAfOB3cO6rUvQXkSk0lNyHdq9WR1aNaihedciUuW4+xJgrZn1DedTDwbeLKTqL8CRAGbWlGDJ1XnARGCAmTUIb2QcEJaJiFQ5US/FV9mYGf27N+XFqb+wYXMetTJ0aUSkShnK70vxvRtumNmlAO4+CrgTGGNm0wEDbnT3lWG9OwmeUwDwF3f/LaHRi4iUE8ogIwzo3oxnP1vA5B9WcNxezZMdjohIwrh7JtCjkPJREd8vJhiVLqz9Pyniib0iIlWJpoVE2K99Q3arlc67M5YkOxQRERERqYCUXEdITTGO7tGM/85ZTs6WrckOR0REREQqGCXXBQzs0YyNuVv5eK7WYBURERGR0lFyXUDfDrtRv2Ya783QA2VEREREpHSUXBeQlprCgO5N+WDWMjbnaWqIiIiIiJSckutCDNyzOes25/FZ1spkhyIiIiIiFYiS60Ic1LERdapX453pmhoiIiIiIiWn5LoQ6dVS6N+tKZNmLWPL1m3JDkdEREREKggl10UYuGdz1mzawhc//ZrsUERERESkglByXYRDOjeiVnqqHigjIiIiIiWm5LoI1dNSOaJbUybOXEaepoaIiIiISAkoud6FY3s047cNuXw5/7dkhyIiIiIiFYCS613o17UJNdNTmfC9poaIiIiISPGiTq7NzAvZLo2ijZvZ4xF1zMxGmNliM9tkZpPNbI9o4yyLGump9O/elHdnLNGqISIiIiJSrLKOXA8BmkdszxVTv3mBbVBY/lpEnRuA64BhwL7AcmCSmdUpY6xRGbRXC1Zv3MKnP+qBMiIiIiKya2VNrle7+9KIbdOuKheouxQ4EZjr7h9DMGoNXA3c5+6vu/sM4HygDnB2GWONyqFdGlO3ejUmfLc4Gd2LiIiISAVS1uT6YTNbaWZfmdmlZlbi85lZbeBMYHREcXugGfB+fkGYsP8POLCQc1xsZplmlrlixYqoX8SupFdLYWCP5kycuZScLVvj0oeIiIiIVA5lSa5vA84AjgJeAf4G3FKK9mcD6ew4laRZ+HVZgbrLIo5t5+5PuXsfd+/TuHHjUnRdOif0asGG3K18NGd53PoQERERkYqvWrQN3f3OiN1pZpYK/Am4q4SnGAK86e7xGXKOob4ddqNR7QzGf7eYgXs2T3Y4IiIiIlJOxXIpvqlAXTNrWlxFM+sF9GHHKSEAS8OvBc/RNOJYwqWmGMfv1Zz/zlnOupwtyQpDRERERMq5WCbXvYAcYHUJ6l4MzAc+KFA+nyCJ7p9fYGbVgUOAz2MTZnQG9WzB5rxtTJpVcMaKiIiIiEggquTazAaZ2RAz62FmHc3sIuAvwFPuvjms09LM5pjZyQXa1gTOAZ5xd488Fu6PBG40s1PMrAcwBlgPvBRNrLGyT5v6tKxfQ6uGiIiIiEiRop1zvQUYCjxEkKDPI7jB8fGIOmlAV6BegbZnALWAZ4s49wNAjfBcDQimmwxw93VRxhoTZsagni14+pN5rNqQS4Na6ckMR0RERETKoahGrt39PXff293ruHstd9/T3R9297yIOgvc3dx9TIG2z7p7NXcvdAjYAyPcvbm7V3f3w8L1rpNuUM/m5G1z3pmhx6GLiIiIyM5iOee60uvevC6dm9TmP98uSnYoIiIiIlIOKbkuBTPjlH1a8dWCVfz864ZkhyMiIiIi5YyS61I6ae8WmME4jV6LiIiISAFKrkupeb0aHNSxEW98s4gCi52IiIiISBWn5DoKp+zTkl9+28jXP69KdigiIiIiUo4ouY7C0Xs0o2Z6Kq9/o6khIlI5mFlvM5tuZllm9oiZWRH1+pnZNDObaWYfR5QfY2Y/hO1vSlzkIiLli5LrKNTKqMYxPZrx1veLydmyNdnhiIjEwpPAEKBzuB1TsIKZ1QeeAE5w9z2A08PyVIJnEwwEugNnmVn3BMUtIlKuKLmO0qn7tGJdTh4fzNbj0EWkYjOz5kBdd58SPil3LHBSIVXPBt5w918A3H15WL4fkOXu89w9F3gFODEBoYuIlDtKrqPUt8NuNK9XnTc0NUREKr6WwMKI/YVhWUFdgAZmNtnMvjazwRHts0vQXkSk0lNyHaXUFOOkvVvy8dwVrFi3OdnhiIgkQjWgN3AccDTwZzPrUtLGZnaxmWWaWeaKFSviFaOISFIpuS6DU/ZuydZtzvjvCn2Su4hIRbEIaBWx3yosK2ghMNHdN7j7SuB/QM+wbuvi2rv7U+7ex937NG7cOGbBi4iUJ0quy6Bz0zrs1aoe/8rM1prXIlJhufsSYK2Z9Q1XCRkMvFlI1TeBg82smpnVBPYHZgNfAZ3NrL2ZpQNnAuMTFL6ISLmi5LqMzti3NXOWruP7hWuSHYqISFkMBZ4GsoCfgHcBzOxSM7sUwN1nA+8B3wNfAk+7+wx3zwOuACYSJNuvufvMxL8EEZHkq5bsACq6E3q24K63ZvPKV9n0bF0/2eGIiETF3TOBHoWUjyqw/1fgr4XUewd4J24BiohUEBq5LqM61dM4bq/mjJ+2iA2b85IdjoiIiIgkkZLrGDhz39ZsyN3K29OXJDsUEREREUkiJdcx0LttAzo2rsWrX2UXX1lEREREKi0l1zFgZpy5bxu+/nkVPy5bl+xwRERERCRJlFzHyMn7tCQt1TR6LSIiIlKFKbmOkUa1M+jfvSlvfLuIzXlbkx2OiIiIiCRBmZNrM2tkZovMzM2sUTF1TzGziWa2Iqzfr8Dxhmb2qJnNMbNNZpZtZk+a2W5ljTMRzty3Db9tyGXSrGXJDkVEREREkiAWI9fPAtNKWLcW8DlwbRHHWwAtgRuAPYFzgUOBl8sYY0Ic3KkRLevX4MUpvyQ7FBERERFJgjI9RMbMrgJqAncDxxZX392fD9sVOsLt7jOAUyKKssxsOPCWmdV197VliTfeUlKMc/q24YH3fuDHZevo3LROskMSERERkQSKeuTazPYGbgQGA9tiFtHO6gKbgY1x7CNmzujTmvTUFF6Y8nOyQxERERGRBIsquTazWsArwDB3XxTbkHbopz5wJzDa3Xd6/KGZXWxmmWaWuWLFiniFUSq71c7guL2a8/o3i1ivJzaKiIiIVCnRjlw/Anzq7q/HMphIZlYbmAAsIpiDvRN3f8rd+7h7n8aNG8crlFI774C2rN+cx7hv4/Z3h4iIiIiUQ9Em10cCF5hZnpnlAR+G5UvN7O6yBhUm1u+Eu8e7e05Zz5lIe7euT4+WdXn+iwW4e7LDEREREZEEiTa5HgD0BHqF20VheT+CUe2omVkd4D0gFTjW3deX5XzJYGYM7tuOucvWM3X+b8kOR0REREQSJKrk2t3nuvuM/A2YHx6a4+7LAMysZbhe9cn57cJ1rHsBPcKiTmbWy8yahcfrAO8DDYALgFpm1izc0qN6hUkyqGcL6tVI4/kvdGOjiIiISFURzyc0pgFdgXoRZScA3wIfhfujw/1Lw/3eQF+gOzAXWBKxHRjHWGOuRnoqp/duxcSZS1m2tkLNahERERGRKMUkuXb3ye5u7r4yomxBWDYmomxMWFZwG1HgPIVtk2MRayKd27cteducF7Usn4iIiEiVEM+R6yqvXaNaHNWtCS9M/YWcLVuTHY6IiIiIxJmS6zi78OAO/LYhV8vyiYiIiFQBSq7jrG+HhuzRoi7PfDqfbdu0LJ+IiIhIZabkOs7MjIsOaU/W8vV8/GP5eIqkiIiIiMSHkusEOG7PFjStm8HTn8xLdigiIiIiEkdKrhMgvVoKFxzYns+yfmXW4rXJDkdERERE4kTJdYKcvV8baqSl8syn84uvLCIiIiIVkpLrBKlXM40/9GnF+O8WsWTNpmSHIyIiIiJxoOQ6gS46pAPbHJ7+RKPXIiIiIpWRkusEat2wJif2bMFLU3/htw25yQ5HRERERGJMyXWCXdavI5u2bGXMZxq9FhEREalslFwnWOemdTh6j6aM+XwB63K2JDscEREREYkhJddJMLRfJ9bm5PHi1F+SHYqIiIiIxJCS6yTo2bo+h3RuxNOfzCdny9ZkhyMigpn1NrPpZpZlZo+Yme2i7r5mlmdmp0WUnW9mP4bb+YmJWkSk/FFynSSXH96Jles381pmdrJDEREBeBIYAnQOt2MKq2RmqcD9wPsRZQ2B24H9gf2A282sQbwDFhEpj5RcJ8n+7RvSp20DnvjoJ41ei0hSmVlzoK67T3F3B8YCJxVRfRjwOrA8ouxoYJK7/+buq4BJFJGci4hUdkquk8TMuLZ/F5auzeHlLzX3WkSSqiWwMGJ/YVi2AzNrCZxMMMpdsH3kx3BFtb/YzDLNLHPFihVlDlpEpDxScp1EB3ZqRN8ODXn8o5/YlKvRaxEp90YCN7r7tmgau/tT7t7H3fs0btw4xqGJiJQPSq6T7LoBXVm5fjNjv1iQ7FBEpOpaBLSK2G8VlhXUB3jFzBYApwFPmNlJYd3WJWgvIlLpKblOsn3bNeTQLo0Z9fFPrN+cl+xwRKQKcvclwFoz6xuuEjIYeLOQeu3dvZ27twP+DQx19/8AE4EBZtYgvJFxQFgmIlLlKLkuB67t34VVG7foqY0ikkxDgaeBLOAn4F0AM7vUzC7dVUN3/w24E/gq3P4SlomIVDlRJddm1tPMXjazbDPbZGY/mNkNZrbL85nZGDPzAtuUAnUmF1LnlWjirCh6ta7PUd2a8NT/5rFmk57aKCKJ5+6Z7t7D3Tu6+xXhqiG4+yh3H1VI/Qvc/d8R+/90907h9mwiYxcRKU+iHbnuDawAzgP2IFjf9M/ATSVo+wHQPGI7tpA6zxaoc0mUcVYY1/bvyrrNeTwxOSvZoYiIiIhIlKpF08jd/1mgaJ6Z7QOcCtxTTPPN7r60mDobS1CnUuneoi4n792SZz9bwHl929KqQc1khyQiIiIipRTLOdd1gVUlqHewmS03s7lmNtrMmhRS50wzW2lmM83sQTOrU9iJKtuaqdcP6IoBf3t/brJDEREREZEoxCS5DketL2DnBwsU9B7BXehHAtcRPCb3v2aWEVHnJeAc4HCCG2ROJXga2E4q25qpLerX4P8Obs+4bxcxY9GaZIcjIiIiIqVU5uTazLoCbwMj3b3QJDifu7/i7uPdfbq7TwAGAl2B4yLqPOXuE8M6rwBnAP3DBL7Su6xfRxrWSufut2cT3k8kIiIiIhVEmZJrM9sdmAy84u4luZlxB+6+mOAxuZ13US0T2FpMnUqjbvU0rjqyM1/M+5WPflie7HBEREREpBSiTq7NrDtBYv0vd78mynM0AloCS3ZRbU8gtZg6lcrZ+7ehQ6Na3PnWbDbn6bHoIiIiIhVFtOtc7wF8RJBc32NmzfK3iDotzWyOmZ0c7tcOb048wMzamVk/YAKwHBgX1uloZreZWZ+wzrHAK8C3wGdleJ0VSlpqCrefsAfzV27gmU/1YBkRERGRiiLakevTgSYE86GXFNjypRHMp64X7m8lGIV+E5gLPAf8ABzg7uvCOrkENztODI89ArwPHOXuVWoI97AujRnQvSmPfpjFkjWbkh2OiIiIiJRAVMm1u49wdytsi6izICwbE+5vcvej3b2Ju6e7e9vwCV/ZEW2y3f0wd9/N3TPCJ31dVVUfo/vn47uzzZ273p6d7FBEREREpARiuc61xFjrhjUZ2q8Tb3+/hM+zViY7HBEREREphpLrcu6SwzrQumENbhs/Uzc3ioiIiJRzSq7LueppqfzlhB5kLV/PEx/9lOxwRERERGQXlFxXAIfv3oQTe7XgiclZzF22rvgGIiIiIpIUSq4riNuO707tjGrc+Pr3bN2mJzeKiIiIlEdKriuI3WpncNug7nz7y2rGfrEg2eGIiIiISCGUXFcgJ/VqSb+ujfnrxB/I/m1jssMRERERkQKUXFcgZsbdJ+9JqhnXvjZN00NEREREyhkl1xVMy/o1GHHCHny1YBVP/W9essMRERERkQhKriugU/ZpybF7NuOhST8wY9GaZIcjIiIiIiEl1xWQmXH3SXvSoGY617w6jZwteriMiIiISHmg5LqCalArnb+e3pMfl6/n7rdnJzscEREREUHJdYV2WJfGXHRwe56f8jMTvluc7HBEREREqjwl1xXcjQN3p3fbBtz0+vf8tGJ9ssMRERERqdKUXFdwaakpPHb23mSkpTL0hW/YlKv51yIiIiLJouS6EmherwYjz+jF3OXr+NO46bhr/WsRERGRZFByXUkc2qUxVx/ZhTe+XcToT7T+tYiIiEgyVEt2ABI7w47oxNxl67j33Tl0aFSbo7o3TXZIIiIiIlWKRq4rkZQU48HTe9KjRT2ueuVb5ixdm+yQRERERKoUJdeVTI30VEYP7kPt6tW4cEwmy9fmJDskEakAzKy3mU03sywze8TMrJA655jZ92G9z82sZ8SxY8zsh7D9TYmNXkSk/Ig6uTazNmY2wcw2mNnK8M04vZg2Hc1snJmtMLO1ZvaamTWNON7PzLyI7fRoY61qmtWrztOD92XVxlwG//NL1mzakuyQRKT8exIYAnQOt2MKqTMfOMzd9wTuBJ4CMLNU4HFgINAdOMvMuiciaBGR8iaq5Dp8I30bqAMcApwFnAb8bRdtagHvAwYcARwEpAMTzCw/js+B5gW2e4H1wLvRxFpV7dmqHv84rzc/rVjPRc99pSX6RKRIZtYcqOvuUzxYbmgscFLBeu7+ubuvCnenAK3C7/cDstx9nrvnAq8AJyYgdBGRcifakesBwB7Aee7+jbtPAm4AhphZ3SLaHAS0B/7o7tPdfTpwPtCHINnG3XPdfWnkRpC0v+zuekJKKR3SuTEjz9ibzJ9XccVL37Bl67ZkhyQi5VNLYGHE/sKwbFcu5PdBj5ZAdnHtzexiM8s0s8wVK1aUIVwRkfIr2uT6AGC2u0e+mU4EMoDeRbTJAByInAScA2wDDi6sgZn1I/h48qko46zyjturOXee2IMP5yzn6lenKcEWkTIzs8P5f/buPD6q6vzj+OfJSsK+CgIBZFVQUSOKrXVFabVutVqtitaKFbTaurYu9adVEfelqKit4ob+fmpdcSmKWhEVFGVXBExA9j0Ess3z++Pe4BCSEIaZTJbv+/W6r8k999x7ztzMTJ6cee65QXB99c7s5+5j3T3X3XPbt2+fmM6JiCRZrFPxdQSWVyhbBZSF2yozhSC94w4zK/9AHgWkEqR/VGY4MN3dp1a20cyGh3XIycmpcecbm7MO7sbm4jJueXMOkYhz/xn7kZ6qa1lFZKsl/JjiQfjzksoqmtk+wGPAzzQjab0AACAASURBVN19ddT+XWuyv4hIQ1drEZa7rwR+TXDBy0ZgPdAK+IJg9HobZtYWOAV4tJpjahSkhi742R5cf/xeTJi5jJHPfEFxqUawRSTg7kuBDWZ2cDhLyDnAKxXrmVkO8BJBSuA3UZs+B3qbWY/wwvbfAK/WQtdFROqcWIPrZUDFO5S0IxiFXlbVTu7+jrv3BDoA7dz9bIK8vMpuKXgOwUj4MzH2USo4/6c9+J8T+vPO7OVc+NRUCotLk90lEak7RhCMSM8HviPMpzazP5jZH8I6NwBtgTFmNt3MpgK4eylwMUF64BzgBXefVcv9FxGpE2JNC/kEuM7Murh7+UUwQ4AiYNqOdnb3VQBmdiRBoF3ZCMfvgf919/Ux9lEqMeyQ7mSkpXDtyzM449FPeXxYLu2aZSa7WyKSZGH63YBKyh+O+vn3BJ/Nle3/JvBmwjooIlJPxDpy/Q4wCxhnZvuZ2dHAHcCj7r4BwMwGmdlcMxtUvpOZnWdmg8P5rs8C/he4x93nRR/czH5KMFdqlSkhErszBuXw8FkHMG/ZBn710GQWrtqU7C6JiIiINAgxBdfuXgYcBxQCHwPPAy8CV0RVywb6ho/l+gIvE3xteANwS4V9yl1AMBvJx7H0T3bsmP4defaCg9m4pZRTxnzM5O9WJbtLIiIiIvVezBc0unueux/v7tnu3tbd/+juRVHbJ7m7ufukqLJr3L2ju2e4ex93vzu8YUHFYw9zd93dK8H2z2nNixcdQpumGZz9+Gc89tECKvl1iIiIiEgNaT62Rq5Hu6b8e+RPOHrPDvz9jTlcOn66LnQUERERiZGCa6F5k3Qe+u0BXHlsX177+geOv/+/fL14XbK7JSIiIlLvKLgWAFJSjJFH9OKZ3x/E5pIyThkzmX+8P5+yiNJERERERGpKwbVs45Ce7Xjr0p8xdEBH7nh7Hr9+eDLzlm1MdrdERERE6gUF17KdltnpPHDGftx7+kAWrtrEcfd/xOi35rK5uCzZXRMRERGp0xRcS6XMjJP268zEyw/n5P06M2bSdxxz7we88fVSzSgiIiIiUgUF11KtNk0zuOPX+/LcBQfTNCONkc9+wcljJvP5ojXJ7pqIiIhInaPgWmpkcM+2vPHHQxl96j4sXb+ZXz/8Cec/8Tlf5K1NdtdERERE6gwF11JjqSnGabldmXTFEVx5bF+m5a3llDGTOWPsFP777Sqli4iIiEijl5bsDkj9k5WRysgjenHuId157rM8Hv1oAWc9/il9dmvGWQd34+T9OtO8SXqyuykiIiJS6zRyLTFrmpnG7w/dgw+vOoLRp+5Dk/RUbnhlFgfdOpG/vDSDqYvWENE82SIiItKIaORadllmWiqn5XbltNyufJW/jqenfM/LXy7muc/y6Nwqi+P37cQJ++7OXp1aYGbJ7q6IiIhIwii4lrjat2sr9u3air+d0J93Zy/j1ek/8NhHC3nkgwV0bpXFkf06cGS/Dgzu2ZYm6anJ7q6IiIhIXCm4loRolpnGyft14eT9urBmUzFvz1rGxDkr+L9pi3lqyvc0SU/hoB5tGdSjDQfv0Ya9O7ciI01ZSiIiIlK/KbiWhGvTNIMzBuVwxqActpSU8enCNbw3ZzmTv1vNHW/PA6BJegr7dW3Nfjmt2LtzSwZ0bkmX1llKIxEREZF6RcG11Kom6akc1qc9h/VpD8DqgiI+X7SWzxau4bNFqxn74QJKw4sgW2enM6BzS/rv3pLeHZrRe7dm9GzfjKaZetmKiIhI3aQoRZKqbbNMhg7oyNABHQHYUlLGvGUbmbFkPTMWr2fGkvU8/t8FlJT9OOtI51ZZ9OrQjF4dmtG9XVNy2mST0yabzq2ylFoiIiIiSaXgWuqUJumpWy+KLFdSFuH71YXMX7GR+SsK+HZFAfNXFPDpwtVsKYlsrWcGu7fMomubrK0Bd6eWWXRq2YSOLZvQqWUWWRm6iFJEREQSR8G11HnpqSlbR6qjRSLOyoIi8tYUkre6kLw1heSvKeT7NYW8P28lKzcWbXesllnp2wTb5T+3b55J+2aZtG+eSdumGaSlagRcREREdp6Ca6m3UlKM3Vo0YbcWTTiwe5vttm8pKWPZ+i0sXb+FZRs2s3T9Fpau+3F95pL1rCoo3m4/M2iTnUG7MNhu1ywjfMzc+ti2WQats4OlSXqKLrwUERERQMG1NGBN0lPp3q4p3ds1rbJOUWkZKzYUsWJjESs3FrGqIHhcWVDEqvDx+7xNrNxYtE0KSrSMtBRaZ6fTOjuDVlGPrbIzaJ2dTqusDJo1SaNpZhrNwqVpZirNM9NpmpnaYEbJ3Z3SiFNa5pREIpSWOaVlEUoi4WOZUxqWl5RFKIv4dmWlkfAxLC8pc9yd1JQU0lKMtFQjNcVIS0khLdVISwnWM1JTaJKRSnZGKtnpaTTJSCE7I42s9FRSU/SPj4iI1J6YgmsLhun+BgwHWgOfAiPdfdYO9msB/B04FWgL5AN/dfcXouqMAK4EOgGzgMvc/aNY+imyI5lpqXRtk03XNtnV1nN3NhWXBYH3xiLWbCpibWEJawuLWVdYwtpNxazbXMK6wmK+XVHAurC8tAa3f89MSwkD7iAYzEhLCZbU4DGzfL3859QUUlIMw0ixYKTdzDDCRwMDUsyIuFMWCZbSqMfI1vXINuWlFQLc8sC4qoB5m/I6eqv7zLQUsjNSaZGVTqusdFpmZ9AqKz34BygrPSjPzqBdsww6NG9ChxaZtMnOIEVBuYiIxCDWkeurgMuBc4F5wA3Au2bW1903VraDmaUD7wJrgNOAxUAXoCiqzunAfcAI4L/h4wQz28vd82Lsq8guM7Oto849qhkJj+buFBSVsq6whIKiUjYVlYaPZRQUlVBQVMambcpL2VISobgsQnFphKLSMgoLSykqLV8PthWVlOEOHrYRcXA8KIv6OeJOipWP9IaPqSmkWPR68Jga1ktPDUaE01OCYD47NYX0sF7a1p9TSE/9cfQ4PbV8VLni9srrBu1sv39522kp4T7h8Sz8JyE6+C8LR8hLw38OSkojbC4pY3NxGYXFZRSWlLG5uJTC4jI2l5RRWFTGhi0lrCssYf3mEvLXFLKusJj1m0uo7H+CtBSjffNMOjTPpH3zJuzWIpMurbPp2iYreGydRZumGUoHEhGR7ex0cB2OWl8GjHL3F8OyYcAK4EzgkSp2PQ9oDxzq7uWJrosq1Pkz8IS7PxquX2JmQ4GLgL/sbF9FksnMaN4kneZN0pPdFalCJOJsLCplXWExqwqKWLGhiOUbtrBiY5AqtHzDFhavLWTq92tYV1iyzb7ZGal0DQPunDZN6dG+KXu0a0rfjs1p1ywzSc9IRESSLZaR6x5AR+Cd8gJ332xmHwKHUHVwfRLwMfCAmZ1IMIL9AnCLu5eYWQZwAHBnhf3eCY+7HTMbTpCaQk5OTgxPRUQas5QUo2VWOi2z0unWtvpvJDZuKWHx2s0sXruZ/DWF5K8tJH/NZhavLeTj+avZXFIGwJ+O7sOlR/euje6LiEgdFEtw3TF8XF6hfDnQuZr99gCOBJ4FjgO6A/8AmgFXAO2A1CqOe3RlB3T3scBYgNzc3LqZ8CkiDULzJuns2SmdPTu12G6bu7N8QxELVhWwe8usJPRORETqih0G12b2W7YdjT4uxrZSCFJHLnD3MmCambUF7jGzK2M8pohI0pkZHcM500VEpHGryRxgrwIDo5ZVYfluFertBiyr5jhLgW/CwLrcHCCbYNR6FVAWw3FFRGQXWOB+M5tvZl+b2f5V1DvAzGaE9e4Pr8HBzNqY2btm9m342Lp2n4GISN2xw+Da3Te6+/zyBZhNEOwOKa9jZk2AQ4HJ1RzqY6CXmUW32QcoBFaFFzlOiz5uaMgOjisiIrvm50DvcBkOPFRFvYeAC6LqDg3LrwEmuntvYGK4LiLSKO303Svc3YF7gavN7BQzGwA8ARQQ5FMDYGYTzey2qF0fAtoA95lZXzM7FvgfYEx4TIC7gXPN7PdmtqeZ3QfsDjwcw3MTEZGaOREY54EpQCsz6xRdIVxv4e5Tws/scQQXqpfv/2T485NR5SIijU6s81yPBrIILkgsv4nMMRXmuO5JcJMYANw938yOIQigpxOMfv+T4KYy5XWeD/OwryO4icxM4Bfu/n2M/RQRkR3rTNTnNcF9CDoTpPNF11lcSR2A3dy9vO4ytk/vA+Izw1P3a96Iab+aWDSq8kuKqioXEalMTMF1OGpxY7hUVad7JWVTqGJavag6Y4AxsfRLRESSy93dzCqdvSkeMzwp0BWRum6n00JERKT+M7ORZjbdzKYTjFB3jdrcBVhSYZclYXlldZaXp5GEjysS02sRkbpPwbWISCPk7v9w94HuPhD4N3BOOGvIwcD6qDSP8vpLgQ1mdnA4S8g5wCvh5leBYeHPw6LKRUQaHQXXIiLyJrAAmA88Cowo3xCObJcbATwW1vsOmBCWjwKGmNm3BDf9GlULfRYRqZNivaBRREQaiPA6mpFVbBsY9fNUYEAldVYDRyWsgyIi9YhGrkVERERE4kTBtYiIiIhInCi4FhERERGJEwXXIiIiIiJxYj/eebx+M7OVQCx3cmwHrIpzd+o7nZPK6bxsS+ejcrGel27u3j7enamrduEze2fV9utU7dX/NtVe/W6vttqs8jO7wQTXsTKzqe6em+x+1CU6J5XTedmWzkfldF7qltr+fai9+t+m2qvf7SWrzWhKCxERERERiRMF1yIiIiIicaLgGsYmuwN1kM5J5XRetqXzUTmdl7qltn8faq/+t6n26nd7yWpzq0afcy0iIiIiEi8auRYRERERiRMF1yIiIiIicaLgWkRE6jUz62pmC82sTbjeOlzvbmZvmdk6M3u9FtobaGafmNksM/vazE6vhTYPM7MvzGx62O4fEtxe93C9hZktNrMHE92emZWFz2+6mb1aC+3lmNk7ZjbHzGaXP+cEtnle1PObbmZbzOykBLbX3cxGh6+XOWZ2v5lZgtu73cxmhkvM74tY3utm1sPMPjWz+Wb2vJll7NozrQF3bzALYMCNwA/AZmAS0H8H+6QDNwDfAVuAr4ChFeo0B+4luOHBZmAycGCyn28iz0uF/c8AHHi9gZ2XHOA1YBPBZPP3Axk72CcTeCCsvwl4FehSoc59wNTw9bQo2c8zwedjUvjaiF7GV6jTGngKWB8uTwGtkv18d+K87PTvsybvufp+XuraAlwFjA1/fgT4S/jzUcAvK35+JaI9oA/QOyzbHVgaz99pFW1mAJlhWTNgEbB7Is9puH4f8CzwYC38Dgtq+TUzCRgSdU6zE91m1PY2wJp4tVnFa+YQ4GMgNVw+AQ5PYHvHAe8CaUBT4HOgRQJ+b5W+14EXgN+EPz8MXJSI19M2bSa6gdpcgKuBjcCvgAHhCf0BaF7NPreHH4DHAXsAFxH8Mdwvqs7zwBzgcKAXwR/N9UDnZD/nRJ2XqH33ABYDH1bygq235yX8QJkRfojuDwwJz8kDO9jvobDekHC/ScB0IDWqzgPAJQRXKy9K9nNN8PmYBPwT6Bi1tKxQZwIwCxgcLrOA15L9nHfi3Oz077Mm77n6fl7q2kIwUPI1cFl4LtOjth1e8fMrke1F1fmKMNiujTaBtkAe8QuuK20POAAYD5xLfIPrqtpLVHC9XXvAXsB/k/E6DbcPB55J8HMcDEwDsoBsgsGDPRPY3pXA9VF1HgdOS8Q5rPheJxjoWAWkheuDgbcT9fvd2m6iG6itJTyBS4Fro8qywj9wF1az3w/ApRXKXgSejjpGKXBihTrTgL8n+3kn6ryE9dKBT4FhwBMVXrD1/bz8HIgAXaPKziIYnaz0P2qgJVAM/DaqrGt4nGMrqX8F9Se43unzEdaZRDV/XIE9CUazfxJV9tOwrG+yn/dOnqMa/T5r8p5rSOelLi3AseE5HFKhfJs/uIluL9w2iGDwISXRbYafQ18DhcDIRLZHkE46CehCnIPrap5fKUEAOAU4KcHP7yTgdeAl4EvgDqIGT2rhdfMecHwtnNM7gXUEA2K3JPicHkMwUp5NcFvyBcDliTiHFd/rYXvzo9a7AjPj+XwrWxpSznUPglGzd8oL3H0zwYjrIdXsl0kQQETbTPCHDoKvMVJ3UKcui/W8ANxCEEw8Wcm2+n5eBgNz3D0/quxtgtfDAVXscwDBPxzR5zKf4A/ojs5lXRfL+Sj3GzNbFebv3WlmzSsct4AgZajcxwSpJ/X9nFWlJu+5xnheasPPCf6xGZDM9sysE0Gaz3nuHkl0m+6e7+77EHyDOMzMdktgeyOAN919cRzbqK49gG4e3Mr6TOBeM+uZwPbSgEMJ/pk+kODb23Pj2F5lbQJbXzd7E3z2Jqw9M+tF8A9+F6AzcKSZHZqo9tz9HeBNgs+75wjSUMri2UZd05CC647h4/IK5cujtlXmbeAyM+trZilmNgQ4BegE4O4bCV4I15lZZzNLNbOzCP44dorrM0iMmM6LmR0DnAZcWNn2BnJeKp6TVQRv+KrOS8dw+6oK5Tt6jdUHsZwPCHIufwscAdxMkAbxYoXjrvRwyAAg/HnFDo5bn9XkPdcYz0tCmdlAgnSmg4E/hYFKrbdnZi2ANwi+uZhSG22Wc/cfgJkEwWGi2hsMXGxmiwhGP88xs1EJbA93XxI+LiAYNd8vge0tBqa7+wJ3LwX+TZAqFxc7+B2eBrzs7iUJbu9kYIq7F7h7AUGK2uAEtoe73+LuA919CMG3e9/Eu40qrAZamVlauN4FWBJr2zVVb4NrM/utmRWULwQjirG4FJgHzCb4yv9B4F8EX5GXOztcXwwUAX8k+O8r3iMSuywe58XM2hOkgQxz93XVVK0350USw93Huvvb7j7D3ccDpwNDzCxuf4xEdiSc6eAh4DJ3zyP4Kv/O2m4vnIXgZWCcu/9fLbXZxcyywjqtCb45nJeo9tz9t+6e4+7dCUZ3x7n7NYlqL5wNIjOs0w74CcHf64S0R3CxXavw7yDAkfFobwdtljuD4G9oXFTTXh5wmJmlmVk6cBjBN7AJaS8cfGsb1tkH2Ieob/bi9JwqFQ5avA+cGhYNA16Jpe2dUW+Da4JZGgZGLeWjiRW/DtsNWFbVQdx9pbufRHAFazegH8HXtQui6nzn7ocRXDXc1d0HEQStCyo5ZLLF47z0Jxh9nmhmpWZWCpwD/CJc7wv17rxUtIztz0k7glSXqs7LsnB7uwrl1b7G6olYzkdlphKMdveOOm776Gmewp877ORx65Py51Xde64xnpdEugDIc/d3w/UxwJ4WTFP3EfC/wFEWTB13bKLaI5gZ4WfAufbjtGoD49BedW2eD3xqZl8BHxAEwDMS1Z6ZHRaHY9e4PYJAbGr4/N4HRrl7PILdqtr7KcE/DRPNbAbBKOujcWivyjbD12l3gnzgD+LUVpXtEXzGfEdwEftXwFfu/loC2/sp8JGZzSa4MPys8FuBuLWxg/f61cCfzWw+wUW/j8fYds0lOqm7thZ+vIjor1FlTYAN7ODCvQrHSQfmA7dWU6c1wYUAw5P9vBNxXgj+0RhQYfk3wZt+AFVMz1bPzkv5BXxdosrOpGYXNJ4ZVdaFhnVBY43PRxXH2ZfgIpOfhevlF+4dElXnEOrhhXs1/X3W5D3XkM6LFi1atGjZdkl6B+L6ZIL/TtYT5EwPIJgqqOL0VxOB26LWDwrr70GQpzaRYOS1VVSdY8PgowdBns90gquWt5t6qS4usZyXSo7xBNtPxVdvzws/Tj33HkH+3tEEeVgPRNUZBMwFBkWVPUSQBnN0uN/7bD8VXy+Cbw3uDs9z+bcI1c4ZXd/OB9CTYI74XKA78AuCrxa/qHA+JoTHLp9ybgb1aMq5Hf0+CS4ImgucHLVPTd5z9fq8aNGiRYuWypfyBO+GYjTBlFf/IBhF/RQ4xoOL78r1BKJnRGgC/J0guC4guKL1bN8217glcBvBKOUaggu2rvU4XnSQYLGcl5qot+fF3cvM7DiCr5Q+Jpjl5BmC+TjLZQN9w8dylxFMC/U8wTmdCJzj7tFXPj9GkMNW7svwsQfBDR7qnBjPRzHBpP2XEqQG5RNcyPU/Fc7HmQRzRZdfAf8qcHFinklC7Oj3mU5wXlpG1anJe66+nxcREamEufuOa4mIiIiIyA7V5wsaRURERETqFAXXIiIiIiJxouBaRERERCROFFyLiIiIiMSJgmsRERERkThRcC0iIiIiEicKrkVERERE4kTBtYiIiIhInCi4FhERERGJEwXXIiIiIiJxouBaRERERCROFFyLiIiIiMSJgmsRERERkThRcC0iIiIiEicKrkVERERE4kTBtYiIiIhInCi4FhERERGJEwXXIiIiIiJxouBaRERERCROFFyLiIiIiMSJgmsRERERkThRcC0iIiIiEicKrkVERERE4iQt2R2Il3bt2nn37t2T3Q0RkZhMmzZtlbu3T3Y/aos+s0WkPqvuM7vBBNfdu3dn6tSpye6GiEhMzOz7ZPehNukzW0Tqs+o+s5UWIiIiIiISJwquRURERETiRMG1iIiIiEicKLgWEREREYkTBdciIiIiInGi4FpERDCzW8ws38wKdlDvL2Y238zmmdmxUeVDw7L5ZnZN4nssIlI3KbgWERGA14BB1VUws72A3wD9gaHAGDNLNbNU4B/Az4G9gDPCuiIijU6DmedaRERi5+5TAMysumonAuPdvQhYaGbz+TEgn+/uC8JjjA/rzk5cj0VE6iYF1yIiUlOdgSlR64vDMoD8CuUHVdzZzIYDwwFycnJi6kD3a96Iab+aWDTquIQdW0QaD6WFVOHcc8/FzDj88MO323bjjTdiZtstTZs2pXfv3gwbNozPPvssof2bMmUK9913H2eddRb9+vUjJSUFM+Oaa2qW6rhp0yZGjRpFbm4uLVq0oGnTpvTv35/rrruO9evXx7Wvixcvpnnz5lvP06RJkyqtV1ZWxq233kqvXr3IzMykW7duXHPNNRQVFVV57FmzZpGRkcEJJ5wQ1z6LSPy5+1h3z3X33PbtG82d3kWkkdHI9S5ISUkh+g/E6tWrmT9/PvPnz+fpp5/mrrvu4rLLLktI20OHDo05CM7Ly+PYY49l7ty5AGRlZZGWlsbs2bOZPXs248aNY9KkSeyxxx5x6evFF19MQUG110gBMGLECMaOHQtA06ZNycvL4/bbb2fGjBm88Ublo1UjRowgLS2N+++/Py59FZFqLQG6Rq13CcuoplxEpFHRyPUu6Nq1K8uWLdu6bNmyhY8//piBAwcSiUS4/PLLmTlzZkLazsrKYtCgQYwcOZJ//etfDBw4sEb7RSIRTjnlFObOnUvHjh2ZMGECBQUFbNiwgc8++4wBAwaQn5/PL3/5S0pLS3e5n6+88gqvvPIKBx203TfE25g3bx6PPvoorVq1YvLkyRQUFDBz5ky6dOnCm2++yX/+85/t9hk3bhwffvgh1157Ld27d9/lvorIDr0K/MbMMs2sB9Ab+Az4HOhtZj3MLIPgosdXk9hPEZGkUXAdR6mpqRxyyCH8+9//Jj09nUgkwtNPP52QthYvXsynn37Kgw8+yLnnnkvLli1rtN9rr73GtGnTAHjyyScZOnQoKSnBy+DAAw/c2vfZs2fzr3/9a5f6WFBQwCWXXEKzZs246667qq373nvv4e5ccMEFDB48GID+/ftz1VVXATBx4sRt6q9bt44rr7ySPn36cOWVV+5SP0UEzGy0mS0Gss1ssZndGJafYGY3Abj7LOAFggsV3wJGunuZu5cCFwNvA3OAF8K6IiKNjoLrBOjWrRt9+vQBYPbsxFwsn5qaGtN+EyZMAGDPPffkmGOO2W57z549t+Yvjxs3LvYOAtdffz35+fnceOONdO7cudq6q1evBtguFaVXr14ArFq1apvyv/71r6xYsYIHH3yQjIyMXeqniIC7X+XuXdw9JXy8MSx/1d1viKp3i7v3dPe+7j4hqvxNd+8TbrslCU9BRKROUHCdIO4OBBfpVSb6osja9P333wPQt2/fKuv069cPgMmTJ1NYWBhTO19++SUPPPAAAwYM4NJLL91h/bZt2wKwYMGCbcq/++67bbYDTJ06lUceeYTTTjuNIUOGxNQ/ERERkURQcJ0AixYt4ttvvwW2H4lNtvJgvqqgH9iaax2JRJgzZ85OtxGJRBg+fDiRSIQxY8aQlrbj62aPOOIIAB599FGmTAlm+pozZw6jR48G4Kijjtp67IsuuoimTZty991373TfRERERBJJwXUclZWV8cknn3DyySdTUlICwFlnnZXkXm2rW7duANUGzdGpLEuXLt3pNh588EGmTp3KOeecw6GHHlqjffr168f555/PunXrGDx4MM2aNWOvvfYiPz+foUOHcvTRRwPw8MMPM3Xq1BqlmoiIiIjUNgXXuyA/P5+OHTtuXbKysjjkkEOYPn06EKR+VDVLxo033oi7b00fqS3ledbz58/n5Zdf3m77zJkzefPNN7eub9y4caeOv2TJEq677jpat27NHXfcsVP7PvLII9x888306NGD4uJiunTpwhVXXMFLL72EmbFixQquvfZaBgwYwB//+EcAxo8fzz777EOTJk3IycnhhhtuiMssJyIiIiKx0DzXuyASibB8+fLtyps0acKLL77IL37xiyT0qnonnHAC++67L1999RW/+93v2LBhAyeeeCKZmZm89957XHzxxaSkpGxNGymfSaSm/vjHP7Jx40bGjBnDzt4kIjU1leuuu47rrruu0u1XXHEF69ev59VXXyUtLY2nnnqKc845h912243TTz+dqVOncvPNN/PDDz/w2GOP7VTbIiIiIvGgketd0K1bt62jz8XFxcydO5eLLrqILVu2cOGFF7Jo0aJkd3E7qampvPTSS/Ts2ZN169Zx7rnn0rp1a7Kzszn++ONZsWLF1jxngFatWtX42K+//jovvfQSubm5XHjhhXHt94cfmZR+RwAAIABJREFUfrg1mD700EMpKSnhyiuvJCsriylTpvDkk08ydepU9t57bx5//HFmzJgR1/ZFREREakLBdZykp6fTt29fxowZwwUXXMDixYs544wziEQiye7advbYYw+mT5/O6NGj+dnPfka3bt3Yc889Of/885k2bdo2N6Tp3bt3jY87cuRIzIzRo0dTWFhIQUHB1iV61pHNmzdTUFDA5s2ba3TckpISRowYQatWrbYG/lOnTmX58uUcf/zxW28gk5WVxQUXXABQ5R0dRURERBJJaSEJcPvtt/PCCy8wZcoUnnrqKYYNG5bsLm2nWbNmXHnllZXegKU857pDhw47NdtJXl4eAEceeWS19crTZQ477DAmTZq0w+Pec889zJo1i3/84x906NAB+HFKwR49emxTt3xe7PLtIiIiIrVJI9cJ0Lp1a0aOHAkEFy7Wtwvsxo8fD8CZZ56Z5J4EF43edNNNHHDAAfzhD3/YbvuWLVu2Wa/paLiIiIhIIii4TpBLLrmEzMxMFi1alLBboCfC2LFj+fzzz8nOzq7RzV+ileefV7YsXLhwa733338fd6/RqPWll17K5s2beeihh7a5uLJ8SsHyW7mX+/zzzwG2poqIiIiI1CYF1wnSsWNHzj77bABuu+227XKvd/UOjQUFBaxatWrrUj6v9ubNm7cpr+wOi2PHjuWpp57aZqaTvLw8rr76ai666CIA7rzzzkoD1EmTJm3td02C410xYcIEXn75ZS644AIOPPDAbbbl5ubSoUMHPv74Y5544gncnalTp/Lwww8D1MmZWkRERKThU3CdQFdccQUpKSl88803PP/883E99sUXX0z79u23LpMnTwbg/vvv36Y8euaPcpMnT+acc86hY8eOZGdn06JFC7p168bo0aNJTU3l7rvv3hpkJ8uWLVu2Psfbbrttu+3p6emMGjUKgPPOO4+mTZty4IEHsm7dOs4//3z23nvv2u6yiIiIiILrROrbty8nnHACALfeemut3zCmKsOGDWPYsGH069ePtLQ0ysrK6N27NyNGjOCrr77iT3/6U7K7yK233sqCBQu4/fbbad26daV1zjvvPJ5++mkGDBhAWVkZXbp04frrr986ei0iIiJS26yuBHy7Kjc316dOnZrsboiIxMTMprl7brL7UVti/czufk3iptlcNOq4hB1bRBqW6j6zNXItIiIiIhInCq5FREREROJEwbWIiIiISJwouBYRERERiRMF1yIiIiIicaLgWkREREQkTmIKrs1sXzN7zszyzWyzmc0zs6vMrNrjmdnNZjbXzDaZ2Vozm2hmh0Rtb2NmD4R1NofHf8jM2sbSTxERERGR2pQW434HACuBs4E8YBDwaHi8W6vZbx4wElgIZAF/At4ys97uvhzYHegMXAXMDn8eAzwHHBNjX0VEREREakVMwbW7/7NC0QIz2x/4FdUE1+7+dPS6mf0ZOB8YCLzt7jOBU6KqzDezK4HXzayFu2+Ipb8iIom2rrCYf368iKP6dWDfrq2S3R0REUmSWEeuK9MCWFvTymaWAQwHNgDTd3DcIqCwkmMMD49BTk7OzvRVRCQuVhcU8dh/F/LUJ99TUFRKk/QUBdciIo1YXILrcNT6XOC3Nah7PDAeyAaWAkPClJDK6rYCbgYedffSitvdfSwwFoJb6cbafxGRnbVi4xYe/XABT0/JY0tpGcft3YmLj+xFv44tkt01ERFJol0Ors2sL/AGcK+7v1iDXd4nSANpB1wAvGBmg919aYXjNgNeA5YQ5GCLiCTdsvVbePiD73juszxKyiKcNLAzI47oRa8OzZLdNRERqQN2Kbg2s34EwfJ4d7+mJvu4+yZgfrhMMbNvgd8TjFCXH7cZ8Ga4ery7b9mVfoqI7Kpl67fw0KT5PPdZPhF3Ttm/MyMO70X3dk2T3TUREalDYg6uzWwv4D3gBXf/0y70IQXIjDpuc2ACYMBQdy/YhWOLiOyS5Ru28NCk73j2szwiEefXuV0YcXgvurbJTnbXRESkDoopuDaz/gSB9fvArWbWsXybuy8L63QGJgJ/cfeXzawFQXrHawS51u0JpuXrArwQ7tMceIfgIsaTgKZmVj4stMbdi2Ppr4jIzqoYVJ96QBdGHqGgWkREqhfryPWvgQ7A6eESzcLHdKAv0DJcLwX6A78D2gKrgc+Bn7n712GdA4CDw5+/qXDcI4BJMfZXRKRGVmzYwkMffMezn+ZRGnFO3b8LFx/Z8INqMzsAeILgHgRvApe6u1eo0xJ4Gsgh+Ptxp7v/K9w2DLgurPp3d3+ylrouIlKnxDrP9Y3AjTuos4gfA23cvRA4eQf7TIreR0SktqzYsIWHP1jAM59+T2nE+dX+nbn4iN7ktG3YQXWUhwguMv+UILgeSpCiF20kMNvdf2lm7YF5ZvYM0Az4G5ALODDNzF519xpPzyoi0lDEc55rEZF6Z8XGLTzywQKenhIE1afs15mLj+xFt7aN50JFM+sEtHD3KeH6OILUvIrBtQPNzcwIAuo1BN9KHgu86+5rwv3fJQjOn6udZyAiUncouBaRRmnlxiIe+eA7nv70e0rKnJP368zFRzTa2T86A4uj1heHZRU9CLwK/AA0B05390h4jU3+jvbXjb9EpDFQcC0ijcraTcU88uECnpy8iKLSMk7erwuXHNlog+qddSzBHXWPBHoC75rZRzXdWTf+EpHGQMG1iDQKG7eU8Ph/F/L4RwspKC7lxH1359Kj+9BDQTUEN+vqErXeJSyr6DxgVHih43wzWwj0C+seXmH/SQnpqYhIHafgWkQatM3FZYz7ZBEPf/AdawtLOLb/bvx5SF/6dmye7K7VGe6+1Mw2mNnBBBc0ngM8UEnVPOAo4CMz241gRqgFBDcFu9XMWof1jgH+kviei4jUPQquRaRBKi6NMP7zPB58bz4rNhbxsz7tueKYPuzTpVWyu1ZXjeDHqfgmhAtm9gcAd3+Y4E66T5jZDIKZna5291VhvZsJplcFuKn84kYRkcZGwbWINCilZRFe+nIJ9/3nW5as28yg7m148Mz9GdSjTbK7Vqe5+1RgQCXlD0f9/APBqHRl+/8T+GfCOigiUk8ouBaRBiEScd6YsZR7/vMNC1ZuYu/OLbn1lL35We92BDPHiYiIJJ6CaxGp19ydiXNWcNe73zBn6Qb67NaMh886gGP776agWkREap2CaxGptybPX8Xot+cxPX8d3dpmc+/pA/nlvruTmqKgWkREkkPBtYjUOzOXrOf2t+by0ber6NSyCbedsjenHtCF9NSUZHdNREQaOQXXIlJvfL96E3e+8w2vffUDrbLTufYXe3L24G40SU9NdtdEREQABdciUg+s3FjEA+99y7Of5pGWaow8oicXHtaTFk3Sk901ERGRbSi4FpE6a+OWEh79cAGP/XchRaURTj+wK5ce1ZvdWjRJdtdEREQqpeBaROqcotIynpmSx4Pvz2fNpmKO27sTlx/Thz3aN0t210RERKql4FpE6oyyiPPK9CXc/e43LF67mUN6tuXqof3Yt6vuqigiIvWDgmsRSTp3Z9K8ldz+1lzmLttI/91bcOvJe3OobgAjIiL1jIJrEUmqL/PWMmrCXD5duIZubbO5/4z9OH7vTqRormoREamHFFyLSFIsWrWJ0W/P5c0Zy2jXLIObTuzPbw7MISNNc1WLiEj9peBaRGrV2k3F3P/etzw95XvSU1O47OjeXHDoHjTN1MeRiIjUfzH/NTOz+4CfAAOAZe7evQb73Az8GugKFANfANe7++SoOj2BO4GfApnAW8Al7r481r6KSPJtKSnjycmLePD9+WwqKuX0A3P409G96aBp9UREpAHZlaGiFOBJYG/gmBruMw8YCSwEsoA/AW+ZWW93X25mTYF3gBnAkeE+NwOvmdnB7h7Zhf6KSBJEIs6rX/3AHW/PY8m6zRzRtz1/+cWe9NmtebK7JiIiEncxB9fufgmAmV1BDYNrd386et3M/gycDwwE3iYYCe8B5Lr72rDOMGAtQbD9n1j7KyK175PvVnPrm3OYsWQ9/XdvwR2n7sMhvdolu1siIiIJk7QkRzPLAIYDG4DpYXEm4MCWqKpbgAhBmoiCa5F6YP6Kjdz25lwmzl3B7i2bcPdp+3LSwM6aAURERBq8Wg+uzex4YDyQDSwFhkTlU08BCoA7zOzqsGwUkAp0quRYwwkCdHJychLccxHZkZUbi7jnP9/w/Of5ZKenctXQvvzuJz1okp6a7K6JiIjUimSMXL9PkAbSDrgAeMHMBrv7UndfaWa/Bh4CRhCMWD9HcOHjdvnW7j4WGAuQm5vrtdR/EamgsLiUxz5ayCMffEdRaYSzD+7GJUf2om2zzGR3TUREpFbVenDt7puA+eEyxcy+BX5PcOEi7v4O0NPM2gGl7r7OzJYBC2q7ryJSvbKI8+K0xdz17jyWbyhiaP+OXDW0L3u0b5bsromIiCRFXZhYNoUg13ob7r4KwMyOBDoAr9Zyv0SkGh/PX8XNr89m7rKNDOzaigfP3J8Du7dJdrdERESSalfmue4FNAN2BzLMbGC4aba7F5tZZ2Ai8Bd3f9nMWgBXAa8R5Fq3J5iWrwvwQtRxzwPmAiuAwcB9wD3uPi/WvopI/CxYWcCtb87hP3NW0KV1Fg+euR/H7d0JM12sKCIisisj148Bh0Wtfxk+9gAWAelAX6BlWF4K9Ad+B7QFVgOfAz9z96+jjtMXuA1oEx7nFuCeXeiniMTB+sIS7pv4LeM+WUST9FSuHtqP837SXRcrioiIRNmVea4P38H2RYBFrRcCJ9fguNcA18TaLxGJr5KyCM9+msc9//mG9ZtL+M2BXfnzkL60b66LFUVERCqqCznXIlJHvT9vBX9/fTbfrdzEIT3bct1xe7HX7i2S3S0REZE6S8G1iGznm+Ub+fsbc/jwm5V0b5vNo+fkcvSeHZRXLSIisgMKrkVkqzWbirnn3W949rM8sjNSue64PTlncHcy0lKS3TUREZF6QcG1iFBcGmHcJ4u4b+K3FBaX8duDcrjs6D60aZqR7K6JiIjUKwquRRoxd+fd2cu59c05LFpdyGF92nPdcXvSe7fmye6aiIhIvaTgWqSRmv3DBm5+fTafLFhNrw7N+Nd5B3JE3w7J7paIiEi9puBapJFZVVDEXe/MY/zn+bTKSuemE/tzxqAc0lOVV92YmdkBwBNAFvAmcKm7eyX1DgfuJbiXwSp3PywsH0pw069U4DF3H1U7PRcRqVsUXIs0EiVlEZ6cHORVby4u47xDenDpUb1pmZ2e7K5J3fAQcAHwKUFwPRSYEF3BzFoBY4Ch7p5nZh3C8lTgH8AQYDHwuZm96u6za7H/IiJ1goJrkUbgg29WctNrs/hu5SYO69Oe64/fi14dmiW7W1JHmFknoIW7TwnXxwEnUSG4Bs4EXnL3PAB3XxGWDwLmu/uCcP/xwImAgmsRaXQUXIs0YItWbeLvb8zmP3NW0L1tNo8Py+XIfpqvWrbTmWDEudzisKyiPkC6mU0CmgP3ufu4sG5+hf0PqrizmQ0HhgPk5OTEpeMiInWNgmuRBqigqJQH3vuWf/53IRmpKVzz836c95PuZKalJrtrUr+lAQcARxHkZn9iZlNqurO7jwXGAuTm5m6Xzy0i0hAouBZpQCIR5+UvlzDqrbms3FjEr/bvwtVD+9KhRZNkd03qtiVAl6j1LmFZRYuB1e6+CdhkZh8C+4blXWuwv4hIg6fgWqSBmJ6/jhtfncX0/HXs27UVY88+gP1yWie7W1IPuPtSM9tgZgcTXNB4DvBAJVVfAR40szQggyD14x5gLtDbzHoQBNW/IcjPFhFpdBRci9RzKzZuYfRb8/i/aYtp3zyTO3+9L6fs15mUFOVVy04ZwY9T8U0IF8zsDwDu/rC7zzGzt4CvgQjBlHszw3oXA28TTMX3T3efVevPQESkDlBwLVJPFZdG+NfHC3ngvfkUlZZx4WF7cMmRvWmWqbe17Dx3nwoMqKT84QrrdwB3VFLvTYIp/EREGjX9FRaph96bu5ybX5/DwlWbOKpfB647fi96tGua7G6JiIg0egquReqR71YWcPPrs5k0byV7tG/KE+cdyOG6ZbmIiEidoeBapB7YsKWEByZ+y78+XkRWeirXHbcn5wzuTkaablkuIiJSlyi4FqnD3J2XvljCbRPmsnpTEacd0JUrju1L++aZye6aiIiIVELBtUgdNfuHDdzwykymfr+W/XJa8c9zc9mnS6tkd0tERESqoeBapI5Zv7mEe979hnGfLKJVdgajT92HU/fvoqn1RERE6oGYEzbNLMfMXjOzTWa2yszuN7OMGu5rZjbBzNzMTq2wbVFYHr2MirWfIvVFJOL837TFHHXXJMZ9soizDu7G+5cfzmm5XRVYi4iI1BMxjVybWSrwBrAaOBRoCzwJGHBJDQ5xOcENCKpyE/BQ1HpBLP0UqS9m/bCeG16ZxbTv17J/TiueOG8QAzq3THa3REREZCfFmhZyDNAf6Obu+QBmdhXwmJld6+4bqtrRzA4ELgUOAJZXUW2juy+LsW8i9cb6zSXc/c48npryPa2VAiIiIlLvxRpcDwbmlAfWobeBTIKg+f3KdjKz5sCzwHB3X2FWZQBxhZn9BcgH/he4w92LKznecGA4QE5OToxPRaT2RSLOi18sZtSEuawtLOasg7tx+ZC+tMxOT3bXREREZBfEGlx3ZPtR51VAWbitKg8Db7n7hGrq3A98SZByMggYBfQAfl+xoruPBcYC5Obmek07L5JMM5es54ZXZvJF3jr2z2nFk79TCoiIiEhDUWuzhZjZ2cC+QG519dz97qjVr81sA/C8mV3t7qsT2UeRRFpfWMJd787j6TAF5I5T9+FXSgERERFpUGINrpcBP6lQ1g5IDbdV5ihgL6CgQjrI82b2ibv/tIr9Pg0fexGMZovUK5GI839fLOb2MAXk7IO78WelgIiIiDRIsQbXnwDXmVkXd18clg0BioBpVexzLXBnhbIZwBXAK9W0NTB8XBpjX0WSJjoF5IBurRl34iD6764UEBERkYYq1uD6HWAWMM7MLieYiu8O4NHymULMbBAwDjjH3T9z9yXAkuiDhCPY+e6+IFwfDBxMcEHkeuBA4B7gVXfPi7GvIrVOKSAiIiKNU0zBtbuXmdlxwBjgY2Az8AxwZVS1bKBv+FhTRcDpwN8IZh75HngUGB1LP0VqW6UpIMf0pWWWUkBEREQag5gvaAxHko+vZvskgpvKVHcMq7D+BcHItUi9M3PJeq5/ZSZf5q0jVykgIiIijVKtzRYi0lCtLyzhznfm8cyn39OmaQZ3/XpfTtm/M9XM4y4iIiINlIJrkRhFIs7/TVvMqLfmsq6wmHMGd+dPQ/ooBURERKQRU3AtEoOKKSA3nXgQe+3eItndEhERkSRTcC2yE9YVFocpIHm0VQqIiIiIVKDgWqQGIhHnf6flc/tb81hXWMwwpYCIiIhIJRRci+zAjMVBCsj0/HUc2L01/3OCUkBERESkcgquRaqwrrCYO96ex7Of5dG2aSZ3n7YvJ++nFBARERGpmoJrkQoiEeeFqfnc/tZc1m8u4dxDghSQFk2UAiIiIiLVU3AtEqViCshNJw5gz05KAREREZGaUXAtglJAREREJD4UXEujFp0CsmFLqVJAREREZJcouJZG6+vF67j+lVl8pRQQERERiZOUZHdApLat3VTMX1+ewYn/+Jglazdzz+n78sKFgxVYS6NmZgeY2Qwzm29m91s1OVFmdqCZlZrZqVFlw8zs23AZVju9FhGpezRyLY1GJOI8PzWf0WEKyHmH9OCyIb2VAiISeAi4APgUeBMYCkyoWMnMUoHbgXeiytoAfwNyAQemmdmr7r62FvotIlKnKLiWRiE6BWRQ9zbcdFJ/+nXUSLUIgJl1Alq4+5RwfRxwEpUE18AlwIvAgVFlxwLvuvuacP93CYLz5xLZbxGRukjBtTRoazcVc8c783juszzaNcvk3tMHcuLA3TULiMi2OgOLo9YXh2XbMLPOwMnAEWwbXHcG8ne0v4hIY6DgWhqk8hSQ29+ay8YtpfzuJz247OjeNFcKiMiuuBe42t0jsfyDambDgeEAOTk5ce6aiEjdoOBaGpyv8tdxwysz+Wrxegb1aMNNJyoFRGQHlgBdota7hGUV5QLjw8C6HfALMysN6x5eYf9JFXd297HAWIDc3FyPQ79FROocBdfSYKzdVMzot+cx/nOlgIjsDHdfamYbzOxgggsazwEeqKRej/KfzewJ4HV3/3d4QeOtZtY63HwM8JfE91xEpO5RcC31XlnEGf95Hne8PU8pICKxGwE8AWQRXMg4AcDM/gDg7g9XtaO7rzGzm4HPw6Kbyi9uFBFpbGIKrsP5T/9GkDvXmmCkY6S7z6rh/mcAzwJvuPvxUeWpwI3AWUAnYCnwDHCju5fG0ldp2KaHKSBfhykgN584gL4dmye7WyL1jrtPBQZUUl5pUO3u51ZY/yfwz4R0TkSkHol15Poq4HLgXGAecAPwrpn1dfeN1e1oZnsAdwAfVbL5amAkMAyYAewDPAkUATfH2FdpgNZsKuaOt+cy/vN82jXL5L7fDOSEfZUCIiIiIsm108F1OGp9GTDK3V8My4YBK4AzgUeq2TedYN7TawmmcmpXocohwGvu/lq4vsjMXgUO2tl+SsNUMQXk/J/04FKlgIiIiEgdEcvIdQ+gI1F353L3zWb2IUFwXGVwDdwCLHL3J83siEq2/xcYYWb93H2ume0FHAncFkM/pYGZnr+O6/89kxlL1nNQjzbcpBQQERERqWNiCa47ho/LK5Qvp5qbBpjZMcBpwMBqjn070ByYbWZlYf9ucfcxVRxTc6Y2Ams2FTP6rbk8PzWf9koBERERkTpsh8G1mf2WbUejj9vZRsysPcFV6Ge4+7pqqp5OMAXUmcAsgkD8PjNb6O6PV6ysOVMbtrKI89xnQQpIQZFSQERERKTuq8nI9asEs4GUywwfdwPyosp3A5ZVcYz+BLN/TIwabUwBCG9A0N/d5xFc6Hinu48P68wws24E86VuF1xLw/Vl3lpueGUWM5as5+A9ghSQPrspBURERETqth0G1+HsH1tnAAkvaFwGDCGc09TMmgCHAldWcZjPgb0rlP2dYBq/kcDCsCwbKKtQr4wwEJeGrzwFZPzn+XRonsn9Z+zHL/fppBQQERERqRd2Oufa3d3M7gX+amZzgW+A64ACgrmrATCzicBn7v4Xd98EzIw+jpmtA9LcPbr8NeAaM1tIkBayH/BnYNzO9lPql+gUkE1FpQz/2R788ajeNMvUfY5ERESk/og1chlNcBevf/DjTWSOqTDHdU8gfyePewnBfNZjgA4EN5F5FLgpxn5KPfBl3lquf2UmM5dsYPAebbnpxP70VgqIiIiI1EMxBdfu7gR3Uryxmjrdd3CMcysp20gwh/ZlsfRL6pfVBUWMfmsez0/NZ7cWSgERERGR+k/fuUutK4s4z3z6PXe+PY/C4jKlgIiIiEiDoWhGatW079dw/b9nMXvpBg7p2Zb/OUEpICIiItJwKLiWWrFyYxGjJszlxS8W07FFEx48cz+O21spICIiItKwKLiWhCoti/DUlO+5+91v2FJSxkWH9+TiI3rRVCkgIiIi0gApwpGE+WzhGm54ZSZzl23k0N7tuPGE/vRs3yzZ3RIRERFJGAXXEncrNmzhtglzefnLJXRulcXDZ+3Psf07KgVEREREGjwF1xI3JWURnpy8iHv/8y3FpREuPqIXI4/oRVZGarK7JiIi8v/s3Xl8FdX5x/HPQxL2XTZl3xcBUeJaqaiguNaldW0Vq1jFqnWr/rQqat13tC6gVdFWrVUrKiKKIi6gBkXZ90hAdg37EpLn98ed4OWSkOTm3swN+b5fr3klM3POnGcmNzdPzj1zRqRCKLmWhJi0YA23jp7O3BUb6N+1KbeetC/tm9QJOywRERGRCqXkWspl+dot3DlmFm9/9yOtGtVixB/6MrBHcw0BERERkSpJybXEZdv2Ap77fBHDx88jr8C54ujODO3fkZoZGgIiIiIiVZeSaymzz+ev5pa3prNg1UYGdG/GzSf2oO1eGgIiIiIiouRaSu3H3M3c+e4s3p22jDaNa/PPwZkc1a152GGJiIiIpAwl11KirdvzefazRTw2fj4F7lw9sAsX/7qDhoCIiIiIxFByLbs1ce4qho2ewcLVGzmmR3NuPrEHrRvXDjssERERkZSk5FqKlPPTJu58dxZjZyynfZM6PH/BgfTv2izssERERERSmpJr2cnmbfk8+ckCnv5kAdXMuO7YrlzUrz010jUERERERKQkSq4FAHfnvenLufPdWSzN3czJ++3D/x3fjb0b1Ao7NBEREZFKQ8m1MGf5eoaNnsGkhWvo1qIer158CAd32CvssEREREQqHSXXVdjaTXk8/OFcXpz8A/VqpnPHKT05+8DWpKdVCzs0ERERkUpJyXUVlF/g/Ccrh/vfn0Pupm2cc3AbrhnYlUZ1qocdmoiIiEilpuS6ipnyw0/cOnoG05eu46B2jbn15B7su0+DsMMSkZCZWV/geaAWMAa40t09psy5wPWAAeuBS939u2DfIOBRIA14xt3vqbjoRURSR1yf/5vZaWb2vpmtMjM3s/5lrH+4mW03s+kx24eY2adm9rOZ5ZrZx2Z2eDwxys5WrNvCVa9O5fQnJ7F6/TYePasPr/7pECXWIlLoSWAI0DlYBhVRZhFwhLv3Au4ARgCYWRrwD+A4oAdwtpn1qIigRURSTbw913WAL4CXgFFlqWhmjYI644GWMbv7A68CVwCbgKuA982sj7vPizPWKm3r9nye+zybx8bPIy/fuezIjgzt34k6NfShhYhEmNneQH13nxysjwJOAd6LLufuX0StTgZaBd8fBMx394VB/VeA3wAzkxy6iEjKiSvDcvcXAcysSRzVnwWhshyfAAAgAElEQVReIPKx4m9jjntu9LqZXUrkDX4QoOS6jD6evZLb35nJotUbGdC9GTef2IO2e9UJOywRST0tgSVR60vYtfMj1oX8kny3BHJi6h8cW8HMLgYuBmjTpk28sYqIpLQK7b40s6FAc+DvwM2lqFIdqAn8XMzx9EZdhOzVG7n9nZl8NHslHZrU4bkLDuRIPV1RRBLEzI4kklyXadieu48gGEqSmZnpJRQXEamUKiy5NrNewK3AIe6eb2alqfZ3YAMwuqideqPe2cat23n84/k8++kiMtKM/zuuGxf8qj3V0zW1nojs1lJ+GeJB8P3SogqaWW/gGeA4d18TVb91aeqLiOzpSkyug7vDn47adJy7f1qWRsysBpGx1Ne6+6JS1rkS+BMwwN3XlaW9qqagwBn93Y/c/d4sVqzbymkHtOSGQd1oVr9m2KGJSCXg7svMbJ2ZHQJ8CZwHPBZbzszaAG8Af3D3uVG7vgY6m1l7Ikn1WcA5yY9cRCT1lKbnejSRN9tC8fRG7A10B54zs+eCbdUAM7PtwPHuPq6wsJn9hcid6Me5+1dxtFdlTM3J5ba3Z/Dt4lx6tWzAE+ceQN+2jcMOS0Qqn6H8MhXfe8GCmV0C4O5PAbcAewFPBJ8+bnf3THffbmZ/Bt4nMhXfP919RoWfgYhICigxuXb39UTmMy2PpUCvmG1DgYHAqUB24UYzuxq4DTjB3T8rZ7t7rOVrt3Df2Nm88e1SmtarwX2/7c1vD2hFtWqlGm4jIrITd88Cehax/amo7y8CLiqm/hgi82OLiFRpcY25NrPGQBugYbCpk5nlAsvdfXlQZhSAu5/n7nlA7JzWK4Gt7j49att1wJ3A74G5ZtYi2LXZ3dfGE+ueZktePiMnLuSJCQvIL3Au7d+Ry47sRF1NrSciIiISungzspOB56LWRwZfbwOGBd/HM33HZUAGkfHZ0V4ABsdxvD2Gu/PutGXcPWY2S3M3M2jfFtx4fHfa7FU77NBEREREJBDvPNfPExmbt7sy/UvYP4xfEvHCbe3iiWdPN23JWm5/ZwZfZ/9M973r88Dv9uPQjnuFHZaIiIiIxNBYghS2cv0W7h87h/9+s4TGtatz16m9OPPA1qRpXLWIiIhISlJynYK25OXzz88X8Y+P5rMtv4Ah/Trw56M6Ub9mRtihiYiIiMhuKLlOIe7O+zNWcOeYmeT8tJkB3Ztz0wndad9EjywXERERqQyUXKeIWcvWcfvbM5m0cA1dmtflxQsPol/npmGHJSIiIiJloOQ6ZGs2bOWBcXN59evFNKiVwR2/2ZezD2pDepoeWS4iIiJS2Si5DsmWvHxe+CKbxz+az+a8fM4/rB1/OboLDWprXLWIiIhIZaXkuoK5O+98v4x7x85myc+bOapbM248vhudmtULOzQRERERKScl1xVoyg8/8/d3Z/Lt4ly6712ff13Um191ahJ2WCIiIiKSIEquK0DOT5u4Z+xs3v1+Gc3q1eC+03tzet9Wmq9aREREZA+j5DqJ1m7O44mP5/Pc59mkVTOuPLozF/+6A3Vq6LKLiIiI7ImU5SVBXn4BL3+1mIc/mEvu5jxOP6AV1x7TlRYNaoYdmoiIiIgkkZLrBHJ3Ppq9kjvHzGLhqo0c2mEvbjqhOz1bNgg7NBERERGpAEquE2TGj2u5891ZfLFgDR2a1OGZ8zI5unszzDSuWkRERKSqUHJdTivWbeGB9+fw32+W0LBWBredvC/nHNyGDD0ERkRERKTKUXIdp41btzNi4kJGTFxIfoEzpF8HLjuyEw1q6SEwIiIiIlWVkusyyssv4NWvc3jkw3ms3rCVE3rtzQ3HdaN149phhyYiIiIiIVNyXUruzvszVnDf2NksXL2Rg9o1ZuR5fdm/TaOwQxMRERGRFKHkuhSysn/i7vdmM+WHn+nUrK5uVhQRERGRIim53o35Kzdw39jZjJu5gmb1anDPab34bd9WpOtmRREREREpgpLrIqxct4VHxs/j1a9zqJWRxrXHdOGPh7endnVdLhEREREpXpmzRTPLAP4OHAd0BNYBHwM3uPvi3dQ7Argb6ArUBn4AnnH3B6LKDAaeK6J6LXffUtZYy2pDMAPIyIkLycsv4A+HtOXyozqxV90ayW5aRERERPYA8XTF1gYOAO4EpgINgAeBsWbW2923F1NvAzAcmAZsAn4FPG1mm9z9iahym4gk7TskO7HOyy/gla8W8+j4eazesI0Teu/Ndcd0pV2TOslsVkRERET2MGVOrt19LTAwepuZ/QmYAXQnkjwXVW8KMCVq0yIzOw3oBzyxc1FfXta44uHujJ2+nPven8Oi1Rs5uH1jnjm/O31aN6yI5kVERERkD5OoQcT1g68/l7aCme0PHAYMi9lVy8x+ANKI9Izf7O7fJiLIWENGZfHhrJV0aV6Xfw7O5MiumgFEREREROJX7uTazKoTGRbytrsvKUX5JUDToO3b3P2pqN1zgD8C3wH1gCuBz81sP3efV8SxLgYuBmjTpk2ZYz923xYc06MFp/dtRVo1JdUiIiIiUj4lJtdmdi7wdNSm49z902BfOvAS0BA4uZRt9gPqAocA95rZInd/EcDdJwGTotr+gkjv9eXAFbEHcvcRwAiAzMxML2X7O/wus3VZq4iIiIiIFKs0PdejgS+j1pfCjsT6ZaAX0N/d15SmQXdfFHw7zcyaExkW8mIxZfPNLAvoXJpji4iIiIiEqcSnobj7enefH7VsDqbjexXoDRxZjhsQqwHFznNnkQHQvYFlcR5fRERKYBHDzWy+mX1vZgcUU66vmU0Lyg0P3qMxs8Zm9oGZzQu+NqrYMxARSR1lftRg0GP9GpFhHWcDbmYtgqVWVLlRZjYqav1yMzvRzDoHy4XAtUSGlRSWudXMjjWzDmbWB3iWSHIdPS5bREQS6zginxB2JnIfy5PFlHsSGBJVdlCw/QZgvLt3BsYH6yIiVVI8NzS2An4TfD8lZt8FwPPB97F3GKYB9wLtgO3AAiJvwNGJc0MiY6hbAGuBb4Ffu/tXccQpIiKl8xtglLs7MNnMGprZ3u6+41NDM9sbqO/uk4P1UcApwHtB/f5B0ReACcD1FRd+crW74d2kHDf7nhOSclwRCVc881xnAyVOreHu/WPWHwEeKaHOVcBVZY1JRETKpSWQE7W+JNi2LKbMkiLKADSPSsSXA82LaqS8MzxBOAmpkmARKYsyDwsREREpTtD7XeTsTe4+wt0z3T2zadOmFRyZiEjFUHItIlIFmdllZjbVzKYS6aGOnpu0FcHMUFGWBtuLKrMiGDZSOHxkZXKiFhFJfUquRUSqIHf/h7v3cfc+wP+A84JZQw4B1kaPtw7KLwPWmdkhwSwh5wFvBbtHA+cH358ftV1EpMpRci0iImOAhcB8YCQwtHBH0LNdaCjwTFBuAZGbGQHuAQaa2TxgQLAuIlIllfvx5yIiUrkF46QvK2Zfn6jvs4CeRZRZAxydtABFRCoR9VyLiIiIiCSIkmsRERERkQRRci0iIiIikiBKrkVEREREEsQi97FUfma2CvghjqpNgNUJDqey0zXZla7JrnRNdlWea9LW3avMk1XK8Z5dVhX9OlV7lb9NtVe526uoNot9z95jkut4mVmWu2eGHUcq0TXZla7JrnRNdqVrknoq+mei9ip/m2qvcrcXVpvRNCxERERERCRBlFyLiIiIiCSIkmsYEXYAKUjXZFe6JrvSNdmVrknqqeifidqr/G2qvcrdXlht7lDlx1yLiIiIiCSKeq5FRERERBJEybWIiFRqZtbazBaZWeNgvVGw3s7MxppZrpm9UwHt9TGzSWY2w8y+N7MzK6DNI8zsGzObGrR7SZLbaxes1zezJWb2eLLbM7P84PymmtnoCmivjZmNM7NZZjaz8JyT2OYFUec31cy2mNkpSWyvnZndF7xeZpnZcDOzJLd3r5lND5a4fy/i+V03s/Zm9qWZzTezV82sevnOtBTcfY9agNOA94FVgAP9S1nvCGAKsAVYCFxSRJmhwKKgzBSgX9jnW8pzM2AY8COwGZgA7FtCncHB9Ytdau4J1ySIvQ3wNrCRyHyYw4HqJdSpATwWlN8IjAZalfe4qbLEeU0mFPE6eSWmTCPgRWBtsLwINAz7fEt5TR4FsoLXeHYp65T4O1eZr0kqLsBfgRHB908D/xd8fzRwEvBOstsDugCdg237AMsS+TMtps3qQI1gW10gG9gnmdc0WH8U+DfweAX8DDdU8GtmAjAw6prWTnabUfsbAz8lqs1iXjOHAZ8DacEyiVLmSnG2dwLwAZAO1AG+Buon4edW5O868B/grOD7p4BLk/F62qnNZDdQ0QvwB+DW4GupkmugPZFk4jGgOzAEyANOjypzZrBtSFDmMWAD0Cbscy7F+V0PrAdOB3oGL7QfgXq7qTM4uCYtopeYMpX5mqQB04I30QOAgcE1eayEek8G5QYG9SYAU4G08hw3FZZyXJMJwD9jXisNYsq8B8wADg2WGcDbYZ9zKa/LY8DlRG6QyS5lnRJ/5yrzNUnFBcgAvgf+ElzLjKh9/Ul8cl1se1FlviNItiuiTWAvYDGJS66LbA/oC7wS/J1IZHJdXHvJSq53aQ/oAXwWxus02H8x8K8kn+OhRDrDagG1iXQedE9ie9cBN0eVeRY4IxnXMPZ3nUhHx2ogPVg/FHg/WT/fHe0mu4GwFiJP5yltcn0vMC9m2zPApKj1L4GRMWXmAXeHfa4lnJsR6T25KWpbLSJ/+P+0m3qDS3pDq6zXJIjzOKAAaB217fdEeieL/I8aaABsA86N2tY6OM6x8R43VZZ4YyeSXBf7B5bIP14O/Cpq2+HBtq5hn3cZrs+1lCK5Ls3v3J5yTVJtAY4NruHAmO07/cFNdnvBvoOAWUC1ZLcZvA99D2wCLktme0SGk04AWpHg5Ho357edSAI4GTglyed3CvAO8AbwLXA/QedJBb1uPgJOrIBr+gCQS+RTszuTfE2PIdJTXptIbrYQuCYZ1zD2dz1ob37UemtgeiLPt6hFY64jDgXGxWx7H8g0s4xgfE7fIsqMI/LxSiprT6QncUfs7r4ZmEjJsdcysx+CcXXvmNn+hTsq+TWByM98lrvnRG17n8iwj77F1OlL5D/m6GuZQ+QPaOE5x3PcVFGe2M8ys9XBGL4HzKxezHE3AF9EbfucyCcjleG1Ulal+Z2ratekohxH5B+bnmG2Z2Z7Exnmc4G7FyS7TXfPcffeQCfgfDNrnsT2hgJj3H1JAtvYXXsQecx0JnAO8IiZdUxie+lAPyL/TB8IdCDyT0Qi7e5104vI+27S2jOzTkT+wW8FtASOMrN+yWrP3ccBY4i8371MZBhKfiLbSDVKriNaACtitq0g8kvWJFjSiinTIunRlU9hfGWNfQ7wR+A3wNlEei8/N7POwf7KfE2g6J/5aiK/8MXF3yLYvzpme/Q5x3PcVBFv7P8GzgWOBO4gMhTi9ZjjrvKg2wAg+H5lCcetrErzO1fVrknSmVkfIkOZDgGuChKVCm/PzOoD7xL55GJyRbRZyN1/BKYTSQ6T1d6hwJ/NLJtI7+d5ZnZPEtvD3ZcGXxcS6TXfv7hjJKC9JcBUd1/o7tuB/xEZJpcQJfwMzwDedPe8JLd3KjDZ3Te4+wYiQ9QOTWJ7uPud7t7H3QcS+XRvbqLbKMYaoKGZpQfrrYCl8bZdWpU6uTazc81sQ9SSyP+8KqXYa0Kkp7XM3H2Su7/g7lPd/VMi46sXEBl7KrKDu49w9/fdfZq7v0LktTLQzBL2B0lkd4KZDp4E/uLui4l8lP9ARbcXfKL3JjDK3f9bQW22MrNaQZlGRIYXzUlWe+5+rru3cfd2RHp3R7n7DclqL5gNokZQpgnwK2BmstojcrNdQzNrGhQ9KhHtldBmobOJ9OwmxG7aWwwcYWbpZpZBZEKHWclqz8zSzGyvoExvoDe7fupd3nMqUtBp8THw22DT+cBb8bRdFpU6uSYyU0OfqCUrzuMsB2I/RmtOZJzXan7pvSuqzPI420yW2GtS2MtartjdPZ/I9S3sua5M16QoRf3MC3vji4t/ebC/Scz26HOO57ipIlGxZxF5bRS+VpYDTaOnegq+b1bG41YWhee0u9+NqnZNkm0IsNjdPwjWnwC6W2Sauk+B14CjgyFuxyarPSIzI/waGGy/TKvWJwHt7a7NC4Evzew74BMiCfC0ZLVnZkck4Nilbo9IIpYVnN/HwD3unohkt7j2DifyT8N4M5tGpJd1ZALaK7bN4HXajsh44E8S1Fax7RF5j1lA5Ab274Dv3P3tJLZ3OPCpmc0kcmP474NPBRLWRgm/69cDV5vZfCI3/T4bZ9ull+xB3WEtlP2Gxrkx20aw6w2NI2LKzCXFb97jl5urbozaVhNYx25uaCzmOFOAf1b2axLEWXjzXquobedQuhsaz4na1oqib2gs9XFTZUlU7MB+we/er4P1wpv3DosqcxiV7OY9yn5DY7G/c3vKNdGiRYsWLbsuoQeQ8BOKzBHZh8gdow5cFKy3iCozishHWYXrhVPxPRL80bsoSKJip+LbFuzrTmSOzw1EbrQI/bxLuCbXE7kj+DQig/9fYddpwcZHJ8VEpjM8lsjNHH2ITLWWBxy0h1yTwmnnPiIyfm8AkXFYj0WVOQiYHXPOTxIZkzcgqPcxRU/FV+xxU3WJ55oAHYFbgEygHXA8kY8XvyHqDnsiY/qm8cu0c9OoJNPOEblJrA/wUPB7U/ipUPVgf8vgmpwaVac0v3OV9ppo0aJFi5bil9ADSPgJFf/wk2FRZSYAE2LqHREkBFuJPBSluIfIZAdlphD0zKX6wi8PtFhGpBfyE6BnTJls4Pmo9YeBH4JzXUnk7uVD95RrEsTehsiUS5uI3PQwnOBhDMH+/sR8+sEvD5FZE9R7m6ip60pz3FReynpN+OVjzDXBa2A+kX+yGscctxHwEpHe23XB95XigSkU/ZAcB9oF+9sF64Oj6pTmd67SXhMtWrRo0VL8Yu47blYXEREREZFyqOw3NIqIiIiIpAwl1yIiIiIiCaLkWkREREQkQZRci4iIiIgkiJJrEREREZEEUXItIiIiIpIgSq5FRERERBJEybWIiIiISIIouRYRERERSRAl1yIiIiIiCaLkWkREREQkQZRci4iIiIgkiJJrEREREZEEUXItIiIiIpIgSq5FRERERBJEybWIiIiISIIouRYRERERSRAl1yIiIiIiCaLkWkREREQkQZRci4iIiIgkiJJrEREREZEEUXItIiIiIpIgSq5FRERERBIkPewAEqVJkyberl27sMMQEYnLlClTVrt707DjqCh6zxaRymx379l7THLdrl07srKywg5DRCQuZvZD2DFUJL1ni0hltrv3bA0LERERERFJECXXIiIiIiIJouRaRERERCRBlFyLiIiIiCSIkmsRERERkQRRci0iIphZXzObZmbzzWy4mVkRZa4zs6nBMt3M8s2scbCvoZn918xmm9ksMzu04s9CRCR8Sq5FRATgSWAI0DlYBsUWcPf73b2Pu/cB/g/4xN1/CnY/Cox1927AfsCsiglbRCS1KLkWEanizGxvoL67T3Z3B0YBp5RQ7Wzg5aB+A+DXwLMA7r7N3XOTGLKISMraYx4iIyIicWsJLIlaXxJsK5KZ1SbSs/3nYFN7YBXwnJntB0wBrnT3jTH1LgYuBmjTpk3CgpfyaXfDu0k7dvY9JyTt2CKpSj3XxRg8eDBmRv/+/XfZN2zYMMxsl6VOnTp07tyZ888/n6+++iqp8U2ePJlHH32U3//+93Tr1o1q1aphZtxwww2lPkZWVhZnnXUW++yzDzVr1qRNmzZcdNFFzJ8/P66YsrOzi7wuxS0//LDzw43y8/O566676NSpEzVq1KBt27bccMMNbN26tdg2Z8yYQfXq1Tn55JPjillE4nIS8HnUkJB04ADgSXffH9gI7PJm5O4j3D3T3TObNq0yT3oXkSpGPdflUK1aNaL/QKxZs4b58+czf/58XnrpJR588EH+8pe/JKXtQYMGsXbt2rjrv/DCC1x00UVs374dM6N+/frk5OTw7LPP8sorrzB69GiOOuqoMh0zLS2N5s2b77bMzz//zLZt22jevDktW+7cMTZ06FBGjBgBQJ06dVi8eDH33nsv06ZN4913i+5ZGTp0KOnp6QwfPrxMsYrITpYCraLWWwXbinMWwZCQwBJgibt/Gaz/lyKSaxGRqkA91+XQunVrli9fvmPZsmULn3/+OX369KGgoIBrrrmG6dOnJ6XtWrVqcdBBB3HZZZfx3HPP0adPn1LX/f777xkyZAjbt2/n3HPPZcWKFeTm5pKdnc3AgQPZuHEjp59+OqtWrSpTTLHXI3bJycmhfv36AJx77rmkp//yv92cOXMYOXIkDRs25IsvvmDDhg1Mnz6dVq1aMWbMGD788MNd2hs1ahQTJ07kpptuol27dmWKVUR+4e7LgHVmdkgwS8h5wFtFlQ3GVx8Rvd/dlwM5ZtY12HQ0MDO5UYuIpCYl1wmUlpbGYYcdxv/+9z8yMjIoKCjgpZdeSkpbS5Ys4csvv+Txxx9n8ODBNGjQoNR1b7nlFvLy8sjMzOSFF17Y0fvetm1b3njjDVq3bk1ubi733HNPQmN+9913Wb16NQDnn3/+Tvs++ugj3J0hQ4Zw6KGRGbz23Xdf/vrXvwIwfvz4ncrn5uZy3XXX0aVLF6677rqExilSRQ0FngHmAwuA9wDM7BIzuySq3KnAuNjx1MDlwL/M7HugD3BX8kMWEUk9Sq6ToG3btnTp0gWAmTOT03mTlpYWV73c3FzGjBkDwNVXX73LcerWrcsll0T+jr788stEJg5IjBdeeAGA/fffn969e++0b82aNQB06NBhp+2dOnUC2JGUF7rxxhtZuXIljz/+ONWrV09YjCJVlbtnuXtPd+/o7n8OZg3B3Z9y96eiyj3v7mcVUX9qMJ66t7uf4u4/V2T8IiKpQsl1khQmpfn5+UXuj74psiJ99tln5OXlAXDMMccUWebYY48FYNmyZcyalZipalevXr1j3HRsrzXAXnvtBcDChQt32r5gwYKd9kPkRsynn36aM844g4EDByYkPhEREZFEUHKdBNnZ2cybNw/YtSc2bIU96S1atNgpYY3Wo0ePXcqX18svv0xeXh4ZGRmcc845u+w/8sgjARg5ciSTJ08GYNasWdx3330AHH300QAUFBRw6aWXUqdOHR566KGExCYiIiKSKEquEyg/P59JkyZx6qmn7ugd/v3vfx9yVDtbtmwZAPvss0+xZWrVqkXDhg13Kl9ezz//PADHH388RU3B1a1bNy688EJyc3M59NBDqVu3Lj169CAnJ4dBgwYxYMAAAJ566imysrIYNmzYLrONiIiIiIRNyXU55OTk0KJFix1LrVq1OOyww5g6dSoQGfpx8MEHF1l32LBhuHtCxzSXxsaNkXuQatWqtdtytWvXBmDDhg3lbnP69Ol88803QNFDQgo9/fTT3HHHHbRv355t27bRqlUrrr32Wt544w3MjJUrV3LTTTfRs2dPrrjiCgBeeeUVevfuvWOe7ltuuYXt27eXO2YRERGReGie63IoKChgxYoVu2yvWbMmr7/+Oscff3wIUaWewhsZ99prL044ofindaWlpfG3v/2Nv/3tb0Xuv/baa1m7di2jR48mPT2dF198kfPOO4/mzZtz5plnkpWVxR133MGPP/7IM888k5RzEREREdkd9VyXQ9u2bXf0Pm/bto3Zs2dz6aWXsmXLFv70pz+RnZ0ddoi7qFOnDgCbN2/ebblNmzYBkdlDyiM/P59//etfAJxzzjlxz+wxceLEHcl0v379yMvL47rrrqNWrVpMnjyZF154gaysLHr16sWzzz7LtGnTyhW3iIiISDyUXCdIRkYGXbt25YknnmDIkCEsWbKEs88+m4KCgrBD20nhWOsff/yx2DKbN28mNzcXgL333rtc7Y0bN27HuO3dDQnZnby8PIYOHUrDhg133OCYlZXFihUrOPHEE3c8QKZWrVoMGTIEoNgnOoqIiIgkk5LrJLj33ntp0KABkydP5sUXXww7nJ0UzgSyfPnyHXNLx4qeISR65pB4FN7I2LNnT/r27RvXMR5++GFmzJjBnXfeSbNmzQD44YcfAGjfvv1OZQvnxS7cLyIiIlKRlFwnQaNGjbjsssuAyI2LqXSD3eGHH05GRgZAkY8Uh0hvM0R6ubt37x53W7m5uYwePRqIv9c6JyeH22+/nb59++54uE20LVu27LRe0nAXERERkWRScp0kl19+OTVq1CA7Oztpj0CPR4MGDXbcaPnQQw/tMmxl48aNPPVU5GFsZ599drkecvPqq6+yZcsW0tLSOPfcc+M6xpVXXsnmzZt58sknqVbtl5dr27ZtAZgyZcpO5b/++muAHUNFRERERCqSkuskadGiBX/4wx8AuPvuu3dJYsv7hMYNGzawevXqHUvhvNqbN2/eaXvhjYnRbrvtNjIyMvjqq68YPHjwjkeLL168mNNOO43FixfTsGFDrr/++l3qTpgwYUfcEyZM2G2MhbOEHHPMMXGN3X7vvfd48803GTJkCAceeOBO+zIzM2nWrBmff/45zz//PO5OVlbWjn8MNFOLiIiIhEHJdRJde+21VKtWjblz5/Lqq68m9Nh//vOfadq06Y7liy++AGD48OE7bS+8ATDafvvtx8iRI3dMZ9esWTMaNmxI27ZtGTduHHXq1OH1118v8mEvpTV37lwmTZoEwODBg8tcf8uWLTvO8e67795lf0ZGBvfccw8AF1xwAXXq1OHAAw8kNzeXCy+8kF69esUdu4iIiEi8lFwnUdeuXTn55JMBuOuuuyr8gTG7c/755zNp0iTOOOMMmjdvzubNm2ndujV//OMfmTp1KkcddVS5jj9q1CgAGjZsuOMalMVdd93FwoULuffee2nUqFGRZTr8v+IAACAASURBVC644AJeeuklevbsSX5+Pq1ateLmm2/e0XstIiIiUtEslRK+8sjMzPSsrKywwxARiYuZTXH3zLDjqCh6z04d7W5I3tSl2fcU/+Awkcpsd+/Z6rkWEREREUkQJdciIiIiIgmi5FpEREREJEGUXIuIiIiIJIiSaxERERGRBFFyLSIiIiKSIHEn12bmRSyX7KZ8hpnda2bfm9lGM1tmZv82szYx5VqY2YtmttzMNpnZd2YW37OzRUREREQqUHo56w8B3olaX7ubsrWBA4A7galAA+BBYKyZ9Xb37UG5UUBj4DfAKuBU4EUzy3H3ieWMV0REREQkacqbXOe6+/LSFHT3tcDA6G1m9idgBtAdmBZsPgy43N2/DNYfNLMrgIOAhCbXS3M3syx3M5ntGifysCIiIiJSRZU3uX7UzJ4CFgHPAiPcvaAM9esHX3+O2vYZcIaZjQ62nwQ0BT6MrWxmFwMXA7Rp0yZ2d4ku//c3rFy/lY+u6U/1dA0/FxGR8CXriYl6WqJIxShPRnkLcCYwAHiFyBCPG0tb2cyqB3XedvclUbvOABxYDWwF/gWc7e5TY4/h7iPcPdPdM5s2bVrmE7j86M4s+Xkzr2bllLmuiIiIiEisuHuu3f2OqNWpZpYG3AT8vaS6ZpYOvAQ0BE6O2f13oAmRpH01cAowysx+7e7fxRtvUfp3aUpm20Y8/tE8fte3FTUz0hJ5eBERERGpYhI5FuJLoL6ZNd9doSCxfhnoDRzt7mui9nUELgeGuPt4d//O3W8Dvg62J5SZce2xXVmxbisvTvoh0YcXERERkSomkcl1H2ALkFtcATPLAF4lklgfWcTNkLWDr/kx2/NJ0pzch3TYi36dm/DkJwvYsHV7yRVERERERIoRV8JqZieZ2RAz62lmHc3sIuB2Ijc0bg3KtDSz2WZ2arCeDrwGHAKcDXgwp3ULM6sVHHo2MB94wswOCo59DZFZRt4s15nuxjXHdOWnjdv452eLktWEiIiIiFQB8fYG5wFDgUnA98CVRG5wvCaqTAbQlch81gCtiMxdvQ8wBVgWtZwJ4O55wPFE5rd+Ozj2ecAF7v52nLGWqE/rhgzs0ZyRExeSu2lbspoREUlZZtbXzKaZ2XwzG25mVky5/mY21cxmmNknUdv/aWYrzWx6xUUtIpJ64kqu3X2su+/v7vXcvY6793L3R6MeBIO7Z7u7ufvzMetFLc9H1Zvn7qe7e/Pg2Pu5+wvlPdGSXHNMFzZs287TExcmuykRkVT0JJEHg3UOlkGxBcysIfAEcLK77wv8Lmr380XVERGpajS5c6Bbi/qc1Hsfnv88m5Xrt4QdjohUMvkFzjkjJzP6ux/DDqXMzGxvoL67T3Z3J/Kk3FOKKHoO8Ia7LwZw95WFO4In6P5UEfGKiKQyJddRrhrYhW35BTzx8YKwQxGRSuad73/kiwVrSK9W5GiKVNcSiH7ewJJgW6wuQCMzm2BmU8zsvLI0YmYXm1mWmWWtWrWqHOGKiKQuJddR2jepw28PaMW/v1zM0tzNYYcjIpVEfoEzfPw8ujavx6B9W4QdTjKlA32BE4BjgZvNrEtpK5f3wV8iIpWBkusYVwzoDMBj4+eFHImIVBbvfP8jC1Zt5MoBnalWOXuulxK56bxQq2BbrCXA++6+0d1XAxOB/SogPhGRSkPJdYyWDWtxzsFteG3KEhat3hh2OCKS4vaEXmt3XwasM7NDgllCzgPeKqLoW8DhZpZuZrWBg4FZFRiqiEjKU3JdhKFHdiQjzXjkw7lhhyIiKe7t7yK91lccXWl7rQsNBZ4h8qyBBcB7AGZ2iZldAuDus4CxRKZJ/Qp4xt2nB+VeJjI9a1czW2JmF1b8KYiIhC897ABSUbN6NRl8WHuenriAof070bVFvbBDEpEUtD2/gEfHz6Nbi3oc17Ny9loXcvcsoGcR25+KWb8fuL+IcmcnLzoRkcpDPdfFuOSIDtStns6D4+aEHYqIpKg3v13KotUbuXpgl8reay0iIgmi5LoYDWtX56J+HRg3cwXf5eSGHY6IpJi8/AKGfzSPni3rM7BH87DDERGRFKHkejf+eHg7GtXO4AH1XotIjP9OWULOT5u5emAXinlSuIiIVEFKrnejXs0MhvbvxKfzVvPFgtVhhyMiKWLr9nwe/2g+fVo35MiuzcIOR0REUoiS6xL84dC27N2gJveOnUPkqcAiUtX9J2sJS3PVay0iIrtScl2CmhlpXDWgC9/l5DJ2+vKwwxGRkG3Jy+cfH80ns20j+nVuEnY4IiKSYpRcl8JpB7SkU7O63D9uDtvzC8IOR0RC9PJXi1m+bgtXH6NeaxER2ZWS61JIT6vGdcd2ZeGqjbw2ZUnY4YhISDZvy+eJCQs4pENjDuuoXmsREdmVkutSOqZHcw5o05BHPpzL5m35YYcjIiF4afIPrFq/lasHdg07FBERSVFKrkvJzLjhuO6sWLeV57/IDjscEalgG7du56lPFtCvcxMOat847HBERCRFKbkug4PaN+aobs14csJ8cjdtCzscEalAoyb9wJqN27hqYJewQxERkRSm5LqM/jqoK+u3bufJCQvCDkVEKsj6LXk8PXEBR3ZtygFtGoUdjoiIpDAl12XUrUV9Tu3Tkue/yGbZ2s1hhyMiFeD5z7PJ3ZSnXmsRESlRuZNrM2tiZkvNzM1st7fPW8QwM/vRzDab2QQz2zemTHZwrOjlnvLGmUhXDeyCOzzywbywQxGRJMvdtI0Rny5kQPfm9G7VMOxwREQkxSWi5/o5YGopy/4VuAa4HDgQWAl8YGb1YsrdDuwdtfw9AXEmTOvGtTn3kDa8NiWH+SvXhx2OiCTRU58sZMPW7Vx3rGYIERGRkpUruTazK4HawIOlKGvAX4B73P11d58OnA/UA86JKb7e3ZdHLRvKE2cy/PnITtSuns79788JOxQRSZKV67bw/BeLOKVPS7q2iO0DEBER2VXcybWZ7Q9cD5wHlOaxhe2BFsC4wg3uvhmYCBwWU/ZaM1tjZlPN7CYzqx5vnMmyV90aDOnXgfdnrOCbxT+HHY6IJMHwj+axPd/5y4DOYYciIiKVRFzJtZnVAV4BLnf3paWs1iL4uiJm+4qofQDDgbOBI4HHgauAJ4qJ42IzyzKzrFWrVpU2/IS5qF97mtStzr3vzcbdK7x9EUmexWs28cpXOZx1UGva7lUn7HBERKSSiLfnejjwmbu/nshgANz9IXf/2N2/d/dngKHAhWa2VxFlR7h7prtnNm3aNNGhlKhOjXSuOLozXy76iY9mr6zw9kUkeR7+cC7pacYVR6nXWkRESi/e5PpoYLCZbTez7cD4YPtyM7uzmDrLg6/NY7Y3j9pXlC+Dr53iijTJzj6oDR2a1OGuMbPYnl+a0TEikupmL1/H/6Yu5fzD2tGsfs2wwxERkUok3uT6GGA/oE+wXBRs70+kV7soi4gk0QMLN5hZTaAf8MVu2uoTfF0WZ6xJlZFWjRuO68aCVRt55eucsMMRkQR4cNxc6lZP59IjOoYdioiIVDJxJdfuPtfdpxcuRBJngNnuvgLAzFqa2WwzOzWo48AjwPVmdpqZ9QSeBzYA/w7qHGpmV5lZHzNrb2ZnEBlvPdrdF5fnRJNpYI/mHNS+MY98OJf1W/LCDkdEyuGbxT/zwcwVXPzrDjSsnXL3UouISIpL5hMaM4CuQIOobfcBDwP/ALKIzGF9jLsXTha9FTgTmADMJDLf9UgiNzimLDPjpuO7s3rDNp7+ZGHY4YhInNyd+8fOoUnd6vzx8PZhhyMiIpVQeiIO4u4TAIvZll3ENgeGBUtRx/kGOCQRMVW0/Vo35OT99mHkpws595A27N2gVtghiUgZfT5/DZMWruHWk3pQp0ZC3h5FRKSKSWbPdZVz3bFdcYcH3p8bdigiUkbuzv3vz6Zlw1qcc3CbsMMREZFKSsl1ArVuXJsLftWON75dwvSla8MOR0TK4P0Zy/luyVquHNCZGulpYYcjIiKVlJLrBBt6ZCca1MrgrjGz9GAZkUpie34B970/h45N63Da/i3DDkdERCoxJdcJ1qBWBlce3ZkvFqxhwpyKf2qkiJTdq1k5LFy1kesHdSM9TW+LIiISP/0VSYJzD25Lu71q68EyIpXAxq3beeTDeRzYrhEDe8Q+40pERKRslFwnQfX0yINl5q3cwGtTloQdjojsxjOfLmLV+q3ccFx3zKzkCnsoM+trZtPMbL6ZDbfdXAwzOzB4Qu9vo7a1MbNxZjbLzGaaWbuKiFtEJNUouU6SY/dtQWbbRjw4bi4btm4POxwRKcKq9Vt5euICjuvZgr5tG4UdTtieBIYAnYNlUFGFzCwNuBcYF7NrFHC/u3cHDgJWJi9UEZHUpeQ6ScyMm07ozuoNW3ni4/lhhyMiRRg+fh7bthdw3bFdww4lVGa2N1Df3ScHzyMYBZxSTPHLgdeJSp7NrAeQ7u4fALj7BnfflOSwRURSkpLrJNq/TSNO3b8lz3y2iJyf9HdGJJUsWLWBf3+1mLMPakOHpnXDDidsLYHoMWxLgm07MbOWwKlEermjdQFyzewNM/vWzO4Perhj619sZllmlrVqlW74FpE9k5LrJLt+UDfSzLhrzKywQxGRKPePnUPN9GpccXTnsEOpTB4Brnf32Du104F+wLXAgUAHYHBsZXcf4e6Z7p7ZtGnTZMcqIhIKJddJ1qJBTYb278h705czacGasMMREWDKDz8zdsZy/nRER5rWqxF2OKlgKdAqar1VsC1WJvCKmWUDvwWeMLNTiPR0T3X3he6+HfgfcEByQxYRSU1KrivAkF93oGXDWtz+zkzyC/RgGZEwuTt3j5lF03o1uKhf+7DDSQnuvgxYZ2aHBLOEnAe8VUS59u7ezt3bAf8Fhrr7/4CvgYZmVtgdfRQws2KiFxFJLUquK0DNjDRuPL47s5at49Wvc8IOR6RKGzdzBVk//MxVA7pQu3p62OGkkqHAM8B8YAHwHoCZXWJml+yuorvnExkSMt7MpgEGjExuuCIiqUl/WSrI8b1acFC7xjwwbg4n9N6bBrUywg5JpMrJyy/g3rGz6di0Dmdktiq5QhXi7llAzyK2P1VM+cEx6x8AvZMSnIhIJaKe6wpiZtxyUg9+3rSNx8bPCzsckSrp318uZuGqjdxwXHc95lxERJJCf10qUM+WDTgzszXPf5HNglUbwg5HpEpZuymPhz+cy6867cWA7s3CDkdERPZQSq4r2DXHdKVmRhp3vqup+UQq0qPj57Fucx5/O6FHlX7MuYiIJJeS6wrWtF4Nrji6Ex/NXsknc/UQBZGKsHDVBkZNyubMA1vTfe/6YYcjIiJ7MCXXIRh8WHva7VWb29+ewbbtsc9iEJFEu2vMbGpmpHH1wKr9mHMREUk+JdchqJ5ejVtO6sGCVRv55+eLwg5HZI/2+fzVfDhrBUOP1ANjREQk+ZRch+Sobs0Z0L05w8fPY9nazWGHI7JHyi9w7nhnJq0a1eKPv9IDY0REJPniSq7NbD8ze9nMcsxss5nNMbO/mtluj2dmdc3sMTNbElXvqpgyI81sQbB/lZm9ZWbd44kz1d16Ug/yC5y/6+ZGkaT4T1YOs5ev5/+O607NjLSwwxERkSog3p7rvsAq4A/AvsCtwM3ADSXUewg4IajXHbgTuMfM/hBVJgsYHOw/lsiTvj40sz3uqSutG9dmaP9OvPv9Mj6fvzrscET2KOu35PHguDkc2K4Rx/dqEXY4IiJSRcSVXLv7P939Cnef4O4L3f0V4Eng9BKqHga86O4fu3u2u48CJgMHRx37aXf/NNj/DfA3YB+gQzyxpro/HdGBNo1rc8tb03Vzo0gCPTFhAas3bNPUeyIiUqESOea6PvBzCWU+A04ys9YAZnYY0AcYW1RhM6sDXAAsBrKL2H+xmWWZWdaqVZVzWruaGWkMO1k3N4ok0uI1m3j2s0Wctn9L9mvdMOxwRESkCklIcm1mBxAZyvFkCUWvAL4DFptZHvAJcL27vxNzvKFmtgHYABwHHO3uW2MP5u4j3D3T3TObNm2agDMJh25uFEms29+ZSUY146+DuoUdioiIVDHlTq7NrCvwLvCIu79eQvHLiQwNOZnIuO2rgAfMbFBMuX8B+wNHAHOB18ysdnljTWW6uVEkMT6evZIPZ63g8qM706JBzbDDERGRKqZcybWZdQMmAK+4+25vZjSzWsDdwF/d/W13/97dHwdeAa6NLuvua919nrtPBH4LdKHk8dyVWuvGtbm0f0fd3ChSDlu353Pb2zPo0KSOpt4TEZFQxJ1cm1kPIon1a+5+VQnFATKCJT9me34JcViw7PFPf7jkiI66uVGkHJ75dBHZazYx7OR9qZ6uafxFRKTixTvP9b7Ax0SS67vMrEXhElWmpZnNNrNTAdx9HZEx1veYWX8za29mg4HzgDeDOp3M7Hoz62tmbYIbHl8DtgI7jcveE0Xf3Djy04VhhyNSqSxbu5nHP5rPMT2a8+sulfceDBERqdzi7dr5HdAMOBNYFrMUygC6Ag2itp0FfE1kTPVMIvNi3ww8HuzfCvQH3gPmA68C64FD3X15nLFWKkd1a85xPVswfPw8sldvDDsckUrjzndnUeDOzSf2CDsUERGpwtLjqeTuw4BhJZTJJjKcI3rbciJT6xVXJ4fI7CBV2q0n7cun81bzt/9N58ULD9IcvSIl+GLBat75fhl/GdCZ1o336HufRUQkxWlQYgpq0aAmfx3Ulc/mr+atqT+GHY5ISsvLL2DY6Bm0alSLS47oGHY4IiJSxSm5TlHnHtyWPq0bcsc7M8ndtC3scERS1ouTfmDuig3cfGIPamakhR2OiIhUcUquU1RaNeOuU3uRuzmPu8fMDjsckZS0ct0WHv5gLr/u0pRjejQPOxwREREl16msxz71uejw9ryalcOXC9eEHY5IyrntnZlszS/gtpP31b0JIiKSEpRcp7grB3SmVaNa3PjmNLZuj50iXKTqmjBnJe9+v4zL+neifZM6YYcjIiICKLlOebWrp3PHKT1ZsGojT3+iua9FALbk5XPzW9Pp0LQOl/TvEHY4IiIiOyi5rgSO7NqME3vvzeMfzWf+yvVhhyMSusc+mkfOT5v5+yk9qZGumxhFRCR1KLmuJG49aV9q10jjr//9nvwCDzsckdDMW7GeERMXctr+LTmsY5OwwxEREdmJkutKomm9Ggw7aV++WZzL819khx2OSCgKCpyb3pxO7erp3HhC97DDERER2YWS60rkN3324ehuzbj//dn8sEaPRpeq57/fLOGr7J/4v+O60aRujbDDERER2YWS60rEzLjz1F5kVKvG9a9/T4GGh0gV8tPGbdw9ZhaZbRtxRmbrsMMREREpkpLrSqZFg5r87cTuTF74E//+anHY4YhUmL+/M5P1W7Zz56m9qFZNc1onmpn1NbNpZjbfzIZbEROHm9lvzOx7M5tqZllmdnjUvrFmlmtm71Rs5CIiqUXJdSV0RmZrDu/UhLvHzGJp7uawwxFJuo9nr+SNb5dyaf+OdG1RL+xw9lRPAkOAzsEyqIgy44H93L0P8Efgmah99wN/SHaQIiKpTsl1JWRm3H1aLxy48Y1puGt4iOy51m/J48Y3p9G5WV3+fFSnsMPZI5nZ3kB9d5/skTeUUcApseXcfYP/8oZTB/CofeMBzRUqIlWekutKqnXj2txwXDc+mbuK16YsCTsckaS5d+xslq/bwr2/7a05rZOnJRD9RrIk2LYLMzvVzGYD7xLpvRYRkShKriux3x/cloPbN+b2t2ey5OdNYYcjknCTF67hpcmL+eOv2nNAm0ZhhyOAu7/p7t2I9GzfUZa6ZnZxMFY7a9WqVckJUEQkZEquK7Fq1YwHfrcf7s61r32n2UNkj7IlL58bXv+eNo1rc80xXcIOZ0+3FGgVtd4q2FYsd58IdDCzUj/Jx91HuHumu2c2bdo0vkhFRFKckutKrnXj2tx60r5MXvgT//x8UdjhiCTMwx/MJXvNJu45rRe1q6eHHc4ezd2XAevM7JBglpDzgLdiy5lZp8JZRMzsAKAGsKZCgxURSXFKrvcAv8tsxYDuzbnv/TnMW6H7iaTy+y4nl5GfLuTsg1pzWCc94ryCDCUy+8d8YAHwHoCZXWJmlwRlTgemm9lU4B/AmYU3OJrZp8BrwNFmtsTMjq3oExARSQXqDtoDFM4ecuwjE7nqP1N549JfUT1d/zdJ5bQlL5/r/vsdTevV4P+O1yPOK4q7ZwE9i9j+VNT39wL3FlO/X/KiExGpPOLOwMzs0eDGlC1mll2Gel3M7I3gYQObzOwbM+setb+GmT1mZqvNbKOZjTazVrs7pkDTejW469ReTF+6jsc/mhd2OCJxe/iDucxdsYF7Tu9N/ZoZYYcjIiJSJuXp3qwGvEBkPtRSMbP2wOfAIuAoIr0kfwM2RBV7hMhHj2cD/YD6wDtmpjm4SjCoZwtOO6Al/5iwgG8X/xx2OCJl9nX2T4z4dCHnHNyGI7s2CzscERGRMos7uXb3y939MWBuGardCYxz92vc/Rt3X+juY9w9B8DMGgAXAte5+wfu/g2RJ371BgbEG2tVMuzkfWlerwZX/+c7Nm3bHnY4IqW2cet2rvnPd7RuVJubNBxEREQqqQobmGtm1YCTgJlmNtbMVpnZ12Z2ZlSxvkAGMK5wQ5B4zwIOq6hYK7P6NTN44Iz9yF6zkdtGzww7HJFSu3PMLHJ+3sSDZ+xHnRq6HURERCqnirzrrRlQF7iRSPI8EHgZ+JeZnRCUaQHkA6tj6q4I9u1EDyQo2mEdmzC0f0dezcrh7e9+DDsckRJ9PGcl//5yMRf368CB7RqHHY6IiEjcKjK5LmzrLXd/yN2nuvtDwH+AP8dzQD2QoHh/GdCF/ds05MY3ppHzk57eKKkrd9M2rv/v93RpXperBuphMSIiUrlVZHK9GtgOxI5VmAW0Cb5fDqQBsRPbNg/2SSllpFVj+Fn7A3DFK9+Sl18QckQiu3J3bvrfdH7auI2HzuhDzQzdtywiIpVbhSXX7r4N+BroGrOrC/BD8P0UII/IkBEAgmn4ugNfVECYe5TWjWtz12m9+HZxLo9+qOn5JPX8f3t3Hh9Vee9x/PNLQhJC2AmLQNhBEFmU1aJorVavS11areJaBa1621qt1dZavNe2au2ipS64FuuC2lpcimtFQAVFDJsgm8iOBEUMSyDhd/84J94xJCQZZuZk+b5fr/OazFnm+Z1nTmZ+85znPOfp99fy4vwNXH1cb/p3bB51OCIiIgfsQMa57mlmg4CDgEwzGxROmeHyjma2xMxOj9nsduDssK90TzMbC3yf4E5fuPsXwIPA7Wb2LTMbDDwKzAdeizfWhuyUgQdx9pDO/HXact5eUb4ru0h0Vm4uYvxzixjRvRWXj+4RdTgiIiIJcSAt1w8AHwBXAx3Cvz8gSLYhGPWjD/BVc5S7/wsYB1wLLAD+G7jA3V+Med2fAM8CkwnGxC4CTnH30gOItUH79an96NamCVdPLuCz7bujDkeE4pJSfvTkB2RmpPHnsweTnmZRhyQiIpIQBzLO9dHubhVMq8Llq8Lnj5Tb7hF37+3ujd19gLs/UW55cTiGdmt3z3H3U8rGwZb45GRm8JdzBvP5jj38ZHIBpXs96pCkgbvj5Y9YuG4bt585gPbNs6MOR0REJGFSeUGjROiQg5oz/pRDmL50MxP+szzqcKQBe3PpZu6f8THnjcjn+EP2GWFTRESkTlNy3YCcM6wzZxzWkT+/vpQZyzQuuKReYVEx1zw1j97tcrnxpH5RhyMiIpJwSq4bEDPjltP607ttU378ZAHrt+6MOiRpQPbudX761Dy27drDXecM1rB7IiJSLym5bmByMjO4+7zD2F2yl6sen8vuEo1/Lakx4Y3lTF+6mV+f0o+D2zeLOhwREZGkUHLdAPXIy+W2Mwcwd/VWfjd1cdThSAMwc1khf3ptKacP7si5w/Kr3kBERKSOUnLdQJ00oAMXf6MrD7+1iufmrY86HKnHNn6xix8/+QG92ubym9P7Y6Zh90REpP5Sct2A3XBiX4Z2bcl1z8xj4bovog5H6qE9pUH3o517Srl7zOHkZGZEHZKIiEhSKbluwDIz0rjnvMNplZPJuElzKCwqjjokqWduf2kJcz75nFvPHEDPtrlRhyMiIpJ0Sq4buDa5WUy8YAif7djND//+vi5wlISZumAD98/4mAtHduHUgQdVvYGIiEg9oORa6N+xObd/dyDvrfqc8c8vijocqQcWb9jGNU/PY3B+C35xUt+owxEREUkZdYAUAE4deBCLN2zjnmkr6NehGeeN6BJ1SFJHfbZ9N2MnzaFpdgb3nXc4WRkaz1pERBoOtVzLV649vg/H9Mlj/HOLeHtFYdThSB20p3QvVz42l0+/LOa+84fQtll21CGJiIiklJJr+Up6mnHnOYPp1qYJlz/6Pss//TLqkKSO+c2Li3ln5RZ+d/qhDOrcIupwREREUk7JtXxNs+xGPHTRUDIz0rno4ffY/KVGEJHqmfzeah55exWXjurGmYd3ijocERGRSCi5ln10bpXDgxcOobComEsnzWHn7tKoQ5JabvbKLfzqX4s4slcbrj/x4KjDERERiYySa6nQwM4tuOv7g5m/dis/mfwBpXs96pCkllqxuYhxj75P51aNmXDOYWSk62NFREQaLn0LSqWOP6Q9vzqpHy8v2sRv/7046nCkFiosKubih9+jUbrxyMXDaJ7TKOqQREREIqWh+GS/fjCqG6s/28GDMz8mr2kWl4/uEXVIUkvs2lPK2Elz2LRtF0+OG0HnVjlRhyQiIhI5JddSpV+d3I/ComJunbqEljmNOHtoftQhScT27nWunlxAwZqt3DPmMAbnt4w6JBERkVpBybVUKT3N+ONZg/hi5x5u+OcCmjfO5IT+7aMOSyLi7tzy4mKmLtzIjSf15YT+iXxUKAAAG5FJREFUHaIOSUREpNZQn2uplsyMNO47/3AGdm7Bj574QDeZacAm/Gc5D731MRcd0ZVLRnWLOhwREZFaJe7k2szyzex5M9tuZoVmdpeZZVaxzf1mtsLMdprZZjObYmZ9Y5YfbWZeyfS9eGOVxMjJzODhi4bStU0OY/82h/lrt0YdkqTYo7M+4Q+vLuX0wR256eR+mFnUIUmCmNnhZrbAzJaHn+f7vLlmdrCZvWNmxWZ2bbllq8LtC8xsTuoiFxGpXeJKrs0sHXgRaAocCZwDfBf4QxWbzgEuAvoC3wYMeM3MyoYYeBvoUG76HVAETI0nVkmsFjmZTPrBcFo2yeSCh97lw/Xbog5JUmRKwTpumrKQYw9uy+3fHUBamhLreuYeYCzQK5xOqGCdz4AfAXdU8hrHuPsgdx+SnBBFRGq/eFuujwcOAc5397nu/ipwHTDWzJpVtpG73+fuM9x9lbvPBW4EDgK6h8t3u/vG2IkgaX/C3YvijFUSrH3zbB6/dAQ5jdIZ88AslmxUgl3fTfvoU655ah5Du7Tir2MOo5HGsq5XzKwD0MzdZ7m7A5OA08qv5+6fuvt7wJ5UxygiUlfE+w05Eljs7mti5r0MZAGHV+cFzKwJcDGwGlhVyTpHE7SgTIwzTkmS/NY5PDFuBFkZ6Yy5fzZLN30ZdUiSJG+vKOTyv79P73ZNeeCiIWQ3So86JEm8jsDamOdrw3k14cArZva+mY2raAUzG2dmc8xszubNm+MMVUSkdos3uW4PbCo3rxAoDZdVysyuMLMigq4eJwLHuntxJauPAwrcvcL+e/qgjlaX1k14YtwIMtKNc++fxTIl2PXOOyu28INH3iO/VQ6TLhlGs2zdJEYqNcrdDyP4XL/SzI4qv4K7T3T3Ie4+JC8vL/URioikQBTndh8DBgOjgaXA02a2z90nzKw1cAZwf2UvpA/q6HVr04THx47AzDjn/tl8tFEJdn0xe2WQWHdumcPjY0fQJjcr6pAkedYBnWKedwrnVZu7rwsfPwWeBYYlLDoRkTok3uR6I9Cu3Lw2QHq4rFLu/oW7L3P36QT9qXsDZ1aw6gUELeGPxRmjpEiPvFyeGDuC9DQ4e+I7zFujUUTqunc//oyLH3mPji0bK7FuANx9A7DNzEaEo4RcAEyp7vZm1sTMmpb9TXBdzsKkBCsiUsvFm1y/A/Q1s9iWjuOAYuD9GryOhVNF39yXAk+7+xdxxigp1LNtLk9fdgRNszMY88BsZq3cEnVIEqe3VxRy0cPv0qF5No+PHU5eUyXWDcQVwAPAcmAF4QhNZna5mV0e/t3ezNYCPwVuNLO14UXs7YCZZjYPeBd40d1fimInRESiFu8dGl8BFgGTzOwaoDXwe+B+d98GYGbDCK44v8Dd3zWzngQt1K8BmwlOO15PkJC/EPviZjYK6EfQ51rqiPzWOTx92RGc/+BsLnzoXe4973COObht1GFJDbz64SaufHwu3Vo34dFLhtG2aXbUIUmKhNe29K9g/r0xf2/k691HymwDBiYvOhGRuiOulmt3LwVOAnYAbwGTgX8AsTcVyAH6hI8QJNFHE7SGLA+3+RIYGX5gxxpLMBrJW/HEJ9Fp3zybyZeNpHe7poydNIcpBTXqtikRevaDtVz+9/fp26EZky8bQdtmSqxFRERqKt6Wa9x9NXDyfpZPI+jyUfZ8DcFV5NV57QvjjUui16pJJo+PHc6lf5vDj58sYP3WXVw+urvu5leLTXpnFTdNWcQRPVoz8YIh5GbF/dEgIiLSoOlOEJIUTbMbMemSYZw68CBue2kJv/zXQkpK90YdlpTj7vzhlY+4acoijuvXjocuGqrEWkRE5ADoW1SSJisjnT+fPYhOLRtz97QVrN+6kwnnHqbkrZYoLinlumfmM6VgPWcP6cwtp/fXnRdFREQOkL5JJanS0ozrTjiY355+KDOWFXLWve+wbuvOqMNq8Lbu2M35D77LlIL1/Ozbfbj1zEOVWIuIiCSAvk0lJc4dns+DFw5hzWc7OOUvM3lnhYbqi8qqwu2ccc/bFKzeyl3nDObKY3qqP7yIiEiCKLmWlDm6T1umXPUNWjXJ5LwHZ/PQzI9x96jDalDeWPIpp06Yyefbd/PY2OGcOvCgqEMSERGpV5RcS0p1z8vl2SuO4NiD2/I/L3zINU/NY8fukqjDqvf27nX+8voyfvC39+jUMofnrhrF0K6tog5LRESk3lFyLSnXNLsR9553OFd/qzfPFqzj1AlvsWTjtqjDqre27drD5X9/nz+8upTvDDyIf/zwCDq3yql6QxEREakxJdcSibQ048ff6sWjPxjO1h17+M6Et3hs9ifqJpJgc1d/zn/dOYPXl3zKTSf3409nD6JxZnrUYYmIiNRbSq4lUqN6tWHqj49kWLdW/PLZhVz5+Fy27tgddVh13t69zt3TlvO9e9/BHZ66bCQ/GNVNFy6KiIgkmQYclsjlNc3ibxcPY+KMldzx8ke8t+pzfnf6oXyrX7uoQ6uT1m/dyc+emcdby7dw0qEd+O0Zh9K8caOowxIREWkQ1HIttUJamnH56B7868pv0LpJJpdOmsPVkwvUil0D7s4T767m+D9NZ+4nW7n1jEOZcO5gJdYiIiIppJZrqVX6d2zOc1eNYsIby7n7jeXMXF7I+FMO4b8Oba8uDfux5rMd3PDPBcxcXsjI7q257cwB5LfWRYsiIiKppuRaap3MjDR+elxvju/Xjuuemc+Vj89lVM82jD/1EHq2zY06vFqluKSUh2au4i//WYYBt5zWn3OH5ZOWph8iIiIiUVByLbVW0Ir9DR6bvZo7XvmIE++cziWjunPVN3uSm6VD982lm7n5uUWsLNzO8f3acdMp/ejUUq3VIiIiUVKGIrVaRnoaFx7RlZMGdODWqUu4980VPD1nDVd9syfnDs8nK6PhDSu3dNOX3P7SR7y2eBPd2jThkYuHcnSftlGHJSIiIii5ljqiTW4Wd3xvIOeP6MJtLy3h5uc/5MGZH3PN8b05dWBH0htAN4g1n+3gT68u5dmCdeRmZnDdCX24ZFS3BvkDQ0REpLZSci11ysDOLXjs0uHMWFbIbS8t4erJ87jztWVcNroHZxzWsV4mmp9s2c7E6St5as4a0swYd2R3Lh/dg5ZNMqMOTURERMpRci11jplxVO88RvVswysfbuTuaSu44Z8L+NOrS7lkVDfOHtqZFjl1P/Gct2YrE6evZOrCDWSkpXHWkM789zd70b55dtShiYiISCWUXEudlZZmnNC/A98+pD1vLd/CPW8u53dTl/CHV5dy8oAOjBnehcPyW9SpIfx27C7hhfkbeOLd1XyweitNszO4bHQPLj6iK22bKakWERGp7ZRcS51nZozq1YZRvdqweMM2Hpv9Cc/OXcc/566jT7umnDroIE4e0IEurZtEHWqFSvc67636jOfnree5gvV8WVxCj7wm3HhSX84e2pmm2boJjIiISF0RV3JtQVPgr4FxQEtgNnCluy+qYrsfAz8EugBbgCnAz929KFw+PnzdWJvcvX08cUrD07dDM2457VCuP7EvUwrW8Y/31/L7lz/i9y9/xIBOzTmxfwdG986jb4emkbZo79pTyvuffM4rizby74Ub2fxlMdmN0jixfwfOGZbP0K4t61SLu4iIiATibbm+DrgGuAj4CLgJeNXM+rj7lxVtYGbnArcDlwIzgO7Ag0A2cEnMqh8BR8c8L40zRmnAcrMyGDO8C2OGd2Hd1p28OH89z8/bwG0vLeG2l5aQ1zSLo3rlMbxbKwbnt6BHXm5Sb7yya08pi9Zv44PVnzNjWSGzP97Crj17ycpI45g+bTlpQAe+eXBbmmj8bhERkTqtxt/kYav1T4Bb3f0f4bwLgU+Bc4H7Ktn0CGCWuz8aPl9lZpOAM8utV+LuG2sal0hlOrZozLijejDuqB5s2raL6Us38+bSzby+ZBP/mLsWgKZZGRzaqTm92ubSs20uPfJy6dQyh7bNsshuVL0RSNyd7btL2fjFLj4u3M7HhUWs3LydReu3sXjDNkr2OgA98prw/aH5HNmrDSO6t1ZCLSIiUo/E863eDWgPvFI2w913mtl0ggS6suR6JnC+mY1w91lmlg+cCvy73HrdzWw9UEzQ3eQX7r4yjjhF9tGuWTbfG9KZ7w3pzN69zsrC7RSs2coHqz9nwbov+MfcdRQVl3xtm6bZGeTlZtE4M53sRulkN0ojPS2N3SWl7C7Zy+7SvXy+fQ+FRcUUl+z92ratm2TSp31Txh3VnYGdWzCocwva6cJEERGReiue5Lqs//OmcvM3AR0r28jdnzSz1sD0sPU7A3gU+HnMarMJuposAdoCNwJvm9kh7r6l/Gua2TiCft/k5+fHsSvSkKWlGT3DlurvHt4JCFqfN20rZsXmItZt3cnmL4uDqaiYXbtL2VVSyq49eynZW0pWeho5mRk0Tzd6t21K69xM2uRmkdc0i25tmtC9TS7Nc3QxooiISENSZXJtZmP4emv0SfEUZGajgV8BVxAk0T2BO4GbCfps4+5Ty20zC1gJXAj8sfxruvtEYCLAkCFDPJ64RGKZGe2bZ2ssaREREYlLdVqunyNIhstkhY/tgNUx89sB++srfQvwhLs/ED5fYGZNgAfM7H/cvaT8Bu5eZGaLgF7ViFNEREREJFJpVa3g7l+6+/KyCfiQIIk+rmwdM8sGjgTe3s9L5bDvyB+lQKVDNISvezCwoao4RUQkPha4y8yWm9l8MzuskvUON7MF4Xp3hV38MLNWZvaqmS0LH1umdg9ERGqPKpPr8tzdgT8DPzezM8ysP/AIUAQ8Xraemb1uZr+L2fR5YJyZfd/MupnZccD/Ai+UtVqb2R1mNjpcPhx4BmgC/C3O/RMRkaqdSHCGsBfBdSz3VLLePcDYmHVPCOdfD7zu7r2A18PnIiINUrxjgN0ONAb+yv/fROb4cmNc9wDWxDy/BXCChLoTUEiQcP8yZp1OwBNAG2AzMAsY4e6fxBmniIhU7TvApLDxZJaZtTCzDu7+1VlDM+sANHP3WeHzScBpwNRw+6PDVf8GTOPrF6uLiDQYcSXX4Qfw+HCqbJ2u5Z6XEFy8ePN+tvl+PPGIiMgB6cjXG0PWhvM2lFtnbQXrALSLScQ3ElyDsw+N8FQ9q26Na9yAOlOeSH1X424hIiIilQkbXyocvcndJ7r7EHcfkpeXl+LIRERSQ8m1iEgDZGZXmlmBmRUQtFB3jlncCVhXbpN14fyK1tkUdhsp6z7yaXKiFhGp/ZRci4g0QO7+V3cf5O6DgH8BF4SjhowAvojtbx2uvwHYZmYjwlFCLgCmhIufI7gfAeHjFEREGigl1yIi8m+CG3YtB+4nuNkXAGHLdpkrgAfC9VYQXMwIcCtwnJktA74VPhcRaZDiHS1ERETqibCf9JWVLBsU8/ccoH8F62wBjk1agCIidYharkVEREREEkTJtYiIiIhIgii5FhERERFJECXXIiIiIiIJYsF1LHWfmW0G4rlNehuCW7HL16le9qU62ZfqZF/x1kkXd28wd1Y5gM/smkr1Mary6n6ZKq9ul5eqMiv9zK43yXW8zGyOuw+JOo7aRvWyL9XJvlQn+1Kd1C6pfj9UXt0vU+XV7fKiKjOWuoWIiIiIiCSIkmsRERERkQRRcg0Tow6gllK97Et1si/Vyb5UJ7VLqt8PlVf3y1R5dbu8qMr8SoPvcy0iIiIikihquRYRERERSRAl1yIiIiIiCaLkWkRE6jQz62xmH5tZq/B5y/B5VzN7ycy2mtkLKShvkJm9Y2aLzGy+mZ2dgjJHm9lcMysIy708yeV1DZ83M7O1ZjYh2eWZWWm4fwVm9lwKyss3s1fMbLGZfVi2z0ks8+KY/Ssws11mdloSy+tqZreHx8tiM7vLzCzJ5d1mZgvDKe7/i3j+182sm5nNNrPlZjbZzDIPbE+rwd3rzQScAbwMbAYcOLqG248CSoCFFSw7E/gQKA4fT496f2uwXwaMB9YDO4FpwCFVbDMtrMPy06KYdQ4BngFWhsvGR72vyayT6hwHwP8CS4DtwOfA68ARUe9vDeolH3g+jL8QuAvIrMZ2w4BXgSLgS+BtoE3M8lUVHEu3Rr2/1ayTO4E5wC5gVTW3qfI4AO4HVoTH32ZgCtA36v2tqxNwHTAx/Ps+4Ibw72OBU4AXkl0e0BvoFc47CNgAtEhymZlAVjgvN/xfOyiZdRo+vxN4HJiQgvewKMXHzDTguJg6zUl2mTHLWwGfJarMSo6ZI4C3gPRweoca5ks1LO+k8PshA2gCvAc0S8L7VuH/OvAU8P3w73uBHybjePpamckuIJUTcD7w6/CxRsk10JIgSXyZcsk1MJIg6f4l0Dd8LAGGR73P1dy3nxMkPGcC/cMDbT3QdD/btALax0xdgG3Ar2PWGQrcAZwb1t34qPc1yXVS5XEAnBf+g3cn+PHxQFhv7aLe52rUSTqwIPxiOQw4LqyTv1Sx3XBga1gf/QkSjDOA5jHrrAJuLndM5Ua9z9Wsl78A/01w9fmqam5T5XEAXAYcCXQN6/s5YB3QKOp9rosT0AiYD/wEWBRbj8DR5b9wk1lezDrzCJPtVJQJtAZWk7jkusLygMOBJ4GLSGxyXVl5yUqu9ykP6AfMjOI4DZePAx5L8j6OBN4HGgM5BI0HCflhX0l5PwN+FbPOg8BZyajD8v/rBA1phUBG+Hwk8HKy3t+vyk12AVFMBLe9rGly/U+CxHw8+ybXk4FXy817DXgi6n2txn4ZQevJL2PmNSZILC+rweuMIUgkO1eyfCF1JLmOt07iOQ6AZuGx+O2o97sa9XIisDf2PSZIEnexn1YGglbq31Tx2quAa6PexwOsn2upZnIdz3EADAjX6RP1vtbVCfh2WIfHlZv/tS/cZJcXLhsGLAbSkl0m0DlMNnYAVyazPILupNOATiQ4ud7P/pUQJICzgNOSvH+nAS+EOcEHwO+B9BQeN/8BTk5Bnd5B0CjyRVWf3wmo0+MJWspzCPKzlcA1yajD8v/rYXnLY553poLeCYme1OcaMLMrgHbALZWsMhJ4pdy8lwlOrdR23QhaCb+K3913AtOpWfxjgZfcfU1iw4tEvHVSo+Mg7Nc1jqDFsuAA4k2VkcDicu/xy0AWQUvVPsysbbjdBjObaWafmtkMMzu2gtWvNbMtYZ/CX6ak31stUJ3jwMyaABcTtDquSllw9c+JBD+c+0dZnpl1AB4FLnb3vcku093XuPsAoCdwoZm1S2J5VwD/dve1CSxjf+UBdPHgVtbnAn82sx5JLC+D4IzStQRnZ7sT/IhIpP0dN4cSfO4mrTwz60lw9rUT0BH4ppkdmazy3P0V4N8EDTFPEHRDKU1kGbVNg0+uzexQghbr89y9sje7PbCp3LxN4fzarizGuOM3s97AaII+ovVBvHVSrePAzE42syKCFt+rCX5Zl9+uNqpo/woJPgQrq5fu4ePNwEMErQkzgJfNbGDMencB5wDHABMI6uXuxIRdO1XnODCzK8J1igi+LI519+LUR1v3mdkggq5MI4Crw0Ql5eWZWTPgRYIzY7NSUWYZd19PcBYxIYlSJeWNBK4ys1UErZ8XmNmtSSwPd18XPq4kaDUfnMTy1gIF7r7S3UuAfxF020qIKt7Ds4Bn3X1Pkss7HZjl7kXuXgRMJXhfk1Ue7v4bdx/k7scRnD1emugyKrEFaGFmGeHzTgTd75KqzibXZjbGzIpiphp/mJhZFsGp/mvd/ePER5l65euFoG/SgRpL8AvxxQS8VsolqU725w1gEEGL9kvAU8n+oo9Q2WfIfe7+kLt/4O6/ILhg5atRC9z9j+7+hrvPd/cHCFq/LjGz1hHEnCrVOQ4eI0gURhN82TxtZjkpjbIeCEc6uAf4ibuvJjiVf0eqywvPUjwLTHL3Z1JUZiczaxyu05LgwvyPklWeu49x93x370rQujvJ3a9PVnnhaBBZ4TptgG8QXEyelPIIPrtamFleuOo3E1FeFWWWOYegZTch9lPeamC0mWWYWSOCz5/FySrPzNLLPuvNbABBF7jyZ4EPdJ8q5EFfkDeA74azLiS4eDyp6mxyTXDxz6CYaU4cr9GB4NTIw2ZWYmYlwE3AIeHz48P1NhJ0G4nVLpxf25Svl8Jwflzxh18WFwIPh7/i66JE1Um1jgN33+7uy919lrtfAuwBLo0z9lSqaP/aEFzoWFm9bAgfy3/5fEgw8khlZoePPWsSYF1SnePA3b9w92XuPp3gw783wUW2UjNjgdXu/mr4/G6grwXD1M0AngaOtWDouG8nqzyCkRGOAi6y/x9WbVACyttfmZcAs81sHvAmQQK8IFnlmdnoBLx2tcsjSMTmhPv3BsEoQ4lIdisrbxTBj4bXzWwBQStros7a7u847UrQH/jNBJVVaXkEn+crCC5gnwfMc/fnk1jeKGCGmX1IcGH4eQeQT8Tzv/5z4Kdmtpzgot8H4yy72url7c/DX7ebgWPcfdp+1msE9Ck3+wqC0w2nE1y8VGRmk4GW7n58zLavAFvc/ZxEx59I4a+8shEffhvOywY+BX7m7vdVsf1ZBFeF9wxPyVW23kLgGXcfn6jYkyXeOon3ODCzFQQXPd6YwN1IODM7keDsRH5Zf0ozO5egu0dbd99WwTZGcBr1IXf/Vcz8GcACd7+ikrK+Q3C6tUvY+lDrmdm1wFVhi1082+/3OAhb5z4HfhS27ouISB2UUfUqdYcFg4rnAy3CWT3NbCuw0d03hutMAnD3C8J+TQvLvcanQLG7x86/E5huZtcTJASnE/QdHZXM/UkEd3cz+zPwCzNbQnDq+UaCPp6Pl61nZq8D77r7DeVeYhzwekWJddiq3S98mg20D1tpitx9eeL3JjEOoE72exxY0NfyOoJxojcAecCVBH28nkrBrh2oVwiGNZpkZtcQ/ML/PXB/WWJtZsOAScAF7v5uWJe/B242s/kEV9efRdAX7qpwm5Hh8zcIrkwfCvwJeK4uJNYWXPyTSzBucWZMS+SH7r7bzDoSjGN9g7s/W53jIHzNMwlGm9kcLrueYPz0hN3sREREIlDZMCJ1cSK4otcrmMbHrDMNmLaf1xhPxTeR+S7BTSF2E/RNOiPq/a1BvZTdMGUDwcVVbwL9y62zCnik3LzuBEOzVTgeJcH4vBXVd6X1W1umA6iTSo8DgmGGniVoFS8OH6dQR8ZDD/chnyC520FwIchdhDeoCJcfTQXDXBKcdltNcNOUd4FvxSw7jGAIra0EN0xZEtZ9wm7MkOQ6mVbJcd41XF72f3BRdY8DgtO/UwnOluwG1hD0vz446v3VpEmTJk0HNtXLbiEiIiIiIlGoyxc0ioiIiIjUKkquRUREREQSRMm1iIiIiEiCKLkWEREREUkQJdciIiIiIgmi5FpEREREJEGUXIuIiIiIJIiSaxERERGRBPk/HN5E0/U9pXgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x1296 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ExNN(meta_info=meta_info,\n",
    "               subnet_num=10,\n",
    "               subnet_arch=[10, 6],\n",
    "               task_type=task_type,\n",
    "               activation_func=tf.tanh,\n",
    "               batch_size=min(1000, int(train_x.shape[0] * 0.2)),\n",
    "               training_epochs=10000,\n",
    "               lr_bp=0.001,\n",
    "               lr_cl=0.1,\n",
    "               beta_threshold=0.05,\n",
    "               tuning_epochs=100,\n",
    "               l1_proj=best_l1_prob,\n",
    "               l1_subnet=best_l1_subnet,\n",
    "               smooth_lambda=10**(-6),\n",
    "               verbose=True,\n",
    "               val_ratio=0.2,\n",
    "               early_stop_thres=500)\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"exnn_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.01115 1.03267 0.99258]\n"
     ]
    }
   ],
   "source": [
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "\n",
    "mse_stat = np.hstack([np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(tr_pred) - meta_info[\"Y\"][\"scaler\"].inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(val_pred) - meta_info[\"Y\"][\"scaler\"].inverse_transform(model.val_y))**2),5),\\\n",
    "               np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(pred_test) - meta_info[\"Y\"][\"scaler\"].inverse_transform(test_y))**2),5)])\n",
    "print(mse_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape_cl:\n",
    "    with tf.GradientTape() as tape_bp:\n",
    "        pred = model.apply(inputs, training=True)\n",
    "        pred_loss = model.loss_fn(labels, pred)\n",
    "        regularization_loss = tf.math.add_n(model.proj_layer.losses + model.output_layer.losses)\n",
    "        cl_loss = pred_loss + regularization_loss\n",
    "        bp_loss = pred_loss + regularization_loss\n",
    "        if model.smooth_lambda > 0:\n",
    "            smoothness_loss = model.subnet_blocks.smooth_loss\n",
    "            bp_loss += smoothness_loss\n",
    "\n",
    "train_weights_list = []\n",
    "trainable_weights_names = [self.trainable_weights[j].name for j in range(len(model.trainable_weights))]\n",
    "for i in range(len(model.trainable_weights)):\n",
    "    if self.trainable_weights[i].name != model.proj_layer.weights[0].name:\n",
    "        train_weights_list.append(self.trainable_weights[i])\n",
    "\n",
    "grad_proj = tape_cl.gradient(cl_loss, model.proj_layer.weights)\n",
    "grad_nets = tape_bp.gradient(bp_loss, train_weights_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
