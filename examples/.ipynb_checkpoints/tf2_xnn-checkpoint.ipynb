{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "from xnn.xnn import xNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(label, pred, scaler):\n",
    "    pred = scaler.inverse_transform(pred.reshape([-1, 1]))\n",
    "    label = scaler.inverse_transform(label.reshape([-1, 1]))\n",
    "    return np.mean((pred - label)**2)\n",
    "\n",
    "def simu_loader(generator, datanum, testnum, noise_sigma):\n",
    "    def wrapper(rand_seed=0):\n",
    "        return generator(datanum, testnum=testnum, noise_sigma=noise_sigma, rand_seed=rand_seed)\n",
    "    return wrapper\n",
    "\n",
    "def data_generator1(datanum, testnum=10000, noise_sigma=1, rand_seed=0):\n",
    "    \n",
    "    corr = 0.5\n",
    "    np.random.seed(rand_seed)\n",
    "    proj_matrix = np.zeros((10, 4))\n",
    "    proj_matrix[:7, 0] = np.array([1,0,0,0,0,0,0])\n",
    "    proj_matrix[:7, 1] = np.array([0,1,0,0,0,0,0])\n",
    "    proj_matrix[:7, 2] = np.array([0,0,0.5,0.5,0,0,0])\n",
    "    proj_matrix[:7, 3] = np.array([0,0,0,0,0.2,0.3,0.5])\n",
    "    u = np.random.uniform(-1, 1, [datanum + testnum, 1])\n",
    "    t = np.sqrt(corr / (1 - corr))\n",
    "    x = np.zeros((datanum + testnum, 10))\n",
    "    for i in range(10):\n",
    "        x[:, i:i + 1] = (np.random.uniform(-1, 1, [datanum + testnum, 1]) + t * u) / (1 + t)\n",
    "\n",
    "    y = np.reshape(2 * np.dot(x, proj_matrix[:, 0]) + 0.2 * np.exp(-4 * np.dot(x, proj_matrix[:, 1])) + \\\n",
    "                   3 * (np.dot(x, proj_matrix[:, 2]))**2 + 2.5 * np.sin(np.pi * np.dot(x, proj_matrix[:, 3])), [-1, 1]) + \\\n",
    "              noise_sigma * np.random.normal(0, 1, [datanum + testnum, 1])\n",
    "    \n",
    "    task_type = \"Regression\"\n",
    "    meta_info = {\"X1\":{\"type\":\"continuous\"},\n",
    "             \"X2\":{\"type\":\"continuous\"},\n",
    "             \"X3\":{\"type\":\"continuous\"},\n",
    "             \"X4\":{\"type\":\"continuous\"},\n",
    "             \"X5\":{\"type\":\"continuous\"},\n",
    "             \"X6\":{\"type\":\"continuous\"},\n",
    "             \"X7\":{\"type\":\"continuous\"},\n",
    "             \"X8\":{\"type\":\"continuous\"},\n",
    "             \"X9\":{\"type\":\"continuous\"},\n",
    "             \"X10\":{\"type\":\"continuous\"},\n",
    "             \"Y\":{\"type\":\"target\"}}\n",
    "    for i, (key, item) in enumerate(meta_info.items()):\n",
    "        if item['type'] == \"target\":\n",
    "            sy = MinMaxScaler((-1, 1))\n",
    "            y = sy.fit_transform(y)\n",
    "            meta_info[key][\"scaler\"] = sy\n",
    "        elif item['type'] == \"categorical\":\n",
    "            enc = OrdinalEncoder()\n",
    "            enc.fit(x[:,[i]])\n",
    "            ordinal_feature = enc.transform(x[:,[i]])\n",
    "            x[:,[i]] = ordinal_feature\n",
    "            meta_info[key][\"values\"] = enc.categories_[0].tolist()\n",
    "        else:\n",
    "            sx = MinMaxScaler((-1, 1))\n",
    "            x[:,[i]] = sx.fit_transform(x[:,[i]])\n",
    "            meta_info[key][\"scaler\"] = sx\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=testnum, random_state=rand_seed)\n",
    "    return train_x, test_x, train_y, test_y, task_type, meta_info\n",
    "\n",
    "train_x, test_x, train_y, test_y, task_type, meta_info = data_generator1(datanum=10000, testnum=10000, noise_sigma=1, rand_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xnn_repeat(folder, name, data_generator,\n",
    "                  subnet_num=10,\n",
    "                  subnet_arch=[10, 6],\n",
    "                  task=\"Regression\",\n",
    "                  activation_func=tf.tanh,\n",
    "                  lr_bp=0.001,\n",
    "                  l1_proj=0.001,\n",
    "                  l1_subnet=0.001,\n",
    "                  batch_size=1000,\n",
    "                  training_epochs=5000,\n",
    "                  tuning_epochs=500,\n",
    "                  beta_threshold=0.05,\n",
    "                  verbose=False,\n",
    "                  val_ratio=0.2,\n",
    "                  early_stop_thres=1000,\n",
    "                  rand_seed=0):\n",
    "\n",
    "    train_x, test_x, train_y, test_y, task_type, meta_info = data_generator(rand_seed)\n",
    "\n",
    "    input_num = train_x.shape[1]\n",
    "    model = xNN(input_num=input_num,\n",
    "                   meta_info=meta_info,\n",
    "                   subnet_num=10,\n",
    "                   subnet_arch=subnet_arch,\n",
    "                   task_type=task_type,\n",
    "                   activation_func=tf.tanh,\n",
    "                   batch_size=min(batch_size, int(train_x.shape[0] * 0.2)),\n",
    "                   training_epochs=training_epochs,\n",
    "                   lr_bp=lr_bp,\n",
    "                   beta_threshold=beta_threshold,\n",
    "                   tuning_epochs=tuning_epochs,\n",
    "                   l1_proj=l1_proj,\n",
    "                   l1_subnet=l1_subnet,\n",
    "                   verbose=verbose,\n",
    "                   val_ratio=val_ratio,\n",
    "                   early_stop_thres=early_stop_thres)\n",
    "    model.fit(train_x, train_y)\n",
    "    model.visualize(folder=folder,\n",
    "                    name=name,\n",
    "                    save_eps=False)\n",
    "\n",
    "    tr_pred = model.predict(model.tr_x)\n",
    "    val_pred = model.predict(model.val_x)\n",
    "    pred_test = model.predict(test_x)\n",
    "\n",
    "    if task_type == \"Regression\":\n",
    "        stat = np.hstack([np.round(mse(model.tr_y, tr_pred, meta_info[\"Y\"][\"scaler\"]), 5),\\\n",
    "                              np.round(mse(model.val_y, val_pred, meta_info[\"Y\"][\"scaler\"]), 5),\\\n",
    "                              np.round(mse(test_y, pred_test, meta_info[\"Y\"][\"scaler\"]), 5)])\n",
    "    elif task_type == \"Classification\":\n",
    "        stat = np.hstack([np.round(auc(model.tr_y, tr_pred), 5),\\\n",
    "                          np.round(auc(model.val_y, val_pred), 5),\\\n",
    "                          np.round(auc(test_y, pred_test), 5)])\n",
    "\n",
    "    res_stat = pd.DataFrame(np.vstack([stat[0], stat[1], stat[2]]).T, columns=['train_metric', \"val_metric\", \"test_metric\"])\n",
    "    res_stat[\"Subnet_Number\"] = min(input_num, 10)\n",
    "    res_stat[\"lr_BP\"] = lr_bp\n",
    "    res_stat[\"L1_Penalty_Proj\"] = l1_proj\n",
    "    res_stat[\"L1_Penalty_Subnet\"] = l1_subnet\n",
    "    res_stat[\"Training_Epochs\"] = training_epochs\n",
    "    return res_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_metric</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>Subnet_Number</th>\n",
       "      <th>lr_BP</th>\n",
       "      <th>L1_Penalty_Proj</th>\n",
       "      <th>L1_Penalty_Subnet</th>\n",
       "      <th>Training_Epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99941</td>\n",
       "      <td>1.02802</td>\n",
       "      <td>0.99131</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00006</td>\n",
       "      <td>1.02963</td>\n",
       "      <td>0.99138</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00077</td>\n",
       "      <td>1.03102</td>\n",
       "      <td>0.99087</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00694</td>\n",
       "      <td>1.03506</td>\n",
       "      <td>0.99288</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00873</td>\n",
       "      <td>1.03596</td>\n",
       "      <td>0.99593</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.01114</td>\n",
       "      <td>1.03796</td>\n",
       "      <td>0.99696</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08260</td>\n",
       "      <td>1.11844</td>\n",
       "      <td>1.06516</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08183</td>\n",
       "      <td>1.12035</td>\n",
       "      <td>1.06763</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08229</td>\n",
       "      <td>1.12094</td>\n",
       "      <td>1.06543</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.08268</td>\n",
       "      <td>1.12336</td>\n",
       "      <td>1.06828</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.09455</td>\n",
       "      <td>1.12725</td>\n",
       "      <td>1.06972</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.12611</td>\n",
       "      <td>1.15771</td>\n",
       "      <td>1.10765</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.12714</td>\n",
       "      <td>1.16161</td>\n",
       "      <td>1.10889</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20808</td>\n",
       "      <td>1.23993</td>\n",
       "      <td>1.20099</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20015</td>\n",
       "      <td>1.24123</td>\n",
       "      <td>1.19787</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20246</td>\n",
       "      <td>1.24270</td>\n",
       "      <td>1.19894</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20623</td>\n",
       "      <td>1.24299</td>\n",
       "      <td>1.19839</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20499</td>\n",
       "      <td>1.24318</td>\n",
       "      <td>1.20091</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20472</td>\n",
       "      <td>1.24319</td>\n",
       "      <td>1.19759</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20349</td>\n",
       "      <td>1.24482</td>\n",
       "      <td>1.20041</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.20706</td>\n",
       "      <td>1.24552</td>\n",
       "      <td>1.20216</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.21744</td>\n",
       "      <td>1.25519</td>\n",
       "      <td>1.20869</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.22369</td>\n",
       "      <td>2.26434</td>\n",
       "      <td>2.19919</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.21995</td>\n",
       "      <td>2.26515</td>\n",
       "      <td>2.19670</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.21911</td>\n",
       "      <td>2.26902</td>\n",
       "      <td>2.19824</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_metric  val_metric  test_metric  Subnet_Number  lr_BP  \\\n",
       "0       0.99941     1.02802      0.99131             10  0.001   \n",
       "0       1.00006     1.02963      0.99138             10  0.001   \n",
       "0       1.00077     1.03102      0.99087             10  0.001   \n",
       "0       1.00694     1.03506      0.99288             10  0.001   \n",
       "0       1.00873     1.03596      0.99593             10  0.001   \n",
       "0       1.01114     1.03796      0.99696             10  0.001   \n",
       "0       1.08260     1.11844      1.06516             10  0.001   \n",
       "0       1.08183     1.12035      1.06763             10  0.001   \n",
       "0       1.08229     1.12094      1.06543             10  0.001   \n",
       "0       1.08268     1.12336      1.06828             10  0.001   \n",
       "0       1.09455     1.12725      1.06972             10  0.001   \n",
       "0       1.12611     1.15771      1.10765             10  0.001   \n",
       "0       1.12714     1.16161      1.10889             10  0.001   \n",
       "0       1.20808     1.23993      1.20099             10  0.001   \n",
       "0       1.20015     1.24123      1.19787             10  0.001   \n",
       "0       1.20246     1.24270      1.19894             10  0.001   \n",
       "0       1.20623     1.24299      1.19839             10  0.001   \n",
       "0       1.20499     1.24318      1.20091             10  0.001   \n",
       "0       1.20472     1.24319      1.19759             10  0.001   \n",
       "0       1.20349     1.24482      1.20041             10  0.001   \n",
       "0       1.20706     1.24552      1.20216             10  0.001   \n",
       "0       1.21744     1.25519      1.20869             10  0.001   \n",
       "0       2.22369     2.26434      2.19919             10  0.001   \n",
       "0       2.21995     2.26515      2.19670             10  0.001   \n",
       "0       2.21911     2.26902      2.19824             10  0.001   \n",
       "\n",
       "   L1_Penalty_Proj  L1_Penalty_Subnet  Training_Epochs  \n",
       "0         0.000100           0.000100            10000  \n",
       "0         0.000100           0.000316            10000  \n",
       "0         0.000100           0.001000            10000  \n",
       "0         0.000316           0.000100            10000  \n",
       "0         0.000316           0.000316            10000  \n",
       "0         0.000316           0.001000            10000  \n",
       "0         0.003162           0.001000            10000  \n",
       "0         0.003162           0.000100            10000  \n",
       "0         0.001000           0.001000            10000  \n",
       "0         0.003162           0.000316            10000  \n",
       "0         0.001000           0.003162            10000  \n",
       "0         0.001000           0.000100            10000  \n",
       "0         0.001000           0.000316            10000  \n",
       "0         0.010000           0.001000            10000  \n",
       "0         0.000100           0.010000            10000  \n",
       "0         0.000100           0.003162            10000  \n",
       "0         0.010000           0.000316            10000  \n",
       "0         0.000316           0.010000            10000  \n",
       "0         0.000316           0.003162            10000  \n",
       "0         0.001000           0.010000            10000  \n",
       "0         0.010000           0.000100            10000  \n",
       "0         0.010000           0.003162            10000  \n",
       "0         0.003162           0.003162            10000  \n",
       "0         0.003162           0.010000            10000  \n",
       "0         0.010000           0.010000            10000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = Parallel(n_jobs=25)(delayed(xnn_repeat)(folder=\"./results/S1_xnn/\",\n",
    "                      name=str(i + 1).zfill(2) + \"_\" + str(j + 1).zfill(2),\n",
    "                      data_generator=simu_loader(data_generator1, 10000, 10000, 1),\n",
    "                      task=task_type,\n",
    "                      subnet_arch=[10, 6],\n",
    "                      beta_threshold=0.05,\n",
    "                      l1_proj=10**(-2 - i*0.5),\n",
    "                      l1_subnet=10**(-2 - j*0.5),\n",
    "                      training_epochs=10000,\n",
    "                      lr_bp=0.001,\n",
    "                      batch_size=1000,\n",
    "                      early_stop_thres=500,\n",
    "                      tuning_epochs=100,\n",
    "                      rand_seed=0) for i in range(5) for j in range(5) for k in [1])\n",
    "sosxnn_stat_all = pd.concat(cv_results)\n",
    "sosxnn_stat_all.sort_values(\"val_metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_l1_prob = sosxnn_stat_all.sort_values(\"val_metric\").loc[:,\"L1_Penalty_Proj\"].iloc[0]\n",
    "best_l1_subnet = sosxnn_stat_all.sort_values(\"val_metric\").loc[:,\"L1_Penalty_Subnet\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training.\n",
      "Training epoch: 1, train loss: 0.05167, val loss: 0.05170\n",
      "Training epoch: 2, train loss: 0.04446, val loss: 0.04462\n",
      "Training epoch: 3, train loss: 0.04094, val loss: 0.04136\n",
      "Training epoch: 4, train loss: 0.03951, val loss: 0.03995\n",
      "Training epoch: 5, train loss: 0.03852, val loss: 0.03902\n",
      "Training epoch: 6, train loss: 0.03759, val loss: 0.03819\n",
      "Training epoch: 7, train loss: 0.03690, val loss: 0.03754\n",
      "Training epoch: 8, train loss: 0.03621, val loss: 0.03688\n",
      "Training epoch: 9, train loss: 0.03550, val loss: 0.03618\n",
      "Training epoch: 10, train loss: 0.03480, val loss: 0.03549\n",
      "Training epoch: 11, train loss: 0.03410, val loss: 0.03481\n",
      "Training epoch: 12, train loss: 0.03345, val loss: 0.03415\n",
      "Training epoch: 13, train loss: 0.03285, val loss: 0.03356\n",
      "Training epoch: 14, train loss: 0.03235, val loss: 0.03308\n",
      "Training epoch: 15, train loss: 0.03191, val loss: 0.03266\n",
      "Training epoch: 16, train loss: 0.03154, val loss: 0.03228\n",
      "Training epoch: 17, train loss: 0.03118, val loss: 0.03195\n",
      "Training epoch: 18, train loss: 0.03080, val loss: 0.03161\n",
      "Training epoch: 19, train loss: 0.03041, val loss: 0.03121\n",
      "Training epoch: 20, train loss: 0.03000, val loss: 0.03082\n",
      "Training epoch: 21, train loss: 0.02959, val loss: 0.03043\n",
      "Training epoch: 22, train loss: 0.02915, val loss: 0.03002\n",
      "Training epoch: 23, train loss: 0.02875, val loss: 0.02962\n",
      "Training epoch: 24, train loss: 0.02831, val loss: 0.02923\n",
      "Training epoch: 25, train loss: 0.02791, val loss: 0.02886\n",
      "Training epoch: 26, train loss: 0.02751, val loss: 0.02848\n",
      "Training epoch: 27, train loss: 0.02714, val loss: 0.02809\n",
      "Training epoch: 28, train loss: 0.02677, val loss: 0.02777\n",
      "Training epoch: 29, train loss: 0.02642, val loss: 0.02743\n",
      "Training epoch: 30, train loss: 0.02610, val loss: 0.02714\n",
      "Training epoch: 31, train loss: 0.02580, val loss: 0.02684\n",
      "Training epoch: 32, train loss: 0.02543, val loss: 0.02649\n",
      "Training epoch: 33, train loss: 0.02514, val loss: 0.02616\n",
      "Training epoch: 34, train loss: 0.02481, val loss: 0.02591\n",
      "Training epoch: 35, train loss: 0.02441, val loss: 0.02550\n",
      "Training epoch: 36, train loss: 0.02409, val loss: 0.02519\n",
      "Training epoch: 37, train loss: 0.02375, val loss: 0.02484\n",
      "Training epoch: 38, train loss: 0.02347, val loss: 0.02457\n",
      "Training epoch: 39, train loss: 0.02320, val loss: 0.02432\n",
      "Training epoch: 40, train loss: 0.02281, val loss: 0.02393\n",
      "Training epoch: 41, train loss: 0.02257, val loss: 0.02366\n",
      "Training epoch: 42, train loss: 0.02225, val loss: 0.02337\n",
      "Training epoch: 43, train loss: 0.02201, val loss: 0.02314\n",
      "Training epoch: 44, train loss: 0.02176, val loss: 0.02288\n",
      "Training epoch: 45, train loss: 0.02152, val loss: 0.02267\n",
      "Training epoch: 46, train loss: 0.02131, val loss: 0.02246\n",
      "Training epoch: 47, train loss: 0.02117, val loss: 0.02229\n",
      "Training epoch: 48, train loss: 0.02101, val loss: 0.02211\n",
      "Training epoch: 49, train loss: 0.02076, val loss: 0.02189\n",
      "Training epoch: 50, train loss: 0.02078, val loss: 0.02190\n",
      "Training epoch: 51, train loss: 0.02054, val loss: 0.02164\n",
      "Training epoch: 52, train loss: 0.02035, val loss: 0.02143\n",
      "Training epoch: 53, train loss: 0.02026, val loss: 0.02138\n",
      "Training epoch: 54, train loss: 0.02016, val loss: 0.02126\n",
      "Training epoch: 55, train loss: 0.02000, val loss: 0.02112\n",
      "Training epoch: 56, train loss: 0.01992, val loss: 0.02099\n",
      "Training epoch: 57, train loss: 0.01978, val loss: 0.02086\n",
      "Training epoch: 58, train loss: 0.01976, val loss: 0.02080\n",
      "Training epoch: 59, train loss: 0.01982, val loss: 0.02089\n",
      "Training epoch: 60, train loss: 0.01958, val loss: 0.02062\n",
      "Training epoch: 61, train loss: 0.01957, val loss: 0.02063\n",
      "Training epoch: 62, train loss: 0.01935, val loss: 0.02039\n",
      "Training epoch: 63, train loss: 0.01926, val loss: 0.02029\n",
      "Training epoch: 64, train loss: 0.01915, val loss: 0.02019\n",
      "Training epoch: 65, train loss: 0.01909, val loss: 0.02011\n",
      "Training epoch: 66, train loss: 0.01904, val loss: 0.02006\n",
      "Training epoch: 67, train loss: 0.01901, val loss: 0.02002\n",
      "Training epoch: 68, train loss: 0.01886, val loss: 0.01986\n",
      "Training epoch: 69, train loss: 0.01878, val loss: 0.01976\n",
      "Training epoch: 70, train loss: 0.01882, val loss: 0.01981\n",
      "Training epoch: 71, train loss: 0.01863, val loss: 0.01962\n",
      "Training epoch: 72, train loss: 0.01868, val loss: 0.01968\n",
      "Training epoch: 73, train loss: 0.01851, val loss: 0.01949\n",
      "Training epoch: 74, train loss: 0.01844, val loss: 0.01941\n",
      "Training epoch: 75, train loss: 0.01841, val loss: 0.01936\n",
      "Training epoch: 76, train loss: 0.01836, val loss: 0.01932\n",
      "Training epoch: 77, train loss: 0.01825, val loss: 0.01919\n",
      "Training epoch: 78, train loss: 0.01820, val loss: 0.01914\n",
      "Training epoch: 79, train loss: 0.01814, val loss: 0.01906\n",
      "Training epoch: 80, train loss: 0.01816, val loss: 0.01908\n",
      "Training epoch: 81, train loss: 0.01804, val loss: 0.01895\n",
      "Training epoch: 82, train loss: 0.01798, val loss: 0.01891\n",
      "Training epoch: 83, train loss: 0.01795, val loss: 0.01886\n",
      "Training epoch: 84, train loss: 0.01789, val loss: 0.01877\n",
      "Training epoch: 85, train loss: 0.01784, val loss: 0.01874\n",
      "Training epoch: 86, train loss: 0.01794, val loss: 0.01884\n",
      "Training epoch: 87, train loss: 0.01773, val loss: 0.01861\n",
      "Training epoch: 88, train loss: 0.01768, val loss: 0.01855\n",
      "Training epoch: 89, train loss: 0.01764, val loss: 0.01850\n",
      "Training epoch: 90, train loss: 0.01760, val loss: 0.01845\n",
      "Training epoch: 91, train loss: 0.01758, val loss: 0.01844\n",
      "Training epoch: 92, train loss: 0.01755, val loss: 0.01837\n",
      "Training epoch: 93, train loss: 0.01749, val loss: 0.01833\n",
      "Training epoch: 94, train loss: 0.01747, val loss: 0.01827\n",
      "Training epoch: 95, train loss: 0.01745, val loss: 0.01829\n",
      "Training epoch: 96, train loss: 0.01738, val loss: 0.01820\n",
      "Training epoch: 97, train loss: 0.01733, val loss: 0.01813\n",
      "Training epoch: 98, train loss: 0.01733, val loss: 0.01816\n",
      "Training epoch: 99, train loss: 0.01731, val loss: 0.01808\n",
      "Training epoch: 100, train loss: 0.01726, val loss: 0.01806\n",
      "Training epoch: 101, train loss: 0.01724, val loss: 0.01803\n",
      "Training epoch: 102, train loss: 0.01717, val loss: 0.01796\n",
      "Training epoch: 103, train loss: 0.01715, val loss: 0.01793\n",
      "Training epoch: 104, train loss: 0.01716, val loss: 0.01792\n",
      "Training epoch: 105, train loss: 0.01720, val loss: 0.01796\n",
      "Training epoch: 106, train loss: 0.01706, val loss: 0.01782\n",
      "Training epoch: 107, train loss: 0.01709, val loss: 0.01785\n",
      "Training epoch: 108, train loss: 0.01700, val loss: 0.01774\n",
      "Training epoch: 109, train loss: 0.01701, val loss: 0.01772\n",
      "Training epoch: 110, train loss: 0.01695, val loss: 0.01767\n",
      "Training epoch: 111, train loss: 0.01694, val loss: 0.01767\n",
      "Training epoch: 112, train loss: 0.01694, val loss: 0.01766\n",
      "Training epoch: 113, train loss: 0.01695, val loss: 0.01769\n",
      "Training epoch: 114, train loss: 0.01686, val loss: 0.01756\n",
      "Training epoch: 115, train loss: 0.01691, val loss: 0.01761\n",
      "Training epoch: 116, train loss: 0.01681, val loss: 0.01750\n",
      "Training epoch: 117, train loss: 0.01679, val loss: 0.01749\n",
      "Training epoch: 118, train loss: 0.01679, val loss: 0.01746\n",
      "Training epoch: 119, train loss: 0.01675, val loss: 0.01743\n",
      "Training epoch: 120, train loss: 0.01673, val loss: 0.01740\n",
      "Training epoch: 121, train loss: 0.01669, val loss: 0.01737\n",
      "Training epoch: 122, train loss: 0.01668, val loss: 0.01732\n",
      "Training epoch: 123, train loss: 0.01668, val loss: 0.01733\n",
      "Training epoch: 124, train loss: 0.01663, val loss: 0.01729\n",
      "Training epoch: 125, train loss: 0.01661, val loss: 0.01724\n",
      "Training epoch: 126, train loss: 0.01659, val loss: 0.01721\n",
      "Training epoch: 127, train loss: 0.01669, val loss: 0.01734\n",
      "Training epoch: 128, train loss: 0.01657, val loss: 0.01722\n",
      "Training epoch: 129, train loss: 0.01653, val loss: 0.01715\n",
      "Training epoch: 130, train loss: 0.01650, val loss: 0.01711\n",
      "Training epoch: 131, train loss: 0.01648, val loss: 0.01709\n",
      "Training epoch: 132, train loss: 0.01646, val loss: 0.01706\n",
      "Training epoch: 133, train loss: 0.01646, val loss: 0.01707\n",
      "Training epoch: 134, train loss: 0.01644, val loss: 0.01706\n",
      "Training epoch: 135, train loss: 0.01640, val loss: 0.01700\n",
      "Training epoch: 136, train loss: 0.01639, val loss: 0.01699\n",
      "Training epoch: 137, train loss: 0.01638, val loss: 0.01695\n",
      "Training epoch: 138, train loss: 0.01635, val loss: 0.01693\n",
      "Training epoch: 139, train loss: 0.01637, val loss: 0.01696\n",
      "Training epoch: 140, train loss: 0.01634, val loss: 0.01689\n",
      "Training epoch: 141, train loss: 0.01633, val loss: 0.01687\n",
      "Training epoch: 142, train loss: 0.01629, val loss: 0.01685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 143, train loss: 0.01627, val loss: 0.01682\n",
      "Training epoch: 144, train loss: 0.01632, val loss: 0.01686\n",
      "Training epoch: 145, train loss: 0.01626, val loss: 0.01679\n",
      "Training epoch: 146, train loss: 0.01626, val loss: 0.01683\n",
      "Training epoch: 147, train loss: 0.01625, val loss: 0.01681\n",
      "Training epoch: 148, train loss: 0.01624, val loss: 0.01678\n",
      "Training epoch: 149, train loss: 0.01616, val loss: 0.01670\n",
      "Training epoch: 150, train loss: 0.01613, val loss: 0.01665\n",
      "Training epoch: 151, train loss: 0.01612, val loss: 0.01663\n",
      "Training epoch: 152, train loss: 0.01613, val loss: 0.01663\n",
      "Training epoch: 153, train loss: 0.01612, val loss: 0.01663\n",
      "Training epoch: 154, train loss: 0.01606, val loss: 0.01658\n",
      "Training epoch: 155, train loss: 0.01606, val loss: 0.01658\n",
      "Training epoch: 156, train loss: 0.01611, val loss: 0.01661\n",
      "Training epoch: 157, train loss: 0.01607, val loss: 0.01654\n",
      "Training epoch: 158, train loss: 0.01604, val loss: 0.01656\n",
      "Training epoch: 159, train loss: 0.01607, val loss: 0.01656\n",
      "Training epoch: 160, train loss: 0.01604, val loss: 0.01656\n",
      "Training epoch: 161, train loss: 0.01602, val loss: 0.01651\n",
      "Training epoch: 162, train loss: 0.01595, val loss: 0.01644\n",
      "Training epoch: 163, train loss: 0.01594, val loss: 0.01644\n",
      "Training epoch: 164, train loss: 0.01594, val loss: 0.01644\n",
      "Training epoch: 165, train loss: 0.01592, val loss: 0.01642\n",
      "Training epoch: 166, train loss: 0.01610, val loss: 0.01654\n",
      "Training epoch: 167, train loss: 0.01619, val loss: 0.01660\n",
      "Training epoch: 168, train loss: 0.01590, val loss: 0.01637\n",
      "Training epoch: 169, train loss: 0.01586, val loss: 0.01632\n",
      "Training epoch: 170, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 171, train loss: 0.01584, val loss: 0.01632\n",
      "Training epoch: 172, train loss: 0.01587, val loss: 0.01633\n",
      "Training epoch: 173, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 174, train loss: 0.01582, val loss: 0.01629\n",
      "Training epoch: 175, train loss: 0.01585, val loss: 0.01633\n",
      "Training epoch: 176, train loss: 0.01600, val loss: 0.01651\n",
      "Training epoch: 177, train loss: 0.01581, val loss: 0.01628\n",
      "Training epoch: 178, train loss: 0.01579, val loss: 0.01622\n",
      "Training epoch: 179, train loss: 0.01580, val loss: 0.01622\n",
      "Training epoch: 180, train loss: 0.01587, val loss: 0.01631\n",
      "Training epoch: 181, train loss: 0.01580, val loss: 0.01622\n",
      "Training epoch: 182, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 183, train loss: 0.01574, val loss: 0.01621\n",
      "Training epoch: 184, train loss: 0.01574, val loss: 0.01620\n",
      "Training epoch: 185, train loss: 0.01575, val loss: 0.01624\n",
      "Training epoch: 186, train loss: 0.01571, val loss: 0.01615\n",
      "Training epoch: 187, train loss: 0.01578, val loss: 0.01626\n",
      "Training epoch: 188, train loss: 0.01570, val loss: 0.01616\n",
      "Training epoch: 189, train loss: 0.01568, val loss: 0.01613\n",
      "Training epoch: 190, train loss: 0.01572, val loss: 0.01619\n",
      "Training epoch: 191, train loss: 0.01565, val loss: 0.01610\n",
      "Training epoch: 192, train loss: 0.01566, val loss: 0.01608\n",
      "Training epoch: 193, train loss: 0.01564, val loss: 0.01608\n",
      "Training epoch: 194, train loss: 0.01564, val loss: 0.01610\n",
      "Training epoch: 195, train loss: 0.01567, val loss: 0.01613\n",
      "Training epoch: 196, train loss: 0.01564, val loss: 0.01609\n",
      "Training epoch: 197, train loss: 0.01565, val loss: 0.01612\n",
      "Training epoch: 198, train loss: 0.01562, val loss: 0.01605\n",
      "Training epoch: 199, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 200, train loss: 0.01570, val loss: 0.01611\n",
      "Training epoch: 201, train loss: 0.01568, val loss: 0.01609\n",
      "Training epoch: 202, train loss: 0.01566, val loss: 0.01610\n",
      "Training epoch: 203, train loss: 0.01561, val loss: 0.01604\n",
      "Training epoch: 204, train loss: 0.01563, val loss: 0.01608\n",
      "Training epoch: 205, train loss: 0.01558, val loss: 0.01603\n",
      "Training epoch: 206, train loss: 0.01557, val loss: 0.01602\n",
      "Training epoch: 207, train loss: 0.01558, val loss: 0.01600\n",
      "Training epoch: 208, train loss: 0.01558, val loss: 0.01602\n",
      "Training epoch: 209, train loss: 0.01561, val loss: 0.01603\n",
      "Training epoch: 210, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 211, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 212, train loss: 0.01560, val loss: 0.01606\n",
      "Training epoch: 213, train loss: 0.01556, val loss: 0.01601\n",
      "Training epoch: 214, train loss: 0.01562, val loss: 0.01602\n",
      "Training epoch: 215, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 216, train loss: 0.01555, val loss: 0.01599\n",
      "Training epoch: 217, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 218, train loss: 0.01556, val loss: 0.01600\n",
      "Training epoch: 219, train loss: 0.01552, val loss: 0.01596\n",
      "Training epoch: 220, train loss: 0.01563, val loss: 0.01611\n",
      "Training epoch: 221, train loss: 0.01553, val loss: 0.01600\n",
      "Training epoch: 222, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 223, train loss: 0.01551, val loss: 0.01597\n",
      "Training epoch: 224, train loss: 0.01559, val loss: 0.01604\n",
      "Training epoch: 225, train loss: 0.01551, val loss: 0.01599\n",
      "Training epoch: 226, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 227, train loss: 0.01552, val loss: 0.01597\n",
      "Training epoch: 228, train loss: 0.01551, val loss: 0.01595\n",
      "Training epoch: 229, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 230, train loss: 0.01553, val loss: 0.01598\n",
      "Training epoch: 231, train loss: 0.01558, val loss: 0.01604\n",
      "Training epoch: 232, train loss: 0.01553, val loss: 0.01601\n",
      "Training epoch: 233, train loss: 0.01571, val loss: 0.01621\n",
      "Training epoch: 234, train loss: 0.01553, val loss: 0.01602\n",
      "Training epoch: 235, train loss: 0.01550, val loss: 0.01594\n",
      "Training epoch: 236, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 237, train loss: 0.01548, val loss: 0.01595\n",
      "Training epoch: 238, train loss: 0.01549, val loss: 0.01595\n",
      "Training epoch: 239, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 240, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 241, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 242, train loss: 0.01547, val loss: 0.01594\n",
      "Training epoch: 243, train loss: 0.01545, val loss: 0.01591\n",
      "Training epoch: 244, train loss: 0.01552, val loss: 0.01601\n",
      "Training epoch: 245, train loss: 0.01548, val loss: 0.01594\n",
      "Training epoch: 246, train loss: 0.01546, val loss: 0.01593\n",
      "Training epoch: 247, train loss: 0.01551, val loss: 0.01593\n",
      "Training epoch: 248, train loss: 0.01555, val loss: 0.01600\n",
      "Training epoch: 249, train loss: 0.01553, val loss: 0.01596\n",
      "Training epoch: 250, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 251, train loss: 0.01548, val loss: 0.01592\n",
      "Training epoch: 252, train loss: 0.01554, val loss: 0.01599\n",
      "Training epoch: 253, train loss: 0.01545, val loss: 0.01589\n",
      "Training epoch: 254, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 255, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 256, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 257, train loss: 0.01542, val loss: 0.01590\n",
      "Training epoch: 258, train loss: 0.01548, val loss: 0.01598\n",
      "Training epoch: 259, train loss: 0.01544, val loss: 0.01590\n",
      "Training epoch: 260, train loss: 0.01554, val loss: 0.01601\n",
      "Training epoch: 261, train loss: 0.01557, val loss: 0.01607\n",
      "Training epoch: 262, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 263, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 264, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 265, train loss: 0.01553, val loss: 0.01599\n",
      "Training epoch: 266, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 267, train loss: 0.01542, val loss: 0.01590\n",
      "Training epoch: 268, train loss: 0.01541, val loss: 0.01588\n",
      "Training epoch: 269, train loss: 0.01545, val loss: 0.01593\n",
      "Training epoch: 270, train loss: 0.01546, val loss: 0.01592\n",
      "Training epoch: 271, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 272, train loss: 0.01543, val loss: 0.01587\n",
      "Training epoch: 273, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 274, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 275, train loss: 0.01540, val loss: 0.01589\n",
      "Training epoch: 276, train loss: 0.01544, val loss: 0.01594\n",
      "Training epoch: 277, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 278, train loss: 0.01539, val loss: 0.01588\n",
      "Training epoch: 279, train loss: 0.01545, val loss: 0.01596\n",
      "Training epoch: 280, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 281, train loss: 0.01538, val loss: 0.01585\n",
      "Training epoch: 282, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 283, train loss: 0.01537, val loss: 0.01584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 284, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 285, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 286, train loss: 0.01540, val loss: 0.01585\n",
      "Training epoch: 287, train loss: 0.01542, val loss: 0.01588\n",
      "Training epoch: 288, train loss: 0.01563, val loss: 0.01602\n",
      "Training epoch: 289, train loss: 0.01536, val loss: 0.01584\n",
      "Training epoch: 290, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 291, train loss: 0.01544, val loss: 0.01593\n",
      "Training epoch: 292, train loss: 0.01556, val loss: 0.01608\n",
      "Training epoch: 293, train loss: 0.01537, val loss: 0.01585\n",
      "Training epoch: 294, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 295, train loss: 0.01546, val loss: 0.01589\n",
      "Training epoch: 296, train loss: 0.01547, val loss: 0.01589\n",
      "Training epoch: 297, train loss: 0.01541, val loss: 0.01586\n",
      "Training epoch: 298, train loss: 0.01550, val loss: 0.01601\n",
      "Training epoch: 299, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 300, train loss: 0.01536, val loss: 0.01581\n",
      "Training epoch: 301, train loss: 0.01545, val loss: 0.01586\n",
      "Training epoch: 302, train loss: 0.01545, val loss: 0.01590\n",
      "Training epoch: 303, train loss: 0.01542, val loss: 0.01585\n",
      "Training epoch: 304, train loss: 0.01535, val loss: 0.01581\n",
      "Training epoch: 305, train loss: 0.01536, val loss: 0.01584\n",
      "Training epoch: 306, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 307, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 308, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 309, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 310, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 311, train loss: 0.01540, val loss: 0.01586\n",
      "Training epoch: 312, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 313, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 314, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 315, train loss: 0.01537, val loss: 0.01583\n",
      "Training epoch: 316, train loss: 0.01536, val loss: 0.01584\n",
      "Training epoch: 317, train loss: 0.01537, val loss: 0.01581\n",
      "Training epoch: 318, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 319, train loss: 0.01533, val loss: 0.01580\n",
      "Training epoch: 320, train loss: 0.01535, val loss: 0.01583\n",
      "Training epoch: 321, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 322, train loss: 0.01534, val loss: 0.01581\n",
      "Training epoch: 323, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 324, train loss: 0.01548, val loss: 0.01597\n",
      "Training epoch: 325, train loss: 0.01546, val loss: 0.01595\n",
      "Training epoch: 326, train loss: 0.01544, val loss: 0.01594\n",
      "Training epoch: 327, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 328, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 329, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 330, train loss: 0.01533, val loss: 0.01578\n",
      "Training epoch: 331, train loss: 0.01538, val loss: 0.01583\n",
      "Training epoch: 332, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 333, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 334, train loss: 0.01537, val loss: 0.01586\n",
      "Training epoch: 335, train loss: 0.01546, val loss: 0.01596\n",
      "Training epoch: 336, train loss: 0.01540, val loss: 0.01588\n",
      "Training epoch: 337, train loss: 0.01545, val loss: 0.01594\n",
      "Training epoch: 338, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 339, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 340, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 341, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 342, train loss: 0.01533, val loss: 0.01580\n",
      "Training epoch: 343, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 344, train loss: 0.01536, val loss: 0.01580\n",
      "Training epoch: 345, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 346, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 347, train loss: 0.01533, val loss: 0.01581\n",
      "Training epoch: 348, train loss: 0.01544, val loss: 0.01593\n",
      "Training epoch: 349, train loss: 0.01539, val loss: 0.01587\n",
      "Training epoch: 350, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 351, train loss: 0.01534, val loss: 0.01578\n",
      "Training epoch: 352, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 353, train loss: 0.01541, val loss: 0.01584\n",
      "Training epoch: 354, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 355, train loss: 0.01532, val loss: 0.01580\n",
      "Training epoch: 356, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 357, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 358, train loss: 0.01537, val loss: 0.01580\n",
      "Training epoch: 359, train loss: 0.01535, val loss: 0.01579\n",
      "Training epoch: 360, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 361, train loss: 0.01536, val loss: 0.01584\n",
      "Training epoch: 362, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 363, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 364, train loss: 0.01545, val loss: 0.01594\n",
      "Training epoch: 365, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 366, train loss: 0.01531, val loss: 0.01579\n",
      "Training epoch: 367, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 368, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 369, train loss: 0.01528, val loss: 0.01576\n",
      "Training epoch: 370, train loss: 0.01533, val loss: 0.01581\n",
      "Training epoch: 371, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 372, train loss: 0.01539, val loss: 0.01581\n",
      "Training epoch: 373, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 374, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 375, train loss: 0.01528, val loss: 0.01576\n",
      "Training epoch: 376, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 377, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 378, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 379, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 380, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 381, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 382, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 383, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 384, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 385, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 386, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 387, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 388, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 389, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 390, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 391, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 392, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 393, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 394, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 395, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 396, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 397, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 398, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 399, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 400, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 401, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 402, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 403, train loss: 0.01527, val loss: 0.01574\n",
      "Training epoch: 404, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 405, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 406, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 407, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 408, train loss: 0.01544, val loss: 0.01587\n",
      "Training epoch: 409, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 410, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 411, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 412, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 413, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 414, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 415, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 416, train loss: 0.01534, val loss: 0.01582\n",
      "Training epoch: 417, train loss: 0.01537, val loss: 0.01586\n",
      "Training epoch: 418, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 419, train loss: 0.01550, val loss: 0.01600\n",
      "Training epoch: 420, train loss: 0.01549, val loss: 0.01599\n",
      "Training epoch: 421, train loss: 0.01537, val loss: 0.01585\n",
      "Training epoch: 422, train loss: 0.01534, val loss: 0.01583\n",
      "Training epoch: 423, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 424, train loss: 0.01540, val loss: 0.01588\n",
      "Training epoch: 425, train loss: 0.01552, val loss: 0.01602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 426, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 427, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 428, train loss: 0.01531, val loss: 0.01579\n",
      "Training epoch: 429, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 430, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 431, train loss: 0.01533, val loss: 0.01582\n",
      "Training epoch: 432, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 433, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 434, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 435, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 436, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 437, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 438, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 439, train loss: 0.01541, val loss: 0.01590\n",
      "Training epoch: 440, train loss: 0.01549, val loss: 0.01597\n",
      "Training epoch: 441, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 442, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 443, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 444, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 445, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 446, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 447, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 448, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 449, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 450, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 451, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 452, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 453, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 454, train loss: 0.01542, val loss: 0.01591\n",
      "Training epoch: 455, train loss: 0.01544, val loss: 0.01592\n",
      "Training epoch: 456, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 457, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 458, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 459, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 460, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 461, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 462, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 463, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 464, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 465, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 466, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 467, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 468, train loss: 0.01534, val loss: 0.01577\n",
      "Training epoch: 469, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 470, train loss: 0.01540, val loss: 0.01582\n",
      "Training epoch: 471, train loss: 0.01532, val loss: 0.01576\n",
      "Training epoch: 472, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 473, train loss: 0.01549, val loss: 0.01592\n",
      "Training epoch: 474, train loss: 0.01530, val loss: 0.01574\n",
      "Training epoch: 475, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 476, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 477, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 478, train loss: 0.01562, val loss: 0.01614\n",
      "Training epoch: 479, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 480, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 481, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 482, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 483, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 484, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 485, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 486, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 487, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 488, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 489, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 490, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 491, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 492, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 493, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 494, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 495, train loss: 0.01532, val loss: 0.01577\n",
      "Training epoch: 496, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 497, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 498, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 499, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 500, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 501, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 502, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 503, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 504, train loss: 0.01524, val loss: 0.01568\n",
      "Training epoch: 505, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 506, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 507, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 508, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 509, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 510, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 511, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 512, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 513, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 514, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 515, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 516, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 517, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 518, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 519, train loss: 0.01537, val loss: 0.01586\n",
      "Training epoch: 520, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 521, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 522, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 523, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 524, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 525, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 526, train loss: 0.01544, val loss: 0.01593\n",
      "Training epoch: 527, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 528, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 529, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 530, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 531, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 532, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 533, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 534, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 535, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 536, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 537, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 538, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 539, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 540, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 541, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 542, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 543, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 544, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 545, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 546, train loss: 0.01541, val loss: 0.01591\n",
      "Training epoch: 547, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 548, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 549, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 550, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 551, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 552, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 553, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 554, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 555, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 556, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 557, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 558, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 559, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 560, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 561, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 562, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 563, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 564, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 565, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 566, train loss: 0.01521, val loss: 0.01565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 567, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 568, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 569, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 570, train loss: 0.01530, val loss: 0.01573\n",
      "Training epoch: 571, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 572, train loss: 0.01528, val loss: 0.01571\n",
      "Training epoch: 573, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 574, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 575, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 576, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 577, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 578, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 579, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 580, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 581, train loss: 0.01542, val loss: 0.01591\n",
      "Training epoch: 582, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 583, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 584, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 585, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 586, train loss: 0.01524, val loss: 0.01568\n",
      "Training epoch: 587, train loss: 0.01539, val loss: 0.01582\n",
      "Training epoch: 588, train loss: 0.01537, val loss: 0.01578\n",
      "Training epoch: 589, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 590, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 591, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 592, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 593, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 594, train loss: 0.01523, val loss: 0.01571\n",
      "Training epoch: 595, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 596, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 597, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 598, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 599, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 600, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 601, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 602, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 603, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 604, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 605, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 606, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 607, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 608, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 609, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 610, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 611, train loss: 0.01548, val loss: 0.01590\n",
      "Training epoch: 612, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 613, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 614, train loss: 0.01526, val loss: 0.01575\n",
      "Training epoch: 615, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 616, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 617, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 618, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 619, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 620, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 621, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 622, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 623, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 624, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 625, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 626, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 627, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 628, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 629, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 630, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 631, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 632, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 633, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 634, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 635, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 636, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 637, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 638, train loss: 0.01524, val loss: 0.01572\n",
      "Training epoch: 639, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 640, train loss: 0.01538, val loss: 0.01586\n",
      "Training epoch: 641, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 642, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 643, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 644, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 645, train loss: 0.01540, val loss: 0.01583\n",
      "Training epoch: 646, train loss: 0.01546, val loss: 0.01588\n",
      "Training epoch: 647, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 648, train loss: 0.01525, val loss: 0.01568\n",
      "Training epoch: 649, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 650, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 651, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 652, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 653, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 654, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 655, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 656, train loss: 0.01526, val loss: 0.01568\n",
      "Training epoch: 657, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 658, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 659, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 660, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 661, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 662, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 663, train loss: 0.01520, val loss: 0.01568\n",
      "Training epoch: 664, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 665, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 666, train loss: 0.01538, val loss: 0.01587\n",
      "Training epoch: 667, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 668, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 669, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 670, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 671, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 672, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 673, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 674, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 675, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 676, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 677, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 678, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 679, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 680, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 681, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 682, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 683, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 684, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 685, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 686, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 687, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 688, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 689, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 690, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 691, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 692, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 693, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 694, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 695, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 696, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 697, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 698, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 699, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 700, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 701, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 702, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 703, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 704, train loss: 0.01518, val loss: 0.01564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 705, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 706, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 707, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 708, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 709, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 710, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 711, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 712, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 713, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 714, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 715, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 716, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 717, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 718, train loss: 0.01542, val loss: 0.01589\n",
      "Training epoch: 719, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 720, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 721, train loss: 0.01545, val loss: 0.01585\n",
      "Training epoch: 722, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 723, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 724, train loss: 0.01535, val loss: 0.01582\n",
      "Training epoch: 725, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 726, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 727, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 728, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 729, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 730, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 731, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 732, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 733, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 734, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 735, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 736, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 737, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 738, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 739, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 740, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 741, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 742, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 743, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 744, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 745, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 746, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 747, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 748, train loss: 0.01523, val loss: 0.01571\n",
      "Training epoch: 749, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 750, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 751, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 752, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 753, train loss: 0.01518, val loss: 0.01566\n",
      "Training epoch: 754, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 755, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 756, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 757, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 758, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 759, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 760, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 761, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 762, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 763, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 764, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 765, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 766, train loss: 0.01517, val loss: 0.01564\n",
      "Training epoch: 767, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 768, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 769, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 770, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 771, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 772, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 773, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 774, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 775, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 776, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 777, train loss: 0.01537, val loss: 0.01584\n",
      "Training epoch: 778, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 779, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 780, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 781, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 782, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 783, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 784, train loss: 0.01528, val loss: 0.01572\n",
      "Training epoch: 785, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 786, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 787, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 788, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 789, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 790, train loss: 0.01531, val loss: 0.01579\n",
      "Training epoch: 791, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 792, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 793, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 794, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 795, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 796, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 797, train loss: 0.01536, val loss: 0.01577\n",
      "Training epoch: 798, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 799, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 800, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 801, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 802, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 803, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 804, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 805, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 806, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 807, train loss: 0.01533, val loss: 0.01575\n",
      "Training epoch: 808, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 809, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 810, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 811, train loss: 0.01534, val loss: 0.01581\n",
      "Training epoch: 812, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 813, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 814, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 815, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 816, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 817, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 818, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 819, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 820, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 821, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 822, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 823, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 824, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 825, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 826, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 827, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 828, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 829, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 830, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 831, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 832, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 833, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 834, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 835, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 836, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 837, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 838, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 839, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 840, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 841, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 842, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 843, train loss: 0.01525, val loss: 0.01567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 844, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 845, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 846, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 847, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 848, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 849, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 850, train loss: 0.01529, val loss: 0.01577\n",
      "Training epoch: 851, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 852, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 853, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 854, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 855, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 856, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 857, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 858, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 859, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 860, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 861, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 862, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 863, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 864, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 865, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 866, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 867, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 868, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 869, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 870, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 871, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 872, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 873, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 874, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 875, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 876, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 877, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 878, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 879, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 880, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 881, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 882, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 883, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 884, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 885, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 886, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 887, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 888, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 889, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 890, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 891, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 892, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 893, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 894, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 895, train loss: 0.01539, val loss: 0.01578\n",
      "Training epoch: 896, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 897, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 898, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 899, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 900, train loss: 0.01519, val loss: 0.01567\n",
      "Training epoch: 901, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 902, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 903, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 904, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 905, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 906, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 907, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 908, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 909, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 910, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 911, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 912, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 913, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 914, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 915, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 916, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 917, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 918, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 919, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 920, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 921, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 922, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 923, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 924, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 925, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 926, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 927, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 928, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 929, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 930, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 931, train loss: 0.01566, val loss: 0.01615\n",
      "Training epoch: 932, train loss: 0.01527, val loss: 0.01573\n",
      "Training epoch: 933, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 934, train loss: 0.01528, val loss: 0.01569\n",
      "Training epoch: 935, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 936, train loss: 0.01527, val loss: 0.01568\n",
      "Training epoch: 937, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 938, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 939, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 940, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 941, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 942, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 943, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 944, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 945, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 946, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 947, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 948, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 949, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 950, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 951, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 952, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 953, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 954, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 955, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 956, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 957, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 958, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 959, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 960, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 961, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 962, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 963, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 964, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 965, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 966, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 967, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 968, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 969, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 970, train loss: 0.01524, val loss: 0.01568\n",
      "Training epoch: 971, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 972, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 973, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 974, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 975, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 976, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 977, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 978, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 979, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 980, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 981, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 982, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 983, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 984, train loss: 0.01515, val loss: 0.01559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 985, train loss: 0.01529, val loss: 0.01572\n",
      "Training epoch: 986, train loss: 0.01538, val loss: 0.01580\n",
      "Training epoch: 987, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 988, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 989, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 990, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 991, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 992, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 993, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 994, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 995, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 996, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 997, train loss: 0.01533, val loss: 0.01579\n",
      "Training epoch: 998, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 999, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1000, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1001, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1002, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1003, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1004, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1005, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 1006, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1007, train loss: 0.01534, val loss: 0.01582\n",
      "Training epoch: 1008, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 1009, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 1010, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1011, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1012, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 1013, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 1014, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 1015, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1016, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1017, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1018, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1019, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1020, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 1021, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 1022, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1023, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1024, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1025, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1026, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1027, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 1028, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 1029, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1030, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1031, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 1032, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1033, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1034, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1035, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 1036, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1037, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1038, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1039, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1040, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1041, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1042, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1043, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 1044, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1045, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 1046, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1047, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1048, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1049, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1050, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1051, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1052, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 1053, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1054, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1055, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1056, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1057, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1058, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1059, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1060, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 1061, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1062, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1063, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1064, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1065, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 1066, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1067, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1068, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1069, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1070, train loss: 0.01543, val loss: 0.01589\n",
      "Training epoch: 1071, train loss: 0.01530, val loss: 0.01578\n",
      "Training epoch: 1072, train loss: 0.01547, val loss: 0.01597\n",
      "Training epoch: 1073, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 1074, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 1075, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1076, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1077, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1078, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1079, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1080, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1081, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1082, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 1083, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1084, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1085, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1086, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1087, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1088, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1089, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1090, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1091, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1092, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1093, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 1094, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 1095, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1096, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1097, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1098, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 1099, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 1100, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1101, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1102, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1103, train loss: 0.01526, val loss: 0.01573\n",
      "Training epoch: 1104, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1105, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 1106, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 1107, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1108, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1109, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 1110, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1111, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 1112, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1113, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 1114, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1115, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 1116, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1117, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1118, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1119, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1120, train loss: 0.01519, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1121, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1122, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 1123, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1124, train loss: 0.01526, val loss: 0.01570\n",
      "Training epoch: 1125, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1126, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1127, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1128, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1129, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1130, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 1131, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 1132, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 1133, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1134, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1135, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1136, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1137, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1138, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1139, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1140, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1141, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 1142, train loss: 0.01529, val loss: 0.01570\n",
      "Training epoch: 1143, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 1144, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1145, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1146, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1147, train loss: 0.01515, val loss: 0.01562\n",
      "Training epoch: 1148, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1149, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 1150, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1151, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1152, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1153, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 1154, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 1155, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1156, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1157, train loss: 0.01526, val loss: 0.01569\n",
      "Training epoch: 1158, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1159, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1160, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1161, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1162, train loss: 0.01532, val loss: 0.01578\n",
      "Training epoch: 1163, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1164, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1165, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1166, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1167, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 1168, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1169, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 1170, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1171, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 1172, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1173, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1174, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1175, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1176, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1177, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1178, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1179, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1180, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 1181, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1182, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 1183, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 1184, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1185, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1186, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1187, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 1188, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1189, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1190, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 1191, train loss: 0.01529, val loss: 0.01574\n",
      "Training epoch: 1192, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 1193, train loss: 0.01534, val loss: 0.01579\n",
      "Training epoch: 1194, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 1195, train loss: 0.01519, val loss: 0.01559\n",
      "Training epoch: 1196, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1197, train loss: 0.01536, val loss: 0.01578\n",
      "Training epoch: 1198, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 1199, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1200, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1201, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1202, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1203, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 1204, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1205, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1206, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 1207, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1208, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 1209, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1210, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1211, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1212, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1213, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1214, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1215, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1216, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 1217, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1218, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1219, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1220, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1221, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1222, train loss: 0.01528, val loss: 0.01568\n",
      "Training epoch: 1223, train loss: 0.01533, val loss: 0.01574\n",
      "Training epoch: 1224, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1225, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1226, train loss: 0.01539, val loss: 0.01586\n",
      "Training epoch: 1227, train loss: 0.01541, val loss: 0.01587\n",
      "Training epoch: 1228, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 1229, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1230, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1231, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1232, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 1233, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 1234, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 1235, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1236, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1237, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1238, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1239, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1240, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1241, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1242, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1243, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1244, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1245, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 1246, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1247, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1248, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 1249, train loss: 0.01545, val loss: 0.01588\n",
      "Training epoch: 1250, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 1251, train loss: 0.01525, val loss: 0.01569\n",
      "Training epoch: 1252, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1253, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1254, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1255, train loss: 0.01513, val loss: 0.01556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1256, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1257, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1258, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1259, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1260, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1261, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1262, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 1263, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1264, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1265, train loss: 0.01525, val loss: 0.01567\n",
      "Training epoch: 1266, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1267, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1268, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1269, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 1270, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 1271, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 1272, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1273, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1274, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 1275, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 1276, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1277, train loss: 0.01527, val loss: 0.01570\n",
      "Training epoch: 1278, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1279, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 1280, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1281, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1282, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1283, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1284, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1285, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1286, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1287, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1288, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1289, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1290, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1291, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1292, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1293, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 1294, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1295, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1296, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 1297, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 1298, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1299, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1300, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 1301, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1302, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1303, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 1304, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1305, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 1306, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1307, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1308, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1309, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1310, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1311, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1312, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1313, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1314, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1315, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 1316, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1317, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 1318, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1319, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 1320, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 1321, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1322, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1323, train loss: 0.01531, val loss: 0.01578\n",
      "Training epoch: 1324, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1325, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1326, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1327, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1328, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1329, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1330, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1331, train loss: 0.01538, val loss: 0.01584\n",
      "Training epoch: 1332, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1333, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1334, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1335, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1336, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1337, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 1338, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1339, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1340, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 1341, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1342, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1343, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1344, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1345, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 1346, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 1347, train loss: 0.01522, val loss: 0.01562\n",
      "Training epoch: 1348, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1349, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1350, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1351, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 1352, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1353, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1354, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1355, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1356, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 1357, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1358, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1359, train loss: 0.01518, val loss: 0.01558\n",
      "Training epoch: 1360, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1361, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1362, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1363, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1364, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1365, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1366, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1367, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1368, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1369, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1370, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1371, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1372, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1373, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1374, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1375, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1376, train loss: 0.01552, val loss: 0.01599\n",
      "Training epoch: 1377, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1378, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 1379, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1380, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1381, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1382, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1383, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 1384, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 1385, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 1386, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1387, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1388, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1389, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1390, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1391, train loss: 0.01515, val loss: 0.01559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1392, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1393, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1394, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 1395, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1396, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1397, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1398, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1399, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1400, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1401, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1402, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 1403, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1404, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1405, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1406, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1407, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1408, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1409, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1410, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1411, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1412, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1413, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1414, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 1415, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 1416, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1417, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1418, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1419, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1420, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1421, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1422, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 1423, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1424, train loss: 0.01539, val loss: 0.01585\n",
      "Training epoch: 1425, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1426, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1427, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1428, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1429, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1430, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1431, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 1432, train loss: 0.01524, val loss: 0.01565\n",
      "Training epoch: 1433, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 1434, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 1435, train loss: 0.01525, val loss: 0.01566\n",
      "Training epoch: 1436, train loss: 0.01523, val loss: 0.01563\n",
      "Training epoch: 1437, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1438, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1439, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1440, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1441, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1442, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1443, train loss: 0.01520, val loss: 0.01561\n",
      "Training epoch: 1444, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1445, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1446, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1447, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1448, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1449, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1450, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1451, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1452, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1453, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1454, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1455, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1456, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1457, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1458, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1459, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1460, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1461, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1462, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 1463, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1464, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1465, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 1466, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1467, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1468, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 1469, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1470, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1471, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1472, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1473, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1474, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1475, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 1476, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1477, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1478, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 1479, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1480, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1481, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1482, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1483, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1484, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1485, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1486, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1487, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1488, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1489, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1490, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1491, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1492, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 1493, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1494, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1495, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1496, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1497, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1498, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1499, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1500, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1501, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 1502, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1503, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 1504, train loss: 0.01534, val loss: 0.01573\n",
      "Training epoch: 1505, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1506, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1507, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1508, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1509, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 1510, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1511, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1512, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1513, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1514, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1515, train loss: 0.01526, val loss: 0.01566\n",
      "Training epoch: 1516, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1517, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1518, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1519, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1520, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1521, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1522, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1523, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1524, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1525, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1526, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1527, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1528, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1529, train loss: 0.01513, val loss: 0.01558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1530, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1531, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1532, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1533, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1534, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1535, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1536, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1537, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1538, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1539, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 1540, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1541, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1542, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1543, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1544, train loss: 0.01528, val loss: 0.01567\n",
      "Training epoch: 1545, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1546, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1547, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1548, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1549, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1550, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1551, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1552, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1553, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1554, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1555, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1556, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1557, train loss: 0.01527, val loss: 0.01567\n",
      "Training epoch: 1558, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1559, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1560, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1561, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1562, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1563, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1564, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1565, train loss: 0.01529, val loss: 0.01575\n",
      "Training epoch: 1566, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 1567, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 1568, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1569, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1570, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1571, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 1572, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1573, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 1574, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1575, train loss: 0.01518, val loss: 0.01558\n",
      "Training epoch: 1576, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1577, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1578, train loss: 0.01523, val loss: 0.01570\n",
      "Training epoch: 1579, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1580, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1581, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1582, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1583, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1584, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1585, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 1586, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1587, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1588, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1589, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1590, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1591, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1592, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1593, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1594, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1595, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1596, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1597, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1598, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1599, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1600, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1601, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1602, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1603, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1604, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1605, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1606, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1607, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 1608, train loss: 0.01523, val loss: 0.01564\n",
      "Training epoch: 1609, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1610, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 1611, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1612, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1613, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1614, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1615, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1616, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1617, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1618, train loss: 0.01532, val loss: 0.01580\n",
      "Training epoch: 1619, train loss: 0.01541, val loss: 0.01589\n",
      "Training epoch: 1620, train loss: 0.01536, val loss: 0.01582\n",
      "Training epoch: 1621, train loss: 0.01536, val loss: 0.01583\n",
      "Training epoch: 1622, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 1623, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1624, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1625, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1626, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 1627, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 1628, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1629, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1630, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1631, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1632, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1633, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1634, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1635, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1636, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1637, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1638, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 1639, train loss: 0.01523, val loss: 0.01566\n",
      "Training epoch: 1640, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1641, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 1642, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1643, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1644, train loss: 0.01529, val loss: 0.01571\n",
      "Training epoch: 1645, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1646, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1647, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1648, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1649, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1650, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1651, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1652, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1653, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1654, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1655, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 1656, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1657, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1658, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1659, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1660, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1661, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1662, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1663, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1664, train loss: 0.01523, val loss: 0.01568\n",
      "Training epoch: 1665, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 1666, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 1667, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1668, train loss: 0.01509, val loss: 0.01554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1669, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1670, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1671, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1672, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1673, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1674, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1675, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1676, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 1677, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1678, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 1679, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1680, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1681, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 1682, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1683, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1684, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1685, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 1686, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1687, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1688, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 1689, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1690, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1691, train loss: 0.01525, val loss: 0.01565\n",
      "Training epoch: 1692, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 1693, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1694, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1695, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1696, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1697, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1698, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1699, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1700, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 1701, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1702, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1703, train loss: 0.01531, val loss: 0.01573\n",
      "Training epoch: 1704, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1705, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1706, train loss: 0.01533, val loss: 0.01576\n",
      "Training epoch: 1707, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 1708, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1709, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1710, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1711, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 1712, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1713, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 1714, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1715, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1716, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1717, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1718, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1719, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1720, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1721, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 1722, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1723, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 1724, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1725, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1726, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1727, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1728, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1729, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1730, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 1731, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1732, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1733, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1734, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1735, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1736, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1737, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1738, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 1739, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1740, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1741, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1742, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1743, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 1744, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1745, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1746, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1747, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 1748, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1749, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1750, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1751, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1752, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1753, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1754, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 1755, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1756, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1757, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1758, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1759, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 1760, train loss: 0.01535, val loss: 0.01580\n",
      "Training epoch: 1761, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 1762, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1763, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1764, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1765, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1766, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1767, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1768, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1769, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1770, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1771, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1772, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 1773, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1774, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1775, train loss: 0.01519, val loss: 0.01561\n",
      "Training epoch: 1776, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1777, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1778, train loss: 0.01526, val loss: 0.01571\n",
      "Training epoch: 1779, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1780, train loss: 0.01516, val loss: 0.01564\n",
      "Training epoch: 1781, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 1782, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1783, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1784, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1785, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1786, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1787, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1788, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1789, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1790, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1791, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 1792, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1793, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 1794, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1795, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1796, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1797, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1798, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1799, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 1800, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1801, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1802, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1803, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1804, train loss: 0.01512, val loss: 0.01555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1805, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1806, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 1807, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1808, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 1809, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1810, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1811, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1812, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1813, train loss: 0.01530, val loss: 0.01571\n",
      "Training epoch: 1814, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1815, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1816, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1817, train loss: 0.01515, val loss: 0.01554\n",
      "Training epoch: 1818, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1819, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1820, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 1821, train loss: 0.01544, val loss: 0.01591\n",
      "Training epoch: 1822, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1823, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1824, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1825, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1826, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1827, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1828, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 1829, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1830, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1831, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1832, train loss: 0.01513, val loss: 0.01553\n",
      "Training epoch: 1833, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1834, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1835, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 1836, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 1837, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1838, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1839, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1840, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1841, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1842, train loss: 0.01517, val loss: 0.01557\n",
      "Training epoch: 1843, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1844, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1845, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1846, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1847, train loss: 0.01537, val loss: 0.01579\n",
      "Training epoch: 1848, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1849, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1850, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1851, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1852, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1853, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1854, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1855, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1856, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1857, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1858, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1859, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1860, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 1861, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1862, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1863, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1864, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1865, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1866, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1867, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1868, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1869, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1870, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1871, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 1872, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1873, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1874, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1875, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 1876, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 1877, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1878, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1879, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1880, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 1881, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1882, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1883, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1884, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1885, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 1886, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1887, train loss: 0.01522, val loss: 0.01567\n",
      "Training epoch: 1888, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 1889, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 1890, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 1891, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 1892, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 1893, train loss: 0.01532, val loss: 0.01580\n",
      "Training epoch: 1894, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1895, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1896, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1897, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1898, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1899, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1900, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1901, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1902, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 1903, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1904, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1905, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1906, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1907, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1908, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 1909, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1910, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1911, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 1912, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 1913, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1914, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1915, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1916, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 1917, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1918, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1919, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 1920, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1921, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 1922, train loss: 0.01539, val loss: 0.01584\n",
      "Training epoch: 1923, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1924, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 1925, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1926, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1927, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1928, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1929, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1930, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1931, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1932, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1933, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1934, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 1935, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 1936, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 1937, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1938, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 1939, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1940, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1941, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1942, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 1943, train loss: 0.01527, val loss: 0.01573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1944, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 1945, train loss: 0.01522, val loss: 0.01565\n",
      "Training epoch: 1946, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1947, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 1948, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1949, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 1950, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1951, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 1952, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 1953, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1954, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 1955, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1956, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 1957, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 1958, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1959, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 1960, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1961, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 1962, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1963, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1964, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 1965, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1966, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1967, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 1968, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 1969, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1970, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 1971, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1972, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1973, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 1974, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 1975, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 1976, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1977, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 1978, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 1979, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1980, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1981, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1982, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 1983, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 1984, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 1985, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 1986, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1987, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 1988, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 1989, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 1990, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 1991, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 1992, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 1993, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 1994, train loss: 0.01529, val loss: 0.01576\n",
      "Training epoch: 1995, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 1996, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 1997, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 1998, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 1999, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2000, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2001, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2002, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2003, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2004, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2005, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2006, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2007, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2008, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2009, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2010, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2011, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2012, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 2013, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2014, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 2015, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2016, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2017, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2018, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2019, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2020, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2021, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2022, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2023, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 2024, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2025, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2026, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2027, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2028, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2029, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2030, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2031, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2032, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2033, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2034, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2035, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2036, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2037, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2038, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2039, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2040, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2041, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2042, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2043, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2044, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2045, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2046, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2047, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 2048, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2049, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2050, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2051, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2052, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2053, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2054, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 2055, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2056, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2057, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2058, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2059, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2060, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 2061, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2062, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2063, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 2064, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 2065, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2066, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2067, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2068, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2069, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2070, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2071, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2072, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2073, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2074, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2075, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 2076, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2077, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2078, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2079, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2080, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2081, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2082, train loss: 0.01511, val loss: 0.01554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2083, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2084, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2085, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 2086, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 2087, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2088, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2089, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2090, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2091, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2092, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2093, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 2094, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2095, train loss: 0.01517, val loss: 0.01565\n",
      "Training epoch: 2096, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2097, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2098, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2099, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2100, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2101, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2102, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2103, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2104, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2105, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2106, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2107, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2108, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2109, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 2110, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 2111, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2112, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2113, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2114, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2115, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2116, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2117, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2118, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2119, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2120, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2121, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2122, train loss: 0.01513, val loss: 0.01552\n",
      "Training epoch: 2123, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2124, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2125, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2126, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2127, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2128, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2129, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2130, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2131, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2132, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2133, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2134, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2135, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2136, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2137, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2138, train loss: 0.01532, val loss: 0.01572\n",
      "Training epoch: 2139, train loss: 0.01525, val loss: 0.01570\n",
      "Training epoch: 2140, train loss: 0.01529, val loss: 0.01569\n",
      "Training epoch: 2141, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2142, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2143, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2144, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 2145, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2146, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2147, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2148, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2149, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2150, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 2151, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2152, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2153, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2154, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2155, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2156, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2157, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2158, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 2159, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 2160, train loss: 0.01517, val loss: 0.01564\n",
      "Training epoch: 2161, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2162, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2163, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2164, train loss: 0.01523, val loss: 0.01565\n",
      "Training epoch: 2165, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2166, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2167, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2168, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2169, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2170, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2171, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2172, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2173, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2174, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2175, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2176, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2177, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2178, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2179, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2180, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2181, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2182, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2183, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2184, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2185, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2186, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2187, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 2188, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2189, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 2190, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2191, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2192, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2193, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2194, train loss: 0.01522, val loss: 0.01569\n",
      "Training epoch: 2195, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 2196, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2197, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2198, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2199, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2200, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2201, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2202, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2203, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2204, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2205, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2206, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 2207, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2208, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2209, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2210, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 2211, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2212, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2213, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 2214, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2215, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2216, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2217, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2218, train loss: 0.01514, val loss: 0.01560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2219, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 2220, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2221, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2222, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2223, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2224, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2225, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2226, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2227, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2228, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2229, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2230, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2231, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 2232, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2233, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2234, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2235, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 2236, train loss: 0.01525, val loss: 0.01564\n",
      "Training epoch: 2237, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2238, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2239, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2240, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2241, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 2242, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2243, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2244, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2245, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2246, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2247, train loss: 0.01521, val loss: 0.01564\n",
      "Training epoch: 2248, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2249, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2250, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2251, train loss: 0.01517, val loss: 0.01558\n",
      "Training epoch: 2252, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2253, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2254, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 2255, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2256, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2257, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2258, train loss: 0.01528, val loss: 0.01570\n",
      "Training epoch: 2259, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2260, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2261, train loss: 0.01530, val loss: 0.01577\n",
      "Training epoch: 2262, train loss: 0.01522, val loss: 0.01566\n",
      "Training epoch: 2263, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2264, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2265, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2266, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2267, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2268, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2269, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 2270, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 2271, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2272, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 2273, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2274, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2275, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2276, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2277, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2278, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2279, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2280, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2281, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2282, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2283, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2284, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2285, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2286, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2287, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2288, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2289, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2290, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2291, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2292, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2293, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2294, train loss: 0.01518, val loss: 0.01566\n",
      "Training epoch: 2295, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 2296, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2297, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2298, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2299, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2300, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2301, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2302, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2303, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2304, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 2305, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 2306, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2307, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2308, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2309, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2310, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 2311, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 2312, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2313, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2314, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2315, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2316, train loss: 0.01524, val loss: 0.01571\n",
      "Training epoch: 2317, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2318, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 2319, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2320, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2321, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2322, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2323, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2324, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2325, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2326, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2327, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2328, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 2329, train loss: 0.01522, val loss: 0.01563\n",
      "Training epoch: 2330, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 2331, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2332, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2333, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2334, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2335, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2336, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2337, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2338, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 2339, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2340, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 2341, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2342, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 2343, train loss: 0.01563, val loss: 0.01605\n",
      "Training epoch: 2344, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2345, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2346, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2347, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2348, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2349, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2350, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2351, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2352, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2353, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2354, train loss: 0.01509, val loss: 0.01553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2355, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2356, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2357, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2358, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 2359, train loss: 0.01527, val loss: 0.01574\n",
      "Training epoch: 2360, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2361, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2362, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2363, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2364, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2365, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2366, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2367, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2368, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2369, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 2370, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 2371, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2372, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2373, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2374, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2375, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2376, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2377, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2378, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2379, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2380, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 2381, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2382, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2383, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2384, train loss: 0.01518, val loss: 0.01563\n",
      "Training epoch: 2385, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 2386, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2387, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2388, train loss: 0.01527, val loss: 0.01574\n",
      "Training epoch: 2389, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 2390, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2391, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2392, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2393, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2394, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2395, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2396, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2397, train loss: 0.01524, val loss: 0.01563\n",
      "Training epoch: 2398, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 2399, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2400, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2401, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2402, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2403, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2404, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2405, train loss: 0.01519, val loss: 0.01560\n",
      "Training epoch: 2406, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2407, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2408, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2409, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2410, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2411, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2412, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2413, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2414, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 2415, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2416, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 2417, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2418, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 2419, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2420, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2421, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2422, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2423, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2424, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2425, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2426, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2427, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2428, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2429, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 2430, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2431, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2432, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 2433, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2434, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2435, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2436, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2437, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 2438, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2439, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2440, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2441, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2442, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 2443, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2444, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2445, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2446, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 2447, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2448, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2449, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2450, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2451, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2452, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2453, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2454, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2455, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2456, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 2457, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2458, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2459, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2460, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2461, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2462, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2463, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2464, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2465, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2466, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 2467, train loss: 0.01531, val loss: 0.01577\n",
      "Training epoch: 2468, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2469, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2470, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2471, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2472, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 2473, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2474, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 2475, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2476, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2477, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2478, train loss: 0.01511, val loss: 0.01551\n",
      "Training epoch: 2479, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2480, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 2481, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2482, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2483, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2484, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2485, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2486, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 2487, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2488, train loss: 0.01517, val loss: 0.01563\n",
      "Training epoch: 2489, train loss: 0.01508, val loss: 0.01553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2490, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2491, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2492, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2493, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2494, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2495, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2496, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2497, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2498, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2499, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2500, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2501, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2502, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2503, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2504, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2505, train loss: 0.01528, val loss: 0.01574\n",
      "Training epoch: 2506, train loss: 0.01524, val loss: 0.01569\n",
      "Training epoch: 2507, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 2508, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 2509, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2510, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2511, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2512, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2513, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2514, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2515, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2516, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2517, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2518, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2519, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2520, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2521, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2522, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2523, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2524, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2525, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 2526, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2527, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 2528, train loss: 0.01524, val loss: 0.01566\n",
      "Training epoch: 2529, train loss: 0.01524, val loss: 0.01564\n",
      "Training epoch: 2530, train loss: 0.01524, val loss: 0.01566\n",
      "Training epoch: 2531, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2532, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2533, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2534, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2535, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2536, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2537, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 2538, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2539, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2540, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2541, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2542, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 2543, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2544, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2545, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2546, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2547, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2548, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2549, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2550, train loss: 0.01515, val loss: 0.01556\n",
      "Training epoch: 2551, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2552, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2553, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2554, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2555, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2556, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2557, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2558, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2559, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2560, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2561, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2562, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 2563, train loss: 0.01511, val loss: 0.01552\n",
      "Training epoch: 2564, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2565, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2566, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2567, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2568, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2569, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2570, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2571, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2572, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2573, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2574, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2575, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2576, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2577, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 2578, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 2579, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 2580, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2581, train loss: 0.01528, val loss: 0.01573\n",
      "Training epoch: 2582, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2583, train loss: 0.01524, val loss: 0.01567\n",
      "Training epoch: 2584, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2585, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2586, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2587, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2588, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2589, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 2590, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2591, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2592, train loss: 0.01540, val loss: 0.01584\n",
      "Training epoch: 2593, train loss: 0.01520, val loss: 0.01561\n",
      "Training epoch: 2594, train loss: 0.01519, val loss: 0.01562\n",
      "Training epoch: 2595, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2596, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2597, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2598, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2599, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2600, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2601, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2602, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2603, train loss: 0.01527, val loss: 0.01571\n",
      "Training epoch: 2604, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2605, train loss: 0.01520, val loss: 0.01562\n",
      "Training epoch: 2606, train loss: 0.01520, val loss: 0.01560\n",
      "Training epoch: 2607, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2608, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2609, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2610, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2611, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 2612, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 2613, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2614, train loss: 0.01516, val loss: 0.01556\n",
      "Training epoch: 2615, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2616, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2617, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2618, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2619, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2620, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2621, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2622, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2623, train loss: 0.01515, val loss: 0.01562\n",
      "Training epoch: 2624, train loss: 0.01509, val loss: 0.01553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2625, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2626, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 2627, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2628, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 2629, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2630, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2631, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2632, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2633, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2634, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2635, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2636, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 2637, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2638, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2639, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2640, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2641, train loss: 0.01515, val loss: 0.01555\n",
      "Training epoch: 2642, train loss: 0.01514, val loss: 0.01562\n",
      "Training epoch: 2643, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2644, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2645, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2646, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2647, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2648, train loss: 0.01532, val loss: 0.01573\n",
      "Training epoch: 2649, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2650, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2651, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2652, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2653, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2654, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2655, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2656, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2657, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2658, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2659, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2660, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2661, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2662, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2663, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2664, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2665, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2666, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2667, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2668, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2669, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2670, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2671, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2672, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2673, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2674, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 2675, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2676, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 2677, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2678, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2679, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2680, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2681, train loss: 0.01521, val loss: 0.01567\n",
      "Training epoch: 2682, train loss: 0.01519, val loss: 0.01558\n",
      "Training epoch: 2683, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2684, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 2685, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2686, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2687, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2688, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2689, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2690, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2691, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2692, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 2693, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2694, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2695, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2696, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2697, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2698, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2699, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2700, train loss: 0.01531, val loss: 0.01576\n",
      "Training epoch: 2701, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2702, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2703, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2704, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2705, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2706, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2707, train loss: 0.01516, val loss: 0.01557\n",
      "Training epoch: 2708, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2709, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2710, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 2711, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2712, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2713, train loss: 0.01517, val loss: 0.01564\n",
      "Training epoch: 2714, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2715, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2716, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2717, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2718, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2719, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 2720, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2721, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 2722, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 2723, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2724, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2725, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 2726, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2727, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2728, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2729, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2730, train loss: 0.01511, val loss: 0.01551\n",
      "Training epoch: 2731, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2732, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2733, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2734, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2735, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2736, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2737, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2738, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 2739, train loss: 0.01518, val loss: 0.01561\n",
      "Training epoch: 2740, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 2741, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2742, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2743, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 2744, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2745, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2746, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2747, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 2748, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2749, train loss: 0.01517, val loss: 0.01561\n",
      "Training epoch: 2750, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2751, train loss: 0.01519, val loss: 0.01564\n",
      "Training epoch: 2752, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2753, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2754, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2755, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2756, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2757, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2758, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2759, train loss: 0.01508, val loss: 0.01552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2760, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2761, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2762, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 2763, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2764, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2765, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2766, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2767, train loss: 0.01514, val loss: 0.01559\n",
      "Training epoch: 2768, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 2769, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2770, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2771, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2772, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 2773, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 2774, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2775, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2776, train loss: 0.01514, val loss: 0.01555\n",
      "Training epoch: 2777, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2778, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 2779, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2780, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2781, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2782, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2783, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2784, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2785, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2786, train loss: 0.01521, val loss: 0.01566\n",
      "Training epoch: 2787, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2788, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2789, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2790, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2791, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2792, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 2793, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2794, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2795, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2796, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2797, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 2798, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2799, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2800, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2801, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2802, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2803, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2804, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2805, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2806, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2807, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 2808, train loss: 0.01533, val loss: 0.01572\n",
      "Training epoch: 2809, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 2810, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2811, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2812, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2813, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2814, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2815, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 2816, train loss: 0.01516, val loss: 0.01554\n",
      "Training epoch: 2817, train loss: 0.01517, val loss: 0.01565\n",
      "Training epoch: 2818, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2819, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2820, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2821, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2822, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2823, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2824, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2825, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2826, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2827, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2828, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2829, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2830, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2831, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2832, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2833, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2834, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2835, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2836, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2837, train loss: 0.01535, val loss: 0.01577\n",
      "Training epoch: 2838, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2839, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2840, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2841, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 2842, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2843, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2844, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 2845, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 2846, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2847, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2848, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2849, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2850, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2851, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2852, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2853, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2854, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 2855, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2856, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2857, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2858, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2859, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2860, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2861, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 2862, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2863, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2864, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2865, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 2866, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 2867, train loss: 0.01508, val loss: 0.01549\n",
      "Training epoch: 2868, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2869, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 2870, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2871, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2872, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2873, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2874, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 2875, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2876, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2877, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2878, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2879, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 2880, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 2881, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2882, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 2883, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 2884, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2885, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 2886, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2887, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2888, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2889, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 2890, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 2891, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2892, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2893, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2894, train loss: 0.01508, val loss: 0.01551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 2895, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2896, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2897, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 2898, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2899, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2900, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 2901, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2902, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2903, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2904, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2905, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2906, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2907, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2908, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2909, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2910, train loss: 0.01509, val loss: 0.01550\n",
      "Training epoch: 2911, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 2912, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2913, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2914, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2915, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2916, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 2917, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2918, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2919, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 2920, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 2921, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 2922, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2923, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2924, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2925, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2926, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 2927, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2928, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 2929, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2930, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2931, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 2932, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2933, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 2934, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 2935, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2936, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2937, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 2938, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2939, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2940, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 2941, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2942, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 2943, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 2944, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2945, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 2946, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2947, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 2948, train loss: 0.01521, val loss: 0.01565\n",
      "Training epoch: 2949, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 2950, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 2951, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2952, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 2953, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 2954, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 2955, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2956, train loss: 0.01518, val loss: 0.01562\n",
      "Training epoch: 2957, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2958, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2959, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 2960, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 2961, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2962, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 2963, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 2964, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 2965, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 2966, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2967, train loss: 0.01525, val loss: 0.01571\n",
      "Training epoch: 2968, train loss: 0.01550, val loss: 0.01592\n",
      "Training epoch: 2969, train loss: 0.01523, val loss: 0.01572\n",
      "Training epoch: 2970, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 2971, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 2972, train loss: 0.01523, val loss: 0.01567\n",
      "Training epoch: 2973, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 2974, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 2975, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 2976, train loss: 0.01514, val loss: 0.01556\n",
      "Training epoch: 2977, train loss: 0.01525, val loss: 0.01572\n",
      "Training epoch: 2978, train loss: 0.01520, val loss: 0.01567\n",
      "Training epoch: 2979, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 2980, train loss: 0.01517, val loss: 0.01562\n",
      "Training epoch: 2981, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 2982, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2983, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 2984, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 2985, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2986, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 2987, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2988, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2989, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 2990, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 2991, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 2992, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 2993, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2994, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 2995, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 2996, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 2997, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 2998, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 2999, train loss: 0.01519, val loss: 0.01563\n",
      "Training epoch: 3000, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3001, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3002, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 3003, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3004, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3005, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3006, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3007, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3008, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3009, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3010, train loss: 0.01530, val loss: 0.01572\n",
      "Training epoch: 3011, train loss: 0.01521, val loss: 0.01562\n",
      "Training epoch: 3012, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3013, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 3014, train loss: 0.01534, val loss: 0.01582\n",
      "Training epoch: 3015, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 3016, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3017, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 3018, train loss: 0.01512, val loss: 0.01553\n",
      "Training epoch: 3019, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3020, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3021, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 3022, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3023, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3024, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 3025, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 3026, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3027, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3028, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3029, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 3030, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3031, train loss: 0.01510, val loss: 0.01554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3032, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3033, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 3034, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3035, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3036, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3037, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3038, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 3039, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3040, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3041, train loss: 0.01522, val loss: 0.01568\n",
      "Training epoch: 3042, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3043, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3044, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 3045, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 3046, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3047, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 3048, train loss: 0.01520, val loss: 0.01564\n",
      "Training epoch: 3049, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3050, train loss: 0.01510, val loss: 0.01551\n",
      "Training epoch: 3051, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3052, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3053, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3054, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3055, train loss: 0.01513, val loss: 0.01554\n",
      "Training epoch: 3056, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3057, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3058, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3059, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 3060, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 3061, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 3062, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3063, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3064, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3065, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3066, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3067, train loss: 0.01513, val loss: 0.01557\n",
      "Training epoch: 3068, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 3069, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3070, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3071, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3072, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 3073, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3074, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3075, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3076, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3077, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3078, train loss: 0.01516, val loss: 0.01556\n",
      "Training epoch: 3079, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3080, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3081, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 3082, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3083, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3084, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3085, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3086, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3087, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3088, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3089, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3090, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 3091, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 3092, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3093, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 3094, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3095, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3096, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3097, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3098, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3099, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3100, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3101, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3102, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3103, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3104, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 3105, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3106, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 3107, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3108, train loss: 0.01516, val loss: 0.01562\n",
      "Training epoch: 3109, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3110, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 3111, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 3112, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3113, train loss: 0.01513, val loss: 0.01555\n",
      "Training epoch: 3114, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3115, train loss: 0.01515, val loss: 0.01560\n",
      "Training epoch: 3116, train loss: 0.01534, val loss: 0.01581\n",
      "Training epoch: 3117, train loss: 0.01510, val loss: 0.01558\n",
      "Training epoch: 3118, train loss: 0.01517, val loss: 0.01559\n",
      "Training epoch: 3119, train loss: 0.01531, val loss: 0.01574\n",
      "Training epoch: 3120, train loss: 0.01515, val loss: 0.01558\n",
      "Training epoch: 3121, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3122, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 3123, train loss: 0.01526, val loss: 0.01572\n",
      "Training epoch: 3124, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3125, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 3126, train loss: 0.01517, val loss: 0.01560\n",
      "Training epoch: 3127, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 3128, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3129, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 3130, train loss: 0.01518, val loss: 0.01560\n",
      "Training epoch: 3131, train loss: 0.01527, val loss: 0.01569\n",
      "Training epoch: 3132, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 3133, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 3134, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3135, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3136, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 3137, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3138, train loss: 0.01516, val loss: 0.01561\n",
      "Training epoch: 3139, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3140, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3141, train loss: 0.01534, val loss: 0.01580\n",
      "Training epoch: 3142, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3143, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3144, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3145, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3146, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3147, train loss: 0.01514, val loss: 0.01558\n",
      "Training epoch: 3148, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3149, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 3150, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 3151, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 3152, train loss: 0.01532, val loss: 0.01579\n",
      "Training epoch: 3153, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3154, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 3155, train loss: 0.01524, val loss: 0.01566\n",
      "Training epoch: 3156, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 3157, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3158, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3159, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3160, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3161, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3162, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3163, train loss: 0.01529, val loss: 0.01573\n",
      "Training epoch: 3164, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 3165, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3166, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3167, train loss: 0.01508, val loss: 0.01552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3168, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 3169, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 3170, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3171, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3172, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 3173, train loss: 0.01551, val loss: 0.01596\n",
      "Training epoch: 3174, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3175, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3176, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3177, train loss: 0.01515, val loss: 0.01557\n",
      "Training epoch: 3178, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3179, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 3180, train loss: 0.01510, val loss: 0.01559\n",
      "Training epoch: 3181, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3182, train loss: 0.01511, val loss: 0.01553\n",
      "Training epoch: 3183, train loss: 0.01515, val loss: 0.01562\n",
      "Training epoch: 3184, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3185, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 3186, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3187, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 3188, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3189, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3190, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3191, train loss: 0.01508, val loss: 0.01550\n",
      "Training epoch: 3192, train loss: 0.01514, val loss: 0.01560\n",
      "Training epoch: 3193, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3194, train loss: 0.01519, val loss: 0.01565\n",
      "Training epoch: 3195, train loss: 0.01513, val loss: 0.01559\n",
      "Training epoch: 3196, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3197, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3198, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 3199, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3200, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3201, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3202, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 3203, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3204, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3205, train loss: 0.01530, val loss: 0.01576\n",
      "Training epoch: 3206, train loss: 0.01524, val loss: 0.01570\n",
      "Training epoch: 3207, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3208, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3209, train loss: 0.01510, val loss: 0.01553\n",
      "Training epoch: 3210, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3211, train loss: 0.01523, val loss: 0.01569\n",
      "Training epoch: 3212, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3213, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3214, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3215, train loss: 0.01520, val loss: 0.01560\n",
      "Training epoch: 3216, train loss: 0.01519, val loss: 0.01566\n",
      "Training epoch: 3217, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 3218, train loss: 0.01512, val loss: 0.01558\n",
      "Training epoch: 3219, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3220, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 3221, train loss: 0.01516, val loss: 0.01558\n",
      "Training epoch: 3222, train loss: 0.01520, val loss: 0.01565\n",
      "Training epoch: 3223, train loss: 0.01510, val loss: 0.01554\n",
      "Training epoch: 3224, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3225, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 3226, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3227, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3228, train loss: 0.01518, val loss: 0.01565\n",
      "Training epoch: 3229, train loss: 0.01532, val loss: 0.01575\n",
      "Training epoch: 3230, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3231, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3232, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3233, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 3234, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3235, train loss: 0.01512, val loss: 0.01556\n",
      "Training epoch: 3236, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3237, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 3238, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 3239, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3240, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 3241, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 3242, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 3243, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3244, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3245, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3246, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3247, train loss: 0.01513, val loss: 0.01560\n",
      "Training epoch: 3248, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3249, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3250, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 3251, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3252, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3253, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 3254, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3255, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3256, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3257, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3258, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3259, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 3260, train loss: 0.01512, val loss: 0.01554\n",
      "Training epoch: 3261, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3262, train loss: 0.01509, val loss: 0.01551\n",
      "Training epoch: 3263, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3264, train loss: 0.01514, val loss: 0.01562\n",
      "Training epoch: 3265, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3266, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3267, train loss: 0.01509, val loss: 0.01558\n",
      "Training epoch: 3268, train loss: 0.01516, val loss: 0.01555\n",
      "Training epoch: 3269, train loss: 0.01514, val loss: 0.01561\n",
      "Training epoch: 3270, train loss: 0.01515, val loss: 0.01559\n",
      "Training epoch: 3271, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3272, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3273, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3274, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3275, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3276, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3277, train loss: 0.01516, val loss: 0.01559\n",
      "Training epoch: 3278, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3279, train loss: 0.01510, val loss: 0.01556\n",
      "Training epoch: 3280, train loss: 0.01508, val loss: 0.01551\n",
      "Training epoch: 3281, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3282, train loss: 0.01509, val loss: 0.01556\n",
      "Training epoch: 3283, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3284, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3285, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3286, train loss: 0.01515, val loss: 0.01562\n",
      "Training epoch: 3287, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3288, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3289, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3290, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3291, train loss: 0.01531, val loss: 0.01575\n",
      "Training epoch: 3292, train loss: 0.01508, val loss: 0.01555\n",
      "Training epoch: 3293, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3294, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3295, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3296, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 3297, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3298, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3299, train loss: 0.01516, val loss: 0.01563\n",
      "Training epoch: 3300, train loss: 0.01512, val loss: 0.01552\n",
      "Training epoch: 3301, train loss: 0.01521, val loss: 0.01568\n",
      "Training epoch: 3302, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3303, train loss: 0.01518, val loss: 0.01559\n",
      "Training epoch: 3304, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3305, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3306, train loss: 0.01508, val loss: 0.01554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 3307, train loss: 0.01509, val loss: 0.01552\n",
      "Training epoch: 3308, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3309, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3310, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3311, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3312, train loss: 0.01507, val loss: 0.01553\n",
      "Training epoch: 3313, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 3314, train loss: 0.01511, val loss: 0.01554\n",
      "Training epoch: 3315, train loss: 0.01513, val loss: 0.01558\n",
      "Training epoch: 3316, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 3317, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3318, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3319, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3320, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 3321, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3322, train loss: 0.01518, val loss: 0.01564\n",
      "Training epoch: 3323, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3324, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3325, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3326, train loss: 0.01510, val loss: 0.01555\n",
      "Training epoch: 3327, train loss: 0.01507, val loss: 0.01550\n",
      "Training epoch: 3328, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3329, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3330, train loss: 0.01511, val loss: 0.01558\n",
      "Training epoch: 3331, train loss: 0.01510, val loss: 0.01552\n",
      "Training epoch: 3332, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3333, train loss: 0.01512, val loss: 0.01555\n",
      "Training epoch: 3334, train loss: 0.01527, val loss: 0.01572\n",
      "Training epoch: 3335, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 3336, train loss: 0.01515, val loss: 0.01561\n",
      "Training epoch: 3337, train loss: 0.01522, val loss: 0.01564\n",
      "Training epoch: 3338, train loss: 0.01509, val loss: 0.01555\n",
      "Training epoch: 3339, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3340, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3341, train loss: 0.01510, val loss: 0.01558\n",
      "Training epoch: 3342, train loss: 0.01514, val loss: 0.01554\n",
      "Training epoch: 3343, train loss: 0.01510, val loss: 0.01557\n",
      "Training epoch: 3344, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3345, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3346, train loss: 0.01509, val loss: 0.01554\n",
      "Training epoch: 3347, train loss: 0.01514, val loss: 0.01557\n",
      "Training epoch: 3348, train loss: 0.01508, val loss: 0.01553\n",
      "Training epoch: 3349, train loss: 0.01528, val loss: 0.01575\n",
      "Training epoch: 3350, train loss: 0.01530, val loss: 0.01575\n",
      "Training epoch: 3351, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3352, train loss: 0.01511, val loss: 0.01557\n",
      "Training epoch: 3353, train loss: 0.01511, val loss: 0.01556\n",
      "Training epoch: 3354, train loss: 0.01520, val loss: 0.01563\n",
      "Training epoch: 3355, train loss: 0.01507, val loss: 0.01552\n",
      "Training epoch: 3356, train loss: 0.01509, val loss: 0.01553\n",
      "Training epoch: 3357, train loss: 0.01527, val loss: 0.01575\n",
      "Training epoch: 3358, train loss: 0.01516, val loss: 0.01560\n",
      "Training epoch: 3359, train loss: 0.01512, val loss: 0.01557\n",
      "Training epoch: 3360, train loss: 0.01512, val loss: 0.01559\n",
      "Training epoch: 3361, train loss: 0.01508, val loss: 0.01552\n",
      "Training epoch: 3362, train loss: 0.01508, val loss: 0.01554\n",
      "Training epoch: 3363, train loss: 0.01521, val loss: 0.01563\n",
      "Training epoch: 3364, train loss: 0.01511, val loss: 0.01555\n",
      "Training epoch: 3365, train loss: 0.01507, val loss: 0.01551\n",
      "Training epoch: 3366, train loss: 0.01520, val loss: 0.01566\n",
      "Training epoch: 3367, train loss: 0.01513, val loss: 0.01556\n",
      "Training epoch: 3368, train loss: 0.01513, val loss: 0.01561\n",
      "Early stop at epoch 3368, With Testing Error: 0.01561\n",
      "Subnetwork pruning.\n",
      "Fine tuning.\n",
      "Tuning epoch: 1, train loss: 0.01542, val loss: 0.01588\n",
      "Tuning epoch: 2, train loss: 0.01592, val loss: 0.01629\n",
      "Tuning epoch: 3, train loss: 0.01508, val loss: 0.01554\n",
      "Tuning epoch: 4, train loss: 0.01517, val loss: 0.01563\n",
      "Tuning epoch: 5, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 6, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 7, train loss: 0.01510, val loss: 0.01555\n",
      "Tuning epoch: 8, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 9, train loss: 0.01510, val loss: 0.01552\n",
      "Tuning epoch: 10, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 11, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 12, train loss: 0.01507, val loss: 0.01550\n",
      "Tuning epoch: 13, train loss: 0.01510, val loss: 0.01554\n",
      "Tuning epoch: 14, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 15, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 16, train loss: 0.01507, val loss: 0.01550\n",
      "Tuning epoch: 17, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 18, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 19, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 20, train loss: 0.01510, val loss: 0.01553\n",
      "Tuning epoch: 21, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 22, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 23, train loss: 0.01516, val loss: 0.01562\n",
      "Tuning epoch: 24, train loss: 0.01509, val loss: 0.01552\n",
      "Tuning epoch: 25, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 26, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 27, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 28, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 29, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 30, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 31, train loss: 0.01515, val loss: 0.01562\n",
      "Tuning epoch: 32, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 33, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 34, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 35, train loss: 0.01509, val loss: 0.01554\n",
      "Tuning epoch: 36, train loss: 0.01509, val loss: 0.01552\n",
      "Tuning epoch: 37, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 38, train loss: 0.01509, val loss: 0.01555\n",
      "Tuning epoch: 39, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 40, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 41, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 42, train loss: 0.01508, val loss: 0.01554\n",
      "Tuning epoch: 43, train loss: 0.01509, val loss: 0.01554\n",
      "Tuning epoch: 44, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 45, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 46, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 47, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 48, train loss: 0.01507, val loss: 0.01553\n",
      "Tuning epoch: 49, train loss: 0.01514, val loss: 0.01558\n",
      "Tuning epoch: 50, train loss: 0.01510, val loss: 0.01554\n",
      "Tuning epoch: 51, train loss: 0.01507, val loss: 0.01553\n",
      "Tuning epoch: 52, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 53, train loss: 0.01511, val loss: 0.01556\n",
      "Tuning epoch: 54, train loss: 0.01514, val loss: 0.01559\n",
      "Tuning epoch: 55, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 56, train loss: 0.01507, val loss: 0.01553\n",
      "Tuning epoch: 57, train loss: 0.01511, val loss: 0.01554\n",
      "Tuning epoch: 58, train loss: 0.01507, val loss: 0.01553\n",
      "Tuning epoch: 59, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 60, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 61, train loss: 0.01515, val loss: 0.01561\n",
      "Tuning epoch: 62, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 63, train loss: 0.01510, val loss: 0.01553\n",
      "Tuning epoch: 64, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 65, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 66, train loss: 0.01510, val loss: 0.01553\n",
      "Tuning epoch: 67, train loss: 0.01508, val loss: 0.01554\n",
      "Tuning epoch: 68, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 69, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 70, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 71, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 72, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 73, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 74, train loss: 0.01508, val loss: 0.01554\n",
      "Tuning epoch: 75, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 76, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 77, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 78, train loss: 0.01509, val loss: 0.01552\n",
      "Tuning epoch: 79, train loss: 0.01508, val loss: 0.01554\n",
      "Tuning epoch: 80, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 81, train loss: 0.01509, val loss: 0.01552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning epoch: 82, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 83, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 84, train loss: 0.01509, val loss: 0.01554\n",
      "Tuning epoch: 85, train loss: 0.01532, val loss: 0.01579\n",
      "Tuning epoch: 86, train loss: 0.01509, val loss: 0.01555\n",
      "Tuning epoch: 87, train loss: 0.01529, val loss: 0.01570\n",
      "Tuning epoch: 88, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 89, train loss: 0.01510, val loss: 0.01556\n",
      "Tuning epoch: 90, train loss: 0.01511, val loss: 0.01557\n",
      "Tuning epoch: 91, train loss: 0.01509, val loss: 0.01555\n",
      "Tuning epoch: 92, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 93, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 94, train loss: 0.01508, val loss: 0.01551\n",
      "Tuning epoch: 95, train loss: 0.01507, val loss: 0.01551\n",
      "Tuning epoch: 96, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 97, train loss: 0.01507, val loss: 0.01552\n",
      "Tuning epoch: 98, train loss: 0.01508, val loss: 0.01552\n",
      "Tuning epoch: 99, train loss: 0.01508, val loss: 0.01553\n",
      "Tuning epoch: 100, train loss: 0.01508, val loss: 0.01551\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1584 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = xNN(input_num=train_x.shape[1],\n",
    "               meta_info=meta_info,\n",
    "               subnet_num=10,\n",
    "               subnet_arch=[10, 6],\n",
    "               task_type=task_type,\n",
    "               activation_func=tf.tanh,\n",
    "               batch_size=min(1000, int(train_x.shape[0] * 0.2)),\n",
    "               training_epochs=10000,\n",
    "               lr_bp=0.001,\n",
    "               beta_threshold=0.05,\n",
    "               tuning_epochs=100,\n",
    "               l1_proj=best_l1_prob,\n",
    "               l1_subnet=best_l1_subnet,\n",
    "               verbose=True,\n",
    "               val_ratio=0.2,\n",
    "               early_stop_thres=500)\n",
    "model.fit(train_x, train_y)\n",
    "model.visualize(\"./\", \"xnn_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99941 1.02802 0.99131]\n"
     ]
    }
   ],
   "source": [
    "tr_pred = model.predict(model.tr_x) \n",
    "val_pred = model.predict(model.val_x) \n",
    "pred_test = model.predict(test_x)\n",
    "\n",
    "mse_stat = np.hstack([np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(tr_pred) - meta_info[\"Y\"][\"scaler\"].inverse_transform(model.tr_y))**2),5),\\\n",
    "                             np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(val_pred) - meta_info[\"Y\"][\"scaler\"].inverse_transform(model.val_y))**2),5),\\\n",
    "               np.round(np.mean((meta_info[\"Y\"][\"scaler\"].inverse_transform(pred_test) - meta_info[\"Y\"][\"scaler\"].inverse_transform(test_y))**2),5)])\n",
    "print(mse_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
